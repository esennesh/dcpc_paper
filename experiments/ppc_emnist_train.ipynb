{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ef93e3-8eb5-4883-8265-5fbe97d06e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/ppc_experiments\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94520ed3-45be-4735-a0f2-b8f380838c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import numpy as np\n",
    "import pyro\n",
    "import torch\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "import trainer.trainer as module_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ac4be4-19eb-4713-a88d-9bec118c7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75ea7b9-6fe5-4f8a-a825-3572e421242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1445d1-45bb-43f9-893d-7a2b2bad1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "\n",
    "config = read_json(\"experiments/ppc_emnist_config.json\")\n",
    "config = ConfigParser(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca35026-5f64-47f8-98c7-a5e228013d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MnistPpc(\n",
      "  (prior): GaussianPrior()\n",
      "  (decoder1): ConditionalGaussian(\n",
      "    (decoder): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Linear(in_features=20, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder2): ConditionalGaussian(\n",
      "    (decoder): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (likelihood): MlpBernoulliLikelihood(\n",
      "    (decoder): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Linear(in_features=256, out_features=784, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (graph): PpcGraphicalModel()\n",
      ")\n",
      "Trainable parameters: 319540\n",
      "Initialize particles: train batch 0\n",
      "Initialize particles: train batch 1\n",
      "Initialize particles: train batch 2\n",
      "Initialize particles: train batch 3\n",
      "Initialize particles: train batch 4\n",
      "Initialize particles: train batch 5\n",
      "Initialize particles: train batch 6\n",
      "Initialize particles: train batch 7\n",
      "Initialize particles: train batch 8\n",
      "Initialize particles: train batch 9\n",
      "Initialize particles: train batch 10\n",
      "Initialize particles: train batch 11\n",
      "Initialize particles: train batch 12\n",
      "Initialize particles: train batch 13\n",
      "Initialize particles: train batch 14\n",
      "Initialize particles: train batch 15\n",
      "Initialize particles: train batch 16\n",
      "Initialize particles: train batch 17\n",
      "Initialize particles: train batch 18\n",
      "Initialize particles: train batch 19\n",
      "Initialize particles: train batch 20\n",
      "Initialize particles: train batch 21\n",
      "Initialize particles: train batch 22\n",
      "Initialize particles: train batch 23\n",
      "Initialize particles: train batch 24\n",
      "Initialize particles: train batch 25\n",
      "Initialize particles: train batch 26\n",
      "Initialize particles: train batch 27\n",
      "Initialize particles: train batch 28\n",
      "Initialize particles: train batch 29\n",
      "Initialize particles: train batch 30\n",
      "Initialize particles: train batch 31\n",
      "Initialize particles: train batch 32\n",
      "Initialize particles: train batch 33\n",
      "Initialize particles: train batch 34\n",
      "Initialize particles: train batch 35\n",
      "Initialize particles: train batch 36\n",
      "Initialize particles: train batch 37\n",
      "Initialize particles: train batch 38\n",
      "Initialize particles: train batch 39\n",
      "Initialize particles: train batch 40\n",
      "Initialize particles: train batch 41\n",
      "Initialize particles: train batch 42\n",
      "Initialize particles: train batch 43\n",
      "Initialize particles: train batch 44\n",
      "Initialize particles: train batch 45\n",
      "Initialize particles: train batch 46\n",
      "Initialize particles: train batch 47\n",
      "Initialize particles: train batch 48\n",
      "Initialize particles: train batch 49\n",
      "Initialize particles: train batch 50\n",
      "Initialize particles: train batch 51\n",
      "Initialize particles: train batch 52\n",
      "Initialize particles: train batch 53\n",
      "Initialize particles: train batch 54\n",
      "Initialize particles: train batch 55\n",
      "Initialize particles: train batch 56\n",
      "Initialize particles: train batch 57\n",
      "Initialize particles: train batch 58\n",
      "Initialize particles: train batch 59\n",
      "Initialize particles: train batch 60\n",
      "Initialize particles: train batch 61\n",
      "Initialize particles: train batch 62\n",
      "Initialize particles: train batch 63\n",
      "Initialize particles: train batch 64\n",
      "Initialize particles: train batch 65\n",
      "Initialize particles: train batch 66\n",
      "Initialize particles: train batch 67\n",
      "Initialize particles: train batch 68\n",
      "Initialize particles: train batch 69\n",
      "Initialize particles: train batch 70\n",
      "Initialize particles: train batch 71\n",
      "Initialize particles: train batch 72\n",
      "Initialize particles: train batch 73\n",
      "Initialize particles: train batch 74\n",
      "Initialize particles: train batch 75\n",
      "Initialize particles: train batch 76\n",
      "Initialize particles: train batch 77\n",
      "Initialize particles: train batch 78\n",
      "Initialize particles: train batch 79\n",
      "Initialize particles: train batch 80\n",
      "Initialize particles: train batch 81\n",
      "Initialize particles: train batch 82\n",
      "Initialize particles: train batch 83\n",
      "Initialize particles: train batch 84\n",
      "Initialize particles: train batch 85\n",
      "Initialize particles: train batch 86\n",
      "Initialize particles: train batch 87\n",
      "Initialize particles: train batch 88\n",
      "Initialize particles: train batch 89\n",
      "Initialize particles: train batch 90\n",
      "Initialize particles: train batch 91\n",
      "Initialize particles: train batch 92\n",
      "Initialize particles: train batch 93\n",
      "Initialize particles: train batch 94\n",
      "Initialize particles: train batch 95\n",
      "Initialize particles: train batch 96\n",
      "Initialize particles: train batch 97\n",
      "Initialize particles: train batch 98\n",
      "Initialize particles: train batch 99\n",
      "Initialize particles: train batch 100\n",
      "Initialize particles: train batch 101\n",
      "Initialize particles: train batch 102\n",
      "Initialize particles: train batch 103\n",
      "Initialize particles: train batch 104\n",
      "Initialize particles: train batch 105\n",
      "Initialize particles: train batch 106\n",
      "Initialize particles: train batch 107\n",
      "Initialize particles: train batch 108\n",
      "Initialize particles: train batch 109\n",
      "Initialize particles: train batch 110\n",
      "Initialize particles: train batch 111\n",
      "Initialize particles: train batch 112\n",
      "Initialize particles: train batch 113\n",
      "Initialize particles: train batch 114\n",
      "Initialize particles: train batch 115\n",
      "Initialize particles: train batch 116\n",
      "Initialize particles: train batch 117\n",
      "Initialize particles: train batch 118\n",
      "Initialize particles: train batch 119\n",
      "Initialize particles: train batch 120\n",
      "Initialize particles: train batch 121\n",
      "Initialize particles: train batch 122\n",
      "Initialize particles: train batch 123\n",
      "Initialize particles: train batch 124\n",
      "Initialize particles: train batch 125\n",
      "Initialize particles: train batch 126\n",
      "Initialize particles: train batch 127\n",
      "Initialize particles: train batch 128\n",
      "Initialize particles: train batch 129\n",
      "Initialize particles: train batch 130\n",
      "Initialize particles: train batch 131\n",
      "Initialize particles: train batch 132\n",
      "Initialize particles: train batch 133\n",
      "Initialize particles: train batch 134\n",
      "Initialize particles: train batch 135\n",
      "Initialize particles: train batch 136\n",
      "Initialize particles: train batch 137\n",
      "Initialize particles: train batch 138\n",
      "Initialize particles: train batch 139\n",
      "Initialize particles: train batch 140\n",
      "Initialize particles: train batch 141\n",
      "Initialize particles: train batch 142\n",
      "Initialize particles: train batch 143\n",
      "Initialize particles: train batch 144\n",
      "Initialize particles: train batch 145\n",
      "Initialize particles: train batch 146\n",
      "Initialize particles: train batch 147\n",
      "Initialize particles: train batch 148\n",
      "Initialize particles: train batch 149\n",
      "Initialize particles: train batch 150\n",
      "Initialize particles: train batch 151\n",
      "Initialize particles: train batch 152\n",
      "Initialize particles: train batch 153\n",
      "Initialize particles: train batch 154\n",
      "Initialize particles: train batch 155\n",
      "Initialize particles: train batch 156\n",
      "Initialize particles: train batch 157\n",
      "Initialize particles: train batch 158\n",
      "Initialize particles: train batch 159\n",
      "Initialize particles: train batch 160\n",
      "Initialize particles: train batch 161\n",
      "Initialize particles: train batch 162\n",
      "Initialize particles: train batch 163\n",
      "Initialize particles: train batch 164\n",
      "Initialize particles: train batch 165\n",
      "Initialize particles: train batch 166\n",
      "Initialize particles: train batch 167\n",
      "Initialize particles: train batch 168\n",
      "Initialize particles: train batch 169\n",
      "Initialize particles: train batch 170\n",
      "Initialize particles: train batch 171\n",
      "Initialize particles: train batch 172\n",
      "Initialize particles: train batch 173\n",
      "Initialize particles: train batch 174\n",
      "Initialize particles: train batch 175\n",
      "Initialize particles: train batch 176\n",
      "Initialize particles: train batch 177\n",
      "Initialize particles: train batch 178\n",
      "Initialize particles: train batch 179\n",
      "Initialize particles: train batch 180\n",
      "Initialize particles: train batch 181\n",
      "Initialize particles: train batch 182\n",
      "Initialize particles: train batch 183\n",
      "Initialize particles: train batch 184\n",
      "Initialize particles: train batch 185\n",
      "Initialize particles: train batch 186\n",
      "Initialize particles: train batch 187\n",
      "Initialize particles: train batch 188\n",
      "Initialize particles: train batch 189\n",
      "Initialize particles: train batch 190\n",
      "Initialize particles: train batch 191\n",
      "Initialize particles: train batch 192\n",
      "Initialize particles: train batch 193\n",
      "Initialize particles: train batch 194\n",
      "Initialize particles: train batch 195\n",
      "Initialize particles: train batch 196\n",
      "Initialize particles: train batch 197\n",
      "Initialize particles: train batch 198\n",
      "Initialize particles: valid batch 0\n",
      "Initialize particles: valid batch 1\n",
      "Initialize particles: valid batch 2\n",
      "Initialize particles: valid batch 3\n",
      "Initialize particles: valid batch 4\n",
      "Initialize particles: valid batch 5\n",
      "Initialize particles: valid batch 6\n",
      "Initialize particles: valid batch 7\n",
      "Initialize particles: valid batch 8\n",
      "Initialize particles: valid batch 9\n",
      "Initialize particles: valid batch 10\n",
      "Initialize particles: valid batch 11\n",
      "Initialize particles: valid batch 12\n",
      "Initialize particles: valid batch 13\n",
      "Initialize particles: valid batch 14\n",
      "Initialize particles: valid batch 15\n",
      "Initialize particles: valid batch 16\n",
      "Initialize particles: valid batch 17\n",
      "Initialize particles: valid batch 18\n",
      "Initialize particles: valid batch 19\n",
      "Initialize particles: valid batch 20\n",
      "Initialize particles: valid batch 21\n",
      "Initialize particles: valid batch 22\n"
     ]
    }
   ],
   "source": [
    "logger = config.get_logger('train')\n",
    "\n",
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()\n",
    "\n",
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "logger.info(model)\n",
    "\n",
    "# get function handles of metrics\n",
    "metrics = [getattr(module_metric, met) for met in config['metrics']]\n",
    "\n",
    "# build optimizer.\n",
    "if \"lr_scheduler\" in config:\n",
    "    lr_scheduler = getattr(pyro.optim, config[\"lr_scheduler\"][\"type\"])\n",
    "    lr_scheduler = optimizer = lr_scheduler({\n",
    "        \"optimizer\": getattr(torch.optim, config[\"optimizer\"][\"type\"]),\n",
    "        \"optim_args\": config[\"optimizer\"][\"args\"][\"optim_args\"],\n",
    "        **config[\"lr_scheduler\"][\"args\"]\n",
    "    })\n",
    "else:\n",
    "    optimizer = config.init_obj('optimizer', pyro.optim)\n",
    "    lr_scheduler = None\n",
    "\n",
    "# build trainer\n",
    "# kwargs = config['trainer'].pop('args')\n",
    "trainer = config.init_obj('trainer', module_trainer, model, metrics, optimizer,\n",
    "                          config=config, data_loader=data_loader,\n",
    "                          valid_data_loader=valid_data_loader,\n",
    "                          lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5cdd31-7fae-4e6f-8fe2-ef5c023919de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved/log/EMnist_Ppc/0504_153821\n"
     ]
    }
   ],
   "source": [
    "logger.info(trainer.config.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9820b982-1a39-4bde-850e-1a1d9a92557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/101520 (0%)] Loss: 790.410278\n",
      "Train Epoch: 1 [11264/101520 (11%)] Loss: -224.198120\n",
      "Train Epoch: 1 [22528/101520 (22%)] Loss: -217.371033\n",
      "Train Epoch: 1 [33792/101520 (33%)] Loss: -238.801620\n",
      "Train Epoch: 1 [45056/101520 (44%)] Loss: -242.810501\n",
      "Train Epoch: 1 [56320/101520 (55%)] Loss: -254.510437\n",
      "Train Epoch: 1 [67584/101520 (67%)] Loss: -261.337280\n",
      "Train Epoch: 1 [78848/101520 (78%)] Loss: -262.173340\n",
      "Train Epoch: 1 [90112/101520 (89%)] Loss: -293.183960\n",
      "Train Epoch: 1 [101376/101520 (100%)] Loss: -304.521667\n",
      "    epoch          : 1\n",
      "    loss           : -228.58709693793676\n",
      "    ess            : 1.9554264952789\n",
      "    log_marginal   : 229.0655339303328\n",
      "    log_joint      : 443.8189624038773\n",
      "    val_loss       : -285.528308370839\n",
      "    val_ess        : 1.9630526667055876\n",
      "    val_log_marginal: 285.9007913340693\n",
      "    val_log_joint  : 501.6078915803329\n",
      "Train Epoch: 2 [0/101520 (0%)] Loss: -318.653198\n",
      "Train Epoch: 2 [11264/101520 (11%)] Loss: -319.923920\n",
      "Train Epoch: 2 [22528/101520 (22%)] Loss: -320.514404\n",
      "Train Epoch: 2 [33792/101520 (33%)] Loss: -354.039337\n",
      "Train Epoch: 2 [45056/101520 (44%)] Loss: -351.973267\n",
      "Train Epoch: 2 [56320/101520 (55%)] Loss: -352.189911\n",
      "Train Epoch: 2 [67584/101520 (67%)] Loss: -366.323364\n",
      "Train Epoch: 2 [78848/101520 (78%)] Loss: -368.199158\n",
      "Train Epoch: 2 [90112/101520 (89%)] Loss: -389.346161\n",
      "Train Epoch: 2 [101376/101520 (100%)] Loss: -381.242126\n",
      "    epoch          : 2\n",
      "    loss           : -350.5356247484984\n",
      "    ess            : 1.9819017049655243\n",
      "    log_marginal   : 350.56158753974955\n",
      "    log_joint      : 558.5231025734139\n",
      "    val_loss       : -392.4341138756794\n",
      "    val_ess        : 1.9821092823277349\n",
      "    val_log_marginal: 392.45471324091375\n",
      "    val_log_joint  : 600.7255089801291\n",
      "Train Epoch: 3 [0/101520 (0%)] Loss: -411.033295\n",
      "Train Epoch: 3 [11264/101520 (11%)] Loss: -411.469421\n",
      "Train Epoch: 3 [22528/101520 (22%)] Loss: -441.954468\n",
      "Train Epoch: 3 [33792/101520 (33%)] Loss: -434.911804\n",
      "Train Epoch: 3 [45056/101520 (44%)] Loss: -440.973602\n",
      "Train Epoch: 3 [56320/101520 (55%)] Loss: -445.175964\n",
      "Train Epoch: 3 [67584/101520 (67%)] Loss: -441.540344\n",
      "Train Epoch: 3 [78848/101520 (78%)] Loss: -452.688477\n",
      "Train Epoch: 3 [90112/101520 (89%)] Loss: -470.804260\n",
      "Train Epoch: 3 [101376/101520 (100%)] Loss: -478.966156\n",
      "    epoch          : 3\n",
      "    loss           : -444.616231947089\n",
      "    ess            : 1.9798836630193433\n",
      "    log_marginal   : 444.6326257140193\n",
      "    log_joint      : 652.7127142671364\n",
      "    val_loss       : -482.82441379712975\n",
      "    val_ess        : 1.9796500931615415\n",
      "    val_log_marginal: 482.84331081224525\n",
      "    val_log_joint  : 690.6665622877038\n",
      "Train Epoch: 4 [0/101520 (0%)] Loss: -505.582458\n",
      "Train Epoch: 4 [11264/101520 (11%)] Loss: -506.073242\n",
      "Train Epoch: 4 [22528/101520 (22%)] Loss: -525.501282\n",
      "Train Epoch: 4 [33792/101520 (33%)] Loss: -508.703247\n",
      "Train Epoch: 4 [45056/101520 (44%)] Loss: -509.822266\n",
      "Train Epoch: 4 [56320/101520 (55%)] Loss: -509.635590\n",
      "Train Epoch: 4 [67584/101520 (67%)] Loss: -512.321594\n",
      "Train Epoch: 4 [78848/101520 (78%)] Loss: -522.479126\n",
      "Train Epoch: 4 [90112/101520 (89%)] Loss: -518.715576\n",
      "Train Epoch: 4 [101376/101520 (100%)] Loss: -533.251221\n",
      "    epoch          : 4\n",
      "    loss           : -516.9763363018707\n",
      "    ess            : 1.979780664995088\n",
      "    log_marginal   : 516.9915211739851\n",
      "    log_joint      : 725.3286074537727\n",
      "    val_loss       : -538.8865091075068\n",
      "    val_ess        : 1.9817760664483774\n",
      "    val_log_marginal: 538.900146484375\n",
      "    val_log_joint  : 747.5170579993206\n",
      "Train Epoch: 5 [0/101520 (0%)] Loss: -567.699951\n",
      "Train Epoch: 5 [11264/101520 (11%)] Loss: -561.410889\n",
      "Train Epoch: 5 [22528/101520 (22%)] Loss: -564.297241\n",
      "Train Epoch: 5 [33792/101520 (33%)] Loss: -592.946899\n",
      "Train Epoch: 5 [45056/101520 (44%)] Loss: -597.086792\n",
      "Train Epoch: 5 [56320/101520 (55%)] Loss: -577.566162\n",
      "Train Epoch: 5 [67584/101520 (67%)] Loss: -580.560547\n",
      "Train Epoch: 5 [78848/101520 (78%)] Loss: -596.635803\n",
      "Train Epoch: 5 [90112/101520 (89%)] Loss: -596.392334\n",
      "Train Epoch: 5 [101376/101520 (100%)] Loss: -600.906128\n",
      "    epoch          : 5\n",
      "    loss           : -586.0183823168577\n",
      "    ess            : 1.98059091436204\n",
      "    log_marginal   : 586.0327749587783\n",
      "    log_joint      : 794.1728353069057\n",
      "    val_loss       : -609.3248211404551\n",
      "    val_ess        : 1.9778147521226301\n",
      "    val_log_marginal: 609.3403665293818\n",
      "    val_log_joint  : 817.3645656419837\n",
      "Train Epoch: 6 [0/101520 (0%)] Loss: -628.201782\n",
      "Train Epoch: 6 [11264/101520 (11%)] Loss: -642.682678\n",
      "Train Epoch: 6 [22528/101520 (22%)] Loss: -638.706360\n",
      "Train Epoch: 6 [33792/101520 (33%)] Loss: -645.026367\n",
      "Train Epoch: 6 [45056/101520 (44%)] Loss: -635.546814\n",
      "Train Epoch: 6 [56320/101520 (55%)] Loss: -643.629517\n",
      "Train Epoch: 6 [67584/101520 (67%)] Loss: -636.265747\n",
      "Train Epoch: 6 [78848/101520 (78%)] Loss: -634.935120\n",
      "Train Epoch: 6 [90112/101520 (89%)] Loss: -628.323364\n",
      "Train Epoch: 6 [101376/101520 (100%)] Loss: -650.848755\n",
      "    epoch          : 6\n",
      "    loss           : -639.7447292002003\n",
      "    ess            : 1.9802828865434656\n",
      "    log_marginal   : 639.7593212031838\n",
      "    log_joint      : 847.9161944365381\n",
      "    val_loss       : -646.8944224481997\n",
      "    val_ess        : 1.9805037301519643\n",
      "    val_log_marginal: 646.9076617697011\n",
      "    val_log_joint  : 854.9325402301291\n",
      "Train Epoch: 7 [0/101520 (0%)] Loss: -685.154480\n",
      "Train Epoch: 7 [11264/101520 (11%)] Loss: -684.995361\n",
      "Train Epoch: 7 [22528/101520 (22%)] Loss: -673.430054\n",
      "Train Epoch: 7 [33792/101520 (33%)] Loss: -690.929688\n",
      "Train Epoch: 7 [45056/101520 (44%)] Loss: -687.298767\n",
      "Train Epoch: 7 [56320/101520 (55%)] Loss: -674.866821\n",
      "Train Epoch: 7 [67584/101520 (67%)] Loss: -678.226501\n",
      "Train Epoch: 7 [78848/101520 (78%)] Loss: -678.689026\n",
      "Train Epoch: 7 [90112/101520 (89%)] Loss: -680.504761\n",
      "Train Epoch: 7 [101376/101520 (100%)] Loss: -684.581299\n",
      "    epoch          : 7\n",
      "    loss           : -683.0064080779875\n",
      "    ess            : 1.979946800811806\n",
      "    log_marginal   : 683.0211512886699\n",
      "    log_joint      : 891.2478444468436\n",
      "    val_loss       : -695.6143453846807\n",
      "    val_ess        : 1.9783060654349949\n",
      "    val_log_marginal: 695.629906695822\n",
      "    val_log_joint  : 903.8817775560462\n",
      "Train Epoch: 8 [0/101520 (0%)] Loss: -712.377869\n",
      "Train Epoch: 8 [11264/101520 (11%)] Loss: -714.612061\n",
      "Train Epoch: 8 [22528/101520 (22%)] Loss: -707.985596\n",
      "Train Epoch: 8 [33792/101520 (33%)] Loss: -704.060181\n",
      "Train Epoch: 8 [45056/101520 (44%)] Loss: -696.409363\n",
      "Train Epoch: 8 [56320/101520 (55%)] Loss: -705.296448\n",
      "Train Epoch: 8 [67584/101520 (67%)] Loss: -718.443604\n",
      "Train Epoch: 8 [78848/101520 (78%)] Loss: -707.671143\n",
      "Train Epoch: 8 [90112/101520 (89%)] Loss: -709.970337\n",
      "Train Epoch: 8 [101376/101520 (100%)] Loss: -702.865356\n",
      "    epoch          : 8\n",
      "    loss           : -708.1844746191897\n",
      "    ess            : 1.9805436625552537\n",
      "    log_marginal   : 708.1988942515311\n",
      "    log_joint      : 916.6555108305198\n",
      "    val_loss       : -716.8701198412025\n",
      "    val_ess        : 1.9786858299504155\n",
      "    val_log_marginal: 716.8855245838995\n",
      "    val_log_joint  : 925.4001279084579\n",
      "Train Epoch: 9 [0/101520 (0%)] Loss: -750.375488\n",
      "Train Epoch: 9 [11264/101520 (11%)] Loss: -753.098572\n",
      "Train Epoch: 9 [22528/101520 (22%)] Loss: -749.815552\n",
      "Train Epoch: 9 [33792/101520 (33%)] Loss: -757.288025\n",
      "Train Epoch: 9 [45056/101520 (44%)] Loss: -749.895203\n",
      "Train Epoch: 9 [56320/101520 (55%)] Loss: -747.598450\n",
      "Train Epoch: 9 [67584/101520 (67%)] Loss: -745.140503\n",
      "Train Epoch: 9 [78848/101520 (78%)] Loss: -736.907532\n",
      "Train Epoch: 9 [90112/101520 (89%)] Loss: -742.306885\n",
      "Train Epoch: 9 [101376/101520 (100%)] Loss: -720.805420\n",
      "    epoch          : 9\n",
      "    loss           : -746.7559854325337\n",
      "    ess            : 1.9796617749947398\n",
      "    log_marginal   : 746.771202815837\n",
      "    log_joint      : 954.9854809938364\n",
      "    val_loss       : -739.421434485394\n",
      "    val_ess        : 1.9804893732070923\n",
      "    val_log_marginal: 739.4364969004755\n",
      "    val_log_joint  : 948.2482644786005\n",
      "Train Epoch: 10 [0/101520 (0%)] Loss: -755.238037\n",
      "Train Epoch: 10 [11264/101520 (11%)] Loss: -767.144165\n",
      "Train Epoch: 10 [22528/101520 (22%)] Loss: -765.101318\n",
      "Train Epoch: 10 [33792/101520 (33%)] Loss: -762.119263\n",
      "Train Epoch: 10 [45056/101520 (44%)] Loss: -757.682800\n",
      "Train Epoch: 10 [56320/101520 (55%)] Loss: -774.428467\n",
      "Train Epoch: 10 [67584/101520 (67%)] Loss: -758.778931\n",
      "Train Epoch: 10 [78848/101520 (78%)] Loss: -760.302734\n",
      "Train Epoch: 10 [90112/101520 (89%)] Loss: -764.044434\n",
      "Train Epoch: 10 [101376/101520 (100%)] Loss: -747.627930\n",
      "    epoch          : 10\n",
      "    loss           : -757.308217724364\n",
      "    ess            : 1.9803125426996893\n",
      "    log_marginal   : 757.3231213440248\n",
      "    log_joint      : 965.838299468534\n",
      "    val_loss       : -774.1791434909986\n",
      "    val_ess        : 1.979745600534522\n",
      "    val_log_marginal: 774.1940546450408\n",
      "    val_log_joint  : 982.4760821798574\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch10.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 11 [0/101520 (0%)] Loss: -789.484741\n",
      "Train Epoch: 11 [11264/101520 (11%)] Loss: -787.777222\n",
      "Train Epoch: 11 [22528/101520 (22%)] Loss: -802.291504\n",
      "Train Epoch: 11 [33792/101520 (33%)] Loss: -794.714966\n",
      "Train Epoch: 11 [45056/101520 (44%)] Loss: -798.139282\n",
      "Train Epoch: 11 [56320/101520 (55%)] Loss: -791.901001\n",
      "Train Epoch: 11 [67584/101520 (67%)] Loss: -812.372925\n",
      "Train Epoch: 11 [78848/101520 (78%)] Loss: -793.709717\n",
      "Train Epoch: 11 [90112/101520 (89%)] Loss: -799.179504\n",
      "Train Epoch: 11 [101376/101520 (100%)] Loss: -804.041870\n",
      "    epoch          : 11\n",
      "    loss           : -794.2294808392548\n",
      "    ess            : 1.9790499893265154\n",
      "    log_marginal   : 794.2451886507733\n",
      "    log_joint      : 1002.3393094623508\n",
      "    val_loss       : -798.8569017493206\n",
      "    val_ess        : 1.9793369718219922\n",
      "    val_log_marginal: 798.8717996348505\n",
      "    val_log_joint  : 1007.1332848590354\n",
      "Train Epoch: 12 [0/101520 (0%)] Loss: -826.554688\n",
      "Train Epoch: 12 [11264/101520 (11%)] Loss: -830.702881\n",
      "Train Epoch: 12 [22528/101520 (22%)] Loss: -829.999207\n",
      "Train Epoch: 12 [33792/101520 (33%)] Loss: -819.300049\n",
      "Train Epoch: 12 [45056/101520 (44%)] Loss: -808.947021\n",
      "Train Epoch: 12 [56320/101520 (55%)] Loss: -827.568848\n",
      "Train Epoch: 12 [67584/101520 (67%)] Loss: -803.816162\n",
      "Train Epoch: 12 [78848/101520 (78%)] Loss: -807.861206\n",
      "Train Epoch: 12 [90112/101520 (89%)] Loss: -809.942200\n",
      "Train Epoch: 12 [101376/101520 (100%)] Loss: -801.221436\n",
      "    epoch          : 12\n",
      "    loss           : -816.949046686067\n",
      "    ess            : 1.9790145572106443\n",
      "    log_marginal   : 816.9647339480606\n",
      "    log_joint      : 1024.9380241662413\n",
      "    val_loss       : -811.444895868716\n",
      "    val_ess        : 1.977472450422204\n",
      "    val_log_marginal: 811.465748662534\n",
      "    val_log_joint  : 1019.2664317255435\n",
      "Train Epoch: 13 [0/101520 (0%)] Loss: -812.126587\n",
      "Train Epoch: 13 [11264/101520 (11%)] Loss: -807.426636\n",
      "Train Epoch: 13 [22528/101520 (22%)] Loss: -799.626587\n",
      "Train Epoch: 13 [33792/101520 (33%)] Loss: -821.047119\n",
      "Train Epoch: 13 [45056/101520 (44%)] Loss: -821.503296\n",
      "Train Epoch: 13 [56320/101520 (55%)] Loss: -817.266602\n",
      "Train Epoch: 13 [67584/101520 (67%)] Loss: -806.675293\n",
      "Train Epoch: 13 [78848/101520 (78%)] Loss: -810.076050\n",
      "Train Epoch: 13 [90112/101520 (89%)] Loss: -802.360657\n",
      "Train Epoch: 13 [101376/101520 (100%)] Loss: -807.129517\n",
      "    epoch          : 13\n",
      "    loss           : -812.2315876256281\n",
      "    ess            : 1.9796824982417889\n",
      "    log_marginal   : 812.2468586830638\n",
      "    log_joint      : 1020.6661045707051\n",
      "    val_loss       : -817.157958984375\n",
      "    val_ess        : 1.9801915158396182\n",
      "    val_log_marginal: 817.1756963315217\n",
      "    val_log_joint  : 1025.443359375\n",
      "Train Epoch: 14 [0/101520 (0%)] Loss: -825.245239\n",
      "Train Epoch: 14 [11264/101520 (11%)] Loss: -834.516357\n",
      "Train Epoch: 14 [22528/101520 (22%)] Loss: -836.625305\n",
      "Train Epoch: 14 [33792/101520 (33%)] Loss: -824.046021\n",
      "Train Epoch: 14 [45056/101520 (44%)] Loss: -838.636780\n",
      "Train Epoch: 14 [56320/101520 (55%)] Loss: -844.483398\n",
      "Train Epoch: 14 [67584/101520 (67%)] Loss: -843.797485\n",
      "Train Epoch: 14 [78848/101520 (78%)] Loss: -833.058838\n",
      "Train Epoch: 14 [90112/101520 (89%)] Loss: -832.596191\n",
      "Train Epoch: 14 [101376/101520 (100%)] Loss: -828.530212\n",
      "    epoch          : 14\n",
      "    loss           : -835.8928069301586\n",
      "    ess            : 1.9797552081208731\n",
      "    log_marginal   : 835.907754102544\n",
      "    log_joint      : 1044.1417162717887\n",
      "    val_loss       : -836.1088283372962\n",
      "    val_ess        : 1.980361124743586\n",
      "    val_log_marginal: 836.1233175526495\n",
      "    val_log_joint  : 1044.4034795346467\n",
      "Train Epoch: 15 [0/101520 (0%)] Loss: -855.904541\n",
      "Train Epoch: 15 [11264/101520 (11%)] Loss: -857.997620\n",
      "Train Epoch: 15 [22528/101520 (22%)] Loss: -857.293335\n",
      "Train Epoch: 15 [33792/101520 (33%)] Loss: -858.140198\n",
      "Train Epoch: 15 [45056/101520 (44%)] Loss: -854.006165\n",
      "Train Epoch: 15 [56320/101520 (55%)] Loss: -848.037720\n",
      "Train Epoch: 15 [67584/101520 (67%)] Loss: -842.362183\n",
      "Train Epoch: 15 [78848/101520 (78%)] Loss: -857.428101\n",
      "Train Epoch: 15 [90112/101520 (89%)] Loss: -854.090088\n",
      "Train Epoch: 15 [101376/101520 (100%)] Loss: -863.143250\n",
      "    epoch          : 15\n",
      "    loss           : -850.6262887925958\n",
      "    ess            : 1.9787774942628102\n",
      "    log_marginal   : 850.6422063932946\n",
      "    log_joint      : 1058.637000309163\n",
      "    val_loss       : -850.8257711659307\n",
      "    val_ess        : 1.980312124542568\n",
      "    val_log_marginal: 850.8396208389946\n",
      "    val_log_joint  : 1059.3988514775815\n",
      "Train Epoch: 16 [0/101520 (0%)] Loss: -869.168701\n",
      "Train Epoch: 16 [11264/101520 (11%)] Loss: -859.398987\n",
      "Train Epoch: 16 [22528/101520 (22%)] Loss: -846.539673\n",
      "Train Epoch: 16 [33792/101520 (33%)] Loss: -858.957336\n",
      "Train Epoch: 16 [45056/101520 (44%)] Loss: -855.379456\n",
      "Train Epoch: 16 [56320/101520 (55%)] Loss: -862.102234\n",
      "Train Epoch: 16 [67584/101520 (67%)] Loss: -868.323730\n",
      "Train Epoch: 16 [78848/101520 (78%)] Loss: -864.925293\n",
      "Train Epoch: 16 [90112/101520 (89%)] Loss: -856.788574\n",
      "Train Epoch: 16 [101376/101520 (100%)] Loss: -871.205505\n",
      "    epoch          : 16\n",
      "    loss           : -861.154053654503\n",
      "    ess            : 1.9800252602927049\n",
      "    log_marginal   : 861.1687830632656\n",
      "    log_joint      : 1069.1901045756124\n",
      "    val_loss       : -862.4767907184104\n",
      "    val_ess        : 1.9768004261929055\n",
      "    val_log_marginal: 862.4945201044497\n",
      "    val_log_joint  : 1070.3463930876358\n",
      "Train Epoch: 17 [0/101520 (0%)] Loss: -874.582642\n",
      "Train Epoch: 17 [11264/101520 (11%)] Loss: -874.758179\n",
      "Train Epoch: 17 [22528/101520 (22%)] Loss: -863.298706\n",
      "Train Epoch: 17 [33792/101520 (33%)] Loss: -874.708618\n",
      "Train Epoch: 17 [45056/101520 (44%)] Loss: -875.289795\n",
      "Train Epoch: 17 [56320/101520 (55%)] Loss: -871.435425\n",
      "Train Epoch: 17 [67584/101520 (67%)] Loss: -871.930054\n",
      "Train Epoch: 17 [78848/101520 (78%)] Loss: -875.582825\n",
      "Train Epoch: 17 [90112/101520 (89%)] Loss: -857.954468\n",
      "Train Epoch: 17 [101376/101520 (100%)] Loss: -881.330811\n",
      "    epoch          : 17\n",
      "    loss           : -870.5802514157702\n",
      "    ess            : 1.9790969955262228\n",
      "    log_marginal   : 870.5957537320392\n",
      "    log_joint      : 1078.5499267578125\n",
      "    val_loss       : -870.0476445737092\n",
      "    val_ess        : 1.9809825990511023\n",
      "    val_log_marginal: 870.0616826596467\n",
      "    val_log_joint  : 1078.3151590098505\n",
      "Train Epoch: 18 [0/101520 (0%)] Loss: -889.115479\n",
      "Train Epoch: 18 [11264/101520 (11%)] Loss: -880.101318\n",
      "Train Epoch: 18 [22528/101520 (22%)] Loss: -868.899231\n",
      "Train Epoch: 18 [33792/101520 (33%)] Loss: -872.295044\n",
      "Train Epoch: 18 [45056/101520 (44%)] Loss: -879.921021\n",
      "Train Epoch: 18 [56320/101520 (55%)] Loss: -873.416138\n",
      "Train Epoch: 18 [67584/101520 (67%)] Loss: -876.579834\n",
      "Train Epoch: 18 [78848/101520 (78%)] Loss: -882.671814\n",
      "Train Epoch: 18 [90112/101520 (89%)] Loss: -882.808594\n",
      "Train Epoch: 18 [101376/101520 (100%)] Loss: -888.258850\n",
      "    epoch          : 18\n",
      "    loss           : -879.5053398093986\n",
      "    ess            : 1.979213349783241\n",
      "    log_marginal   : 879.5211359532035\n",
      "    log_joint      : 1087.4940216217808\n",
      "    val_loss       : -878.1263905400815\n",
      "    val_ess        : 1.9796302733214006\n",
      "    val_log_marginal: 878.1415007218071\n",
      "    val_log_joint  : 1086.1272768766983\n",
      "Train Epoch: 19 [0/101520 (0%)] Loss: -885.042053\n",
      "Train Epoch: 19 [11264/101520 (11%)] Loss: -892.138550\n",
      "Train Epoch: 19 [22528/101520 (22%)] Loss: -882.720337\n",
      "Train Epoch: 19 [33792/101520 (33%)] Loss: -882.475830\n",
      "Train Epoch: 19 [45056/101520 (44%)] Loss: -885.361084\n",
      "Train Epoch: 19 [56320/101520 (55%)] Loss: -870.605103\n",
      "Train Epoch: 19 [67584/101520 (67%)] Loss: -869.923218\n",
      "Train Epoch: 19 [78848/101520 (78%)] Loss: -864.921631\n",
      "Train Epoch: 19 [90112/101520 (89%)] Loss: -867.958496\n",
      "Train Epoch: 19 [101376/101520 (100%)] Loss: -859.438171\n",
      "    epoch          : 19\n",
      "    loss           : -877.2279663085938\n",
      "    ess            : 1.9788224140004298\n",
      "    log_marginal   : 877.2438161265311\n",
      "    log_joint      : 1085.3995171168342\n",
      "    val_loss       : -866.3120090650475\n",
      "    val_ess        : 1.9802677994189055\n",
      "    val_log_marginal: 866.3259250806725\n",
      "    val_log_joint  : 1074.817435886549\n",
      "Train Epoch: 20 [0/101520 (0%)] Loss: -880.230774\n",
      "Train Epoch: 20 [11264/101520 (11%)] Loss: -892.955261\n",
      "Train Epoch: 20 [22528/101520 (22%)] Loss: -893.351807\n",
      "Train Epoch: 20 [33792/101520 (33%)] Loss: -893.405396\n",
      "Train Epoch: 20 [45056/101520 (44%)] Loss: -884.375977\n",
      "Train Epoch: 20 [56320/101520 (55%)] Loss: -880.665710\n",
      "Train Epoch: 20 [67584/101520 (67%)] Loss: -888.176636\n",
      "Train Epoch: 20 [78848/101520 (78%)] Loss: -891.361694\n",
      "Train Epoch: 20 [90112/101520 (89%)] Loss: -876.888672\n",
      "Train Epoch: 20 [101376/101520 (100%)] Loss: -885.430115\n",
      "    epoch          : 20\n",
      "    loss           : -886.7156148172503\n",
      "    ess            : 1.9794819978014309\n",
      "    log_marginal   : 886.7309413890742\n",
      "    log_joint      : 1094.71258943644\n",
      "    val_loss       : -880.7934464164402\n",
      "    val_ess        : 1.977972590405008\n",
      "    val_log_marginal: 880.8083575704824\n",
      "    val_log_joint  : 1089.0251093325408\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch20.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 21 [0/101520 (0%)] Loss: -899.019165\n",
      "Train Epoch: 21 [11264/101520 (11%)] Loss: -899.969360\n",
      "Train Epoch: 21 [22528/101520 (22%)] Loss: -887.626953\n",
      "Train Epoch: 21 [33792/101520 (33%)] Loss: -897.773254\n",
      "Train Epoch: 21 [45056/101520 (44%)] Loss: -903.557129\n",
      "Train Epoch: 21 [56320/101520 (55%)] Loss: -895.037964\n",
      "Train Epoch: 21 [67584/101520 (67%)] Loss: -895.269348\n",
      "Train Epoch: 21 [78848/101520 (78%)] Loss: -882.311096\n",
      "Train Epoch: 21 [90112/101520 (89%)] Loss: -895.100830\n",
      "Train Epoch: 21 [101376/101520 (100%)] Loss: -896.460938\n",
      "    epoch          : 21\n",
      "    loss           : -894.4417052915946\n",
      "    ess            : 1.9785319698515849\n",
      "    log_marginal   : 894.4581961320273\n",
      "    log_joint      : 1102.3145236681455\n",
      "    val_loss       : -893.0512881071671\n",
      "    val_ess        : 1.9778097142343936\n",
      "    val_log_marginal: 893.067650836447\n",
      "    val_log_joint  : 1101.100925611413\n",
      "Train Epoch: 22 [0/101520 (0%)] Loss: -909.931946\n",
      "Train Epoch: 22 [11264/101520 (11%)] Loss: -905.751587\n",
      "Train Epoch: 22 [22528/101520 (22%)] Loss: -898.822754\n",
      "Train Epoch: 22 [33792/101520 (33%)] Loss: -895.887085\n",
      "Train Epoch: 22 [45056/101520 (44%)] Loss: -904.431641\n",
      "Train Epoch: 22 [56320/101520 (55%)] Loss: -897.628601\n",
      "Train Epoch: 22 [67584/101520 (67%)] Loss: -893.934204\n",
      "Train Epoch: 22 [78848/101520 (78%)] Loss: -898.122070\n",
      "Train Epoch: 22 [90112/101520 (89%)] Loss: -903.713135\n",
      "Train Epoch: 22 [101376/101520 (100%)] Loss: -899.541016\n",
      "    epoch          : 22\n",
      "    loss           : -899.7073496142823\n",
      "    ess            : 1.978479091845565\n",
      "    log_marginal   : 899.7236288252788\n",
      "    log_joint      : 1107.5973023074357\n",
      "    val_loss       : -894.5913271696671\n",
      "    val_ess        : 1.9789704550867495\n",
      "    val_log_marginal: 894.6080242654551\n",
      "    val_log_joint  : 1102.2214620838995\n",
      "Train Epoch: 23 [0/101520 (0%)] Loss: -905.521484\n",
      "Train Epoch: 23 [11264/101520 (11%)] Loss: -922.919434\n",
      "Train Epoch: 23 [22528/101520 (22%)] Loss: -909.111450\n",
      "Train Epoch: 23 [33792/101520 (33%)] Loss: -908.769409\n",
      "Train Epoch: 23 [45056/101520 (44%)] Loss: -909.619263\n",
      "Train Epoch: 23 [56320/101520 (55%)] Loss: -895.644165\n",
      "Train Epoch: 23 [67584/101520 (67%)] Loss: -898.826294\n",
      "Train Epoch: 23 [78848/101520 (78%)] Loss: -907.369629\n",
      "Train Epoch: 23 [90112/101520 (89%)] Loss: -895.330200\n",
      "Train Epoch: 23 [101376/101520 (100%)] Loss: -912.516479\n",
      "    epoch          : 23\n",
      "    loss           : -903.77764033792\n",
      "    ess            : 1.979063640287773\n",
      "    log_marginal   : 903.7932582836055\n",
      "    log_joint      : 1111.6669020149577\n",
      "    val_loss       : -900.5374225118886\n",
      "    val_ess        : 1.978777139083199\n",
      "    val_log_marginal: 900.552150560462\n",
      "    val_log_joint  : 1108.2432436735733\n",
      "Train Epoch: 24 [0/101520 (0%)] Loss: -912.775024\n",
      "Train Epoch: 24 [11264/101520 (11%)] Loss: -909.892029\n",
      "Train Epoch: 24 [22528/101520 (22%)] Loss: -907.820740\n",
      "Train Epoch: 24 [33792/101520 (33%)] Loss: -902.173828\n",
      "Train Epoch: 24 [45056/101520 (44%)] Loss: -899.419189\n",
      "Train Epoch: 24 [56320/101520 (55%)] Loss: -909.466553\n",
      "Train Epoch: 24 [67584/101520 (67%)] Loss: -906.449219\n",
      "Train Epoch: 24 [78848/101520 (78%)] Loss: -905.196533\n",
      "Train Epoch: 24 [90112/101520 (89%)] Loss: -899.731934\n",
      "Train Epoch: 24 [101376/101520 (100%)] Loss: -906.627380\n",
      "    epoch          : 24\n",
      "    loss           : -908.2662792109963\n",
      "    ess            : 1.9779150030720773\n",
      "    log_marginal   : 908.2831448502277\n",
      "    log_joint      : 1116.143769997448\n",
      "    val_loss       : -905.7715427564538\n",
      "    val_ess        : 1.9797131704247517\n",
      "    val_log_marginal: 905.7860532014266\n",
      "    val_log_joint  : 1113.4609268851902\n",
      "Train Epoch: 25 [0/101520 (0%)] Loss: -919.911621\n",
      "Train Epoch: 25 [11264/101520 (11%)] Loss: -922.281250\n",
      "Train Epoch: 25 [22528/101520 (22%)] Loss: -919.150391\n",
      "Train Epoch: 25 [33792/101520 (33%)] Loss: -906.462891\n",
      "Train Epoch: 25 [45056/101520 (44%)] Loss: -914.678894\n",
      "Train Epoch: 25 [56320/101520 (55%)] Loss: -911.987671\n",
      "Train Epoch: 25 [67584/101520 (67%)] Loss: -900.641113\n",
      "Train Epoch: 25 [78848/101520 (78%)] Loss: -908.052246\n",
      "Train Epoch: 25 [90112/101520 (89%)] Loss: -908.879761\n",
      "Train Epoch: 25 [101376/101520 (100%)] Loss: -906.718750\n",
      "    epoch          : 25\n",
      "    loss           : -912.1823500436755\n",
      "    ess            : 1.9784520079742125\n",
      "    log_marginal   : 912.1989785965962\n",
      "    log_joint      : 1120.0727404110396\n",
      "    val_loss       : -909.2394090735394\n",
      "    val_ess        : 1.9797836231148762\n",
      "    val_log_marginal: 909.2540177055027\n",
      "    val_log_joint  : 1117.2210799507473\n",
      "Train Epoch: 26 [0/101520 (0%)] Loss: -922.165527\n",
      "Train Epoch: 26 [11264/101520 (11%)] Loss: -917.560791\n",
      "Train Epoch: 26 [22528/101520 (22%)] Loss: -907.764832\n",
      "Train Epoch: 26 [33792/101520 (33%)] Loss: -923.970703\n",
      "Train Epoch: 26 [45056/101520 (44%)] Loss: -911.641113\n",
      "Train Epoch: 26 [56320/101520 (55%)] Loss: -922.067505\n",
      "Train Epoch: 26 [67584/101520 (67%)] Loss: -905.379822\n",
      "Train Epoch: 26 [78848/101520 (78%)] Loss: -919.879883\n",
      "Train Epoch: 26 [90112/101520 (89%)] Loss: -913.088867\n",
      "Train Epoch: 26 [101376/101520 (100%)] Loss: -918.447266\n",
      "    epoch          : 26\n",
      "    loss           : -914.632379426429\n",
      "    ess            : 1.9789545799619588\n",
      "    log_marginal   : 914.6484488482452\n",
      "    log_joint      : 1122.4828770316426\n",
      "    val_loss       : -913.6614353345788\n",
      "    val_ess        : 1.9814321839291116\n",
      "    val_log_marginal: 913.6754070779551\n",
      "    val_log_joint  : 1121.1785039487092\n",
      "Train Epoch: 27 [0/101520 (0%)] Loss: -924.661621\n",
      "Train Epoch: 27 [11264/101520 (11%)] Loss: -917.815796\n",
      "Train Epoch: 27 [22528/101520 (22%)] Loss: -921.481018\n",
      "Train Epoch: 27 [33792/101520 (33%)] Loss: -926.516968\n",
      "Train Epoch: 27 [45056/101520 (44%)] Loss: -923.207397\n",
      "Train Epoch: 27 [56320/101520 (55%)] Loss: -912.289734\n",
      "Train Epoch: 27 [67584/101520 (67%)] Loss: -908.301636\n",
      "Train Epoch: 27 [78848/101520 (78%)] Loss: -914.916321\n",
      "Train Epoch: 27 [90112/101520 (89%)] Loss: -920.217163\n",
      "Train Epoch: 27 [101376/101520 (100%)] Loss: -899.292664\n",
      "    epoch          : 27\n",
      "    loss           : -917.9617383180552\n",
      "    ess            : 1.9788777097385732\n",
      "    log_marginal   : 917.97742956727\n",
      "    log_joint      : 1125.84258634481\n",
      "    val_loss       : -917.9064994480299\n",
      "    val_ess        : 1.9766157969184543\n",
      "    val_log_marginal: 917.9243137525475\n",
      "    val_log_joint  : 1125.445848547894\n",
      "Train Epoch: 28 [0/101520 (0%)] Loss: -927.268921\n",
      "Train Epoch: 28 [11264/101520 (11%)] Loss: -918.459717\n",
      "Train Epoch: 28 [22528/101520 (22%)] Loss: -922.257812\n",
      "Train Epoch: 28 [33792/101520 (33%)] Loss: -917.285645\n",
      "Train Epoch: 28 [45056/101520 (44%)] Loss: -922.980896\n",
      "Train Epoch: 28 [56320/101520 (55%)] Loss: -913.160889\n",
      "Train Epoch: 28 [67584/101520 (67%)] Loss: -929.035889\n",
      "Train Epoch: 28 [78848/101520 (78%)] Loss: -915.445068\n",
      "Train Epoch: 28 [90112/101520 (89%)] Loss: -915.601379\n",
      "Train Epoch: 28 [101376/101520 (100%)] Loss: -931.059265\n",
      "    epoch          : 28\n",
      "    loss           : -920.1208186317328\n",
      "    ess            : 1.9781636730510386\n",
      "    log_marginal   : 920.1370772932044\n",
      "    log_joint      : 1127.9418325759657\n",
      "    val_loss       : -917.2603812839674\n",
      "    val_ess        : 1.9783746988877007\n",
      "    val_log_marginal: 917.2782831606658\n",
      "    val_log_joint  : 1125.1682288128397\n",
      "Train Epoch: 29 [0/101520 (0%)] Loss: -919.030457\n",
      "Train Epoch: 29 [11264/101520 (11%)] Loss: -925.035034\n",
      "Train Epoch: 29 [22528/101520 (22%)] Loss: -920.337280\n",
      "Train Epoch: 29 [33792/101520 (33%)] Loss: -914.056152\n",
      "Train Epoch: 29 [45056/101520 (44%)] Loss: -912.739502\n",
      "Train Epoch: 29 [56320/101520 (55%)] Loss: -912.724487\n",
      "Train Epoch: 29 [67584/101520 (67%)] Loss: -907.817139\n",
      "Train Epoch: 29 [78848/101520 (78%)] Loss: -907.163513\n",
      "Train Epoch: 29 [90112/101520 (89%)] Loss: -897.806641\n",
      "Train Epoch: 29 [101376/101520 (100%)] Loss: -896.284485\n",
      "    epoch          : 29\n",
      "    loss           : -911.9454180080088\n",
      "    ess            : 1.978743861668074\n",
      "    log_marginal   : 911.9615285288749\n",
      "    log_joint      : 1120.102324979389\n",
      "    val_loss       : -906.0601488196331\n",
      "    val_ess        : 1.9769494948179827\n",
      "    val_log_marginal: 906.0780506963315\n",
      "    val_log_joint  : 1114.5760869565217\n",
      "Train Epoch: 30 [0/101520 (0%)] Loss: -913.405640\n",
      "Train Epoch: 30 [11264/101520 (11%)] Loss: -910.724365\n",
      "Train Epoch: 30 [22528/101520 (22%)] Loss: -916.719238\n",
      "Train Epoch: 30 [33792/101520 (33%)] Loss: -906.355103\n",
      "Train Epoch: 30 [45056/101520 (44%)] Loss: -920.253418\n",
      "Train Epoch: 30 [56320/101520 (55%)] Loss: -928.322632\n",
      "Train Epoch: 30 [67584/101520 (67%)] Loss: -920.534058\n",
      "Train Epoch: 30 [78848/101520 (78%)] Loss: -917.960144\n",
      "Train Epoch: 30 [90112/101520 (89%)] Loss: -917.301758\n",
      "Train Epoch: 30 [101376/101520 (100%)] Loss: -923.308289\n",
      "    epoch          : 30\n",
      "    loss           : -921.2373669494935\n",
      "    ess            : 1.9787048167319754\n",
      "    log_marginal   : 921.25311678019\n",
      "    log_joint      : 1129.0910619994504\n",
      "    val_loss       : -920.4644589631454\n",
      "    val_ess        : 1.9791781228521597\n",
      "    val_log_marginal: 920.4802166482676\n",
      "    val_log_joint  : 1128.2293701171875\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch30.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 31 [0/101520 (0%)] Loss: -944.479614\n",
      "Train Epoch: 31 [11264/101520 (11%)] Loss: -922.398560\n",
      "Train Epoch: 31 [22528/101520 (22%)] Loss: -932.106812\n",
      "Train Epoch: 31 [33792/101520 (33%)] Loss: -929.946289\n",
      "Train Epoch: 31 [45056/101520 (44%)] Loss: -922.364624\n",
      "Train Epoch: 31 [56320/101520 (55%)] Loss: -932.191711\n",
      "Train Epoch: 31 [67584/101520 (67%)] Loss: -936.389709\n",
      "Train Epoch: 31 [78848/101520 (78%)] Loss: -929.623779\n",
      "Train Epoch: 31 [90112/101520 (89%)] Loss: -927.616699\n",
      "Train Epoch: 31 [101376/101520 (100%)] Loss: -917.421021\n",
      "    epoch          : 31\n",
      "    loss           : -929.6838679481391\n",
      "    ess            : 1.9777058368951232\n",
      "    log_marginal   : 929.7009476704814\n",
      "    log_joint      : 1137.387729050526\n",
      "    val_loss       : -929.8942738408628\n",
      "    val_ess        : 1.9733112013858298\n",
      "    val_log_marginal: 929.9145136294158\n",
      "    val_log_joint  : 1137.8101965862772\n",
      "Train Epoch: 32 [0/101520 (0%)] Loss: -934.851929\n",
      "Train Epoch: 32 [11264/101520 (11%)] Loss: -931.306763\n",
      "Train Epoch: 32 [22528/101520 (22%)] Loss: -925.491699\n",
      "Train Epoch: 32 [33792/101520 (33%)] Loss: -927.232178\n",
      "Train Epoch: 32 [45056/101520 (44%)] Loss: -927.478821\n",
      "Train Epoch: 32 [56320/101520 (55%)] Loss: -927.890381\n",
      "Train Epoch: 32 [67584/101520 (67%)] Loss: -932.273193\n",
      "Train Epoch: 32 [78848/101520 (78%)] Loss: -926.356567\n",
      "Train Epoch: 32 [90112/101520 (89%)] Loss: -917.491638\n",
      "Train Epoch: 32 [101376/101520 (100%)] Loss: -919.990051\n",
      "    epoch          : 32\n",
      "    loss           : -925.7703958635954\n",
      "    ess            : 1.9781118385755836\n",
      "    log_marginal   : 925.7872912536315\n",
      "    log_joint      : 1133.7215416683025\n",
      "    val_loss       : -922.8409344217051\n",
      "    val_ess        : 1.9784717870795208\n",
      "    val_log_marginal: 922.8561719811481\n",
      "    val_log_joint  : 1130.7216106912365\n",
      "Train Epoch: 33 [0/101520 (0%)] Loss: -929.240723\n",
      "Train Epoch: 33 [11264/101520 (11%)] Loss: -925.648560\n",
      "Train Epoch: 33 [22528/101520 (22%)] Loss: -934.170044\n",
      "Train Epoch: 33 [33792/101520 (33%)] Loss: -933.650146\n",
      "Train Epoch: 33 [45056/101520 (44%)] Loss: -930.125122\n",
      "Train Epoch: 33 [56320/101520 (55%)] Loss: -933.198669\n",
      "Train Epoch: 33 [67584/101520 (67%)] Loss: -929.185120\n",
      "Train Epoch: 33 [78848/101520 (78%)] Loss: -928.033203\n",
      "Train Epoch: 33 [90112/101520 (89%)] Loss: -925.094116\n",
      "Train Epoch: 33 [101376/101520 (100%)] Loss: -914.499817\n",
      "    epoch          : 33\n",
      "    loss           : -927.935191398889\n",
      "    ess            : 1.9780972962403418\n",
      "    log_marginal   : 927.951566609905\n",
      "    log_joint      : 1135.849290397299\n",
      "    val_loss       : -927.9527747112771\n",
      "    val_ess        : 1.9803121452746184\n",
      "    val_log_marginal: 927.9672108525815\n",
      "    val_log_joint  : 1135.9203358525815\n",
      "Train Epoch: 34 [0/101520 (0%)] Loss: -942.251343\n",
      "Train Epoch: 34 [11264/101520 (11%)] Loss: -933.894775\n",
      "Train Epoch: 34 [22528/101520 (22%)] Loss: -923.114624\n",
      "Train Epoch: 34 [33792/101520 (33%)] Loss: -936.711975\n",
      "Train Epoch: 34 [45056/101520 (44%)] Loss: -938.682617\n",
      "Train Epoch: 34 [56320/101520 (55%)] Loss: -938.292358\n",
      "Train Epoch: 34 [67584/101520 (67%)] Loss: -935.308716\n",
      "Train Epoch: 34 [78848/101520 (78%)] Loss: -937.790771\n",
      "Train Epoch: 34 [90112/101520 (89%)] Loss: -929.741333\n",
      "Train Epoch: 34 [101376/101520 (100%)] Loss: -934.232300\n",
      "    epoch          : 34\n",
      "    loss           : -934.4129341163826\n",
      "    ess            : 1.978438839241488\n",
      "    log_marginal   : 934.4297911677528\n",
      "    log_joint      : 1142.0687838607098\n",
      "    val_loss       : -932.6498413085938\n",
      "    val_ess        : 1.972347000370855\n",
      "    val_log_marginal: 932.6781456988791\n",
      "    val_log_joint  : 1140.425489342731\n",
      "Train Epoch: 35 [0/101520 (0%)] Loss: -933.285706\n",
      "Train Epoch: 35 [11264/101520 (11%)] Loss: -937.464966\n",
      "Train Epoch: 35 [22528/101520 (22%)] Loss: -938.426147\n",
      "Train Epoch: 35 [33792/101520 (33%)] Loss: -932.532349\n",
      "Train Epoch: 35 [45056/101520 (44%)] Loss: -934.541809\n",
      "Train Epoch: 35 [56320/101520 (55%)] Loss: -938.722351\n",
      "Train Epoch: 35 [67584/101520 (67%)] Loss: -934.962524\n",
      "Train Epoch: 35 [78848/101520 (78%)] Loss: -944.600464\n",
      "Train Epoch: 35 [90112/101520 (89%)] Loss: -944.429138\n",
      "Train Epoch: 35 [101376/101520 (100%)] Loss: -942.288940\n",
      "    epoch          : 35\n",
      "    loss           : -938.7997396651225\n",
      "    ess            : 1.978064203981179\n",
      "    log_marginal   : 938.8163565630889\n",
      "    log_joint      : 1146.33729744916\n",
      "    val_loss       : -936.8633024796196\n",
      "    val_ess        : 1.9775315626807835\n",
      "    val_log_marginal: 936.8808328379755\n",
      "    val_log_joint  : 1144.5990361752717\n",
      "Train Epoch: 36 [0/101520 (0%)] Loss: -949.574585\n",
      "Train Epoch: 36 [11264/101520 (11%)] Loss: -938.685974\n",
      "Train Epoch: 36 [22528/101520 (22%)] Loss: -941.570923\n",
      "Train Epoch: 36 [33792/101520 (33%)] Loss: -935.538025\n",
      "Train Epoch: 36 [45056/101520 (44%)] Loss: -932.193848\n",
      "Train Epoch: 36 [56320/101520 (55%)] Loss: -939.266052\n",
      "Train Epoch: 36 [67584/101520 (67%)] Loss: -926.028625\n",
      "Train Epoch: 36 [78848/101520 (78%)] Loss: -939.907227\n",
      "Train Epoch: 36 [90112/101520 (89%)] Loss: -928.132507\n",
      "Train Epoch: 36 [101376/101520 (100%)] Loss: -927.249329\n",
      "    epoch          : 36\n",
      "    loss           : -935.114276521769\n",
      "    ess            : 1.9781453831111966\n",
      "    log_marginal   : 935.1306716688913\n",
      "    log_joint      : 1142.9603608864636\n",
      "    val_loss       : -931.353879182235\n",
      "    val_ess        : 1.9778061846028203\n",
      "    val_log_marginal: 931.3698465098505\n",
      "    val_log_joint  : 1139.2404572860055\n",
      "Train Epoch: 37 [0/101520 (0%)] Loss: -942.696472\n",
      "Train Epoch: 37 [11264/101520 (11%)] Loss: -937.296631\n",
      "Train Epoch: 37 [22528/101520 (22%)] Loss: -939.470215\n",
      "Train Epoch: 37 [33792/101520 (33%)] Loss: -942.998413\n",
      "Train Epoch: 37 [45056/101520 (44%)] Loss: -939.565918\n",
      "Train Epoch: 37 [56320/101520 (55%)] Loss: -936.328125\n",
      "Train Epoch: 37 [67584/101520 (67%)] Loss: -932.431824\n",
      "Train Epoch: 37 [78848/101520 (78%)] Loss: -928.445374\n",
      "Train Epoch: 37 [90112/101520 (89%)] Loss: -935.060059\n",
      "Train Epoch: 37 [101376/101520 (100%)] Loss: -916.602417\n",
      "    epoch          : 37\n",
      "    loss           : -935.6955035799112\n",
      "    ess            : 1.9778341036945133\n",
      "    log_marginal   : 935.712081525793\n",
      "    log_joint      : 1143.575876207208\n",
      "    val_loss       : -933.88195535411\n",
      "    val_ess        : 1.9760885342307712\n",
      "    val_log_marginal: 933.9012371560801\n",
      "    val_log_joint  : 1141.363090183424\n",
      "Train Epoch: 38 [0/101520 (0%)] Loss: -941.806763\n",
      "Train Epoch: 38 [11264/101520 (11%)] Loss: -946.766907\n",
      "Train Epoch: 38 [22528/101520 (22%)] Loss: -938.494385\n",
      "Train Epoch: 38 [33792/101520 (33%)] Loss: -945.720947\n",
      "Train Epoch: 38 [45056/101520 (44%)] Loss: -944.145508\n",
      "Train Epoch: 38 [56320/101520 (55%)] Loss: -939.044006\n",
      "Train Epoch: 38 [67584/101520 (67%)] Loss: -939.145508\n",
      "Train Epoch: 38 [78848/101520 (78%)] Loss: -940.335449\n",
      "Train Epoch: 38 [90112/101520 (89%)] Loss: -941.161987\n",
      "Train Epoch: 38 [101376/101520 (100%)] Loss: -937.317688\n",
      "    epoch          : 38\n",
      "    loss           : -941.376818479605\n",
      "    ess            : 1.9769516410540098\n",
      "    log_marginal   : 941.3939665981273\n",
      "    log_joint      : 1148.996694286864\n",
      "    val_loss       : -939.5383513077446\n",
      "    val_ess        : 1.9790965111359307\n",
      "    val_log_marginal: 939.5533500339674\n",
      "    val_log_joint  : 1147.0046705163043\n",
      "Train Epoch: 39 [0/101520 (0%)] Loss: -945.070190\n",
      "Train Epoch: 39 [11264/101520 (11%)] Loss: -946.379395\n",
      "Train Epoch: 39 [22528/101520 (22%)] Loss: -941.413940\n",
      "Train Epoch: 39 [33792/101520 (33%)] Loss: -945.672363\n",
      "Train Epoch: 39 [45056/101520 (44%)] Loss: -947.297668\n",
      "Train Epoch: 39 [56320/101520 (55%)] Loss: -941.242981\n",
      "Train Epoch: 39 [67584/101520 (67%)] Loss: -953.374878\n",
      "Train Epoch: 39 [78848/101520 (78%)] Loss: -945.570129\n",
      "Train Epoch: 39 [90112/101520 (89%)] Loss: -939.173584\n",
      "Train Epoch: 39 [101376/101520 (100%)] Loss: -931.599487\n",
      "    epoch          : 39\n",
      "    loss           : -945.0545737108393\n",
      "    ess            : 1.978157381915567\n",
      "    log_marginal   : 945.0710200784195\n",
      "    log_joint      : 1152.6787778001335\n",
      "    val_loss       : -944.5891458262568\n",
      "    val_ess        : 1.9774535065111907\n",
      "    val_log_marginal: 944.6078411599864\n",
      "    val_log_joint  : 1152.2283033288043\n",
      "Train Epoch: 40 [0/101520 (0%)] Loss: -947.624268\n",
      "Train Epoch: 40 [11264/101520 (11%)] Loss: -950.023621\n",
      "Train Epoch: 40 [22528/101520 (22%)] Loss: -940.701843\n",
      "Train Epoch: 40 [33792/101520 (33%)] Loss: -936.225281\n",
      "Train Epoch: 40 [45056/101520 (44%)] Loss: -933.914429\n",
      "Train Epoch: 40 [56320/101520 (55%)] Loss: -935.194702\n",
      "Train Epoch: 40 [67584/101520 (67%)] Loss: -938.253723\n",
      "Train Epoch: 40 [78848/101520 (78%)] Loss: -938.002197\n",
      "Train Epoch: 40 [90112/101520 (89%)] Loss: -947.152588\n",
      "Train Epoch: 40 [101376/101520 (100%)] Loss: -947.118713\n",
      "    epoch          : 40\n",
      "    loss           : -941.2931346797463\n",
      "    ess            : 1.977522070084385\n",
      "    log_marginal   : 941.3103284979585\n",
      "    log_joint      : 1149.063984473147\n",
      "    val_loss       : -937.4746067212975\n",
      "    val_ess        : 1.97782564163208\n",
      "    val_log_marginal: 937.4911525560462\n",
      "    val_log_joint  : 1145.2458283797555\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch40.pth ...\n",
      "Train Epoch: 41 [0/101520 (0%)] Loss: -954.113770\n",
      "Train Epoch: 41 [11264/101520 (11%)] Loss: -952.237854\n",
      "Train Epoch: 41 [22528/101520 (22%)] Loss: -952.348389\n",
      "Train Epoch: 41 [33792/101520 (33%)] Loss: -947.048218\n",
      "Train Epoch: 41 [45056/101520 (44%)] Loss: -939.591370\n",
      "Train Epoch: 41 [56320/101520 (55%)] Loss: -942.866577\n",
      "Train Epoch: 41 [67584/101520 (67%)] Loss: -934.691650\n",
      "Train Epoch: 41 [78848/101520 (78%)] Loss: -938.147522\n",
      "Train Epoch: 41 [90112/101520 (89%)] Loss: -940.698242\n",
      "Train Epoch: 41 [101376/101520 (100%)] Loss: -926.914062\n",
      "    epoch          : 41\n",
      "    loss           : -941.783186255987\n",
      "    ess            : 1.9776477430334043\n",
      "    log_marginal   : 941.7997255564934\n",
      "    log_joint      : 1149.629604933849\n",
      "    val_loss       : -942.9130753226902\n",
      "    val_ess        : 1.9798984320267388\n",
      "    val_log_marginal: 942.9277211064878\n",
      "    val_log_joint  : 1150.5685663637908\n",
      "Train Epoch: 42 [0/101520 (0%)] Loss: -953.765015\n",
      "Train Epoch: 42 [11264/101520 (11%)] Loss: -948.862793\n",
      "Train Epoch: 42 [22528/101520 (22%)] Loss: -949.945190\n",
      "Train Epoch: 42 [33792/101520 (33%)] Loss: -946.414673\n",
      "Train Epoch: 42 [45056/101520 (44%)] Loss: -942.741272\n",
      "Train Epoch: 42 [56320/101520 (55%)] Loss: -937.768494\n",
      "Train Epoch: 42 [67584/101520 (67%)] Loss: -942.488953\n",
      "Train Epoch: 42 [78848/101520 (78%)] Loss: -949.047974\n",
      "Train Epoch: 42 [90112/101520 (89%)] Loss: -951.180359\n",
      "Train Epoch: 42 [101376/101520 (100%)] Loss: -954.306458\n",
      "    epoch          : 42\n",
      "    loss           : -946.9426008828322\n",
      "    ess            : 1.9772734090910484\n",
      "    log_marginal   : 946.9595551610593\n",
      "    log_joint      : 1154.6006184486887\n",
      "    val_loss       : -945.6938503099525\n",
      "    val_ess        : 1.9779880565145742\n",
      "    val_log_marginal: 945.7101387355639\n",
      "    val_log_joint  : 1153.3637270720108\n",
      "Train Epoch: 43 [0/101520 (0%)] Loss: -954.361877\n",
      "Train Epoch: 43 [11264/101520 (11%)] Loss: -955.622803\n",
      "Train Epoch: 43 [22528/101520 (22%)] Loss: -949.044434\n",
      "Train Epoch: 43 [33792/101520 (33%)] Loss: -945.843018\n",
      "Train Epoch: 43 [45056/101520 (44%)] Loss: -954.912720\n",
      "Train Epoch: 43 [56320/101520 (55%)] Loss: -949.132812\n",
      "Train Epoch: 43 [67584/101520 (67%)] Loss: -953.193054\n",
      "Train Epoch: 43 [78848/101520 (78%)] Loss: -948.360474\n",
      "Train Epoch: 43 [90112/101520 (89%)] Loss: -946.212769\n",
      "Train Epoch: 43 [101376/101520 (100%)] Loss: -940.492188\n",
      "    epoch          : 43\n",
      "    loss           : -950.6003589725974\n",
      "    ess            : 1.9773307829046969\n",
      "    log_marginal   : 950.6176064649419\n",
      "    log_joint      : 1158.1798065032192\n",
      "    val_loss       : -948.9010168987771\n",
      "    val_ess        : 1.9748815194420193\n",
      "    val_log_marginal: 948.9204632302989\n",
      "    val_log_joint  : 1156.5221318783967\n",
      "Train Epoch: 44 [0/101520 (0%)] Loss: -952.049744\n",
      "Train Epoch: 44 [11264/101520 (11%)] Loss: -949.295288\n",
      "Train Epoch: 44 [22528/101520 (22%)] Loss: -942.283691\n",
      "Train Epoch: 44 [33792/101520 (33%)] Loss: -941.763306\n",
      "Train Epoch: 44 [45056/101520 (44%)] Loss: -941.700439\n",
      "Train Epoch: 44 [56320/101520 (55%)] Loss: -949.734192\n",
      "Train Epoch: 44 [67584/101520 (67%)] Loss: -946.288818\n",
      "Train Epoch: 44 [78848/101520 (78%)] Loss: -950.033203\n",
      "Train Epoch: 44 [90112/101520 (89%)] Loss: -945.191956\n",
      "Train Epoch: 44 [101376/101520 (100%)] Loss: -954.203247\n",
      "    epoch          : 44\n",
      "    loss           : -947.1464389820195\n",
      "    ess            : 1.9770985566191936\n",
      "    log_marginal   : 947.1639579121191\n",
      "    log_joint      : 1154.8619157800722\n",
      "    val_loss       : -943.7221095872962\n",
      "    val_ess        : 1.9786172638768735\n",
      "    val_log_marginal: 943.7374453337296\n",
      "    val_log_joint  : 1151.8818253226902\n",
      "Train Epoch: 45 [0/101520 (0%)] Loss: -949.450745\n",
      "Train Epoch: 45 [11264/101520 (11%)] Loss: -958.823364\n",
      "Train Epoch: 45 [22528/101520 (22%)] Loss: -954.881409\n",
      "Train Epoch: 45 [33792/101520 (33%)] Loss: -947.410400\n",
      "Train Epoch: 45 [45056/101520 (44%)] Loss: -945.076416\n",
      "Train Epoch: 45 [56320/101520 (55%)] Loss: -951.516235\n",
      "Train Epoch: 45 [67584/101520 (67%)] Loss: -950.314453\n",
      "Train Epoch: 45 [78848/101520 (78%)] Loss: -949.213074\n",
      "Train Epoch: 45 [90112/101520 (89%)] Loss: -943.687256\n",
      "Train Epoch: 45 [101376/101520 (100%)] Loss: -940.149719\n",
      "    epoch          : 45\n",
      "    loss           : -947.5934773354076\n",
      "    ess            : 1.977606147976976\n",
      "    log_marginal   : 947.6103340800684\n",
      "    log_joint      : 1155.3925382527873\n",
      "    val_loss       : -946.2872951341712\n",
      "    val_ess        : 1.9781276972397515\n",
      "    val_log_marginal: 946.3034561820652\n",
      "    val_log_joint  : 1153.89306640625\n",
      "Train Epoch: 46 [0/101520 (0%)] Loss: -958.592529\n",
      "Train Epoch: 46 [11264/101520 (11%)] Loss: -955.453003\n",
      "Train Epoch: 46 [22528/101520 (22%)] Loss: -952.287659\n",
      "Train Epoch: 46 [33792/101520 (33%)] Loss: -946.166138\n",
      "Train Epoch: 46 [45056/101520 (44%)] Loss: -951.729187\n",
      "Train Epoch: 46 [56320/101520 (55%)] Loss: -954.531860\n",
      "Train Epoch: 46 [67584/101520 (67%)] Loss: -952.579956\n",
      "Train Epoch: 46 [78848/101520 (78%)] Loss: -948.519653\n",
      "Train Epoch: 46 [90112/101520 (89%)] Loss: -953.204712\n",
      "Train Epoch: 46 [101376/101520 (100%)] Loss: -951.534180\n",
      "    epoch          : 46\n",
      "    loss           : -951.6536070857215\n",
      "    ess            : 1.9766685351654514\n",
      "    log_marginal   : 951.6712447123311\n",
      "    log_joint      : 1159.2778455264604\n",
      "    val_loss       : -950.6853319251019\n",
      "    val_ess        : 1.9771482685337896\n",
      "    val_log_marginal: 950.7025624150815\n",
      "    val_log_joint  : 1158.4044560971467\n",
      "Train Epoch: 47 [0/101520 (0%)] Loss: -956.536865\n",
      "Train Epoch: 47 [11264/101520 (11%)] Loss: -955.684448\n",
      "Train Epoch: 47 [22528/101520 (22%)] Loss: -955.997070\n",
      "Train Epoch: 47 [33792/101520 (33%)] Loss: -965.253174\n",
      "Train Epoch: 47 [45056/101520 (44%)] Loss: -948.445557\n",
      "Train Epoch: 47 [56320/101520 (55%)] Loss: -953.407227\n",
      "Train Epoch: 47 [67584/101520 (67%)] Loss: -949.327393\n",
      "Train Epoch: 47 [78848/101520 (78%)] Loss: -956.488403\n",
      "Train Epoch: 47 [90112/101520 (89%)] Loss: -957.126465\n",
      "Train Epoch: 47 [101376/101520 (100%)] Loss: -954.679688\n",
      "    epoch          : 47\n",
      "    loss           : -955.2241296816112\n",
      "    ess            : 1.9767455006364603\n",
      "    log_marginal   : 955.2418111676546\n",
      "    log_joint      : 1162.7567887042635\n",
      "    val_loss       : -954.6375838569973\n",
      "    val_ess        : 1.9777326169221296\n",
      "    val_log_marginal: 954.6545038637908\n",
      "    val_log_joint  : 1162.111911939538\n",
      "Train Epoch: 48 [0/101520 (0%)] Loss: -956.979492\n",
      "Train Epoch: 48 [11264/101520 (11%)] Loss: -956.619812\n",
      "Train Epoch: 48 [22528/101520 (22%)] Loss: -952.275146\n",
      "Train Epoch: 48 [33792/101520 (33%)] Loss: -956.276428\n",
      "Train Epoch: 48 [45056/101520 (44%)] Loss: -957.181580\n",
      "Train Epoch: 48 [56320/101520 (55%)] Loss: -945.140869\n",
      "Train Epoch: 48 [67584/101520 (67%)] Loss: -944.929932\n",
      "Train Epoch: 48 [78848/101520 (78%)] Loss: -954.040771\n",
      "Train Epoch: 48 [90112/101520 (89%)] Loss: -952.768311\n",
      "Train Epoch: 48 [101376/101520 (100%)] Loss: -950.557312\n",
      "    epoch          : 48\n",
      "    loss           : -951.2129246697353\n",
      "    ess            : 1.9766393492569276\n",
      "    log_marginal   : 951.2305448139133\n",
      "    log_joint      : 1158.9727979497095\n",
      "    val_loss       : -949.7067048445991\n",
      "    val_ess        : 1.9770513513813848\n",
      "    val_log_marginal: 949.7242590862771\n",
      "    val_log_joint  : 1157.53735882303\n",
      "Train Epoch: 49 [0/101520 (0%)] Loss: -952.851440\n",
      "Train Epoch: 49 [11264/101520 (11%)] Loss: -954.644775\n",
      "Train Epoch: 49 [22528/101520 (22%)] Loss: -950.296326\n",
      "Train Epoch: 49 [33792/101520 (33%)] Loss: -951.925293\n",
      "Train Epoch: 49 [45056/101520 (44%)] Loss: -948.799438\n",
      "Train Epoch: 49 [56320/101520 (55%)] Loss: -948.611755\n",
      "Train Epoch: 49 [67584/101520 (67%)] Loss: -949.314026\n",
      "Train Epoch: 49 [78848/101520 (78%)] Loss: -955.246338\n",
      "Train Epoch: 49 [90112/101520 (89%)] Loss: -943.402649\n",
      "Train Epoch: 49 [101376/101520 (100%)] Loss: -961.939331\n",
      "    epoch          : 49\n",
      "    loss           : -952.1096737348854\n",
      "    ess            : 1.9769911190972256\n",
      "    log_marginal   : 952.127422696981\n",
      "    log_joint      : 1159.8690719221106\n",
      "    val_loss       : -952.2373338782269\n",
      "    val_ess        : 1.9763240814208984\n",
      "    val_log_marginal: 952.2556550399116\n",
      "    val_log_joint  : 1159.9166312839675\n",
      "Train Epoch: 50 [0/101520 (0%)] Loss: -959.035400\n",
      "Train Epoch: 50 [11264/101520 (11%)] Loss: -953.313599\n",
      "Train Epoch: 50 [22528/101520 (22%)] Loss: -955.008057\n",
      "Train Epoch: 50 [33792/101520 (33%)] Loss: -952.899109\n",
      "Train Epoch: 50 [45056/101520 (44%)] Loss: -960.540039\n",
      "Train Epoch: 50 [56320/101520 (55%)] Loss: -960.399353\n",
      "Train Epoch: 50 [67584/101520 (67%)] Loss: -960.578247\n",
      "Train Epoch: 50 [78848/101520 (78%)] Loss: -961.835205\n",
      "Train Epoch: 50 [90112/101520 (89%)] Loss: -946.590454\n",
      "Train Epoch: 50 [101376/101520 (100%)] Loss: -955.258789\n",
      "    epoch          : 50\n",
      "    loss           : -955.8331559531053\n",
      "    ess            : 1.9777913099557312\n",
      "    log_marginal   : 955.8499341801783\n",
      "    log_joint      : 1163.424517607569\n",
      "    val_loss       : -955.3547814410666\n",
      "    val_ess        : 1.9776001080222751\n",
      "    val_log_marginal: 955.3719004755435\n",
      "    val_log_joint  : 1163.0595915421195\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [0/101520 (0%)] Loss: -966.384277\n",
      "Train Epoch: 51 [11264/101520 (11%)] Loss: -964.209290\n",
      "Train Epoch: 51 [22528/101520 (22%)] Loss: -966.489258\n",
      "Train Epoch: 51 [33792/101520 (33%)] Loss: -958.923035\n",
      "Train Epoch: 51 [45056/101520 (44%)] Loss: -962.604187\n",
      "Train Epoch: 51 [56320/101520 (55%)] Loss: -956.395996\n",
      "Train Epoch: 51 [67584/101520 (67%)] Loss: -959.048096\n",
      "Train Epoch: 51 [78848/101520 (78%)] Loss: -961.577271\n",
      "Train Epoch: 51 [90112/101520 (89%)] Loss: -965.520020\n",
      "Train Epoch: 51 [101376/101520 (100%)] Loss: -951.707275\n",
      "    epoch          : 51\n",
      "    loss           : -959.2978227318232\n",
      "    ess            : 1.9766180407461809\n",
      "    log_marginal   : 959.3154214063482\n",
      "    log_joint      : 1166.8602613899577\n",
      "    val_loss       : -959.5845841117527\n",
      "    val_ess        : 1.9769769336866296\n",
      "    val_log_marginal: 959.6016235351562\n",
      "    val_log_joint  : 1166.8664710003397\n",
      "Train Epoch: 52 [0/101520 (0%)] Loss: -960.634766\n",
      "Train Epoch: 52 [11264/101520 (11%)] Loss: -959.463623\n",
      "Train Epoch: 52 [22528/101520 (22%)] Loss: -952.819214\n",
      "Train Epoch: 52 [33792/101520 (33%)] Loss: -949.674316\n",
      "Train Epoch: 52 [45056/101520 (44%)] Loss: -956.555847\n",
      "Train Epoch: 52 [56320/101520 (55%)] Loss: -959.223022\n",
      "Train Epoch: 52 [67584/101520 (67%)] Loss: -957.598816\n",
      "Train Epoch: 52 [78848/101520 (78%)] Loss: -949.248779\n",
      "Train Epoch: 52 [90112/101520 (89%)] Loss: -957.807556\n",
      "Train Epoch: 52 [101376/101520 (100%)] Loss: -944.087891\n",
      "    epoch          : 52\n",
      "    loss           : -954.977652238242\n",
      "    ess            : 1.9769067979937223\n",
      "    log_marginal   : 954.9948475900007\n",
      "    log_joint      : 1162.7518181728958\n",
      "    val_loss       : -953.6346488620924\n",
      "    val_ess        : 1.9785334286482439\n",
      "    val_log_marginal: 953.6499686863111\n",
      "    val_log_joint  : 1161.242240574049\n",
      "Train Epoch: 53 [0/101520 (0%)] Loss: -953.764526\n",
      "Train Epoch: 53 [11264/101520 (11%)] Loss: -961.914062\n",
      "Train Epoch: 53 [22528/101520 (22%)] Loss: -957.869141\n",
      "Train Epoch: 53 [33792/101520 (33%)] Loss: -952.233276\n",
      "Train Epoch: 53 [45056/101520 (44%)] Loss: -959.885620\n",
      "Train Epoch: 53 [56320/101520 (55%)] Loss: -950.584167\n",
      "Train Epoch: 53 [67584/101520 (67%)] Loss: -958.161133\n",
      "Train Epoch: 53 [78848/101520 (78%)] Loss: -949.968872\n",
      "Train Epoch: 53 [90112/101520 (89%)] Loss: -951.667480\n",
      "Train Epoch: 53 [101376/101520 (100%)] Loss: -960.475891\n",
      "    epoch          : 53\n",
      "    loss           : -955.9607283242384\n",
      "    ess            : 1.9770623205894202\n",
      "    log_marginal   : 955.9778399443506\n",
      "    log_joint      : 1163.7234254769944\n",
      "    val_loss       : -957.4586500084919\n",
      "    val_ess        : 1.9777069454607756\n",
      "    val_log_marginal: 957.475830078125\n",
      "    val_log_joint  : 1165.038913892663\n",
      "Train Epoch: 54 [0/101520 (0%)] Loss: -968.756165\n",
      "Train Epoch: 54 [11264/101520 (11%)] Loss: -963.143616\n",
      "Train Epoch: 54 [22528/101520 (22%)] Loss: -963.622009\n",
      "Train Epoch: 54 [33792/101520 (33%)] Loss: -954.306091\n",
      "Train Epoch: 54 [45056/101520 (44%)] Loss: -958.091187\n",
      "Train Epoch: 54 [56320/101520 (55%)] Loss: -958.147339\n",
      "Train Epoch: 54 [67584/101520 (67%)] Loss: -963.508423\n",
      "Train Epoch: 54 [78848/101520 (78%)] Loss: -959.990601\n",
      "Train Epoch: 54 [90112/101520 (89%)] Loss: -952.435913\n",
      "Train Epoch: 54 [101376/101520 (100%)] Loss: -944.486755\n",
      "    epoch          : 54\n",
      "    loss           : -959.6428501761739\n",
      "    ess            : 1.976848088317181\n",
      "    log_marginal   : 959.6602289401108\n",
      "    log_joint      : 1167.199580667007\n",
      "    val_loss       : -958.9722820779551\n",
      "    val_ess        : 1.9775214972703352\n",
      "    val_log_marginal: 958.98840066661\n",
      "    val_log_joint  : 1166.398341966712\n",
      "Train Epoch: 55 [0/101520 (0%)] Loss: -961.856445\n",
      "Train Epoch: 55 [11264/101520 (11%)] Loss: -959.109131\n",
      "Train Epoch: 55 [22528/101520 (22%)] Loss: -969.449951\n",
      "Train Epoch: 55 [33792/101520 (33%)] Loss: -966.696533\n",
      "Train Epoch: 55 [45056/101520 (44%)] Loss: -961.864624\n",
      "Train Epoch: 55 [56320/101520 (55%)] Loss: -969.515991\n",
      "Train Epoch: 55 [67584/101520 (67%)] Loss: -962.655884\n",
      "Train Epoch: 55 [78848/101520 (78%)] Loss: -966.316650\n",
      "Train Epoch: 55 [90112/101520 (89%)] Loss: -977.461182\n",
      "Train Epoch: 55 [101376/101520 (100%)] Loss: -969.270813\n",
      "    epoch          : 55\n",
      "    loss           : -963.2512710034548\n",
      "    ess            : 1.9760525783701757\n",
      "    log_marginal   : 963.2698480807358\n",
      "    log_joint      : 1170.6961706726995\n",
      "    val_loss       : -964.0119788128396\n",
      "    val_ess        : 1.9762783102367236\n",
      "    val_log_marginal: 964.0306024966033\n",
      "    val_log_joint  : 1171.553169582201\n",
      "Train Epoch: 56 [0/101520 (0%)] Loss: -976.718994\n",
      "Train Epoch: 56 [11264/101520 (11%)] Loss: -964.847412\n",
      "Train Epoch: 56 [22528/101520 (22%)] Loss: -954.736084\n",
      "Train Epoch: 56 [33792/101520 (33%)] Loss: -952.815552\n",
      "Train Epoch: 56 [45056/101520 (44%)] Loss: -959.636719\n",
      "Train Epoch: 56 [56320/101520 (55%)] Loss: -952.581360\n",
      "Train Epoch: 56 [67584/101520 (67%)] Loss: -964.157471\n",
      "Train Epoch: 56 [78848/101520 (78%)] Loss: -957.626770\n",
      "Train Epoch: 56 [90112/101520 (89%)] Loss: -948.661865\n",
      "Train Epoch: 56 [101376/101520 (100%)] Loss: -962.511292\n",
      "    epoch          : 56\n",
      "    loss           : -958.9311600114831\n",
      "    ess            : 1.9768108417041337\n",
      "    log_marginal   : 958.9483884878495\n",
      "    log_joint      : 1166.6166035254396\n",
      "    val_loss       : -954.9501581606658\n",
      "    val_ess        : 1.974035874657009\n",
      "    val_log_marginal: 954.969190514606\n",
      "    val_log_joint  : 1162.662013841712\n",
      "Train Epoch: 57 [0/101520 (0%)] Loss: -964.265381\n",
      "Train Epoch: 57 [11264/101520 (11%)] Loss: -973.115173\n",
      "Train Epoch: 57 [22528/101520 (22%)] Loss: -961.735107\n",
      "Train Epoch: 57 [33792/101520 (33%)] Loss: -960.710510\n",
      "Train Epoch: 57 [45056/101520 (44%)] Loss: -963.991699\n",
      "Train Epoch: 57 [56320/101520 (55%)] Loss: -957.752808\n",
      "Train Epoch: 57 [67584/101520 (67%)] Loss: -955.183594\n",
      "Train Epoch: 57 [78848/101520 (78%)] Loss: -949.165894\n",
      "Train Epoch: 57 [90112/101520 (89%)] Loss: -957.741821\n",
      "Train Epoch: 57 [101376/101520 (100%)] Loss: -956.147766\n",
      "    epoch          : 57\n",
      "    loss           : -959.7472221336174\n",
      "    ess            : 1.9762449689845942\n",
      "    log_marginal   : 959.7654808466159\n",
      "    log_joint      : 1167.5052809212077\n",
      "    val_loss       : -960.4372983186141\n",
      "    val_ess        : 1.97642183303833\n",
      "    val_log_marginal: 960.4551656971807\n",
      "    val_log_joint  : 1167.9465225883152\n",
      "Train Epoch: 58 [0/101520 (0%)] Loss: -962.243286\n",
      "Train Epoch: 58 [11264/101520 (11%)] Loss: -962.632263\n",
      "Train Epoch: 58 [22528/101520 (22%)] Loss: -965.636230\n",
      "Train Epoch: 58 [33792/101520 (33%)] Loss: -961.490967\n",
      "Train Epoch: 58 [45056/101520 (44%)] Loss: -962.012329\n",
      "Train Epoch: 58 [56320/101520 (55%)] Loss: -961.306641\n",
      "Train Epoch: 58 [67584/101520 (67%)] Loss: -957.067993\n",
      "Train Epoch: 58 [78848/101520 (78%)] Loss: -959.136475\n",
      "Train Epoch: 58 [90112/101520 (89%)] Loss: -961.151489\n",
      "Train Epoch: 58 [101376/101520 (100%)] Loss: -962.909485\n",
      "    epoch          : 58\n",
      "    loss           : -962.8946674289416\n",
      "    ess            : 1.9767696306334068\n",
      "    log_marginal   : 962.9128316754671\n",
      "    log_joint      : 1170.4727212723776\n",
      "    val_loss       : -961.8486938476562\n",
      "    val_ess        : 1.9767461030379585\n",
      "    val_log_marginal: 961.8658447265625\n",
      "    val_log_joint  : 1169.2277248216712\n",
      "Train Epoch: 59 [0/101520 (0%)] Loss: -968.300293\n",
      "Train Epoch: 59 [11264/101520 (11%)] Loss: -962.163025\n",
      "Train Epoch: 59 [22528/101520 (22%)] Loss: -967.582153\n",
      "Train Epoch: 59 [33792/101520 (33%)] Loss: -972.608093\n",
      "Train Epoch: 59 [45056/101520 (44%)] Loss: -966.500000\n",
      "Train Epoch: 59 [56320/101520 (55%)] Loss: -973.252686\n",
      "Train Epoch: 59 [67584/101520 (67%)] Loss: -966.599121\n",
      "Train Epoch: 59 [78848/101520 (78%)] Loss: -960.721924\n",
      "Train Epoch: 59 [90112/101520 (89%)] Loss: -963.731323\n",
      "Train Epoch: 59 [101376/101520 (100%)] Loss: -966.294495\n",
      "    epoch          : 59\n",
      "    loss           : -966.383374698198\n",
      "    ess            : 1.9758041126644192\n",
      "    log_marginal   : 966.4026894114126\n",
      "    log_joint      : 1173.813567348461\n",
      "    val_loss       : -966.3569070567255\n",
      "    val_ess        : 1.9757734018823374\n",
      "    val_log_marginal: 966.3749177352241\n",
      "    val_log_joint  : 1173.7223324983017\n",
      "Train Epoch: 60 [0/101520 (0%)] Loss: -976.642883\n",
      "Train Epoch: 60 [11264/101520 (11%)] Loss: -965.140564\n",
      "Train Epoch: 60 [22528/101520 (22%)] Loss: -960.244873\n",
      "Train Epoch: 60 [33792/101520 (33%)] Loss: -961.643799\n",
      "Train Epoch: 60 [45056/101520 (44%)] Loss: -957.678345\n",
      "Train Epoch: 60 [56320/101520 (55%)] Loss: -964.744995\n",
      "Train Epoch: 60 [67584/101520 (67%)] Loss: -958.088867\n",
      "Train Epoch: 60 [78848/101520 (78%)] Loss: -963.134216\n",
      "Train Epoch: 60 [90112/101520 (89%)] Loss: -956.891663\n",
      "Train Epoch: 60 [101376/101520 (100%)] Loss: -965.869385\n",
      "    epoch          : 60\n",
      "    loss           : -962.1584362240892\n",
      "    ess            : 1.9764948310564512\n",
      "    log_marginal   : 962.1766541447472\n",
      "    log_joint      : 1169.8324796590373\n",
      "    val_loss       : -960.5463999872622\n",
      "    val_ess        : 1.9765094674151877\n",
      "    val_log_marginal: 960.5638241975204\n",
      "    val_log_joint  : 1168.0130721382473\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch60.pth ...\n",
      "Train Epoch: 61 [0/101520 (0%)] Loss: -961.584106\n",
      "Train Epoch: 61 [11264/101520 (11%)] Loss: -965.321838\n",
      "Train Epoch: 61 [22528/101520 (22%)] Loss: -969.067139\n",
      "Train Epoch: 61 [33792/101520 (33%)] Loss: -962.314453\n",
      "Train Epoch: 61 [45056/101520 (44%)] Loss: -953.600647\n",
      "Train Epoch: 61 [56320/101520 (55%)] Loss: -968.552246\n",
      "Train Epoch: 61 [67584/101520 (67%)] Loss: -965.433838\n",
      "Train Epoch: 61 [78848/101520 (78%)] Loss: -959.415283\n",
      "Train Epoch: 61 [90112/101520 (89%)] Loss: -965.646606\n",
      "Train Epoch: 61 [101376/101520 (100%)] Loss: -977.126282\n",
      "    epoch          : 61\n",
      "    loss           : -963.2374948472833\n",
      "    ess            : 1.9753647407694677\n",
      "    log_marginal   : 963.2565252409509\n",
      "    log_joint      : 1170.9858521121232\n",
      "    val_loss       : -963.8467884893003\n",
      "    val_ess        : 1.974687959836877\n",
      "    val_log_marginal: 963.8653882897419\n",
      "    val_log_joint  : 1171.5329749065897\n",
      "Train Epoch: 62 [0/101520 (0%)] Loss: -980.690918\n",
      "Train Epoch: 62 [11264/101520 (11%)] Loss: -963.075073\n",
      "Train Epoch: 62 [22528/101520 (22%)] Loss: -970.858398\n",
      "Train Epoch: 62 [33792/101520 (33%)] Loss: -965.658569\n",
      "Train Epoch: 62 [45056/101520 (44%)] Loss: -965.775940\n",
      "Train Epoch: 62 [56320/101520 (55%)] Loss: -964.964233\n",
      "Train Epoch: 62 [67584/101520 (67%)] Loss: -961.884460\n",
      "Train Epoch: 62 [78848/101520 (78%)] Loss: -969.194641\n",
      "Train Epoch: 62 [90112/101520 (89%)] Loss: -965.543579\n",
      "Train Epoch: 62 [101376/101520 (100%)] Loss: -983.412537\n",
      "    epoch          : 62\n",
      "    loss           : -966.1632497202811\n",
      "    ess            : 1.9769368519136055\n",
      "    log_marginal   : 966.1805383116756\n",
      "    log_joint      : 1173.7276402765783\n",
      "    val_loss       : -966.6681624702785\n",
      "    val_ess        : 1.975349151569864\n",
      "    val_log_marginal: 966.6869108780571\n",
      "    val_log_joint  : 1174.0277577275815\n",
      "Train Epoch: 63 [0/101520 (0%)] Loss: -971.752197\n",
      "Train Epoch: 63 [11264/101520 (11%)] Loss: -971.056580\n",
      "Train Epoch: 63 [22528/101520 (22%)] Loss: -968.969849\n",
      "Train Epoch: 63 [33792/101520 (33%)] Loss: -969.641357\n",
      "Train Epoch: 63 [45056/101520 (44%)] Loss: -965.621826\n",
      "Train Epoch: 63 [56320/101520 (55%)] Loss: -965.482788\n",
      "Train Epoch: 63 [67584/101520 (67%)] Loss: -971.520508\n",
      "Train Epoch: 63 [78848/101520 (78%)] Loss: -971.586914\n",
      "Train Epoch: 63 [90112/101520 (89%)] Loss: -965.691162\n",
      "Train Epoch: 63 [101376/101520 (100%)] Loss: -962.234680\n",
      "    epoch          : 63\n",
      "    loss           : -969.5237865352151\n",
      "    ess            : 1.9749707708406687\n",
      "    log_marginal   : 969.5430021813168\n",
      "    log_joint      : 1176.9656589833935\n",
      "    val_loss       : -970.0392960258152\n",
      "    val_ess        : 1.9758898227111152\n",
      "    val_log_marginal: 970.0596578846807\n",
      "    val_log_joint  : 1177.3182691491168\n",
      "Train Epoch: 64 [0/101520 (0%)] Loss: -969.729858\n",
      "Train Epoch: 64 [11264/101520 (11%)] Loss: -969.724243\n",
      "Train Epoch: 64 [22528/101520 (22%)] Loss: -974.720947\n",
      "Train Epoch: 64 [33792/101520 (33%)] Loss: -968.875366\n",
      "Train Epoch: 64 [45056/101520 (44%)] Loss: -963.438171\n",
      "Train Epoch: 64 [56320/101520 (55%)] Loss: -961.996338\n",
      "Train Epoch: 64 [67584/101520 (67%)] Loss: -957.155640\n",
      "Train Epoch: 64 [78848/101520 (78%)] Loss: -961.025696\n",
      "Train Epoch: 64 [90112/101520 (89%)] Loss: -960.973267\n",
      "Train Epoch: 64 [101376/101520 (100%)] Loss: -962.433594\n",
      "    epoch          : 64\n",
      "    loss           : -964.8815077585192\n",
      "    ess            : 1.9759299623307272\n",
      "    log_marginal   : 964.8994968740185\n",
      "    log_joint      : 1172.645511493012\n",
      "    val_loss       : -964.9848340905231\n",
      "    val_ess        : 1.973654498224673\n",
      "    val_log_marginal: 965.0052065641984\n",
      "    val_log_joint  : 1172.7614321501358\n",
      "Train Epoch: 65 [0/101520 (0%)] Loss: -968.060120\n",
      "Train Epoch: 65 [11264/101520 (11%)] Loss: -972.555908\n",
      "Train Epoch: 65 [22528/101520 (22%)] Loss: -972.757812\n",
      "Train Epoch: 65 [33792/101520 (33%)] Loss: -966.587036\n",
      "Train Epoch: 65 [45056/101520 (44%)] Loss: -962.997314\n",
      "Train Epoch: 65 [56320/101520 (55%)] Loss: -969.448486\n",
      "Train Epoch: 65 [67584/101520 (67%)] Loss: -963.534241\n",
      "Train Epoch: 65 [78848/101520 (78%)] Loss: -972.441650\n",
      "Train Epoch: 65 [90112/101520 (89%)] Loss: -963.040649\n",
      "Train Epoch: 65 [101376/101520 (100%)] Loss: -960.435547\n",
      "    epoch          : 65\n",
      "    loss           : -966.0515498635757\n",
      "    ess            : 1.9763926698933894\n",
      "    log_marginal   : 966.0695349918539\n",
      "    log_joint      : 1173.733075779287\n",
      "    val_loss       : -965.053883428159\n",
      "    val_ess        : 1.976307371388311\n",
      "    val_log_marginal: 965.0712625254755\n",
      "    val_log_joint  : 1172.5114958389945\n",
      "Train Epoch: 66 [0/101520 (0%)] Loss: -982.716553\n",
      "Train Epoch: 66 [11264/101520 (11%)] Loss: -965.714417\n",
      "Train Epoch: 66 [22528/101520 (22%)] Loss: -973.228271\n",
      "Train Epoch: 66 [33792/101520 (33%)] Loss: -968.546082\n",
      "Train Epoch: 66 [45056/101520 (44%)] Loss: -969.079590\n",
      "Train Epoch: 66 [56320/101520 (55%)] Loss: -971.282959\n",
      "Train Epoch: 66 [67584/101520 (67%)] Loss: -964.239319\n",
      "Train Epoch: 66 [78848/101520 (78%)] Loss: -963.147339\n",
      "Train Epoch: 66 [90112/101520 (89%)] Loss: -976.930542\n",
      "Train Epoch: 66 [101376/101520 (100%)] Loss: -974.142029\n",
      "    epoch          : 66\n",
      "    loss           : -968.9299009696922\n",
      "    ess            : 1.9755460041851254\n",
      "    log_marginal   : 968.9484814207758\n",
      "    log_joint      : 1176.456863786707\n",
      "    val_loss       : -968.153683869735\n",
      "    val_ess        : 1.9739131409188975\n",
      "    val_log_marginal: 968.1737591287364\n",
      "    val_log_joint  : 1175.2945715862772\n",
      "Train Epoch: 67 [0/101520 (0%)] Loss: -966.022156\n",
      "Train Epoch: 67 [11264/101520 (11%)] Loss: -971.837769\n",
      "Train Epoch: 67 [22528/101520 (22%)] Loss: -975.620239\n",
      "Train Epoch: 67 [33792/101520 (33%)] Loss: -966.979614\n",
      "Train Epoch: 67 [45056/101520 (44%)] Loss: -968.913330\n",
      "Train Epoch: 67 [56320/101520 (55%)] Loss: -972.902039\n",
      "Train Epoch: 67 [67584/101520 (67%)] Loss: -977.567139\n",
      "Train Epoch: 67 [78848/101520 (78%)] Loss: -969.712524\n",
      "Train Epoch: 67 [90112/101520 (89%)] Loss: -975.813232\n",
      "Train Epoch: 67 [101376/101520 (100%)] Loss: -988.290466\n",
      "    epoch          : 67\n",
      "    loss           : -972.3677984649812\n",
      "    ess            : 1.975254657280505\n",
      "    log_marginal   : 972.3867282579891\n",
      "    log_joint      : 1179.800592317054\n",
      "    val_loss       : -972.387820036515\n",
      "    val_ess        : 1.9756222496861997\n",
      "    val_log_marginal: 972.4070248811141\n",
      "    val_log_joint  : 1180.0976456351902\n",
      "Train Epoch: 68 [0/101520 (0%)] Loss: -966.675171\n",
      "Train Epoch: 68 [11264/101520 (11%)] Loss: -970.870911\n",
      "Train Epoch: 68 [22528/101520 (22%)] Loss: -968.416565\n",
      "Train Epoch: 68 [33792/101520 (33%)] Loss: -969.336304\n",
      "Train Epoch: 68 [45056/101520 (44%)] Loss: -965.888062\n",
      "Train Epoch: 68 [56320/101520 (55%)] Loss: -968.778564\n",
      "Train Epoch: 68 [67584/101520 (67%)] Loss: -967.339172\n",
      "Train Epoch: 68 [78848/101520 (78%)] Loss: -965.590637\n",
      "Train Epoch: 68 [90112/101520 (89%)] Loss: -969.324341\n",
      "Train Epoch: 68 [101376/101520 (100%)] Loss: -976.039490\n",
      "    epoch          : 68\n",
      "    loss           : -967.7882951132616\n",
      "    ess            : 1.9758836522174241\n",
      "    log_marginal   : 967.806601366206\n",
      "    log_joint      : 1175.4247053136778\n",
      "    val_loss       : -965.8742835003396\n",
      "    val_ess        : 1.9754392375116763\n",
      "    val_log_marginal: 965.8942658797554\n",
      "    val_log_joint  : 1173.6997282608695\n",
      "Train Epoch: 69 [0/101520 (0%)] Loss: -967.187134\n",
      "Train Epoch: 69 [11264/101520 (11%)] Loss: -968.316162\n",
      "Train Epoch: 69 [22528/101520 (22%)] Loss: -977.991211\n",
      "Train Epoch: 69 [33792/101520 (33%)] Loss: -966.912598\n",
      "Train Epoch: 69 [45056/101520 (44%)] Loss: -971.884216\n",
      "Train Epoch: 69 [56320/101520 (55%)] Loss: -965.445557\n",
      "Train Epoch: 69 [67584/101520 (67%)] Loss: -961.806274\n",
      "Train Epoch: 69 [78848/101520 (78%)] Loss: -967.134521\n",
      "Train Epoch: 69 [90112/101520 (89%)] Loss: -968.539673\n",
      "Train Epoch: 69 [101376/101520 (100%)] Loss: -964.086365\n",
      "    epoch          : 69\n",
      "    loss           : -968.7754771170305\n",
      "    ess            : 1.9762183254088588\n",
      "    log_marginal   : 968.7935925967729\n",
      "    log_joint      : 1176.4317682160804\n",
      "    val_loss       : -968.6794698963995\n",
      "    val_ess        : 1.9757903451504915\n",
      "    val_log_marginal: 968.6988684612771\n",
      "    val_log_joint  : 1176.1633513077445\n",
      "Train Epoch: 70 [0/101520 (0%)] Loss: -975.126709\n",
      "Train Epoch: 70 [11264/101520 (11%)] Loss: -970.388672\n",
      "Train Epoch: 70 [22528/101520 (22%)] Loss: -975.081421\n",
      "Train Epoch: 70 [33792/101520 (33%)] Loss: -974.692017\n",
      "Train Epoch: 70 [45056/101520 (44%)] Loss: -973.737671\n",
      "Train Epoch: 70 [56320/101520 (55%)] Loss: -969.987061\n",
      "Train Epoch: 70 [67584/101520 (67%)] Loss: -970.665894\n",
      "Train Epoch: 70 [78848/101520 (78%)] Loss: -970.724854\n",
      "Train Epoch: 70 [90112/101520 (89%)] Loss: -972.379150\n",
      "Train Epoch: 70 [101376/101520 (100%)] Loss: -971.531677\n",
      "    epoch          : 70\n",
      "    loss           : -971.6138824002827\n",
      "    ess            : 1.9753597082205154\n",
      "    log_marginal   : 971.6330649217768\n",
      "    log_joint      : 1179.1166765222597\n",
      "    val_loss       : -972.4650613536005\n",
      "    val_ess        : 1.9770323919213337\n",
      "    val_log_marginal: 972.4833639393682\n",
      "    val_log_joint  : 1179.543897545856\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch70.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 71 [0/101520 (0%)] Loss: -973.503418\n",
      "Train Epoch: 71 [11264/101520 (11%)] Loss: -974.779358\n",
      "Train Epoch: 71 [22528/101520 (22%)] Loss: -974.360352\n",
      "Train Epoch: 71 [33792/101520 (33%)] Loss: -975.039368\n",
      "Train Epoch: 71 [45056/101520 (44%)] Loss: -974.631348\n",
      "Train Epoch: 71 [56320/101520 (55%)] Loss: -973.544861\n",
      "Train Epoch: 71 [67584/101520 (67%)] Loss: -974.364380\n",
      "Train Epoch: 71 [78848/101520 (78%)] Loss: -970.669312\n",
      "Train Epoch: 71 [90112/101520 (89%)] Loss: -972.582642\n",
      "Train Epoch: 71 [101376/101520 (100%)] Loss: -986.546570\n",
      "    epoch          : 71\n",
      "    loss           : -974.9082988183103\n",
      "    ess            : 1.974497442269445\n",
      "    log_marginal   : 974.9280070395924\n",
      "    log_joint      : 1182.309894810969\n",
      "    val_loss       : -974.4313301418139\n",
      "    val_ess        : 1.9737576868223108\n",
      "    val_log_marginal: 974.451588506284\n",
      "    val_log_joint  : 1182.2812075407608\n",
      "Train Epoch: 72 [0/101520 (0%)] Loss: -969.308167\n",
      "Train Epoch: 72 [11264/101520 (11%)] Loss: -973.409973\n",
      "Train Epoch: 72 [22528/101520 (22%)] Loss: -969.878906\n",
      "Train Epoch: 72 [33792/101520 (33%)] Loss: -963.890625\n",
      "Train Epoch: 72 [45056/101520 (44%)] Loss: -974.374268\n",
      "Train Epoch: 72 [56320/101520 (55%)] Loss: -974.306091\n",
      "Train Epoch: 72 [67584/101520 (67%)] Loss: -974.217224\n",
      "Train Epoch: 72 [78848/101520 (78%)] Loss: -966.461060\n",
      "Train Epoch: 72 [90112/101520 (89%)] Loss: -963.479980\n",
      "Train Epoch: 72 [101376/101520 (100%)] Loss: -976.479065\n",
      "    epoch          : 72\n",
      "    loss           : -970.5008600129554\n",
      "    ess            : 1.975364421480265\n",
      "    log_marginal   : 970.5197701765665\n",
      "    log_joint      : 1178.1395466100032\n",
      "    val_loss       : -968.3051200534986\n",
      "    val_ess        : 1.9767426926156748\n",
      "    val_log_marginal: 968.3245557702106\n",
      "    val_log_joint  : 1176.0630519701087\n",
      "Train Epoch: 73 [0/101520 (0%)] Loss: -972.046021\n",
      "Train Epoch: 73 [11264/101520 (11%)] Loss: -974.238037\n",
      "Train Epoch: 73 [22528/101520 (22%)] Loss: -971.148682\n",
      "Train Epoch: 73 [33792/101520 (33%)] Loss: -966.192200\n",
      "Train Epoch: 73 [45056/101520 (44%)] Loss: -975.291809\n",
      "Train Epoch: 73 [56320/101520 (55%)] Loss: -969.398438\n",
      "Train Epoch: 73 [67584/101520 (67%)] Loss: -966.242188\n",
      "Train Epoch: 73 [78848/101520 (78%)] Loss: -976.044922\n",
      "Train Epoch: 73 [90112/101520 (89%)] Loss: -970.168823\n",
      "Train Epoch: 73 [101376/101520 (100%)] Loss: -957.317078\n",
      "    epoch          : 73\n",
      "    loss           : -971.2911490435577\n",
      "    ess            : 1.9750054930921774\n",
      "    log_marginal   : 971.3104573158763\n",
      "    log_joint      : 1179.026903928824\n",
      "    val_loss       : -972.2750376825747\n",
      "    val_ess        : 1.9700785823490308\n",
      "    val_log_marginal: 972.3027662194294\n",
      "    val_log_joint  : 1180.0043573794158\n",
      "Train Epoch: 74 [0/101520 (0%)] Loss: -982.525757\n",
      "Train Epoch: 74 [11264/101520 (11%)] Loss: -977.758179\n",
      "Train Epoch: 74 [22528/101520 (22%)] Loss: -974.429077\n",
      "Train Epoch: 74 [33792/101520 (33%)] Loss: -974.132874\n",
      "Train Epoch: 74 [45056/101520 (44%)] Loss: -973.595215\n",
      "Train Epoch: 74 [56320/101520 (55%)] Loss: -975.211060\n",
      "Train Epoch: 74 [67584/101520 (67%)] Loss: -970.527466\n",
      "Train Epoch: 74 [78848/101520 (78%)] Loss: -967.702087\n",
      "Train Epoch: 74 [90112/101520 (89%)] Loss: -969.392090\n",
      "Train Epoch: 74 [101376/101520 (100%)] Loss: -955.547668\n",
      "    epoch          : 74\n",
      "    loss           : -973.7722051419205\n",
      "    ess            : 1.9750519883093522\n",
      "    log_marginal   : 973.7911879956423\n",
      "    log_joint      : 1181.268877345713\n",
      "    val_loss       : -974.9339971127717\n",
      "    val_ess        : 1.9723430353662241\n",
      "    val_log_marginal: 974.9551205842391\n",
      "    val_log_joint  : 1182.3110988451087\n",
      "Train Epoch: 75 [0/101520 (0%)] Loss: -977.442505\n",
      "Train Epoch: 75 [11264/101520 (11%)] Loss: -973.490967\n",
      "Train Epoch: 75 [22528/101520 (22%)] Loss: -982.270386\n",
      "Train Epoch: 75 [33792/101520 (33%)] Loss: -981.904358\n",
      "Train Epoch: 75 [45056/101520 (44%)] Loss: -975.733704\n",
      "Train Epoch: 75 [56320/101520 (55%)] Loss: -978.072754\n",
      "Train Epoch: 75 [67584/101520 (67%)] Loss: -975.787964\n",
      "Train Epoch: 75 [78848/101520 (78%)] Loss: -972.479248\n",
      "Train Epoch: 75 [90112/101520 (89%)] Loss: -977.057800\n",
      "Train Epoch: 75 [101376/101520 (100%)] Loss: -968.902588\n",
      "    epoch          : 75\n",
      "    loss           : -977.1030288772967\n",
      "    ess            : 1.9746655231744201\n",
      "    log_marginal   : 977.1223828493052\n",
      "    log_joint      : 1184.519151543852\n",
      "    val_loss       : -977.3963676120924\n",
      "    val_ess        : 1.9748783370722895\n",
      "    val_log_marginal: 977.4144419794497\n",
      "    val_log_joint  : 1184.661769701087\n",
      "Train Epoch: 76 [0/101520 (0%)] Loss: -980.320923\n",
      "Train Epoch: 76 [11264/101520 (11%)] Loss: -978.295532\n",
      "Train Epoch: 76 [22528/101520 (22%)] Loss: -970.112976\n",
      "Train Epoch: 76 [33792/101520 (33%)] Loss: -962.880249\n",
      "Train Epoch: 76 [45056/101520 (44%)] Loss: -976.566040\n",
      "Train Epoch: 76 [56320/101520 (55%)] Loss: -972.563721\n",
      "Train Epoch: 76 [67584/101520 (67%)] Loss: -972.597900\n",
      "Train Epoch: 76 [78848/101520 (78%)] Loss: -966.277039\n",
      "Train Epoch: 76 [90112/101520 (89%)] Loss: -972.060303\n",
      "Train Epoch: 76 [101376/101520 (100%)] Loss: -974.136719\n",
      "    epoch          : 76\n",
      "    loss           : -972.8068550148201\n",
      "    ess            : 1.9746623674229762\n",
      "    log_marginal   : 972.8262602072864\n",
      "    log_joint      : 1180.516660450691\n",
      "    val_loss       : -968.6707126783288\n",
      "    val_ess        : 1.9761622211207515\n",
      "    val_log_marginal: 968.6882164996604\n",
      "    val_log_joint  : 1176.313179347826\n",
      "Train Epoch: 77 [0/101520 (0%)] Loss: -972.810913\n",
      "Train Epoch: 77 [11264/101520 (11%)] Loss: -977.858215\n",
      "Train Epoch: 77 [22528/101520 (22%)] Loss: -967.863464\n",
      "Train Epoch: 77 [33792/101520 (33%)] Loss: -978.612732\n",
      "Train Epoch: 77 [45056/101520 (44%)] Loss: -969.864685\n",
      "Train Epoch: 77 [56320/101520 (55%)] Loss: -968.450073\n",
      "Train Epoch: 77 [67584/101520 (67%)] Loss: -977.666199\n",
      "Train Epoch: 77 [78848/101520 (78%)] Loss: -971.729004\n",
      "Train Epoch: 77 [90112/101520 (89%)] Loss: -973.806091\n",
      "Train Epoch: 77 [101376/101520 (100%)] Loss: -975.060547\n",
      "    epoch          : 77\n",
      "    loss           : -973.9611362476445\n",
      "    ess            : 1.9743299106856687\n",
      "    log_marginal   : 973.9808502964039\n",
      "    log_joint      : 1181.6701562009266\n",
      "    val_loss       : -971.6853637695312\n",
      "    val_ess        : 1.972159250922825\n",
      "    val_log_marginal: 971.7079520847486\n",
      "    val_log_joint  : 1179.2827307659647\n",
      "Train Epoch: 78 [0/101520 (0%)] Loss: -983.995178\n",
      "Train Epoch: 78 [11264/101520 (11%)] Loss: -973.967346\n",
      "Train Epoch: 78 [22528/101520 (22%)] Loss: -973.068542\n",
      "Train Epoch: 78 [33792/101520 (33%)] Loss: -970.725281\n",
      "Train Epoch: 78 [45056/101520 (44%)] Loss: -975.636353\n",
      "Train Epoch: 78 [56320/101520 (55%)] Loss: -973.165894\n",
      "Train Epoch: 78 [67584/101520 (67%)] Loss: -970.785583\n",
      "Train Epoch: 78 [78848/101520 (78%)] Loss: -975.109619\n",
      "Train Epoch: 78 [90112/101520 (89%)] Loss: -976.999451\n",
      "Train Epoch: 78 [101376/101520 (100%)] Loss: -982.540283\n",
      "    epoch          : 78\n",
      "    loss           : -976.0337275979507\n",
      "    ess            : 1.9747259323321396\n",
      "    log_marginal   : 976.053146899046\n",
      "    log_joint      : 1183.6334492285646\n",
      "    val_loss       : -977.8129431683084\n",
      "    val_ess        : 1.9718288235042407\n",
      "    val_log_marginal: 977.834496539572\n",
      "    val_log_joint  : 1185.1132706351902\n",
      "Train Epoch: 79 [0/101520 (0%)] Loss: -981.901672\n",
      "Train Epoch: 79 [11264/101520 (11%)] Loss: -976.007874\n",
      "Train Epoch: 79 [22528/101520 (22%)] Loss: -976.624390\n",
      "Train Epoch: 79 [33792/101520 (33%)] Loss: -973.104858\n",
      "Train Epoch: 79 [45056/101520 (44%)] Loss: -976.827026\n",
      "Train Epoch: 79 [56320/101520 (55%)] Loss: -975.171692\n",
      "Train Epoch: 79 [67584/101520 (67%)] Loss: -980.369995\n",
      "Train Epoch: 79 [78848/101520 (78%)] Loss: -973.980469\n",
      "Train Epoch: 79 [90112/101520 (89%)] Loss: -983.968506\n",
      "Train Epoch: 79 [101376/101520 (100%)] Loss: -974.120789\n",
      "    epoch          : 79\n",
      "    loss           : -979.2738644393844\n",
      "    ess            : 1.9748703367147014\n",
      "    log_marginal   : 979.2931451078636\n",
      "    log_joint      : 1186.7040886807083\n",
      "    val_loss       : -981.1449054220449\n",
      "    val_ess        : 1.975159069766169\n",
      "    val_log_marginal: 981.1644950534986\n",
      "    val_log_joint  : 1188.3018321161685\n",
      "Train Epoch: 80 [0/101520 (0%)] Loss: -986.278503\n",
      "Train Epoch: 80 [11264/101520 (11%)] Loss: -971.278259\n",
      "Train Epoch: 80 [22528/101520 (22%)] Loss: -979.961060\n",
      "Train Epoch: 80 [33792/101520 (33%)] Loss: -973.030457\n",
      "Train Epoch: 80 [45056/101520 (44%)] Loss: -973.640259\n",
      "Train Epoch: 80 [56320/101520 (55%)] Loss: -981.155884\n",
      "Train Epoch: 80 [67584/101520 (67%)] Loss: -974.543091\n",
      "Train Epoch: 80 [78848/101520 (78%)] Loss: -973.306885\n",
      "Train Epoch: 80 [90112/101520 (89%)] Loss: -976.404846\n",
      "Train Epoch: 80 [101376/101520 (100%)] Loss: -952.195557\n",
      "    epoch          : 80\n",
      "    loss           : -975.4534774090178\n",
      "    ess            : 1.974294979368622\n",
      "    log_marginal   : 975.4730801222911\n",
      "    log_joint      : 1183.078579543224\n",
      "    val_loss       : -974.3486673106318\n",
      "    val_ess        : 1.9743841160898623\n",
      "    val_log_marginal: 974.3686231530231\n",
      "    val_log_joint  : 1181.8787470278533\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch80.pth ...\n",
      "Train Epoch: 81 [0/101520 (0%)] Loss: -974.802002\n",
      "Train Epoch: 81 [11264/101520 (11%)] Loss: -981.999695\n",
      "Train Epoch: 81 [22528/101520 (22%)] Loss: -970.672974\n",
      "Train Epoch: 81 [33792/101520 (33%)] Loss: -976.065674\n",
      "Train Epoch: 81 [45056/101520 (44%)] Loss: -978.161926\n",
      "Train Epoch: 81 [56320/101520 (55%)] Loss: -978.828491\n",
      "Train Epoch: 81 [67584/101520 (67%)] Loss: -981.881226\n",
      "Train Epoch: 81 [78848/101520 (78%)] Loss: -972.095459\n",
      "Train Epoch: 81 [90112/101520 (89%)] Loss: -970.049805\n",
      "Train Epoch: 81 [101376/101520 (100%)] Loss: -972.033020\n",
      "    epoch          : 81\n",
      "    loss           : -976.4914753209406\n",
      "    ess            : 1.9749867970020927\n",
      "    log_marginal   : 976.5106109159077\n",
      "    log_joint      : 1184.177397608158\n",
      "    val_loss       : -975.9911525560462\n",
      "    val_ess        : 1.973655923553135\n",
      "    val_log_marginal: 976.0096223250679\n",
      "    val_log_joint  : 1183.5169624660325\n",
      "Train Epoch: 82 [0/101520 (0%)] Loss: -978.712402\n",
      "Train Epoch: 82 [11264/101520 (11%)] Loss: -982.527649\n",
      "Train Epoch: 82 [22528/101520 (22%)] Loss: -973.140015\n",
      "Train Epoch: 82 [33792/101520 (33%)] Loss: -980.310059\n",
      "Train Epoch: 82 [45056/101520 (44%)] Loss: -981.912720\n",
      "Train Epoch: 82 [56320/101520 (55%)] Loss: -977.618286\n",
      "Train Epoch: 82 [67584/101520 (67%)] Loss: -980.523193\n",
      "Train Epoch: 82 [78848/101520 (78%)] Loss: -971.595337\n",
      "Train Epoch: 82 [90112/101520 (89%)] Loss: -970.399231\n",
      "Train Epoch: 82 [101376/101520 (100%)] Loss: -967.269958\n",
      "    epoch          : 82\n",
      "    loss           : -978.1312004357726\n",
      "    ess            : 1.9738708554799833\n",
      "    log_marginal   : 978.1517395326241\n",
      "    log_joint      : 1185.673420201594\n",
      "    val_loss       : -976.0038717518682\n",
      "    val_ess        : 1.9734576318574988\n",
      "    val_log_marginal: 976.0226838485054\n",
      "    val_log_joint  : 1183.4874851392663\n",
      "Train Epoch: 83 [0/101520 (0%)] Loss: -988.674438\n",
      "Train Epoch: 83 [11264/101520 (11%)] Loss: -979.904785\n",
      "Train Epoch: 83 [22528/101520 (22%)] Loss: -973.110168\n",
      "Train Epoch: 83 [33792/101520 (33%)] Loss: -983.269714\n",
      "Train Epoch: 83 [45056/101520 (44%)] Loss: -987.294678\n",
      "Train Epoch: 83 [56320/101520 (55%)] Loss: -983.116943\n",
      "Train Epoch: 83 [67584/101520 (67%)] Loss: -979.940125\n",
      "Train Epoch: 83 [78848/101520 (78%)] Loss: -984.696106\n",
      "Train Epoch: 83 [90112/101520 (89%)] Loss: -981.002930\n",
      "Train Epoch: 83 [101376/101520 (100%)] Loss: -963.696716\n",
      "    epoch          : 83\n",
      "    loss           : -981.2205184859846\n",
      "    ess            : 1.9738942672259843\n",
      "    log_marginal   : 981.2404506050761\n",
      "    log_joint      : 1188.6561193418263\n",
      "    val_loss       : -981.0169359290081\n",
      "    val_ess        : 1.9746862598087476\n",
      "    val_log_marginal: 981.0360505477241\n",
      "    val_log_joint  : 1188.625297214674\n",
      "Train Epoch: 84 [0/101520 (0%)] Loss: -979.263672\n",
      "Train Epoch: 84 [11264/101520 (11%)] Loss: -981.042236\n",
      "Train Epoch: 84 [22528/101520 (22%)] Loss: -978.736267\n",
      "Train Epoch: 84 [33792/101520 (33%)] Loss: -973.380127\n",
      "Train Epoch: 84 [45056/101520 (44%)] Loss: -976.322327\n",
      "Train Epoch: 84 [56320/101520 (55%)] Loss: -976.026123\n",
      "Train Epoch: 84 [67584/101520 (67%)] Loss: -973.960510\n",
      "Train Epoch: 84 [78848/101520 (78%)] Loss: -967.663696\n",
      "Train Epoch: 84 [90112/101520 (89%)] Loss: -979.667908\n",
      "Train Epoch: 84 [101376/101520 (100%)] Loss: -974.314270\n",
      "    epoch          : 84\n",
      "    loss           : -977.4309916280622\n",
      "    ess            : 1.9733682092110716\n",
      "    log_marginal   : 977.4517466482805\n",
      "    log_joint      : 1185.0419835996388\n",
      "    val_loss       : -974.694603961447\n",
      "    val_ess        : 1.9734037699906721\n",
      "    val_log_marginal: 974.7143740446671\n",
      "    val_log_joint  : 1182.5648511803668\n",
      "Train Epoch: 85 [0/101520 (0%)] Loss: -978.817810\n",
      "Train Epoch: 85 [11264/101520 (11%)] Loss: -979.618713\n",
      "Train Epoch: 85 [22528/101520 (22%)] Loss: -988.320068\n",
      "Train Epoch: 85 [33792/101520 (33%)] Loss: -985.305664\n",
      "Train Epoch: 85 [45056/101520 (44%)] Loss: -972.061523\n",
      "Train Epoch: 85 [56320/101520 (55%)] Loss: -980.339966\n",
      "Train Epoch: 85 [67584/101520 (67%)] Loss: -987.730896\n",
      "Train Epoch: 85 [78848/101520 (78%)] Loss: -980.265442\n",
      "Train Epoch: 85 [90112/101520 (89%)] Loss: -975.393311\n",
      "Train Epoch: 85 [101376/101520 (100%)] Loss: -985.241455\n",
      "    epoch          : 85\n",
      "    loss           : -978.7698704705166\n",
      "    ess            : 1.9744012200053613\n",
      "    log_marginal   : 978.790281362869\n",
      "    log_joint      : 1186.4714686714824\n",
      "    val_loss       : -980.0942887015965\n",
      "    val_ess        : 1.9749879681545754\n",
      "    val_log_marginal: 980.1136474609375\n",
      "    val_log_joint  : 1187.412788722826\n",
      "Train Epoch: 86 [0/101520 (0%)] Loss: -979.794312\n",
      "Train Epoch: 86 [11264/101520 (11%)] Loss: -982.872192\n",
      "Train Epoch: 86 [22528/101520 (22%)] Loss: -983.811646\n",
      "Train Epoch: 86 [33792/101520 (33%)] Loss: -978.163452\n",
      "Train Epoch: 86 [45056/101520 (44%)] Loss: -979.156555\n",
      "Train Epoch: 86 [56320/101520 (55%)] Loss: -981.345581\n",
      "Train Epoch: 86 [67584/101520 (67%)] Loss: -980.044678\n",
      "Train Epoch: 86 [78848/101520 (78%)] Loss: -982.364014\n",
      "Train Epoch: 86 [90112/101520 (89%)] Loss: -977.550110\n",
      "Train Epoch: 86 [101376/101520 (100%)] Loss: -987.758667\n",
      "    epoch          : 86\n",
      "    loss           : -980.4300423626923\n",
      "    ess            : 1.973614060699041\n",
      "    log_marginal   : 980.45008121663\n",
      "    log_joint      : 1187.949521778816\n",
      "    val_loss       : -980.4516946543818\n",
      "    val_ess        : 1.9737537632817808\n",
      "    val_log_marginal: 980.470875615659\n",
      "    val_log_joint  : 1188.0354269276495\n",
      "Train Epoch: 87 [0/101520 (0%)] Loss: -980.169006\n",
      "Train Epoch: 87 [11264/101520 (11%)] Loss: -986.025391\n",
      "Train Epoch: 87 [22528/101520 (22%)] Loss: -986.992920\n",
      "Train Epoch: 87 [33792/101520 (33%)] Loss: -980.698730\n",
      "Train Epoch: 87 [45056/101520 (44%)] Loss: -992.163452\n",
      "Train Epoch: 87 [56320/101520 (55%)] Loss: -982.732666\n",
      "Train Epoch: 87 [67584/101520 (67%)] Loss: -987.723877\n",
      "Train Epoch: 87 [78848/101520 (78%)] Loss: -981.226318\n",
      "Train Epoch: 87 [90112/101520 (89%)] Loss: -983.462036\n",
      "Train Epoch: 87 [101376/101520 (100%)] Loss: -986.958313\n",
      "    epoch          : 87\n",
      "    loss           : -983.4008509957011\n",
      "    ess            : 1.9738942786077758\n",
      "    log_marginal   : 983.4209833959839\n",
      "    log_joint      : 1190.853714986063\n",
      "    val_loss       : -983.0924894913384\n",
      "    val_ess        : 1.9740089851876963\n",
      "    val_log_marginal: 983.1136421535326\n",
      "    val_log_joint  : 1190.5759118121603\n",
      "Train Epoch: 88 [0/101520 (0%)] Loss: -988.815674\n",
      "Train Epoch: 88 [11264/101520 (11%)] Loss: -984.470032\n",
      "Train Epoch: 88 [22528/101520 (22%)] Loss: -977.689941\n",
      "Train Epoch: 88 [33792/101520 (33%)] Loss: -980.657715\n",
      "Train Epoch: 88 [45056/101520 (44%)] Loss: -979.512207\n",
      "Train Epoch: 88 [56320/101520 (55%)] Loss: -977.894043\n",
      "Train Epoch: 88 [67584/101520 (67%)] Loss: -981.274048\n",
      "Train Epoch: 88 [78848/101520 (78%)] Loss: -974.698120\n",
      "Train Epoch: 88 [90112/101520 (89%)] Loss: -979.323486\n",
      "Train Epoch: 88 [101376/101520 (100%)] Loss: -973.576843\n",
      "    epoch          : 88\n",
      "    loss           : -979.7159880825023\n",
      "    ess            : 1.9733730686369853\n",
      "    log_marginal   : 979.7365621442171\n",
      "    log_joint      : 1187.2880233687972\n",
      "    val_loss       : -977.4933657438859\n",
      "    val_ess        : 1.9737025810324627\n",
      "    val_log_marginal: 977.5155241593071\n",
      "    val_log_joint  : 1185.3884171195652\n",
      "Train Epoch: 89 [0/101520 (0%)] Loss: -981.126770\n",
      "Train Epoch: 89 [11264/101520 (11%)] Loss: -985.132141\n",
      "Train Epoch: 89 [22528/101520 (22%)] Loss: -979.891602\n",
      "Train Epoch: 89 [33792/101520 (33%)] Loss: -982.262329\n",
      "Train Epoch: 89 [45056/101520 (44%)] Loss: -979.541016\n",
      "Train Epoch: 89 [56320/101520 (55%)] Loss: -981.480713\n",
      "Train Epoch: 89 [67584/101520 (67%)] Loss: -976.639038\n",
      "Train Epoch: 89 [78848/101520 (78%)] Loss: -973.961182\n",
      "Train Epoch: 89 [90112/101520 (89%)] Loss: -978.945679\n",
      "Train Epoch: 89 [101376/101520 (100%)] Loss: -979.177429\n",
      "    epoch          : 89\n",
      "    loss           : -981.1625961227033\n",
      "    ess            : 1.9731778990683244\n",
      "    log_marginal   : 981.1835974305119\n",
      "    log_joint      : 1188.7908377335898\n",
      "    val_loss       : -979.8742835003396\n",
      "    val_ess        : 1.9740303443825764\n",
      "    val_log_marginal: 979.8925117824389\n",
      "    val_log_joint  : 1187.6034094769022\n",
      "Train Epoch: 90 [0/101520 (0%)] Loss: -980.185364\n",
      "Train Epoch: 90 [11264/101520 (11%)] Loss: -979.569885\n",
      "Train Epoch: 90 [22528/101520 (22%)] Loss: -985.169983\n",
      "Train Epoch: 90 [33792/101520 (33%)] Loss: -986.241516\n",
      "Train Epoch: 90 [45056/101520 (44%)] Loss: -979.940247\n",
      "Train Epoch: 90 [56320/101520 (55%)] Loss: -976.317261\n",
      "Train Epoch: 90 [67584/101520 (67%)] Loss: -983.141846\n",
      "Train Epoch: 90 [78848/101520 (78%)] Loss: -978.083008\n",
      "Train Epoch: 90 [90112/101520 (89%)] Loss: -979.234009\n",
      "Train Epoch: 90 [101376/101520 (100%)] Loss: -985.936096\n",
      "    epoch          : 90\n",
      "    loss           : -982.1956072476642\n",
      "    ess            : 1.9732867665027254\n",
      "    log_marginal   : 982.2164070474442\n",
      "    log_joint      : 1189.7767088616913\n",
      "    val_loss       : -982.2071772036345\n",
      "    val_ess        : 1.9731242138406504\n",
      "    val_log_marginal: 982.2281679899796\n",
      "    val_log_joint  : 1189.6437245244565\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch90.pth ...\n",
      "Train Epoch: 91 [0/101520 (0%)] Loss: -985.591370\n",
      "Train Epoch: 91 [11264/101520 (11%)] Loss: -979.617432\n",
      "Train Epoch: 91 [22528/101520 (22%)] Loss: -992.871826\n",
      "Train Epoch: 91 [33792/101520 (33%)] Loss: -978.536987\n",
      "Train Epoch: 91 [45056/101520 (44%)] Loss: -986.750122\n",
      "Train Epoch: 91 [56320/101520 (55%)] Loss: -986.369751\n",
      "Train Epoch: 91 [67584/101520 (67%)] Loss: -988.786621\n",
      "Train Epoch: 91 [78848/101520 (78%)] Loss: -984.864746\n",
      "Train Epoch: 91 [90112/101520 (89%)] Loss: -984.297241\n",
      "Train Epoch: 91 [101376/101520 (100%)] Loss: -978.411560\n",
      "    epoch          : 91\n",
      "    loss           : -985.1138333267902\n",
      "    ess            : 1.9730330873374364\n",
      "    log_marginal   : 985.1344834524183\n",
      "    log_joint      : 1192.552153467533\n",
      "    val_loss       : -985.0440965735394\n",
      "    val_ess        : 1.9751920544582864\n",
      "    val_log_marginal: 985.0622983186141\n",
      "    val_log_joint  : 1192.6667586616848\n",
      "Train Epoch: 92 [0/101520 (0%)] Loss: -987.265503\n",
      "Train Epoch: 92 [11264/101520 (11%)] Loss: -985.746887\n",
      "Train Epoch: 92 [22528/101520 (22%)] Loss: -977.853760\n",
      "Train Epoch: 92 [33792/101520 (33%)] Loss: -983.772583\n",
      "Train Epoch: 92 [45056/101520 (44%)] Loss: -980.362305\n",
      "Train Epoch: 92 [56320/101520 (55%)] Loss: -983.561768\n",
      "Train Epoch: 92 [67584/101520 (67%)] Loss: -977.978516\n",
      "Train Epoch: 92 [78848/101520 (78%)] Loss: -977.378296\n",
      "Train Epoch: 92 [90112/101520 (89%)] Loss: -978.125000\n",
      "Train Epoch: 92 [101376/101520 (100%)] Loss: -992.745117\n",
      "    epoch          : 92\n",
      "    loss           : -981.7260604168303\n",
      "    ess            : 1.9731718499456818\n",
      "    log_marginal   : 981.7471534307278\n",
      "    log_joint      : 1189.3325041957837\n",
      "    val_loss       : -979.794141686481\n",
      "    val_ess        : 1.9726599040238753\n",
      "    val_log_marginal: 979.8152757727581\n",
      "    val_log_joint  : 1187.2760434358017\n",
      "Train Epoch: 93 [0/101520 (0%)] Loss: -987.938599\n",
      "Train Epoch: 93 [11264/101520 (11%)] Loss: -980.300964\n",
      "Train Epoch: 93 [22528/101520 (22%)] Loss: -996.506714\n",
      "Train Epoch: 93 [33792/101520 (33%)] Loss: -983.687256\n",
      "Train Epoch: 93 [45056/101520 (44%)] Loss: -982.590637\n",
      "Train Epoch: 93 [56320/101520 (55%)] Loss: -981.210938\n",
      "Train Epoch: 93 [67584/101520 (67%)] Loss: -987.210938\n",
      "Train Epoch: 93 [78848/101520 (78%)] Loss: -974.380615\n",
      "Train Epoch: 93 [90112/101520 (89%)] Loss: -990.618530\n",
      "Train Epoch: 93 [101376/101520 (100%)] Loss: -971.081482\n",
      "    epoch          : 93\n",
      "    loss           : -983.0390085191583\n",
      "    ess            : 1.9735707781422678\n",
      "    log_marginal   : 983.0590900056925\n",
      "    log_joint      : 1190.701331977269\n",
      "    val_loss       : -983.9319245711616\n",
      "    val_ess        : 1.9730740059976992\n",
      "    val_log_marginal: 983.9536637015965\n",
      "    val_log_joint  : 1191.219827403193\n",
      "Train Epoch: 94 [0/101520 (0%)] Loss: -986.328125\n",
      "Train Epoch: 94 [11264/101520 (11%)] Loss: -988.078247\n",
      "Train Epoch: 94 [22528/101520 (22%)] Loss: -985.917847\n",
      "Train Epoch: 94 [33792/101520 (33%)] Loss: -983.945923\n",
      "Train Epoch: 94 [45056/101520 (44%)] Loss: -987.176636\n",
      "Train Epoch: 94 [56320/101520 (55%)] Loss: -987.521240\n",
      "Train Epoch: 94 [67584/101520 (67%)] Loss: -982.921875\n",
      "Train Epoch: 94 [78848/101520 (78%)] Loss: -978.994019\n",
      "Train Epoch: 94 [90112/101520 (89%)] Loss: -977.301025\n",
      "Train Epoch: 94 [101376/101520 (100%)] Loss: -995.207886\n",
      "    epoch          : 94\n",
      "    loss           : -984.3889236833581\n",
      "    ess            : 1.9730690855476725\n",
      "    log_marginal   : 984.4098575151146\n",
      "    log_joint      : 1191.9744057200062\n",
      "    val_loss       : -983.3693529211956\n",
      "    val_ess        : 1.9705503660699595\n",
      "    val_log_marginal: 983.3913759977921\n",
      "    val_log_joint  : 1191.0013533882473\n",
      "Train Epoch: 95 [0/101520 (0%)] Loss: -989.699951\n",
      "Train Epoch: 95 [11264/101520 (11%)] Loss: -993.808533\n",
      "Train Epoch: 95 [22528/101520 (22%)] Loss: -989.360352\n",
      "Train Epoch: 95 [33792/101520 (33%)] Loss: -982.391113\n",
      "Train Epoch: 95 [45056/101520 (44%)] Loss: -993.360229\n",
      "Train Epoch: 95 [56320/101520 (55%)] Loss: -991.114502\n",
      "Train Epoch: 95 [67584/101520 (67%)] Loss: -991.714844\n",
      "Train Epoch: 95 [78848/101520 (78%)] Loss: -990.634338\n",
      "Train Epoch: 95 [90112/101520 (89%)] Loss: -988.931091\n",
      "Train Epoch: 95 [101376/101520 (100%)] Loss: -1001.590088\n",
      "    epoch          : 95\n",
      "    loss           : -987.4617763500117\n",
      "    ess            : 1.9728067041042463\n",
      "    log_marginal   : 987.482952175428\n",
      "    log_joint      : 1194.8627782467022\n",
      "    val_loss       : -987.3135137143342\n",
      "    val_ess        : 1.9740978064744368\n",
      "    val_log_marginal: 987.3333899456521\n",
      "    val_log_joint  : 1194.5747441830842\n",
      "Train Epoch: 96 [0/101520 (0%)] Loss: -992.858521\n",
      "Train Epoch: 96 [11264/101520 (11%)] Loss: -980.753052\n",
      "Train Epoch: 96 [22528/101520 (22%)] Loss: -993.836792\n",
      "Train Epoch: 96 [33792/101520 (33%)] Loss: -983.415283\n",
      "Train Epoch: 96 [45056/101520 (44%)] Loss: -983.971924\n",
      "Train Epoch: 96 [56320/101520 (55%)] Loss: -984.366943\n",
      "Train Epoch: 96 [67584/101520 (67%)] Loss: -980.175293\n",
      "Train Epoch: 96 [78848/101520 (78%)] Loss: -979.148560\n",
      "Train Epoch: 96 [90112/101520 (89%)] Loss: -976.500488\n",
      "Train Epoch: 96 [101376/101520 (100%)] Loss: -976.386292\n",
      "    epoch          : 96\n",
      "    loss           : -983.9010896155582\n",
      "    ess            : 1.972909929165289\n",
      "    log_marginal   : 983.9220946038788\n",
      "    log_joint      : 1191.512604526539\n",
      "    val_loss       : -983.141261888587\n",
      "    val_ess        : 1.9732983837956968\n",
      "    val_log_marginal: 983.1612309994905\n",
      "    val_log_joint  : 1190.6010954483695\n",
      "Train Epoch: 97 [0/101520 (0%)] Loss: -981.057800\n",
      "Train Epoch: 97 [11264/101520 (11%)] Loss: -994.091431\n",
      "Train Epoch: 97 [22528/101520 (22%)] Loss: -987.871338\n",
      "Train Epoch: 97 [33792/101520 (33%)] Loss: -989.253357\n",
      "Train Epoch: 97 [45056/101520 (44%)] Loss: -982.063110\n",
      "Train Epoch: 97 [56320/101520 (55%)] Loss: -982.017395\n",
      "Train Epoch: 97 [67584/101520 (67%)] Loss: -988.101074\n",
      "Train Epoch: 97 [78848/101520 (78%)] Loss: -992.276062\n",
      "Train Epoch: 97 [90112/101520 (89%)] Loss: -982.350281\n",
      "Train Epoch: 97 [101376/101520 (100%)] Loss: -981.430542\n",
      "    epoch          : 97\n",
      "    loss           : -985.2728449375785\n",
      "    ess            : 1.9723003054384012\n",
      "    log_marginal   : 985.294209082522\n",
      "    log_joint      : 1192.9535194281957\n",
      "    val_loss       : -985.9620122494905\n",
      "    val_ess        : 1.9729506554810896\n",
      "    val_log_marginal: 985.9829499617866\n",
      "    val_log_joint  : 1193.4962264351223\n",
      "Train Epoch: 98 [0/101520 (0%)] Loss: -989.856750\n",
      "Train Epoch: 98 [11264/101520 (11%)] Loss: -992.684753\n",
      "Train Epoch: 98 [22528/101520 (22%)] Loss: -987.859131\n",
      "Train Epoch: 98 [33792/101520 (33%)] Loss: -983.062622\n",
      "Train Epoch: 98 [45056/101520 (44%)] Loss: -980.124023\n",
      "Train Epoch: 98 [56320/101520 (55%)] Loss: -983.446289\n",
      "Train Epoch: 98 [67584/101520 (67%)] Loss: -981.018433\n",
      "Train Epoch: 98 [78848/101520 (78%)] Loss: -984.173950\n",
      "Train Epoch: 98 [90112/101520 (89%)] Loss: -981.656982\n",
      "Train Epoch: 98 [101376/101520 (100%)] Loss: -975.931519\n",
      "    epoch          : 98\n",
      "    loss           : -986.1176644330048\n",
      "    ess            : 1.9721926055361878\n",
      "    log_marginal   : 986.1394021499098\n",
      "    log_joint      : 1193.6814897240106\n",
      "    val_loss       : -985.962158203125\n",
      "    val_ess        : 1.973004548445992\n",
      "    val_log_marginal: 985.98181417714\n",
      "    val_log_joint  : 1193.1047150985055\n",
      "Train Epoch: 99 [0/101520 (0%)] Loss: -997.952637\n",
      "Train Epoch: 99 [11264/101520 (11%)] Loss: -986.762329\n",
      "Train Epoch: 99 [22528/101520 (22%)] Loss: -993.291016\n",
      "Train Epoch: 99 [33792/101520 (33%)] Loss: -992.174011\n",
      "Train Epoch: 99 [45056/101520 (44%)] Loss: -987.113220\n",
      "Train Epoch: 99 [56320/101520 (55%)] Loss: -992.689331\n",
      "Train Epoch: 99 [67584/101520 (67%)] Loss: -987.760132\n",
      "Train Epoch: 99 [78848/101520 (78%)] Loss: -990.927307\n",
      "Train Epoch: 99 [90112/101520 (89%)] Loss: -986.548340\n",
      "Train Epoch: 99 [101376/101520 (100%)] Loss: -995.324524\n",
      "    epoch          : 99\n",
      "    loss           : -989.0558263117344\n",
      "    ess            : 1.9728121637698992\n",
      "    log_marginal   : 989.0768306866363\n",
      "    log_joint      : 1196.51708984375\n",
      "    val_loss       : -989.4245446246604\n",
      "    val_ess        : 1.9741668338360994\n",
      "    val_log_marginal: 989.4444925059443\n",
      "    val_log_joint  : 1196.9273363196332\n",
      "Train Epoch: 100 [0/101520 (0%)] Loss: -994.725342\n",
      "Train Epoch: 100 [11264/101520 (11%)] Loss: -997.464111\n",
      "Train Epoch: 100 [22528/101520 (22%)] Loss: -990.021667\n",
      "Train Epoch: 100 [33792/101520 (33%)] Loss: -977.982910\n",
      "Train Epoch: 100 [45056/101520 (44%)] Loss: -997.002136\n",
      "Train Epoch: 100 [56320/101520 (55%)] Loss: -981.462585\n",
      "Train Epoch: 100 [67584/101520 (67%)] Loss: -980.812378\n",
      "Train Epoch: 100 [78848/101520 (78%)] Loss: -984.288208\n",
      "Train Epoch: 100 [90112/101520 (89%)] Loss: -972.652466\n",
      "Train Epoch: 100 [101376/101520 (100%)] Loss: -997.565735\n",
      "    epoch          : 100\n",
      "    loss           : -985.9698713293028\n",
      "    ess            : 1.9723629010981651\n",
      "    log_marginal   : 985.9913603049426\n",
      "    log_joint      : 1193.6076095811086\n",
      "    val_loss       : -983.4349046790081\n",
      "    val_ess        : 1.9690791057503743\n",
      "    val_log_marginal: 983.4590746008831\n",
      "    val_log_joint  : 1191.3730681046195\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [0/101520 (0%)] Loss: -983.151978\n",
      "Train Epoch: 101 [11264/101520 (11%)] Loss: -985.754395\n",
      "Train Epoch: 101 [22528/101520 (22%)] Loss: -984.758057\n",
      "Train Epoch: 101 [33792/101520 (33%)] Loss: -994.439453\n",
      "Train Epoch: 101 [45056/101520 (44%)] Loss: -986.889587\n",
      "Train Epoch: 101 [56320/101520 (55%)] Loss: -988.416870\n",
      "Train Epoch: 101 [67584/101520 (67%)] Loss: -978.833923\n",
      "Train Epoch: 101 [78848/101520 (78%)] Loss: -983.450806\n",
      "Train Epoch: 101 [90112/101520 (89%)] Loss: -981.298462\n",
      "Train Epoch: 101 [101376/101520 (100%)] Loss: -991.372620\n",
      "    epoch          : 101\n",
      "    loss           : -987.4589524772299\n",
      "    ess            : 1.9722236125313457\n",
      "    log_marginal   : 987.4806233315013\n",
      "    log_joint      : 1195.1182180433418\n",
      "    val_loss       : -988.127977454144\n",
      "    val_ess        : 1.9720682009406711\n",
      "    val_log_marginal: 988.1504357379416\n",
      "    val_log_joint  : 1196.0531642747962\n",
      "Train Epoch: 102 [0/101520 (0%)] Loss: -993.837463\n",
      "Train Epoch: 102 [11264/101520 (11%)] Loss: -987.965698\n",
      "Train Epoch: 102 [22528/101520 (22%)] Loss: -985.307495\n",
      "Train Epoch: 102 [33792/101520 (33%)] Loss: -987.026062\n",
      "Train Epoch: 102 [45056/101520 (44%)] Loss: -987.098938\n",
      "Train Epoch: 102 [56320/101520 (55%)] Loss: -982.439697\n",
      "Train Epoch: 102 [67584/101520 (67%)] Loss: -987.800659\n",
      "Train Epoch: 102 [78848/101520 (78%)] Loss: -984.247375\n",
      "Train Epoch: 102 [90112/101520 (89%)] Loss: -984.152222\n",
      "Train Epoch: 102 [101376/101520 (100%)] Loss: -991.963135\n",
      "    epoch          : 102\n",
      "    loss           : -987.9637880564934\n",
      "    ess            : 1.971910080118994\n",
      "    log_marginal   : 987.9854279331227\n",
      "    log_joint      : 1195.5255869189698\n",
      "    val_loss       : -987.4385880180027\n",
      "    val_ess        : 1.9709115961323613\n",
      "    val_log_marginal: 987.461741571841\n",
      "    val_log_joint  : 1195.1344577955163\n",
      "Train Epoch: 103 [0/101520 (0%)] Loss: -987.364075\n",
      "Train Epoch: 103 [11264/101520 (11%)] Loss: -990.019287\n",
      "Train Epoch: 103 [22528/101520 (22%)] Loss: -992.259399\n",
      "Train Epoch: 103 [33792/101520 (33%)] Loss: -995.044312\n",
      "Train Epoch: 103 [45056/101520 (44%)] Loss: -989.766113\n",
      "Train Epoch: 103 [56320/101520 (55%)] Loss: -990.396606\n",
      "Train Epoch: 103 [67584/101520 (67%)] Loss: -993.331604\n",
      "Train Epoch: 103 [78848/101520 (78%)] Loss: -999.912537\n",
      "Train Epoch: 103 [90112/101520 (89%)] Loss: -990.129272\n",
      "Train Epoch: 103 [101376/101520 (100%)] Loss: -994.470947\n",
      "    epoch          : 103\n",
      "    loss           : -990.6649906024262\n",
      "    ess            : 1.9717538955822662\n",
      "    log_marginal   : 990.6875469265271\n",
      "    log_joint      : 1198.1830760246546\n",
      "    val_loss       : -991.9169709578804\n",
      "    val_ess        : 1.9719871749048647\n",
      "    val_log_marginal: 991.937744140625\n",
      "    val_log_joint  : 1199.3137950067935\n",
      "Train Epoch: 104 [0/101520 (0%)] Loss: -986.969604\n",
      "Train Epoch: 104 [11264/101520 (11%)] Loss: -993.445801\n",
      "Train Epoch: 104 [22528/101520 (22%)] Loss: -996.055664\n",
      "Train Epoch: 104 [33792/101520 (33%)] Loss: -991.004639\n",
      "Train Epoch: 104 [45056/101520 (44%)] Loss: -983.611938\n",
      "Train Epoch: 104 [56320/101520 (55%)] Loss: -989.981873\n",
      "Train Epoch: 104 [67584/101520 (67%)] Loss: -992.069275\n",
      "Train Epoch: 104 [78848/101520 (78%)] Loss: -982.674500\n",
      "Train Epoch: 104 [90112/101520 (89%)] Loss: -986.908936\n",
      "Train Epoch: 104 [101376/101520 (100%)] Loss: -984.331848\n",
      "    epoch          : 104\n",
      "    loss           : -987.5032271955481\n",
      "    ess            : 1.9729625831297295\n",
      "    log_marginal   : 987.5239619729507\n",
      "    log_joint      : 1195.160849413081\n",
      "    val_loss       : -985.6066523012908\n",
      "    val_ess        : 1.9765235911244932\n",
      "    val_log_marginal: 985.6245807150136\n",
      "    val_log_joint  : 1192.9899583899457\n",
      "Train Epoch: 105 [0/101520 (0%)] Loss: -987.339355\n",
      "Train Epoch: 105 [11264/101520 (11%)] Loss: -988.063721\n",
      "Train Epoch: 105 [22528/101520 (22%)] Loss: -988.708984\n",
      "Train Epoch: 105 [33792/101520 (33%)] Loss: -985.946899\n",
      "Train Epoch: 105 [45056/101520 (44%)] Loss: -990.314697\n",
      "Train Epoch: 105 [56320/101520 (55%)] Loss: -995.207764\n",
      "Train Epoch: 105 [67584/101520 (67%)] Loss: -988.064941\n",
      "Train Epoch: 105 [78848/101520 (78%)] Loss: -996.340271\n",
      "Train Epoch: 105 [90112/101520 (89%)] Loss: -985.974121\n",
      "Train Epoch: 105 [101376/101520 (100%)] Loss: -978.502197\n",
      "    epoch          : 105\n",
      "    loss           : -989.0487256840845\n",
      "    ess            : 1.9712849202467568\n",
      "    log_marginal   : 989.0715328964156\n",
      "    log_joint      : 1196.7230414769158\n",
      "    val_loss       : -990.364454186481\n",
      "    val_ess        : 1.9695597783378933\n",
      "    val_log_marginal: 990.3878041143003\n",
      "    val_log_joint  : 1198.0793669327445\n",
      "Train Epoch: 106 [0/101520 (0%)] Loss: -988.480042\n",
      "Train Epoch: 106 [11264/101520 (11%)] Loss: -993.165161\n",
      "Train Epoch: 106 [22528/101520 (22%)] Loss: -983.792236\n",
      "Train Epoch: 106 [33792/101520 (33%)] Loss: -994.357788\n",
      "Train Epoch: 106 [45056/101520 (44%)] Loss: -983.905029\n",
      "Train Epoch: 106 [56320/101520 (55%)] Loss: -985.739380\n",
      "Train Epoch: 106 [67584/101520 (67%)] Loss: -994.204041\n",
      "Train Epoch: 106 [78848/101520 (78%)] Loss: -997.933105\n",
      "Train Epoch: 106 [90112/101520 (89%)] Loss: -999.092773\n",
      "Train Epoch: 106 [101376/101520 (100%)] Loss: -987.513367\n",
      "    epoch          : 106\n",
      "    loss           : -989.7851415279523\n",
      "    ess            : 1.9715152541596686\n",
      "    log_marginal   : 989.8075122929099\n",
      "    log_joint      : 1197.3345481067447\n",
      "    val_loss       : -992.2761601987092\n",
      "    val_ess        : 1.9715210873147715\n",
      "    val_log_marginal: 992.2990032693614\n",
      "    val_log_joint  : 1199.352390455163\n",
      "Train Epoch: 107 [0/101520 (0%)] Loss: -994.262085\n",
      "Train Epoch: 107 [11264/101520 (11%)] Loss: -988.520264\n",
      "Train Epoch: 107 [22528/101520 (22%)] Loss: -995.364624\n",
      "Train Epoch: 107 [33792/101520 (33%)] Loss: -984.631470\n",
      "Train Epoch: 107 [45056/101520 (44%)] Loss: -994.519348\n",
      "Train Epoch: 107 [56320/101520 (55%)] Loss: -996.527954\n",
      "Train Epoch: 107 [67584/101520 (67%)] Loss: -998.701660\n",
      "Train Epoch: 107 [78848/101520 (78%)] Loss: -990.385498\n",
      "Train Epoch: 107 [90112/101520 (89%)] Loss: -989.467834\n",
      "Train Epoch: 107 [101376/101520 (100%)] Loss: -1002.797302\n",
      "    epoch          : 107\n",
      "    loss           : -992.4087926203282\n",
      "    ess            : 1.9712304865295565\n",
      "    log_marginal   : 992.4313056984139\n",
      "    log_joint      : 1200.0025781986103\n",
      "    val_loss       : -993.9283792246943\n",
      "    val_ess        : 1.9712559399397478\n",
      "    val_log_marginal: 993.9516999617866\n",
      "    val_log_joint  : 1201.4716584578805\n",
      "Train Epoch: 108 [0/101520 (0%)] Loss: -998.373230\n",
      "Train Epoch: 108 [11264/101520 (11%)] Loss: -994.978760\n",
      "Train Epoch: 108 [22528/101520 (22%)] Loss: -986.413696\n",
      "Train Epoch: 108 [33792/101520 (33%)] Loss: -997.836792\n",
      "Train Epoch: 108 [45056/101520 (44%)] Loss: -988.143066\n",
      "Train Epoch: 108 [56320/101520 (55%)] Loss: -986.894897\n",
      "Train Epoch: 108 [67584/101520 (67%)] Loss: -984.212463\n",
      "Train Epoch: 108 [78848/101520 (78%)] Loss: -989.214661\n",
      "Train Epoch: 108 [90112/101520 (89%)] Loss: -994.243164\n",
      "Train Epoch: 108 [101376/101520 (100%)] Loss: -981.551636\n",
      "    epoch          : 108\n",
      "    loss           : -989.3473507675094\n",
      "    ess            : 1.9708428796212278\n",
      "    log_marginal   : 989.370173315307\n",
      "    log_joint      : 1197.0000920127984\n",
      "    val_loss       : -986.4520608653193\n",
      "    val_ess        : 1.966491320858831\n",
      "    val_log_marginal: 986.4811958644701\n",
      "    val_log_joint  : 1193.928907311481\n",
      "Train Epoch: 109 [0/101520 (0%)] Loss: -987.589844\n",
      "Train Epoch: 109 [11264/101520 (11%)] Loss: -991.956543\n",
      "Train Epoch: 109 [22528/101520 (22%)] Loss: -987.861694\n",
      "Train Epoch: 109 [33792/101520 (33%)] Loss: -989.727173\n",
      "Train Epoch: 109 [45056/101520 (44%)] Loss: -988.256104\n",
      "Train Epoch: 109 [56320/101520 (55%)] Loss: -999.731689\n",
      "Train Epoch: 109 [67584/101520 (67%)] Loss: -985.963562\n",
      "Train Epoch: 109 [78848/101520 (78%)] Loss: -988.341064\n",
      "Train Epoch: 109 [90112/101520 (89%)] Loss: -987.017517\n",
      "Train Epoch: 109 [101376/101520 (100%)] Loss: -983.798706\n",
      "    epoch          : 109\n",
      "    loss           : -990.893723991049\n",
      "    ess            : 1.9705433066765867\n",
      "    log_marginal   : 990.9166998935106\n",
      "    log_joint      : 1198.511415721184\n",
      "    val_loss       : -992.5260142450747\n",
      "    val_ess        : 1.9724795973819236\n",
      "    val_log_marginal: 992.5474986200747\n",
      "    val_log_joint  : 1199.9413744055707\n",
      "Train Epoch: 110 [0/101520 (0%)] Loss: -986.022583\n",
      "Train Epoch: 110 [11264/101520 (11%)] Loss: -994.166138\n",
      "Train Epoch: 110 [22528/101520 (22%)] Loss: -995.744446\n",
      "Train Epoch: 110 [33792/101520 (33%)] Loss: -996.772583\n",
      "Train Epoch: 110 [45056/101520 (44%)] Loss: -990.421387\n",
      "Train Epoch: 110 [56320/101520 (55%)] Loss: -987.623291\n",
      "Train Epoch: 110 [67584/101520 (67%)] Loss: -990.708679\n",
      "Train Epoch: 110 [78848/101520 (78%)] Loss: -990.131836\n",
      "Train Epoch: 110 [90112/101520 (89%)] Loss: -988.357056\n",
      "Train Epoch: 110 [101376/101520 (100%)] Loss: -977.807068\n",
      "    epoch          : 110\n",
      "    loss           : -991.4329818648908\n",
      "    ess            : 1.9708684814635233\n",
      "    log_marginal   : 991.4559139079184\n",
      "    log_joint      : 1199.1101356391332\n",
      "    val_loss       : -989.1137005349864\n",
      "    val_ess        : 1.9681158324946528\n",
      "    val_log_marginal: 989.136843474015\n",
      "    val_log_joint  : 1196.642280910326\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch110.pth ...\n",
      "Train Epoch: 111 [0/101520 (0%)] Loss: -992.497559\n",
      "Train Epoch: 111 [11264/101520 (11%)] Loss: -993.500000\n",
      "Train Epoch: 111 [22528/101520 (22%)] Loss: -996.249756\n",
      "Train Epoch: 111 [33792/101520 (33%)] Loss: -988.144287\n",
      "Train Epoch: 111 [45056/101520 (44%)] Loss: -994.213501\n",
      "Train Epoch: 111 [56320/101520 (55%)] Loss: -995.492432\n",
      "Train Epoch: 111 [67584/101520 (67%)] Loss: -996.259644\n",
      "Train Epoch: 111 [78848/101520 (78%)] Loss: -995.297668\n",
      "Train Epoch: 111 [90112/101520 (89%)] Loss: -995.924316\n",
      "Train Epoch: 111 [101376/101520 (100%)] Loss: -994.800110\n",
      "    epoch          : 111\n",
      "    loss           : -994.2788539867305\n",
      "    ess            : 1.9713931353247944\n",
      "    log_marginal   : 994.30134252807\n",
      "    log_joint      : 1201.794898565091\n",
      "    val_loss       : -995.8805833899456\n",
      "    val_ess        : 1.968075332434281\n",
      "    val_log_marginal: 995.907762610394\n",
      "    val_log_joint  : 1203.1705746858017\n",
      "Train Epoch: 112 [0/101520 (0%)] Loss: -995.565063\n",
      "Train Epoch: 112 [11264/101520 (11%)] Loss: -997.096924\n",
      "Train Epoch: 112 [22528/101520 (22%)] Loss: -1000.813354\n",
      "Train Epoch: 112 [33792/101520 (33%)] Loss: -994.244141\n",
      "Train Epoch: 112 [45056/101520 (44%)] Loss: -988.075806\n",
      "Train Epoch: 112 [56320/101520 (55%)] Loss: -985.364990\n",
      "Train Epoch: 112 [67584/101520 (67%)] Loss: -991.880920\n",
      "Train Epoch: 112 [78848/101520 (78%)] Loss: -986.488892\n",
      "Train Epoch: 112 [90112/101520 (89%)] Loss: -989.365601\n",
      "Train Epoch: 112 [101376/101520 (100%)] Loss: -987.645203\n",
      "    epoch          : 112\n",
      "    loss           : -991.4961784017745\n",
      "    ess            : 1.9704880121365265\n",
      "    log_marginal   : 991.519327288297\n",
      "    log_joint      : 1199.1073071657113\n",
      "    val_loss       : -990.1204303243886\n",
      "    val_ess        : 1.970245589380679\n",
      "    val_log_marginal: 990.1434538468071\n",
      "    val_log_joint  : 1197.4962052055027\n",
      "Train Epoch: 113 [0/101520 (0%)] Loss: -994.117798\n",
      "Train Epoch: 113 [11264/101520 (11%)] Loss: -982.394592\n",
      "Train Epoch: 113 [22528/101520 (22%)] Loss: -992.433228\n",
      "Train Epoch: 113 [33792/101520 (33%)] Loss: -994.991638\n",
      "Train Epoch: 113 [45056/101520 (44%)] Loss: -988.778442\n",
      "Train Epoch: 113 [56320/101520 (55%)] Loss: -988.404297\n",
      "Train Epoch: 113 [67584/101520 (67%)] Loss: -986.484497\n",
      "Train Epoch: 113 [78848/101520 (78%)] Loss: -990.141846\n",
      "Train Epoch: 113 [90112/101520 (89%)] Loss: -990.044250\n",
      "Train Epoch: 113 [101376/101520 (100%)] Loss: -982.909424\n",
      "    epoch          : 113\n",
      "    loss           : -992.787826768118\n",
      "    ess            : 1.970624693075017\n",
      "    log_marginal   : 992.8116835397692\n",
      "    log_joint      : 1200.4863158566268\n",
      "    val_loss       : -992.8413298233696\n",
      "    val_ess        : 1.9716161023015562\n",
      "    val_log_marginal: 992.8632016389266\n",
      "    val_log_joint  : 1200.1372282608695\n",
      "Train Epoch: 114 [0/101520 (0%)] Loss: -1000.813721\n",
      "Train Epoch: 114 [11264/101520 (11%)] Loss: -997.705444\n",
      "Train Epoch: 114 [22528/101520 (22%)] Loss: -998.827881\n",
      "Train Epoch: 114 [33792/101520 (33%)] Loss: -989.714111\n",
      "Train Epoch: 114 [45056/101520 (44%)] Loss: -993.801086\n",
      "Train Epoch: 114 [56320/101520 (55%)] Loss: -997.405212\n",
      "Train Epoch: 114 [67584/101520 (67%)] Loss: -988.886719\n",
      "Train Epoch: 114 [78848/101520 (78%)] Loss: -992.290283\n",
      "Train Epoch: 114 [90112/101520 (89%)] Loss: -994.638428\n",
      "Train Epoch: 114 [101376/101520 (100%)] Loss: -975.275208\n",
      "    epoch          : 114\n",
      "    loss           : -992.9880975311125\n",
      "    ess            : 1.9697614417004226\n",
      "    log_marginal   : 993.0123291015625\n",
      "    log_joint      : 1200.600339956619\n",
      "    val_loss       : -993.2841239597486\n",
      "    val_ess        : 1.9708617459172788\n",
      "    val_log_marginal: 993.3054146144701\n",
      "    val_log_joint  : 1200.8209387737772\n",
      "Train Epoch: 115 [0/101520 (0%)] Loss: -989.773193\n",
      "Train Epoch: 115 [11264/101520 (11%)] Loss: -996.894348\n",
      "Train Epoch: 115 [22528/101520 (22%)] Loss: -993.978638\n",
      "Train Epoch: 115 [33792/101520 (33%)] Loss: -995.765930\n",
      "Train Epoch: 115 [45056/101520 (44%)] Loss: -997.305298\n",
      "Train Epoch: 115 [56320/101520 (55%)] Loss: -1001.042725\n",
      "Train Epoch: 115 [67584/101520 (67%)] Loss: -995.401733\n",
      "Train Epoch: 115 [78848/101520 (78%)] Loss: -991.132568\n",
      "Train Epoch: 115 [90112/101520 (89%)] Loss: -998.881714\n",
      "Train Epoch: 115 [101376/101520 (100%)] Loss: -986.042542\n",
      "    epoch          : 115\n",
      "    loss           : -995.7158098843829\n",
      "    ess            : 1.9700416955516566\n",
      "    log_marginal   : 995.7391704003415\n",
      "    log_joint      : 1203.258297100738\n",
      "    val_loss       : -997.0380195949389\n",
      "    val_ess        : 1.9699671734934268\n",
      "    val_log_marginal: 997.0597640327785\n",
      "    val_log_joint  : 1204.819967518682\n",
      "Train Epoch: 116 [0/101520 (0%)] Loss: -990.159790\n",
      "Train Epoch: 116 [11264/101520 (11%)] Loss: -995.932922\n",
      "Train Epoch: 116 [22528/101520 (22%)] Loss: -997.959412\n",
      "Train Epoch: 116 [33792/101520 (33%)] Loss: -993.376587\n",
      "Train Epoch: 116 [45056/101520 (44%)] Loss: -991.104736\n",
      "Train Epoch: 116 [56320/101520 (55%)] Loss: -982.877869\n",
      "Train Epoch: 116 [67584/101520 (67%)] Loss: -993.979614\n",
      "Train Epoch: 116 [78848/101520 (78%)] Loss: -992.400085\n",
      "Train Epoch: 116 [90112/101520 (89%)] Loss: -985.567383\n",
      "Train Epoch: 116 [101376/101520 (100%)] Loss: -984.290466\n",
      "    epoch          : 116\n",
      "    loss           : -992.9346071176193\n",
      "    ess            : 1.971052810175335\n",
      "    log_marginal   : 992.9572548411\n",
      "    log_joint      : 1200.571273113615\n",
      "    val_loss       : -990.3789407481318\n",
      "    val_ess        : 1.9721258101255998\n",
      "    val_log_marginal: 990.4022588315217\n",
      "    val_log_joint  : 1197.7822584069293\n",
      "Train Epoch: 117 [0/101520 (0%)] Loss: -998.725098\n",
      "Train Epoch: 117 [11264/101520 (11%)] Loss: -989.138550\n",
      "Train Epoch: 117 [22528/101520 (22%)] Loss: -990.116699\n",
      "Train Epoch: 117 [33792/101520 (33%)] Loss: -995.751465\n",
      "Train Epoch: 117 [45056/101520 (44%)] Loss: -993.332031\n",
      "Train Epoch: 117 [56320/101520 (55%)] Loss: -998.567993\n",
      "Train Epoch: 117 [67584/101520 (67%)] Loss: -993.917236\n",
      "Train Epoch: 117 [78848/101520 (78%)] Loss: -989.425720\n",
      "Train Epoch: 117 [90112/101520 (89%)] Loss: -988.180603\n",
      "Train Epoch: 117 [101376/101520 (100%)] Loss: -1001.264099\n",
      "    epoch          : 117\n",
      "    loss           : -994.6771172758323\n",
      "    ess            : 1.9707619610743308\n",
      "    log_marginal   : 994.7001753763936\n",
      "    log_joint      : 1202.3397621653187\n",
      "    val_loss       : -996.0963134765625\n",
      "    val_ess        : 1.970178272413171\n",
      "    val_log_marginal: 996.1208788001019\n",
      "    val_log_joint  : 1203.8158861243207\n",
      "Train Epoch: 118 [0/101520 (0%)] Loss: -988.473572\n",
      "Train Epoch: 118 [11264/101520 (11%)] Loss: -995.819580\n",
      "Train Epoch: 118 [22528/101520 (22%)] Loss: -992.351746\n",
      "Train Epoch: 118 [33792/101520 (33%)] Loss: -989.842834\n",
      "Train Epoch: 118 [45056/101520 (44%)] Loss: -995.704956\n",
      "Train Epoch: 118 [56320/101520 (55%)] Loss: -999.621826\n",
      "Train Epoch: 118 [67584/101520 (67%)] Loss: -993.314453\n",
      "Train Epoch: 118 [78848/101520 (78%)] Loss: -1003.073608\n",
      "Train Epoch: 118 [90112/101520 (89%)] Loss: -997.569641\n",
      "Train Epoch: 118 [101376/101520 (100%)] Loss: -990.542419\n",
      "    epoch          : 118\n",
      "    loss           : -994.8874376766646\n",
      "    ess            : 1.9695031247546326\n",
      "    log_marginal   : 994.9110279179099\n",
      "    log_joint      : 1202.5173253965138\n",
      "    val_loss       : -994.6410787831182\n",
      "    val_ess        : 1.9685474219529524\n",
      "    val_log_marginal: 994.6669311523438\n",
      "    val_log_joint  : 1202.5678392493207\n",
      "Train Epoch: 119 [0/101520 (0%)] Loss: -997.387085\n",
      "Train Epoch: 119 [11264/101520 (11%)] Loss: -999.653442\n",
      "Train Epoch: 119 [22528/101520 (22%)] Loss: -998.865051\n",
      "Train Epoch: 119 [33792/101520 (33%)] Loss: -994.435059\n",
      "Train Epoch: 119 [45056/101520 (44%)] Loss: -993.247498\n",
      "Train Epoch: 119 [56320/101520 (55%)] Loss: -1002.476318\n",
      "Train Epoch: 119 [67584/101520 (67%)] Loss: -1003.106323\n",
      "Train Epoch: 119 [78848/101520 (78%)] Loss: -1001.273254\n",
      "Train Epoch: 119 [90112/101520 (89%)] Loss: -1003.540894\n",
      "Train Epoch: 119 [101376/101520 (100%)] Loss: -996.235596\n",
      "    epoch          : 119\n",
      "    loss           : -997.527817002493\n",
      "    ess            : 1.9700965989175154\n",
      "    log_marginal   : 997.5508097739674\n",
      "    log_joint      : 1205.04176399576\n",
      "    val_loss       : -997.3669619352921\n",
      "    val_ess        : 1.969914400059244\n",
      "    val_log_marginal: 997.3922357973845\n",
      "    val_log_joint  : 1205.0187563688858\n",
      "Train Epoch: 120 [0/101520 (0%)] Loss: -998.307068\n",
      "Train Epoch: 120 [11264/101520 (11%)] Loss: -998.972534\n",
      "Train Epoch: 120 [22528/101520 (22%)] Loss: -993.867615\n",
      "Train Epoch: 120 [33792/101520 (33%)] Loss: -1002.364990\n",
      "Train Epoch: 120 [45056/101520 (44%)] Loss: -988.524536\n",
      "Train Epoch: 120 [56320/101520 (55%)] Loss: -995.571533\n",
      "Train Epoch: 120 [67584/101520 (67%)] Loss: -994.798523\n",
      "Train Epoch: 120 [78848/101520 (78%)] Loss: -986.779846\n",
      "Train Epoch: 120 [90112/101520 (89%)] Loss: -1006.253784\n",
      "Train Epoch: 120 [101376/101520 (100%)] Loss: -995.455505\n",
      "    epoch          : 120\n",
      "    loss           : -994.8686593980646\n",
      "    ess            : 1.9698620315772206\n",
      "    log_marginal   : 994.8924131153816\n",
      "    log_joint      : 1202.4824028590217\n",
      "    val_loss       : -992.8501135784646\n",
      "    val_ess        : 1.968378227689992\n",
      "    val_log_marginal: 992.8756820015285\n",
      "    val_log_joint  : 1200.1019711701767\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch120.pth ...\n",
      "Train Epoch: 121 [0/101520 (0%)] Loss: -995.614380\n",
      "Train Epoch: 121 [11264/101520 (11%)] Loss: -993.766113\n",
      "Train Epoch: 121 [22528/101520 (22%)] Loss: -995.397644\n",
      "Train Epoch: 121 [33792/101520 (33%)] Loss: -986.169189\n",
      "Train Epoch: 121 [45056/101520 (44%)] Loss: -1001.374146\n",
      "Train Epoch: 121 [56320/101520 (55%)] Loss: -996.702026\n",
      "Train Epoch: 121 [67584/101520 (67%)] Loss: -999.493225\n",
      "Train Epoch: 121 [78848/101520 (78%)] Loss: -1000.337891\n",
      "Train Epoch: 121 [90112/101520 (89%)] Loss: -996.543091\n",
      "Train Epoch: 121 [101376/101520 (100%)] Loss: -998.990662\n",
      "    epoch          : 121\n",
      "    loss           : -996.3613900802842\n",
      "    ess            : 1.9696577619667628\n",
      "    log_marginal   : 996.3852950052999\n",
      "    log_joint      : 1204.0431957149026\n",
      "    val_loss       : -997.3263974397079\n",
      "    val_ess        : 1.9690233676329902\n",
      "    val_log_marginal: 997.3497155230979\n",
      "    val_log_joint  : 1204.7376815132473\n",
      "Train Epoch: 122 [0/101520 (0%)] Loss: -1009.608032\n",
      "Train Epoch: 122 [11264/101520 (11%)] Loss: -996.281982\n",
      "Train Epoch: 122 [22528/101520 (22%)] Loss: -998.830566\n",
      "Train Epoch: 122 [33792/101520 (33%)] Loss: -993.383057\n",
      "Train Epoch: 122 [45056/101520 (44%)] Loss: -995.744629\n",
      "Train Epoch: 122 [56320/101520 (55%)] Loss: -999.952759\n",
      "Train Epoch: 122 [67584/101520 (67%)] Loss: -1001.623413\n",
      "Train Epoch: 122 [78848/101520 (78%)] Loss: -995.344116\n",
      "Train Epoch: 122 [90112/101520 (89%)] Loss: -989.687073\n",
      "Train Epoch: 122 [101376/101520 (100%)] Loss: -992.546997\n",
      "    epoch          : 122\n",
      "    loss           : -996.422044303549\n",
      "    ess            : 1.969303654665923\n",
      "    log_marginal   : 996.4461077972873\n",
      "    log_joint      : 1204.0684133558418\n",
      "    val_loss       : -995.9418892238451\n",
      "    val_ess        : 1.9714771250019902\n",
      "    val_log_marginal: 995.9639414911685\n",
      "    val_log_joint  : 1203.7343856148098\n",
      "Train Epoch: 123 [0/101520 (0%)] Loss: -997.792847\n",
      "Train Epoch: 123 [11264/101520 (11%)] Loss: -986.807983\n",
      "Train Epoch: 123 [22528/101520 (22%)] Loss: -997.003235\n",
      "Train Epoch: 123 [33792/101520 (33%)] Loss: -1002.823730\n",
      "Train Epoch: 123 [45056/101520 (44%)] Loss: -1003.805176\n",
      "Train Epoch: 123 [56320/101520 (55%)] Loss: -997.685303\n",
      "Train Epoch: 123 [67584/101520 (67%)] Loss: -1004.823181\n",
      "Train Epoch: 123 [78848/101520 (78%)] Loss: -996.086792\n",
      "Train Epoch: 123 [90112/101520 (89%)] Loss: -999.196777\n",
      "Train Epoch: 123 [101376/101520 (100%)] Loss: -1000.651917\n",
      "    epoch          : 123\n",
      "    loss           : -998.8895266738969\n",
      "    ess            : 1.9695780385079695\n",
      "    log_marginal   : 998.9133061547974\n",
      "    log_joint      : 1206.4409817642902\n",
      "    val_loss       : -1001.375904912534\n",
      "    val_ess        : 1.9725064091060474\n",
      "    val_log_marginal: 1001.3956776494565\n",
      "    val_log_joint  : 1208.6554804262908\n",
      "Train Epoch: 124 [0/101520 (0%)] Loss: -1000.346924\n",
      "Train Epoch: 124 [11264/101520 (11%)] Loss: -1003.010010\n",
      "Train Epoch: 124 [22528/101520 (22%)] Loss: -997.967590\n",
      "Train Epoch: 124 [33792/101520 (33%)] Loss: -995.144531\n",
      "Train Epoch: 124 [45056/101520 (44%)] Loss: -1000.245850\n",
      "Train Epoch: 124 [56320/101520 (55%)] Loss: -996.366943\n",
      "Train Epoch: 124 [67584/101520 (67%)] Loss: -990.844360\n",
      "Train Epoch: 124 [78848/101520 (78%)] Loss: -994.563477\n",
      "Train Epoch: 124 [90112/101520 (89%)] Loss: -991.604370\n",
      "Train Epoch: 124 [101376/101520 (100%)] Loss: -996.180786\n",
      "    epoch          : 124\n",
      "    loss           : -996.4198154468633\n",
      "    ess            : 1.9697280553118068\n",
      "    log_marginal   : 996.4437074900871\n",
      "    log_joint      : 1204.1191737496074\n",
      "    val_loss       : -994.826731806216\n",
      "    val_ess        : 1.9707549188448035\n",
      "    val_log_marginal: 994.8488689920176\n",
      "    val_log_joint  : 1202.4335088315217\n",
      "Train Epoch: 125 [0/101520 (0%)] Loss: -992.038696\n",
      "Train Epoch: 125 [11264/101520 (11%)] Loss: -995.239990\n",
      "Train Epoch: 125 [22528/101520 (22%)] Loss: -1003.117676\n",
      "Train Epoch: 125 [33792/101520 (33%)] Loss: -997.219849\n",
      "Train Epoch: 125 [45056/101520 (44%)] Loss: -1004.809326\n",
      "Train Epoch: 125 [56320/101520 (55%)] Loss: -999.062500\n",
      "Train Epoch: 125 [67584/101520 (67%)] Loss: -998.268555\n",
      "Train Epoch: 125 [78848/101520 (78%)] Loss: -999.684204\n",
      "Train Epoch: 125 [90112/101520 (89%)] Loss: -997.264282\n",
      "Train Epoch: 125 [101376/101520 (100%)] Loss: -1005.003723\n",
      "    epoch          : 125\n",
      "    loss           : -998.1315718607686\n",
      "    ess            : 1.9693273563480855\n",
      "    log_marginal   : 998.155781348147\n",
      "    log_joint      : 1205.7249007488615\n",
      "    val_loss       : -997.4081341287364\n",
      "    val_ess        : 1.9699537235757578\n",
      "    val_log_marginal: 997.431080693784\n",
      "    val_log_joint  : 1204.7433445142663\n",
      "Train Epoch: 126 [0/101520 (0%)] Loss: -1007.023926\n",
      "Train Epoch: 126 [11264/101520 (11%)] Loss: -1004.963196\n",
      "Train Epoch: 126 [22528/101520 (22%)] Loss: -1000.190735\n",
      "Train Epoch: 126 [33792/101520 (33%)] Loss: -1005.898560\n",
      "Train Epoch: 126 [45056/101520 (44%)] Loss: -997.238342\n",
      "Train Epoch: 126 [56320/101520 (55%)] Loss: -990.277954\n",
      "Train Epoch: 126 [67584/101520 (67%)] Loss: -992.065796\n",
      "Train Epoch: 126 [78848/101520 (78%)] Loss: -993.065796\n",
      "Train Epoch: 126 [90112/101520 (89%)] Loss: -1000.805908\n",
      "Train Epoch: 126 [101376/101520 (100%)] Loss: -996.718445\n",
      "    epoch          : 126\n",
      "    loss           : -997.6749966875393\n",
      "    ess            : 1.9695575440948332\n",
      "    log_marginal   : 997.698768807416\n",
      "    log_joint      : 1205.3667458385678\n",
      "    val_loss       : -997.7259919539741\n",
      "    val_ess        : 1.9692543589550515\n",
      "    val_log_marginal: 997.7485192340354\n",
      "    val_log_joint  : 1205.4397184952445\n",
      "Train Epoch: 127 [0/101520 (0%)] Loss: -996.532349\n",
      "Train Epoch: 127 [11264/101520 (11%)] Loss: -993.484985\n",
      "Train Epoch: 127 [22528/101520 (22%)] Loss: -999.351440\n",
      "Train Epoch: 127 [33792/101520 (33%)] Loss: -997.907837\n",
      "Train Epoch: 127 [45056/101520 (44%)] Loss: -996.642578\n",
      "Train Epoch: 127 [56320/101520 (55%)] Loss: -1003.824463\n",
      "Train Epoch: 127 [67584/101520 (67%)] Loss: -1005.374573\n",
      "Train Epoch: 127 [78848/101520 (78%)] Loss: -997.981323\n",
      "Train Epoch: 127 [90112/101520 (89%)] Loss: -999.559570\n",
      "Train Epoch: 127 [101376/101520 (100%)] Loss: -1007.304138\n",
      "    epoch          : 127\n",
      "    loss           : -1000.216193577752\n",
      "    ess            : 1.9686324015334622\n",
      "    log_marginal   : 1000.2409947074239\n",
      "    log_joint      : 1207.8450694635285\n",
      "    val_loss       : -999.6449744183084\n",
      "    val_ess        : 1.9689768760100654\n",
      "    val_log_marginal: 999.6726445737092\n",
      "    val_log_joint  : 1206.9933392068615\n",
      "Train Epoch: 128 [0/101520 (0%)] Loss: -995.525635\n",
      "Train Epoch: 128 [11264/101520 (11%)] Loss: -1002.491699\n",
      "Train Epoch: 128 [22528/101520 (22%)] Loss: -1001.780884\n",
      "Train Epoch: 128 [33792/101520 (33%)] Loss: -997.425598\n",
      "Train Epoch: 128 [45056/101520 (44%)] Loss: -996.471191\n",
      "Train Epoch: 128 [56320/101520 (55%)] Loss: -994.964111\n",
      "Train Epoch: 128 [67584/101520 (67%)] Loss: -999.341125\n",
      "Train Epoch: 128 [78848/101520 (78%)] Loss: -997.414551\n",
      "Train Epoch: 128 [90112/101520 (89%)] Loss: -994.945129\n",
      "Train Epoch: 128 [101376/101520 (100%)] Loss: -993.237732\n",
      "    epoch          : 128\n",
      "    loss           : -998.203049242796\n",
      "    ess            : 1.969891524195072\n",
      "    log_marginal   : 998.226632736436\n",
      "    log_joint      : 1205.8541860915907\n",
      "    val_loss       : -998.1448974609375\n",
      "    val_ess        : 1.9692952114602793\n",
      "    val_log_marginal: 998.1671434485394\n",
      "    val_log_joint  : 1205.6363684612772\n",
      "Train Epoch: 129 [0/101520 (0%)] Loss: -996.571899\n",
      "Train Epoch: 129 [11264/101520 (11%)] Loss: -1001.904785\n",
      "Train Epoch: 129 [22528/101520 (22%)] Loss: -998.350647\n",
      "Train Epoch: 129 [33792/101520 (33%)] Loss: -998.936646\n",
      "Train Epoch: 129 [45056/101520 (44%)] Loss: -1000.412415\n",
      "Train Epoch: 129 [56320/101520 (55%)] Loss: -997.804077\n",
      "Train Epoch: 129 [67584/101520 (67%)] Loss: -995.724121\n",
      "Train Epoch: 129 [78848/101520 (78%)] Loss: -996.400757\n",
      "Train Epoch: 129 [90112/101520 (89%)] Loss: -1001.062744\n",
      "Train Epoch: 129 [101376/101520 (100%)] Loss: -981.754150\n",
      "    epoch          : 129\n",
      "    loss           : -999.9235247894748\n",
      "    ess            : 1.9686544300922797\n",
      "    log_marginal   : 999.947957867953\n",
      "    log_joint      : 1207.5594286127905\n",
      "    val_loss       : -999.3759261421535\n",
      "    val_ess        : 1.9716348699901416\n",
      "    val_log_marginal: 999.3979571798574\n",
      "    val_log_joint  : 1207.2009967306385\n",
      "Train Epoch: 130 [0/101520 (0%)] Loss: -1007.231934\n",
      "Train Epoch: 130 [11264/101520 (11%)] Loss: -1004.282349\n",
      "Train Epoch: 130 [22528/101520 (22%)] Loss: -1002.714661\n",
      "Train Epoch: 130 [33792/101520 (33%)] Loss: -999.257446\n",
      "Train Epoch: 130 [45056/101520 (44%)] Loss: -1001.556641\n",
      "Train Epoch: 130 [56320/101520 (55%)] Loss: -997.787537\n",
      "Train Epoch: 130 [67584/101520 (67%)] Loss: -995.744812\n",
      "Train Epoch: 130 [78848/101520 (78%)] Loss: -998.030273\n",
      "Train Epoch: 130 [90112/101520 (89%)] Loss: -1002.010193\n",
      "Train Epoch: 130 [101376/101520 (100%)] Loss: -994.179382\n",
      "    epoch          : 130\n",
      "    loss           : -999.3289825592808\n",
      "    ess            : 1.9693644088716362\n",
      "    log_marginal   : 999.3536554844535\n",
      "    log_joint      : 1207.001793022731\n",
      "    val_loss       : -999.47179910411\n",
      "    val_ess        : 1.970285794009333\n",
      "    val_log_marginal: 999.4969907014266\n",
      "    val_log_joint  : 1207.0769361413043\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch130.pth ...\n",
      "Train Epoch: 131 [0/101520 (0%)] Loss: -1004.734253\n",
      "Train Epoch: 131 [11264/101520 (11%)] Loss: -1001.325256\n",
      "Train Epoch: 131 [22528/101520 (22%)] Loss: -1007.857666\n",
      "Train Epoch: 131 [33792/101520 (33%)] Loss: -999.727051\n",
      "Train Epoch: 131 [45056/101520 (44%)] Loss: -997.841675\n",
      "Train Epoch: 131 [56320/101520 (55%)] Loss: -994.650024\n",
      "Train Epoch: 131 [67584/101520 (67%)] Loss: -1011.404968\n",
      "Train Epoch: 131 [78848/101520 (78%)] Loss: -1005.552490\n",
      "Train Epoch: 131 [90112/101520 (89%)] Loss: -1002.217896\n",
      "Train Epoch: 131 [101376/101520 (100%)] Loss: -1004.005310\n",
      "    epoch          : 131\n",
      "    loss           : -1001.8099656608237\n",
      "    ess            : 1.9687058817801164\n",
      "    log_marginal   : 1001.8352102921835\n",
      "    log_joint      : 1209.3194077074827\n",
      "    val_loss       : -1002.5249713400136\n",
      "    val_ess        : 1.970876906229102\n",
      "    val_log_marginal: 1002.5470872961956\n",
      "    val_log_joint  : 1209.81201171875\n",
      "Train Epoch: 132 [0/101520 (0%)] Loss: -1002.588989\n",
      "Train Epoch: 132 [11264/101520 (11%)] Loss: -1001.881958\n",
      "Train Epoch: 132 [22528/101520 (22%)] Loss: -1002.039551\n",
      "Train Epoch: 132 [33792/101520 (33%)] Loss: -997.295776\n",
      "Train Epoch: 132 [45056/101520 (44%)] Loss: -998.933472\n",
      "Train Epoch: 132 [56320/101520 (55%)] Loss: -1005.218384\n",
      "Train Epoch: 132 [67584/101520 (67%)] Loss: -994.058838\n",
      "Train Epoch: 132 [78848/101520 (78%)] Loss: -1001.034302\n",
      "Train Epoch: 132 [90112/101520 (89%)] Loss: -1001.073120\n",
      "Train Epoch: 132 [101376/101520 (100%)] Loss: -989.068787\n",
      "    epoch          : 132\n",
      "    loss           : -999.763500731195\n",
      "    ess            : 1.9687599626617815\n",
      "    log_marginal   : 999.788463803392\n",
      "    log_joint      : 1207.4205377473304\n",
      "    val_loss       : -999.9478441321331\n",
      "    val_ess        : 1.9709551852682363\n",
      "    val_log_marginal: 999.96911886464\n",
      "    val_log_joint  : 1207.1114873471467\n",
      "Train Epoch: 133 [0/101520 (0%)] Loss: -1000.424133\n",
      "Train Epoch: 133 [11264/101520 (11%)] Loss: -999.565552\n",
      "Train Epoch: 133 [22528/101520 (22%)] Loss: -998.941650\n",
      "Train Epoch: 133 [33792/101520 (33%)] Loss: -1003.564819\n",
      "Train Epoch: 133 [45056/101520 (44%)] Loss: -1005.695923\n",
      "Train Epoch: 133 [56320/101520 (55%)] Loss: -1013.105713\n",
      "Train Epoch: 133 [67584/101520 (67%)] Loss: -1000.335205\n",
      "Train Epoch: 133 [78848/101520 (78%)] Loss: -993.543457\n",
      "Train Epoch: 133 [90112/101520 (89%)] Loss: -1003.968018\n",
      "Train Epoch: 133 [101376/101520 (100%)] Loss: -995.467651\n",
      "    epoch          : 133\n",
      "    loss           : -1001.5881362991717\n",
      "    ess            : 1.9680936060958172\n",
      "    log_marginal   : 1001.6133619145533\n",
      "    log_joint      : 1209.2268735032585\n",
      "    val_loss       : -1003.2306067425271\n",
      "    val_ess        : 1.9673335811366206\n",
      "    val_log_marginal: 1003.2586510699729\n",
      "    val_log_joint  : 1210.8058020550272\n",
      "Train Epoch: 134 [0/101520 (0%)] Loss: -999.506531\n",
      "Train Epoch: 134 [11264/101520 (11%)] Loss: -998.070190\n",
      "Train Epoch: 134 [22528/101520 (22%)] Loss: -1000.128418\n",
      "Train Epoch: 134 [33792/101520 (33%)] Loss: -994.104004\n",
      "Train Epoch: 134 [45056/101520 (44%)] Loss: -998.541016\n",
      "Train Epoch: 134 [56320/101520 (55%)] Loss: -1005.911987\n",
      "Train Epoch: 134 [67584/101520 (67%)] Loss: -1002.447083\n",
      "Train Epoch: 134 [78848/101520 (78%)] Loss: -1001.930603\n",
      "Train Epoch: 134 [90112/101520 (89%)] Loss: -1000.927490\n",
      "Train Epoch: 134 [101376/101520 (100%)] Loss: -979.599731\n",
      "    epoch          : 134\n",
      "    loss           : -1000.7267649281564\n",
      "    ess            : 1.9681993051988995\n",
      "    log_marginal   : 1000.752085930139\n",
      "    log_joint      : 1208.3875861239792\n",
      "    val_loss       : -999.3526292883831\n",
      "    val_ess        : 1.9694906680480293\n",
      "    val_log_marginal: 999.3762552012568\n",
      "    val_log_joint  : 1207.0762886379075\n",
      "Train Epoch: 135 [0/101520 (0%)] Loss: -1002.458313\n",
      "Train Epoch: 135 [11264/101520 (11%)] Loss: -1003.231995\n",
      "Train Epoch: 135 [22528/101520 (22%)] Loss: -1009.880737\n",
      "Train Epoch: 135 [33792/101520 (33%)] Loss: -1006.701660\n",
      "Train Epoch: 135 [45056/101520 (44%)] Loss: -1001.270203\n",
      "Train Epoch: 135 [56320/101520 (55%)] Loss: -997.456909\n",
      "Train Epoch: 135 [67584/101520 (67%)] Loss: -1009.005493\n",
      "Train Epoch: 135 [78848/101520 (78%)] Loss: -1001.886353\n",
      "Train Epoch: 135 [90112/101520 (89%)] Loss: -1017.740601\n",
      "Train Epoch: 135 [101376/101520 (100%)] Loss: -1005.595703\n",
      "    epoch          : 135\n",
      "    loss           : -1003.1646182573021\n",
      "    ess            : 1.9685367597407433\n",
      "    log_marginal   : 1003.1896120004318\n",
      "    log_joint      : 1210.7525407800722\n",
      "    val_loss       : -1005.6439845872962\n",
      "    val_ess        : 1.9673176744709844\n",
      "    val_log_marginal: 1005.6678042204484\n",
      "    val_log_joint  : 1213.2130923063858\n",
      "Train Epoch: 136 [0/101520 (0%)] Loss: -1004.686768\n",
      "Train Epoch: 136 [11264/101520 (11%)] Loss: -1008.543335\n",
      "Train Epoch: 136 [22528/101520 (22%)] Loss: -1005.513855\n",
      "Train Epoch: 136 [33792/101520 (33%)] Loss: -1003.797180\n",
      "Train Epoch: 136 [45056/101520 (44%)] Loss: -995.627441\n",
      "Train Epoch: 136 [56320/101520 (55%)] Loss: -993.596008\n",
      "Train Epoch: 136 [67584/101520 (67%)] Loss: -989.344482\n",
      "Train Epoch: 136 [78848/101520 (78%)] Loss: -989.503052\n",
      "Train Epoch: 136 [90112/101520 (89%)] Loss: -1004.576904\n",
      "Train Epoch: 136 [101376/101520 (100%)] Loss: -986.194580\n",
      "    epoch          : 136\n",
      "    loss           : -1001.3467398025283\n",
      "    ess            : 1.9683733363846438\n",
      "    log_marginal   : 1001.3718384402481\n",
      "    log_joint      : 1209.0518443045305\n",
      "    val_loss       : -999.8218065344769\n",
      "    val_ess        : 1.9702199438343877\n",
      "    val_log_marginal: 999.8449600883152\n",
      "    val_log_joint  : 1207.3078029466712\n",
      "Train Epoch: 137 [0/101520 (0%)] Loss: -999.741150\n",
      "Train Epoch: 137 [11264/101520 (11%)] Loss: -998.453613\n",
      "Train Epoch: 137 [22528/101520 (22%)] Loss: -1002.720581\n",
      "Train Epoch: 137 [33792/101520 (33%)] Loss: -1013.268921\n",
      "Train Epoch: 137 [45056/101520 (44%)] Loss: -1003.267456\n",
      "Train Epoch: 137 [56320/101520 (55%)] Loss: -1009.809082\n",
      "Train Epoch: 137 [67584/101520 (67%)] Loss: -1003.072998\n",
      "Train Epoch: 137 [78848/101520 (78%)] Loss: -1004.103333\n",
      "Train Epoch: 137 [90112/101520 (89%)] Loss: -1008.316223\n",
      "Train Epoch: 137 [101376/101520 (100%)] Loss: -1015.551208\n",
      "    epoch          : 137\n",
      "    loss           : -1003.4220194600935\n",
      "    ess            : 1.9681461186864269\n",
      "    log_marginal   : 1003.4473668390782\n",
      "    log_joint      : 1211.0599923445352\n",
      "    val_loss       : -1004.0249076511549\n",
      "    val_ess        : 1.967894383098768\n",
      "    val_log_marginal: 1004.0498922596807\n",
      "    val_log_joint  : 1211.7230171535325\n",
      "Train Epoch: 138 [0/101520 (0%)] Loss: -1005.908691\n",
      "Train Epoch: 138 [11264/101520 (11%)] Loss: -1011.266235\n",
      "Train Epoch: 138 [22528/101520 (22%)] Loss: -1009.951904\n",
      "Train Epoch: 138 [33792/101520 (33%)] Loss: -1002.795044\n",
      "Train Epoch: 138 [45056/101520 (44%)] Loss: -999.179138\n",
      "Train Epoch: 138 [56320/101520 (55%)] Loss: -1000.677490\n",
      "Train Epoch: 138 [67584/101520 (67%)] Loss: -997.154236\n",
      "Train Epoch: 138 [78848/101520 (78%)] Loss: -1005.321167\n",
      "Train Epoch: 138 [90112/101520 (89%)] Loss: -1001.225220\n",
      "Train Epoch: 138 [101376/101520 (100%)] Loss: -995.191223\n",
      "    epoch          : 138\n",
      "    loss           : -1002.3112780700377\n",
      "    ess            : 1.9678072354302334\n",
      "    log_marginal   : 1002.3371250785176\n",
      "    log_joint      : 1209.969689143962\n",
      "    val_loss       : -1001.855980914572\n",
      "    val_ess        : 1.9690594724986865\n",
      "    val_log_marginal: 1001.8803498641304\n",
      "    val_log_joint  : 1209.1609523607337\n",
      "Train Epoch: 139 [0/101520 (0%)] Loss: -1006.627380\n",
      "Train Epoch: 139 [11264/101520 (11%)] Loss: -1002.158264\n",
      "Train Epoch: 139 [22528/101520 (22%)] Loss: -1000.525635\n",
      "Train Epoch: 139 [33792/101520 (33%)] Loss: -1007.316650\n",
      "Train Epoch: 139 [45056/101520 (44%)] Loss: -1010.196289\n",
      "Train Epoch: 139 [56320/101520 (55%)] Loss: -1007.146301\n",
      "Train Epoch: 139 [67584/101520 (67%)] Loss: -1004.151367\n",
      "Train Epoch: 139 [78848/101520 (78%)] Loss: -1009.957642\n",
      "Train Epoch: 139 [90112/101520 (89%)] Loss: -1007.874146\n",
      "Train Epoch: 139 [101376/101520 (100%)] Loss: -1006.370239\n",
      "    epoch          : 139\n",
      "    loss           : -1004.4927263882892\n",
      "    ess            : 1.9678729485027755\n",
      "    log_marginal   : 1004.5184402849206\n",
      "    log_joint      : 1212.1489171933888\n",
      "    val_loss       : -1007.5132207455842\n",
      "    val_ess        : 1.968839572823566\n",
      "    val_log_marginal: 1007.5374676248301\n",
      "    val_log_joint  : 1214.784423828125\n",
      "Train Epoch: 140 [0/101520 (0%)] Loss: -1010.683594\n",
      "Train Epoch: 140 [11264/101520 (11%)] Loss: -1001.040283\n",
      "Train Epoch: 140 [22528/101520 (22%)] Loss: -1002.500366\n",
      "Train Epoch: 140 [33792/101520 (33%)] Loss: -1002.314575\n",
      "Train Epoch: 140 [45056/101520 (44%)] Loss: -1004.055115\n",
      "Train Epoch: 140 [56320/101520 (55%)] Loss: -1000.597656\n",
      "Train Epoch: 140 [67584/101520 (67%)] Loss: -1004.868469\n",
      "Train Epoch: 140 [78848/101520 (78%)] Loss: -1005.511475\n",
      "Train Epoch: 140 [90112/101520 (89%)] Loss: -1005.296509\n",
      "Train Epoch: 140 [101376/101520 (100%)] Loss: -1002.463013\n",
      "    epoch          : 140\n",
      "    loss           : -1002.931561187284\n",
      "    ess            : 1.968288141878406\n",
      "    log_marginal   : 1002.9576118507576\n",
      "    log_joint      : 1210.5977789337312\n",
      "    val_loss       : -1004.1304480511209\n",
      "    val_ess        : 1.9668496795322583\n",
      "    val_log_marginal: 1004.1566958220109\n",
      "    val_log_joint  : 1211.7292692764945\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch140.pth ...\n",
      "Train Epoch: 141 [0/101520 (0%)] Loss: -1004.252319\n",
      "Train Epoch: 141 [11264/101520 (11%)] Loss: -1004.588745\n",
      "Train Epoch: 141 [22528/101520 (22%)] Loss: -1004.217041\n",
      "Train Epoch: 141 [33792/101520 (33%)] Loss: -1010.103882\n",
      "Train Epoch: 141 [45056/101520 (44%)] Loss: -1005.973328\n",
      "Train Epoch: 141 [56320/101520 (55%)] Loss: -1006.939636\n",
      "Train Epoch: 141 [67584/101520 (67%)] Loss: -1001.011597\n",
      "Train Epoch: 141 [78848/101520 (78%)] Loss: -1006.637512\n",
      "Train Epoch: 141 [90112/101520 (89%)] Loss: -1004.711121\n",
      "Train Epoch: 141 [101376/101520 (100%)] Loss: -999.170776\n",
      "    epoch          : 141\n",
      "    loss           : -1004.9451321549152\n",
      "    ess            : 1.9677880954502815\n",
      "    log_marginal   : 1004.9711450931414\n",
      "    log_joint      : 1212.5810681827104\n",
      "    val_loss       : -1007.3199409816576\n",
      "    val_ess        : 1.9630548539369002\n",
      "    val_log_marginal: 1007.3533219047215\n",
      "    val_log_joint  : 1214.9576999830163\n",
      "Train Epoch: 142 [0/101520 (0%)] Loss: -1006.119141\n",
      "Train Epoch: 142 [11264/101520 (11%)] Loss: -1000.367676\n",
      "Train Epoch: 142 [22528/101520 (22%)] Loss: -1003.821167\n",
      "Train Epoch: 142 [33792/101520 (33%)] Loss: -999.296204\n",
      "Train Epoch: 142 [45056/101520 (44%)] Loss: -1002.626221\n",
      "Train Epoch: 142 [56320/101520 (55%)] Loss: -996.662109\n",
      "Train Epoch: 142 [67584/101520 (67%)] Loss: -999.456055\n",
      "Train Epoch: 142 [78848/101520 (78%)] Loss: -996.992188\n",
      "Train Epoch: 142 [90112/101520 (89%)] Loss: -1003.608643\n",
      "Train Epoch: 142 [101376/101520 (100%)] Loss: -992.569641\n",
      "    epoch          : 142\n",
      "    loss           : -1003.6240709774459\n",
      "    ess            : 1.9671096741853646\n",
      "    log_marginal   : 1003.650724631458\n",
      "    log_joint      : 1211.330970649144\n",
      "    val_loss       : -1004.9296795388926\n",
      "    val_ess        : 1.9658555518025937\n",
      "    val_log_marginal: 1004.9550117824389\n",
      "    val_log_joint  : 1212.2930112092392\n",
      "Train Epoch: 143 [0/101520 (0%)] Loss: -999.289368\n",
      "Train Epoch: 143 [11264/101520 (11%)] Loss: -1005.366882\n",
      "Train Epoch: 143 [22528/101520 (22%)] Loss: -1003.758911\n",
      "Train Epoch: 143 [33792/101520 (33%)] Loss: -1001.567505\n",
      "Train Epoch: 143 [45056/101520 (44%)] Loss: -1001.225830\n",
      "Train Epoch: 143 [56320/101520 (55%)] Loss: -1006.222046\n",
      "Train Epoch: 143 [67584/101520 (67%)] Loss: -1002.846802\n",
      "Train Epoch: 143 [78848/101520 (78%)] Loss: -1009.707153\n",
      "Train Epoch: 143 [90112/101520 (89%)] Loss: -1003.728516\n",
      "Train Epoch: 143 [101376/101520 (100%)] Loss: -1001.082703\n",
      "    epoch          : 143\n",
      "    loss           : -1005.7548693172896\n",
      "    ess            : 1.967712742000369\n",
      "    log_marginal   : 1005.7808399296287\n",
      "    log_joint      : 1213.3852121937814\n",
      "    val_loss       : -1007.6739369268003\n",
      "    val_ess        : 1.9655134159585703\n",
      "    val_log_marginal: 1007.7048525602921\n",
      "    val_log_joint  : 1215.2736922554348\n",
      "Train Epoch: 144 [0/101520 (0%)] Loss: -1011.952026\n",
      "Train Epoch: 144 [11264/101520 (11%)] Loss: -1002.050415\n",
      "Train Epoch: 144 [22528/101520 (22%)] Loss: -1007.440186\n",
      "Train Epoch: 144 [33792/101520 (33%)] Loss: -1004.208374\n",
      "Train Epoch: 144 [45056/101520 (44%)] Loss: -996.022461\n",
      "Train Epoch: 144 [56320/101520 (55%)] Loss: -1009.451172\n",
      "Train Epoch: 144 [67584/101520 (67%)] Loss: -997.703979\n",
      "Train Epoch: 144 [78848/101520 (78%)] Loss: -1006.667725\n",
      "Train Epoch: 144 [90112/101520 (89%)] Loss: -995.279419\n",
      "Train Epoch: 144 [101376/101520 (100%)] Loss: -998.110474\n",
      "    epoch          : 144\n",
      "    loss           : -1004.3085351685183\n",
      "    ess            : 1.9668249203332107\n",
      "    log_marginal   : 1004.335491237928\n",
      "    log_joint      : 1212.0479006359924\n",
      "    val_loss       : -1003.51720926036\n",
      "    val_ess        : 1.9690570623978325\n",
      "    val_log_marginal: 1003.5405485733696\n",
      "    val_log_joint  : 1211.1878131368885\n",
      "Train Epoch: 145 [0/101520 (0%)] Loss: -1002.295105\n",
      "Train Epoch: 145 [11264/101520 (11%)] Loss: -1005.127808\n",
      "Train Epoch: 145 [22528/101520 (22%)] Loss: -1015.622925\n",
      "Train Epoch: 145 [33792/101520 (33%)] Loss: -1009.191528\n",
      "Train Epoch: 145 [45056/101520 (44%)] Loss: -1006.380554\n",
      "Train Epoch: 145 [56320/101520 (55%)] Loss: -1011.500732\n",
      "Train Epoch: 145 [67584/101520 (67%)] Loss: -1007.490417\n",
      "Train Epoch: 145 [78848/101520 (78%)] Loss: -1010.392151\n",
      "Train Epoch: 145 [90112/101520 (89%)] Loss: -1007.655090\n",
      "Train Epoch: 145 [101376/101520 (100%)] Loss: -1003.118958\n",
      "    epoch          : 145\n",
      "    loss           : -1006.3829502124883\n",
      "    ess            : 1.9674760887970277\n",
      "    log_marginal   : 1006.4084880579657\n",
      "    log_joint      : 1214.009874200102\n",
      "    val_loss       : -1006.3388088060462\n",
      "    val_ess        : 1.968382462211277\n",
      "    val_log_marginal: 1006.3642870032269\n",
      "    val_log_joint  : 1213.883152173913\n",
      "Train Epoch: 146 [0/101520 (0%)] Loss: -1011.485352\n",
      "Train Epoch: 146 [11264/101520 (11%)] Loss: -1008.804443\n",
      "Train Epoch: 146 [22528/101520 (22%)] Loss: -1004.178833\n",
      "Train Epoch: 146 [33792/101520 (33%)] Loss: -1005.308411\n",
      "Train Epoch: 146 [45056/101520 (44%)] Loss: -1005.881592\n",
      "Train Epoch: 146 [56320/101520 (55%)] Loss: -1005.234497\n",
      "Train Epoch: 146 [67584/101520 (67%)] Loss: -1002.880005\n",
      "Train Epoch: 146 [78848/101520 (78%)] Loss: -1000.992798\n",
      "Train Epoch: 146 [90112/101520 (89%)] Loss: -997.213318\n",
      "Train Epoch: 146 [101376/101520 (100%)] Loss: -1006.583130\n",
      "    epoch          : 146\n",
      "    loss           : -1005.0801732048916\n",
      "    ess            : 1.9668668011325088\n",
      "    log_marginal   : 1005.106874705559\n",
      "    log_joint      : 1212.8772800771435\n",
      "    val_loss       : -1004.6123099949049\n",
      "    val_ess        : 1.9692767806675122\n",
      "    val_log_marginal: 1004.6363525390625\n",
      "    val_log_joint  : 1212.4100235648777\n",
      "Train Epoch: 147 [0/101520 (0%)] Loss: -1006.342651\n",
      "Train Epoch: 147 [11264/101520 (11%)] Loss: -1006.780212\n",
      "Train Epoch: 147 [22528/101520 (22%)] Loss: -1015.483521\n",
      "Train Epoch: 147 [33792/101520 (33%)] Loss: -1004.659729\n",
      "Train Epoch: 147 [45056/101520 (44%)] Loss: -1008.002563\n",
      "Train Epoch: 147 [56320/101520 (55%)] Loss: -1010.184204\n",
      "Train Epoch: 147 [67584/101520 (67%)] Loss: -1012.454224\n",
      "Train Epoch: 147 [78848/101520 (78%)] Loss: -1005.846313\n",
      "Train Epoch: 147 [90112/101520 (89%)] Loss: -1002.945435\n",
      "Train Epoch: 147 [101376/101520 (100%)] Loss: -1014.175110\n",
      "    epoch          : 147\n",
      "    loss           : -1007.3775432337469\n",
      "    ess            : 1.9668508432618337\n",
      "    log_marginal   : 1007.4039131816309\n",
      "    log_joint      : 1215.0051183652638\n",
      "    val_loss       : -1009.7610234799592\n",
      "    val_ess        : 1.9638880387596462\n",
      "    val_log_marginal: 1009.7916843580163\n",
      "    val_log_joint  : 1217.1598696501358\n",
      "Train Epoch: 148 [0/101520 (0%)] Loss: -1007.506348\n",
      "Train Epoch: 148 [11264/101520 (11%)] Loss: -1001.841797\n",
      "Train Epoch: 148 [22528/101520 (22%)] Loss: -1008.082092\n",
      "Train Epoch: 148 [33792/101520 (33%)] Loss: -1000.839600\n",
      "Train Epoch: 148 [45056/101520 (44%)] Loss: -1009.612549\n",
      "Train Epoch: 148 [56320/101520 (55%)] Loss: -1010.849487\n",
      "Train Epoch: 148 [67584/101520 (67%)] Loss: -1003.532837\n",
      "Train Epoch: 148 [78848/101520 (78%)] Loss: -1000.419067\n",
      "Train Epoch: 148 [90112/101520 (89%)] Loss: -1003.431702\n",
      "Train Epoch: 148 [101376/101520 (100%)] Loss: -1006.907349\n",
      "    epoch          : 148\n",
      "    loss           : -1006.0067764358904\n",
      "    ess            : 1.966760700671517\n",
      "    log_marginal   : 1006.0342167993286\n",
      "    log_joint      : 1213.710559634108\n",
      "    val_loss       : -1005.8459313434104\n",
      "    val_ess        : 1.964627452518629\n",
      "    val_log_marginal: 1005.8751379925271\n",
      "    val_log_joint  : 1213.3878545346467\n",
      "Train Epoch: 149 [0/101520 (0%)] Loss: -1003.673340\n",
      "Train Epoch: 149 [11264/101520 (11%)] Loss: -1004.226196\n",
      "Train Epoch: 149 [22528/101520 (22%)] Loss: -1010.028320\n",
      "Train Epoch: 149 [33792/101520 (33%)] Loss: -1002.542175\n",
      "Train Epoch: 149 [45056/101520 (44%)] Loss: -1003.922852\n",
      "Train Epoch: 149 [56320/101520 (55%)] Loss: -1014.160522\n",
      "Train Epoch: 149 [67584/101520 (67%)] Loss: -1013.894104\n",
      "Train Epoch: 149 [78848/101520 (78%)] Loss: -1016.314270\n",
      "Train Epoch: 149 [90112/101520 (89%)] Loss: -1007.547180\n",
      "Train Epoch: 149 [101376/101520 (100%)] Loss: -1016.318787\n",
      "    epoch          : 149\n",
      "    loss           : -1008.0797162079931\n",
      "    ess            : 1.9661893191649087\n",
      "    log_marginal   : 1008.1067004946608\n",
      "    log_joint      : 1215.7099130908448\n",
      "    val_loss       : -1007.5770184060801\n",
      "    val_ess        : 1.9608999698058418\n",
      "    val_log_marginal: 1007.6108663807745\n",
      "    val_log_joint  : 1215.5445875084918\n",
      "Train Epoch: 150 [0/101520 (0%)] Loss: -1010.108765\n",
      "Train Epoch: 150 [11264/101520 (11%)] Loss: -1002.636475\n",
      "Train Epoch: 150 [22528/101520 (22%)] Loss: -1002.542786\n",
      "Train Epoch: 150 [33792/101520 (33%)] Loss: -1003.753540\n",
      "Train Epoch: 150 [45056/101520 (44%)] Loss: -1008.448120\n",
      "Train Epoch: 150 [56320/101520 (55%)] Loss: -1000.397461\n",
      "Train Epoch: 150 [67584/101520 (67%)] Loss: -1009.902649\n",
      "Train Epoch: 150 [78848/101520 (78%)] Loss: -1003.547852\n",
      "Train Epoch: 150 [90112/101520 (89%)] Loss: -1003.906982\n",
      "Train Epoch: 150 [101376/101520 (100%)] Loss: -995.934570\n",
      "    epoch          : 150\n",
      "    loss           : -1006.7839079430355\n",
      "    ess            : 1.9659250064111835\n",
      "    log_marginal   : 1006.8117734056023\n",
      "    log_joint      : 1214.5027401411353\n",
      "    val_loss       : -1004.4219094981318\n",
      "    val_ess        : 1.967557844908341\n",
      "    val_log_marginal: 1004.4479662024456\n",
      "    val_log_joint  : 1211.9397131878397\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [0/101520 (0%)] Loss: -1006.606323\n",
      "Train Epoch: 151 [11264/101520 (11%)] Loss: -1012.422241\n",
      "Train Epoch: 151 [22528/101520 (22%)] Loss: -1007.586792\n",
      "Train Epoch: 151 [33792/101520 (33%)] Loss: -1014.691284\n",
      "Train Epoch: 151 [45056/101520 (44%)] Loss: -1012.471375\n",
      "Train Epoch: 151 [56320/101520 (55%)] Loss: -1002.994751\n",
      "Train Epoch: 151 [67584/101520 (67%)] Loss: -1006.514099\n",
      "Train Epoch: 151 [78848/101520 (78%)] Loss: -1004.715149\n",
      "Train Epoch: 151 [90112/101520 (89%)] Loss: -1010.949341\n",
      "Train Epoch: 151 [101376/101520 (100%)] Loss: -1009.940857\n",
      "    epoch          : 151\n",
      "    loss           : -1008.7412002026736\n",
      "    ess            : 1.9660513347117745\n",
      "    log_marginal   : 1008.7690693457522\n",
      "    log_joint      : 1216.429912011228\n",
      "    val_loss       : -1011.145287555197\n",
      "    val_ess        : 1.9660472403401914\n",
      "    val_log_marginal: 1011.1731408160666\n",
      "    val_log_joint  : 1218.5940259850543\n",
      "Train Epoch: 152 [0/101520 (0%)] Loss: -1006.122681\n",
      "Train Epoch: 152 [11264/101520 (11%)] Loss: -1019.288086\n",
      "Train Epoch: 152 [22528/101520 (22%)] Loss: -1008.094177\n",
      "Train Epoch: 152 [33792/101520 (33%)] Loss: -1005.753418\n",
      "Train Epoch: 152 [45056/101520 (44%)] Loss: -994.126099\n",
      "Train Epoch: 152 [56320/101520 (55%)] Loss: -1012.982666\n",
      "Train Epoch: 152 [67584/101520 (67%)] Loss: -1011.629150\n",
      "Train Epoch: 152 [78848/101520 (78%)] Loss: -1007.077148\n",
      "Train Epoch: 152 [90112/101520 (89%)] Loss: -1007.129517\n",
      "Train Epoch: 152 [101376/101520 (100%)] Loss: -999.051331\n",
      "    epoch          : 152\n",
      "    loss           : -1007.7138481715217\n",
      "    ess            : 1.965703453250866\n",
      "    log_marginal   : 1007.7419090079303\n",
      "    log_joint      : 1215.3968383175643\n",
      "    val_loss       : -1006.9665261973505\n",
      "    val_ess        : 1.9665782088818757\n",
      "    val_log_marginal: 1006.9925404424253\n",
      "    val_log_joint  : 1214.5547671110733\n",
      "Train Epoch: 153 [0/101520 (0%)] Loss: -1004.307861\n",
      "Train Epoch: 153 [11264/101520 (11%)] Loss: -1010.442871\n",
      "Train Epoch: 153 [22528/101520 (22%)] Loss: -1010.256287\n",
      "Train Epoch: 153 [33792/101520 (33%)] Loss: -1014.988770\n",
      "Train Epoch: 153 [45056/101520 (44%)] Loss: -999.288147\n",
      "Train Epoch: 153 [56320/101520 (55%)] Loss: -1007.856079\n",
      "Train Epoch: 153 [67584/101520 (67%)] Loss: -1005.655762\n",
      "Train Epoch: 153 [78848/101520 (78%)] Loss: -1002.705688\n",
      "Train Epoch: 153 [90112/101520 (89%)] Loss: -1003.716431\n",
      "Train Epoch: 153 [101376/101520 (100%)] Loss: -1002.843750\n",
      "    epoch          : 153\n",
      "    loss           : -1009.6451922086017\n",
      "    ess            : 1.966345814604256\n",
      "    log_marginal   : 1009.6723252492933\n",
      "    log_joint      : 1217.219842498626\n",
      "    val_loss       : -1010.2712083899456\n",
      "    val_ess        : 1.96815926614015\n",
      "    val_log_marginal: 1010.2978329865829\n",
      "    val_log_joint  : 1217.5638533882473\n",
      "Train Epoch: 154 [0/101520 (0%)] Loss: -1006.815308\n",
      "Train Epoch: 154 [11264/101520 (11%)] Loss: -1017.164734\n",
      "Train Epoch: 154 [22528/101520 (22%)] Loss: -1011.573853\n",
      "Train Epoch: 154 [33792/101520 (33%)] Loss: -999.256714\n",
      "Train Epoch: 154 [45056/101520 (44%)] Loss: -1008.137939\n",
      "Train Epoch: 154 [56320/101520 (55%)] Loss: -999.933105\n",
      "Train Epoch: 154 [67584/101520 (67%)] Loss: -1009.259705\n",
      "Train Epoch: 154 [78848/101520 (78%)] Loss: -1008.396118\n",
      "Train Epoch: 154 [90112/101520 (89%)] Loss: -1009.493530\n",
      "Train Epoch: 154 [101376/101520 (100%)] Loss: -1017.702026\n",
      "    epoch          : 154\n",
      "    loss           : -1007.780343673936\n",
      "    ess            : 1.9665198661574168\n",
      "    log_marginal   : 1007.8075362162374\n",
      "    log_joint      : 1215.532750422032\n",
      "    val_loss       : -1007.0621682871943\n",
      "    val_ess        : 1.9683837320493616\n",
      "    val_log_marginal: 1007.0885620117188\n",
      "    val_log_joint  : 1214.7120042883832\n",
      "Train Epoch: 155 [0/101520 (0%)] Loss: -1011.500000\n",
      "Train Epoch: 155 [11264/101520 (11%)] Loss: -1013.448853\n",
      "Train Epoch: 155 [22528/101520 (22%)] Loss: -1011.394653\n",
      "Train Epoch: 155 [33792/101520 (33%)] Loss: -1008.152222\n",
      "Train Epoch: 155 [45056/101520 (44%)] Loss: -1007.247131\n",
      "Train Epoch: 155 [56320/101520 (55%)] Loss: -1007.288635\n",
      "Train Epoch: 155 [67584/101520 (67%)] Loss: -1011.877319\n",
      "Train Epoch: 155 [78848/101520 (78%)] Loss: -1007.859863\n",
      "Train Epoch: 155 [90112/101520 (89%)] Loss: -1018.667725\n",
      "Train Epoch: 155 [101376/101520 (100%)] Loss: -1005.320557\n",
      "    epoch          : 155\n",
      "    loss           : -1009.5128514275478\n",
      "    ess            : 1.966157211131187\n",
      "    log_marginal   : 1009.5405037271317\n",
      "    log_joint      : 1217.2532762690405\n",
      "    val_loss       : -1010.7916923191236\n",
      "    val_ess        : 1.9672083336374033\n",
      "    val_log_marginal: 1010.8186804729959\n",
      "    val_log_joint  : 1218.3290272588315\n",
      "Train Epoch: 156 [0/101520 (0%)] Loss: -1005.139282\n",
      "Train Epoch: 156 [11264/101520 (11%)] Loss: -1007.697754\n",
      "Train Epoch: 156 [22528/101520 (22%)] Loss: -1009.143555\n",
      "Train Epoch: 156 [33792/101520 (33%)] Loss: -1007.888428\n",
      "Train Epoch: 156 [45056/101520 (44%)] Loss: -1004.476196\n",
      "Train Epoch: 156 [56320/101520 (55%)] Loss: -1015.373474\n",
      "Train Epoch: 156 [67584/101520 (67%)] Loss: -1016.999207\n",
      "Train Epoch: 156 [78848/101520 (78%)] Loss: -1006.455444\n",
      "Train Epoch: 156 [90112/101520 (89%)] Loss: -1013.681396\n",
      "Train Epoch: 156 [101376/101520 (100%)] Loss: -1003.228333\n",
      "    epoch          : 156\n",
      "    loss           : -1008.9075403261425\n",
      "    ess            : 1.9659821370139194\n",
      "    log_marginal   : 1008.9350481656329\n",
      "    log_joint      : 1216.6567370544126\n",
      "    val_loss       : -1010.4762865149456\n",
      "    val_ess        : 1.9659154000489607\n",
      "    val_log_marginal: 1010.503781525985\n",
      "    val_log_joint  : 1218.3872229534647\n",
      "Train Epoch: 157 [0/101520 (0%)] Loss: -1014.154236\n",
      "Train Epoch: 157 [11264/101520 (11%)] Loss: -1015.772461\n",
      "Train Epoch: 157 [22528/101520 (22%)] Loss: -1012.156494\n",
      "Train Epoch: 157 [33792/101520 (33%)] Loss: -1004.596069\n",
      "Train Epoch: 157 [45056/101520 (44%)] Loss: -1012.006775\n",
      "Train Epoch: 157 [56320/101520 (55%)] Loss: -1011.142822\n",
      "Train Epoch: 157 [67584/101520 (67%)] Loss: -1008.003418\n",
      "Train Epoch: 157 [78848/101520 (78%)] Loss: -1008.875732\n",
      "Train Epoch: 157 [90112/101520 (89%)] Loss: -1018.567993\n",
      "Train Epoch: 157 [101376/101520 (100%)] Loss: -1012.897461\n",
      "    epoch          : 157\n",
      "    loss           : -1011.1430596586448\n",
      "    ess            : 1.9658870822820231\n",
      "    log_marginal   : 1011.1705978623587\n",
      "    log_joint      : 1218.743130324474\n",
      "    val_loss       : -1010.9473346212636\n",
      "    val_ess        : 1.9655115604400635\n",
      "    val_log_marginal: 1010.9771887737771\n",
      "    val_log_joint  : 1218.5327891474185\n",
      "Train Epoch: 158 [0/101520 (0%)] Loss: -1017.275391\n",
      "Train Epoch: 158 [11264/101520 (11%)] Loss: -1010.939087\n",
      "Train Epoch: 158 [22528/101520 (22%)] Loss: -1012.104370\n",
      "Train Epoch: 158 [33792/101520 (33%)] Loss: -1013.750366\n",
      "Train Epoch: 158 [45056/101520 (44%)] Loss: -1009.964844\n",
      "Train Epoch: 158 [56320/101520 (55%)] Loss: -1013.895874\n",
      "Train Epoch: 158 [67584/101520 (67%)] Loss: -1015.739380\n",
      "Train Epoch: 158 [78848/101520 (78%)] Loss: -1013.789795\n",
      "Train Epoch: 158 [90112/101520 (89%)] Loss: -1013.356445\n",
      "Train Epoch: 158 [101376/101520 (100%)] Loss: -1006.625549\n",
      "    epoch          : 158\n",
      "    loss           : -1009.6996374082325\n",
      "    ess            : 1.9660264930533404\n",
      "    log_marginal   : 1009.7273507429727\n",
      "    log_joint      : 1217.4055856675957\n",
      "    val_loss       : -1008.8681720236074\n",
      "    val_ess        : 1.966359687888104\n",
      "    val_log_marginal: 1008.8956988790761\n",
      "    val_log_joint  : 1216.3065610139267\n",
      "Train Epoch: 159 [0/101520 (0%)] Loss: -1013.751892\n",
      "Train Epoch: 159 [11264/101520 (11%)] Loss: -1007.095703\n",
      "Train Epoch: 159 [22528/101520 (22%)] Loss: -1014.635620\n",
      "Train Epoch: 159 [33792/101520 (33%)] Loss: -1016.010498\n",
      "Train Epoch: 159 [45056/101520 (44%)] Loss: -1011.707764\n",
      "Train Epoch: 159 [56320/101520 (55%)] Loss: -1007.197266\n",
      "Train Epoch: 159 [67584/101520 (67%)] Loss: -1003.197937\n",
      "Train Epoch: 159 [78848/101520 (78%)] Loss: -1017.303955\n",
      "Train Epoch: 159 [90112/101520 (89%)] Loss: -1010.602173\n",
      "Train Epoch: 159 [101376/101520 (100%)] Loss: -1014.159302\n",
      "    epoch          : 159\n",
      "    loss           : -1011.3136397318624\n",
      "    ess            : 1.9654005855771166\n",
      "    log_marginal   : 1011.3423560061048\n",
      "    log_joint      : 1219.0197674161825\n",
      "    val_loss       : -1011.874315344769\n",
      "    val_ess        : 1.9654013851414556\n",
      "    val_log_marginal: 1011.9019563094429\n",
      "    val_log_joint  : 1219.4080810546875\n",
      "Train Epoch: 160 [0/101520 (0%)] Loss: -1016.411499\n",
      "Train Epoch: 160 [11264/101520 (11%)] Loss: -1012.934753\n",
      "Train Epoch: 160 [22528/101520 (22%)] Loss: -1010.607971\n",
      "Train Epoch: 160 [33792/101520 (33%)] Loss: -1009.946838\n",
      "Train Epoch: 160 [45056/101520 (44%)] Loss: -1009.966675\n",
      "Train Epoch: 160 [56320/101520 (55%)] Loss: -999.418213\n",
      "Train Epoch: 160 [67584/101520 (67%)] Loss: -1005.884644\n",
      "Train Epoch: 160 [78848/101520 (78%)] Loss: -1003.695923\n",
      "Train Epoch: 160 [90112/101520 (89%)] Loss: -1003.599609\n",
      "Train Epoch: 160 [101376/101520 (100%)] Loss: -1000.661926\n",
      "    epoch          : 160\n",
      "    loss           : -1010.329291415574\n",
      "    ess            : 1.9664002045914157\n",
      "    log_marginal   : 1010.3570547439346\n",
      "    log_joint      : 1218.087625014722\n",
      "    val_loss       : -1010.0706574813179\n",
      "    val_ess        : 1.9655337385509326\n",
      "    val_log_marginal: 1010.0980277683424\n",
      "    val_log_joint  : 1217.689750339674\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch160.pth ...\n",
      "Train Epoch: 161 [0/101520 (0%)] Loss: -1011.889404\n",
      "Train Epoch: 161 [11264/101520 (11%)] Loss: -1017.491211\n",
      "Train Epoch: 161 [22528/101520 (22%)] Loss: -1009.381958\n",
      "Train Epoch: 161 [33792/101520 (33%)] Loss: -1018.376831\n",
      "Train Epoch: 161 [45056/101520 (44%)] Loss: -1014.876099\n",
      "Train Epoch: 161 [56320/101520 (55%)] Loss: -1023.414124\n",
      "Train Epoch: 161 [67584/101520 (67%)] Loss: -1018.936523\n",
      "Train Epoch: 161 [78848/101520 (78%)] Loss: -1013.166809\n",
      "Train Epoch: 161 [90112/101520 (89%)] Loss: -1016.473145\n",
      "Train Epoch: 161 [101376/101520 (100%)] Loss: -1007.641724\n",
      "    epoch          : 161\n",
      "    loss           : -1012.1779113462821\n",
      "    ess            : 1.9659821226369196\n",
      "    log_marginal   : 1012.2057795692328\n",
      "    log_joint      : 1219.8152616107884\n",
      "    val_loss       : -1013.5372977878736\n",
      "    val_ess        : 1.9657059856083081\n",
      "    val_log_marginal: 1013.5648007600204\n",
      "    val_log_joint  : 1220.9757716966712\n",
      "Train Epoch: 162 [0/101520 (0%)] Loss: -1012.361328\n",
      "Train Epoch: 162 [11264/101520 (11%)] Loss: -1007.501221\n",
      "Train Epoch: 162 [22528/101520 (22%)] Loss: -1012.590759\n",
      "Train Epoch: 162 [33792/101520 (33%)] Loss: -1011.790771\n",
      "Train Epoch: 162 [45056/101520 (44%)] Loss: -1010.149719\n",
      "Train Epoch: 162 [56320/101520 (55%)] Loss: -1015.011414\n",
      "Train Epoch: 162 [67584/101520 (67%)] Loss: -1009.236450\n",
      "Train Epoch: 162 [78848/101520 (78%)] Loss: -1005.536011\n",
      "Train Epoch: 162 [90112/101520 (89%)] Loss: -1009.088013\n",
      "Train Epoch: 162 [101376/101520 (100%)] Loss: -1019.725586\n",
      "    epoch          : 162\n",
      "    loss           : -1011.564791732098\n",
      "    ess            : 1.9655103994973342\n",
      "    log_marginal   : 1011.5932601852033\n",
      "    log_joint      : 1219.2612746348932\n",
      "    val_loss       : -1012.8363806683084\n",
      "    val_ess        : 1.9641325629275779\n",
      "    val_log_marginal: 1012.8669698963995\n",
      "    val_log_joint  : 1220.3670707370925\n",
      "Train Epoch: 163 [0/101520 (0%)] Loss: -1012.673706\n",
      "Train Epoch: 163 [11264/101520 (11%)] Loss: -1010.798462\n",
      "Train Epoch: 163 [22528/101520 (22%)] Loss: -1013.810059\n",
      "Train Epoch: 163 [33792/101520 (33%)] Loss: -1011.067322\n",
      "Train Epoch: 163 [45056/101520 (44%)] Loss: -1014.001343\n",
      "Train Epoch: 163 [56320/101520 (55%)] Loss: -1019.688538\n",
      "Train Epoch: 163 [67584/101520 (67%)] Loss: -1011.237427\n",
      "Train Epoch: 163 [78848/101520 (78%)] Loss: -1012.277100\n",
      "Train Epoch: 163 [90112/101520 (89%)] Loss: -1016.664062\n",
      "Train Epoch: 163 [101376/101520 (100%)] Loss: -1014.867432\n",
      "    epoch          : 163\n",
      "    loss           : -1012.8690492256203\n",
      "    ess            : 1.9655522647215493\n",
      "    log_marginal   : 1012.8976734870643\n",
      "    log_joint      : 1220.554780126217\n",
      "    val_loss       : -1015.446044921875\n",
      "    val_ess        : 1.9657983779907227\n",
      "    val_log_marginal: 1015.4718176800271\n",
      "    val_log_joint  : 1222.8696660580842\n",
      "Train Epoch: 164 [0/101520 (0%)] Loss: -1016.259155\n",
      "Train Epoch: 164 [11264/101520 (11%)] Loss: -1011.362732\n",
      "Train Epoch: 164 [22528/101520 (22%)] Loss: -1012.267822\n",
      "Train Epoch: 164 [33792/101520 (33%)] Loss: -1012.790161\n",
      "Train Epoch: 164 [45056/101520 (44%)] Loss: -1014.982605\n",
      "Train Epoch: 164 [56320/101520 (55%)] Loss: -1003.536804\n",
      "Train Epoch: 164 [67584/101520 (67%)] Loss: -1007.477783\n",
      "Train Epoch: 164 [78848/101520 (78%)] Loss: -1009.803589\n",
      "Train Epoch: 164 [90112/101520 (89%)] Loss: -1016.845215\n",
      "Train Epoch: 164 [101376/101520 (100%)] Loss: -1013.032104\n",
      "    epoch          : 164\n",
      "    loss           : -1012.2104121069214\n",
      "    ess            : 1.9653524573723875\n",
      "    log_marginal   : 1012.238918285274\n",
      "    log_joint      : 1219.8942509176743\n",
      "    val_loss       : -1012.8171519403872\n",
      "    val_ess        : 1.9680316137230915\n",
      "    val_log_marginal: 1012.84118917714\n",
      "    val_log_joint  : 1220.432081139606\n",
      "Train Epoch: 165 [0/101520 (0%)] Loss: -1004.211182\n",
      "Train Epoch: 165 [11264/101520 (11%)] Loss: -1015.894897\n",
      "Train Epoch: 165 [22528/101520 (22%)] Loss: -1011.434875\n",
      "Train Epoch: 165 [33792/101520 (33%)] Loss: -1008.414917\n",
      "Train Epoch: 165 [45056/101520 (44%)] Loss: -1009.647095\n",
      "Train Epoch: 165 [56320/101520 (55%)] Loss: -1015.770508\n",
      "Train Epoch: 165 [67584/101520 (67%)] Loss: -1012.708191\n",
      "Train Epoch: 165 [78848/101520 (78%)] Loss: -1012.552490\n",
      "Train Epoch: 165 [90112/101520 (89%)] Loss: -1014.392944\n",
      "Train Epoch: 165 [101376/101520 (100%)] Loss: -1025.183350\n",
      "    epoch          : 165\n",
      "    loss           : -1013.4397371378376\n",
      "    ess            : 1.9654695478516009\n",
      "    log_marginal   : 1013.4682408625157\n",
      "    log_joint      : 1221.1551973735866\n",
      "    val_loss       : -1015.3370069420856\n",
      "    val_ess        : 1.9670227247735728\n",
      "    val_log_marginal: 1015.3638544497283\n",
      "    val_log_joint  : 1223.2816745923913\n",
      "Train Epoch: 166 [0/101520 (0%)] Loss: -1015.193359\n",
      "Train Epoch: 166 [11264/101520 (11%)] Loss: -1012.651855\n",
      "Train Epoch: 166 [22528/101520 (22%)] Loss: -1017.307983\n",
      "Train Epoch: 166 [33792/101520 (33%)] Loss: -1010.904785\n",
      "Train Epoch: 166 [45056/101520 (44%)] Loss: -1005.768921\n",
      "Train Epoch: 166 [56320/101520 (55%)] Loss: -1007.449219\n",
      "Train Epoch: 166 [67584/101520 (67%)] Loss: -1009.641357\n",
      "Train Epoch: 166 [78848/101520 (78%)] Loss: -1016.231323\n",
      "Train Epoch: 166 [90112/101520 (89%)] Loss: -1012.906067\n",
      "Train Epoch: 166 [101376/101520 (100%)] Loss: -1001.249573\n",
      "    epoch          : 166\n",
      "    loss           : -1012.7962097474679\n",
      "    ess            : 1.9659166324078736\n",
      "    log_marginal   : 1012.8237954911275\n",
      "    log_joint      : 1220.4986934182632\n",
      "    val_loss       : -1011.5560966159986\n",
      "    val_ess        : 1.96608044790185\n",
      "    val_log_marginal: 1011.5824452275815\n",
      "    val_log_joint  : 1219.398389733356\n",
      "Train Epoch: 167 [0/101520 (0%)] Loss: -1015.693542\n",
      "Train Epoch: 167 [11264/101520 (11%)] Loss: -1012.416138\n",
      "Train Epoch: 167 [22528/101520 (22%)] Loss: -1009.675049\n",
      "Train Epoch: 167 [33792/101520 (33%)] Loss: -1012.790283\n",
      "Train Epoch: 167 [45056/101520 (44%)] Loss: -1008.807556\n",
      "Train Epoch: 167 [56320/101520 (55%)] Loss: -1016.283630\n",
      "Train Epoch: 167 [67584/101520 (67%)] Loss: -1014.252197\n",
      "Train Epoch: 167 [78848/101520 (78%)] Loss: -1019.364624\n",
      "Train Epoch: 167 [90112/101520 (89%)] Loss: -1010.333130\n",
      "Train Epoch: 167 [101376/101520 (100%)] Loss: -1023.676025\n",
      "    epoch          : 167\n",
      "    loss           : -1013.4667290922386\n",
      "    ess            : 1.9657733446389587\n",
      "    log_marginal   : 1013.4948454430355\n",
      "    log_joint      : 1221.1809695449906\n",
      "    val_loss       : -1016.1205630095109\n",
      "    val_ess        : 1.966447026833244\n",
      "    val_log_marginal: 1016.147216796875\n",
      "    val_log_joint  : 1224.0079929517663\n",
      "Train Epoch: 168 [0/101520 (0%)] Loss: -1013.833740\n",
      "Train Epoch: 168 [11264/101520 (11%)] Loss: -1014.263306\n",
      "Train Epoch: 168 [22528/101520 (22%)] Loss: -1019.629639\n",
      "Train Epoch: 168 [33792/101520 (33%)] Loss: -1010.411255\n",
      "Train Epoch: 168 [45056/101520 (44%)] Loss: -1016.571472\n",
      "Train Epoch: 168 [56320/101520 (55%)] Loss: -1019.205933\n",
      "Train Epoch: 168 [67584/101520 (67%)] Loss: -1017.325317\n",
      "Train Epoch: 168 [78848/101520 (78%)] Loss: -1016.412598\n",
      "Train Epoch: 168 [90112/101520 (89%)] Loss: -1011.042603\n",
      "Train Epoch: 168 [101376/101520 (100%)] Loss: -1009.522888\n",
      "    epoch          : 168\n",
      "    loss           : -1014.4471512224206\n",
      "    ess            : 1.9650558299155692\n",
      "    log_marginal   : 1014.4761996628652\n",
      "    log_joint      : 1222.1371542772456\n",
      "    val_loss       : -1017.2278389308764\n",
      "    val_ess        : 1.9641636869181758\n",
      "    val_log_marginal: 1017.2564219599185\n",
      "    val_log_joint  : 1225.0390890370245\n",
      "Train Epoch: 169 [0/101520 (0%)] Loss: -1017.465942\n",
      "Train Epoch: 169 [11264/101520 (11%)] Loss: -1013.159912\n",
      "Train Epoch: 169 [22528/101520 (22%)] Loss: -1004.494446\n",
      "Train Epoch: 169 [33792/101520 (33%)] Loss: -1016.764954\n",
      "Train Epoch: 169 [45056/101520 (44%)] Loss: -1019.922180\n",
      "Train Epoch: 169 [56320/101520 (55%)] Loss: -1009.098938\n",
      "Train Epoch: 169 [67584/101520 (67%)] Loss: -1006.088989\n",
      "Train Epoch: 169 [78848/101520 (78%)] Loss: -1010.049683\n",
      "Train Epoch: 169 [90112/101520 (89%)] Loss: -1003.711914\n",
      "Train Epoch: 169 [101376/101520 (100%)] Loss: -995.987976\n",
      "    epoch          : 169\n",
      "    loss           : -1009.4280554996664\n",
      "    ess            : 1.9649508502615156\n",
      "    log_marginal   : 1009.4570248091042\n",
      "    log_joint      : 1217.4017941268844\n",
      "    val_loss       : -1005.4733754033628\n",
      "    val_ess        : 1.969105145205622\n",
      "    val_log_marginal: 1005.4977868121604\n",
      "    val_log_joint  : 1213.4236582880435\n",
      "Train Epoch: 170 [0/101520 (0%)] Loss: -1012.405640\n",
      "Train Epoch: 170 [11264/101520 (11%)] Loss: -1016.381531\n",
      "Train Epoch: 170 [22528/101520 (22%)] Loss: -1011.993713\n",
      "Train Epoch: 170 [33792/101520 (33%)] Loss: -1011.090759\n",
      "Train Epoch: 170 [45056/101520 (44%)] Loss: -1018.485107\n",
      "Train Epoch: 170 [56320/101520 (55%)] Loss: -1002.315979\n",
      "Train Epoch: 170 [67584/101520 (67%)] Loss: -1012.533386\n",
      "Train Epoch: 170 [78848/101520 (78%)] Loss: -1011.456665\n",
      "Train Epoch: 170 [90112/101520 (89%)] Loss: -1013.269165\n",
      "Train Epoch: 170 [101376/101520 (100%)] Loss: -1010.115479\n",
      "    epoch          : 170\n",
      "    loss           : -1010.6067366863615\n",
      "    ess            : 1.964434705187927\n",
      "    log_marginal   : 1010.636740219653\n",
      "    log_joint      : 1218.4968077693154\n",
      "    val_loss       : -1012.01416015625\n",
      "    val_ess        : 1.9613414795502373\n",
      "    val_log_marginal: 1012.0479152513587\n",
      "    val_log_joint  : 1220.0417215098505\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch170.pth ...\n",
      "Train Epoch: 171 [0/101520 (0%)] Loss: -1016.337158\n",
      "Train Epoch: 171 [11264/101520 (11%)] Loss: -1016.438660\n",
      "Train Epoch: 171 [22528/101520 (22%)] Loss: -1004.370972\n",
      "Train Epoch: 171 [33792/101520 (33%)] Loss: -1009.539795\n",
      "Train Epoch: 171 [45056/101520 (44%)] Loss: -1019.667114\n",
      "Train Epoch: 171 [56320/101520 (55%)] Loss: -1011.151978\n",
      "Train Epoch: 171 [67584/101520 (67%)] Loss: -1014.556274\n",
      "Train Epoch: 171 [78848/101520 (78%)] Loss: -1016.428101\n",
      "Train Epoch: 171 [90112/101520 (89%)] Loss: -1013.549194\n",
      "Train Epoch: 171 [101376/101520 (100%)] Loss: -1030.573853\n",
      "    epoch          : 171\n",
      "    loss           : -1014.2783147917321\n",
      "    ess            : 1.9648692314349228\n",
      "    log_marginal   : 1014.3070283055905\n",
      "    log_joint      : 1222.021692937343\n",
      "    val_loss       : -1015.4593612007473\n",
      "    val_ess        : 1.9641237103420754\n",
      "    val_log_marginal: 1015.4882706351902\n",
      "    val_log_joint  : 1223.026802394701\n",
      "Train Epoch: 172 [0/101520 (0%)] Loss: -1017.782104\n",
      "Train Epoch: 172 [11264/101520 (11%)] Loss: -1014.781250\n",
      "Train Epoch: 172 [22528/101520 (22%)] Loss: -1015.312012\n",
      "Train Epoch: 172 [33792/101520 (33%)] Loss: -1019.923096\n",
      "Train Epoch: 172 [45056/101520 (44%)] Loss: -1020.693848\n",
      "Train Epoch: 172 [56320/101520 (55%)] Loss: -1017.426025\n",
      "Train Epoch: 172 [67584/101520 (67%)] Loss: -1023.916626\n",
      "Train Epoch: 172 [78848/101520 (78%)] Loss: -1011.224121\n",
      "Train Epoch: 172 [90112/101520 (89%)] Loss: -1010.686401\n",
      "Train Epoch: 172 [101376/101520 (100%)] Loss: -1016.267822\n",
      "    epoch          : 172\n",
      "    loss           : -1016.4080249268804\n",
      "    ess            : 1.9646599676141787\n",
      "    log_marginal   : 1016.4373803833621\n",
      "    log_joint      : 1224.0064151323022\n",
      "    val_loss       : -1017.4519096042799\n",
      "    val_ess        : 1.9655311418616253\n",
      "    val_log_marginal: 1017.4797522503396\n",
      "    val_log_joint  : 1224.9465013586957\n",
      "Train Epoch: 173 [0/101520 (0%)] Loss: -1007.388123\n",
      "Train Epoch: 173 [11264/101520 (11%)] Loss: -1017.673096\n",
      "Train Epoch: 173 [22528/101520 (22%)] Loss: -1011.452026\n",
      "Train Epoch: 173 [33792/101520 (33%)] Loss: -1016.088074\n",
      "Train Epoch: 173 [45056/101520 (44%)] Loss: -1012.980713\n",
      "Train Epoch: 173 [56320/101520 (55%)] Loss: -1014.271606\n",
      "Train Epoch: 173 [67584/101520 (67%)] Loss: -1016.118286\n",
      "Train Epoch: 173 [78848/101520 (78%)] Loss: -1014.730225\n",
      "Train Epoch: 173 [90112/101520 (89%)] Loss: -1019.650330\n",
      "Train Epoch: 173 [101376/101520 (100%)] Loss: -1028.614990\n",
      "    epoch          : 173\n",
      "    loss           : -1015.9472107240304\n",
      "    ess            : 1.9643582225444929\n",
      "    log_marginal   : 1015.9772127237751\n",
      "    log_joint      : 1223.684422478604\n",
      "    val_loss       : -1015.306492017663\n",
      "    val_ess        : 1.9648092466851939\n",
      "    val_log_marginal: 1015.3352953040081\n",
      "    val_log_joint  : 1222.6396325152853\n",
      "Train Epoch: 174 [0/101520 (0%)] Loss: -1015.807861\n",
      "Train Epoch: 174 [11264/101520 (11%)] Loss: -1011.649048\n",
      "Train Epoch: 174 [22528/101520 (22%)] Loss: -1021.906372\n",
      "Train Epoch: 174 [33792/101520 (33%)] Loss: -1016.678467\n",
      "Train Epoch: 174 [45056/101520 (44%)] Loss: -1012.738708\n",
      "Train Epoch: 174 [56320/101520 (55%)] Loss: -1017.522766\n",
      "Train Epoch: 174 [67584/101520 (67%)] Loss: -1016.610596\n",
      "Train Epoch: 174 [78848/101520 (78%)] Loss: -1018.537231\n",
      "Train Epoch: 174 [90112/101520 (89%)] Loss: -1013.430237\n",
      "Train Epoch: 174 [101376/101520 (100%)] Loss: -1012.102417\n",
      "    epoch          : 174\n",
      "    loss           : -1017.0663022755379\n",
      "    ess            : 1.9642390869370656\n",
      "    log_marginal   : 1017.0960441857726\n",
      "    log_joint      : 1224.7019901754868\n",
      "    val_loss       : -1019.2562786599864\n",
      "    val_ess        : 1.9648743764213894\n",
      "    val_log_marginal: 1019.2858010996943\n",
      "    val_log_joint  : 1226.8314792798913\n",
      "Train Epoch: 175 [0/101520 (0%)] Loss: -1025.490234\n",
      "Train Epoch: 175 [11264/101520 (11%)] Loss: -1020.100159\n",
      "Train Epoch: 175 [22528/101520 (22%)] Loss: -1014.989380\n",
      "Train Epoch: 175 [33792/101520 (33%)] Loss: -1012.782166\n",
      "Train Epoch: 175 [45056/101520 (44%)] Loss: -1012.975830\n",
      "Train Epoch: 175 [56320/101520 (55%)] Loss: -1020.562317\n",
      "Train Epoch: 175 [67584/101520 (67%)] Loss: -1013.706543\n",
      "Train Epoch: 175 [78848/101520 (78%)] Loss: -1019.788330\n",
      "Train Epoch: 175 [90112/101520 (89%)] Loss: -1013.687256\n",
      "Train Epoch: 175 [101376/101520 (100%)] Loss: -1016.923645\n",
      "    epoch          : 175\n",
      "    loss           : -1014.9838946931925\n",
      "    ess            : 1.9652186883753868\n",
      "    log_marginal   : 1015.0129323988104\n",
      "    log_joint      : 1222.678983908802\n",
      "    val_loss       : -1013.1379474142324\n",
      "    val_ess        : 1.9653620875400046\n",
      "    val_log_marginal: 1013.16736105214\n",
      "    val_log_joint  : 1220.916408372962\n",
      "Train Epoch: 176 [0/101520 (0%)] Loss: -1015.772766\n",
      "Train Epoch: 176 [11264/101520 (11%)] Loss: -1010.452148\n",
      "Train Epoch: 176 [22528/101520 (22%)] Loss: -1014.753296\n",
      "Train Epoch: 176 [33792/101520 (33%)] Loss: -1019.239136\n",
      "Train Epoch: 176 [45056/101520 (44%)] Loss: -1020.844727\n",
      "Train Epoch: 176 [56320/101520 (55%)] Loss: -1017.134827\n",
      "Train Epoch: 176 [67584/101520 (67%)] Loss: -1018.168213\n",
      "Train Epoch: 176 [78848/101520 (78%)] Loss: -1017.687927\n",
      "Train Epoch: 176 [90112/101520 (89%)] Loss: -1013.983948\n",
      "Train Epoch: 176 [101376/101520 (100%)] Loss: -1008.916321\n",
      "    epoch          : 176\n",
      "    loss           : -1015.9890878955323\n",
      "    ess            : 1.9636264434411896\n",
      "    log_marginal   : 1016.019679083896\n",
      "    log_joint      : 1223.803291972558\n",
      "    val_loss       : -1015.4398989470109\n",
      "    val_ess        : 1.9672805848328963\n",
      "    val_log_marginal: 1015.4670463230299\n",
      "    val_log_joint  : 1223.0935164741848\n",
      "Train Epoch: 177 [0/101520 (0%)] Loss: -1018.758545\n",
      "Train Epoch: 177 [11264/101520 (11%)] Loss: -1017.195068\n",
      "Train Epoch: 177 [22528/101520 (22%)] Loss: -1013.738159\n",
      "Train Epoch: 177 [33792/101520 (33%)] Loss: -1022.677368\n",
      "Train Epoch: 177 [45056/101520 (44%)] Loss: -1019.064026\n",
      "Train Epoch: 177 [56320/101520 (55%)] Loss: -1014.858154\n",
      "Train Epoch: 177 [67584/101520 (67%)] Loss: -1015.244507\n",
      "Train Epoch: 177 [78848/101520 (78%)] Loss: -1011.253113\n",
      "Train Epoch: 177 [90112/101520 (89%)] Loss: -1011.638611\n",
      "Train Epoch: 177 [101376/101520 (100%)] Loss: -1025.725586\n",
      "    epoch          : 177\n",
      "    loss           : -1015.6756990519001\n",
      "    ess            : 1.9640721250419042\n",
      "    log_marginal   : 1015.7058191347362\n",
      "    log_joint      : 1223.5016083837154\n",
      "    val_loss       : -1016.66906472911\n",
      "    val_ess        : 1.9656089440636013\n",
      "    val_log_marginal: 1016.6963952105979\n",
      "    val_log_joint  : 1224.373237941576\n",
      "Train Epoch: 178 [0/101520 (0%)] Loss: -1016.886475\n",
      "Train Epoch: 178 [11264/101520 (11%)] Loss: -1012.226746\n",
      "Train Epoch: 178 [22528/101520 (22%)] Loss: -1019.498657\n",
      "Train Epoch: 178 [33792/101520 (33%)] Loss: -1014.507507\n",
      "Train Epoch: 178 [45056/101520 (44%)] Loss: -1009.600098\n",
      "Train Epoch: 178 [56320/101520 (55%)] Loss: -1023.871521\n",
      "Train Epoch: 178 [67584/101520 (67%)] Loss: -1017.721252\n",
      "Train Epoch: 178 [78848/101520 (78%)] Loss: -1015.427551\n",
      "Train Epoch: 178 [90112/101520 (89%)] Loss: -1019.079773\n",
      "Train Epoch: 178 [101376/101520 (100%)] Loss: -1030.606812\n",
      "    epoch          : 178\n",
      "    loss           : -1017.7883911132812\n",
      "    ess            : 1.9643138904667379\n",
      "    log_marginal   : 1017.8175772662139\n",
      "    log_joint      : 1225.481120814031\n",
      "    val_loss       : -1021.4360325025475\n",
      "    val_ess        : 1.9688203542128853\n",
      "    val_log_marginal: 1021.4608738111413\n",
      "    val_log_joint  : 1229.226068911345\n",
      "Train Epoch: 179 [0/101520 (0%)] Loss: -1019.973145\n",
      "Train Epoch: 179 [11264/101520 (11%)] Loss: -1018.412048\n",
      "Train Epoch: 179 [22528/101520 (22%)] Loss: -1019.605347\n",
      "Train Epoch: 179 [33792/101520 (33%)] Loss: -1018.852783\n",
      "Train Epoch: 179 [45056/101520 (44%)] Loss: -1014.703918\n",
      "Train Epoch: 179 [56320/101520 (55%)] Loss: -1017.013672\n",
      "Train Epoch: 179 [67584/101520 (67%)] Loss: -1021.936890\n",
      "Train Epoch: 179 [78848/101520 (78%)] Loss: -1020.577576\n",
      "Train Epoch: 179 [90112/101520 (89%)] Loss: -1013.372253\n",
      "Train Epoch: 179 [101376/101520 (100%)] Loss: -998.759766\n",
      "    epoch          : 179\n",
      "    loss           : -1016.4160435355489\n",
      "    ess            : 1.9638211038244429\n",
      "    log_marginal   : 1016.4461737397927\n",
      "    log_joint      : 1224.1718062971106\n",
      "    val_loss       : -1016.0707795516304\n",
      "    val_ess        : 1.9677691718806392\n",
      "    val_log_marginal: 1016.0963692043139\n",
      "    val_log_joint  : 1223.6899042544158\n",
      "Train Epoch: 180 [0/101520 (0%)] Loss: -1019.795166\n",
      "Train Epoch: 180 [11264/101520 (11%)] Loss: -1011.816956\n",
      "Train Epoch: 180 [22528/101520 (22%)] Loss: -1019.394653\n",
      "Train Epoch: 180 [33792/101520 (33%)] Loss: -1023.946350\n",
      "Train Epoch: 180 [45056/101520 (44%)] Loss: -1017.223083\n",
      "Train Epoch: 180 [56320/101520 (55%)] Loss: -1014.563782\n",
      "Train Epoch: 180 [67584/101520 (67%)] Loss: -1019.225708\n",
      "Train Epoch: 180 [78848/101520 (78%)] Loss: -1020.766113\n",
      "Train Epoch: 180 [90112/101520 (89%)] Loss: -1021.028442\n",
      "Train Epoch: 180 [101376/101520 (100%)] Loss: -1017.916321\n",
      "    epoch          : 180\n",
      "    loss           : -1017.9337115263819\n",
      "    ess            : 1.9633669919105032\n",
      "    log_marginal   : 1017.9646045167242\n",
      "    log_joint      : 1225.6992929736573\n",
      "    val_loss       : -1018.6439633576766\n",
      "    val_ess        : 1.9653702145037444\n",
      "    val_log_marginal: 1018.6759431258491\n",
      "    val_log_joint  : 1226.3054995329483\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch180.pth ...\n",
      "Train Epoch: 181 [0/101520 (0%)] Loss: -1016.555786\n",
      "Train Epoch: 181 [11264/101520 (11%)] Loss: -1019.431702\n",
      "Train Epoch: 181 [22528/101520 (22%)] Loss: -1024.918335\n",
      "Train Epoch: 181 [33792/101520 (33%)] Loss: -1015.810974\n",
      "Train Epoch: 181 [45056/101520 (44%)] Loss: -1022.673340\n",
      "Train Epoch: 181 [56320/101520 (55%)] Loss: -1022.925049\n",
      "Train Epoch: 181 [67584/101520 (67%)] Loss: -1010.502808\n",
      "Train Epoch: 181 [78848/101520 (78%)] Loss: -1016.184265\n",
      "Train Epoch: 181 [90112/101520 (89%)] Loss: -1011.482239\n",
      "Train Epoch: 181 [101376/101520 (100%)] Loss: -1017.914734\n",
      "    epoch          : 181\n",
      "    loss           : -1017.0640059427999\n",
      "    ess            : 1.9634041037391778\n",
      "    log_marginal   : 1017.0943232397339\n",
      "    log_joint      : 1224.777849820391\n",
      "    val_loss       : -1015.600758428159\n",
      "    val_ess        : 1.967286519382311\n",
      "    val_log_marginal: 1015.627077849015\n",
      "    val_log_joint  : 1223.062399159307\n",
      "Train Epoch: 182 [0/101520 (0%)] Loss: -1012.022156\n",
      "Train Epoch: 182 [11264/101520 (11%)] Loss: -1009.747620\n",
      "Train Epoch: 182 [22528/101520 (22%)] Loss: -1018.869385\n",
      "Train Epoch: 182 [33792/101520 (33%)] Loss: -1021.901489\n",
      "Train Epoch: 182 [45056/101520 (44%)] Loss: -1018.683655\n",
      "Train Epoch: 182 [56320/101520 (55%)] Loss: -1019.428406\n",
      "Train Epoch: 182 [67584/101520 (67%)] Loss: -1019.606079\n",
      "Train Epoch: 182 [78848/101520 (78%)] Loss: -1019.878174\n",
      "Train Epoch: 182 [90112/101520 (89%)] Loss: -1013.845093\n",
      "Train Epoch: 182 [101376/101520 (100%)] Loss: -1021.958435\n",
      "    epoch          : 182\n",
      "    loss           : -1018.7054081442368\n",
      "    ess            : 1.963979652778587\n",
      "    log_marginal   : 1018.7356969172032\n",
      "    log_joint      : 1226.5197974736966\n",
      "    val_loss       : -1020.4050850246264\n",
      "    val_ess        : 1.965158892714459\n",
      "    val_log_marginal: 1020.4356158712636\n",
      "    val_log_joint  : 1228.0208156419837\n",
      "Train Epoch: 183 [0/101520 (0%)] Loss: -1026.756470\n",
      "Train Epoch: 183 [11264/101520 (11%)] Loss: -1016.540283\n",
      "Train Epoch: 183 [22528/101520 (22%)] Loss: -1023.444519\n",
      "Train Epoch: 183 [33792/101520 (33%)] Loss: -1018.695801\n",
      "Train Epoch: 183 [45056/101520 (44%)] Loss: -1015.044678\n",
      "Train Epoch: 183 [56320/101520 (55%)] Loss: -1016.597229\n",
      "Train Epoch: 183 [67584/101520 (67%)] Loss: -1014.407288\n",
      "Train Epoch: 183 [78848/101520 (78%)] Loss: -1012.617615\n",
      "Train Epoch: 183 [90112/101520 (89%)] Loss: -1020.278809\n",
      "Train Epoch: 183 [101376/101520 (100%)] Loss: -1010.913208\n",
      "    epoch          : 183\n",
      "    loss           : -1017.4769719569528\n",
      "    ess            : 1.9628237875262697\n",
      "    log_marginal   : 1017.5086025832286\n",
      "    log_joint      : 1225.261408973579\n",
      "    val_loss       : -1016.2057176672894\n",
      "    val_ess        : 1.957642534504766\n",
      "    val_log_marginal: 1016.2422458814538\n",
      "    val_log_joint  : 1223.9820503566575\n",
      "Train Epoch: 184 [0/101520 (0%)] Loss: -1026.345581\n",
      "Train Epoch: 184 [11264/101520 (11%)] Loss: -1014.605591\n",
      "Train Epoch: 184 [22528/101520 (22%)] Loss: -1021.936035\n",
      "Train Epoch: 184 [33792/101520 (33%)] Loss: -1022.206848\n",
      "Train Epoch: 184 [45056/101520 (44%)] Loss: -1017.515503\n",
      "Train Epoch: 184 [56320/101520 (55%)] Loss: -1021.284424\n",
      "Train Epoch: 184 [67584/101520 (67%)] Loss: -1025.039429\n",
      "Train Epoch: 184 [78848/101520 (78%)] Loss: -1022.734985\n",
      "Train Epoch: 184 [90112/101520 (89%)] Loss: -1021.922913\n",
      "Train Epoch: 184 [101376/101520 (100%)] Loss: -1039.357422\n",
      "    epoch          : 184\n",
      "    loss           : -1019.1624151642\n",
      "    ess            : 1.9631750230214104\n",
      "    log_marginal   : 1019.194169394335\n",
      "    log_joint      : 1226.8635462468592\n",
      "    val_loss       : -1019.308986497962\n",
      "    val_ess        : 1.9581261821415112\n",
      "    val_log_marginal: 1019.3441400942595\n",
      "    val_log_joint  : 1227.31566852072\n",
      "Train Epoch: 185 [0/101520 (0%)] Loss: -1017.146240\n",
      "Train Epoch: 185 [11264/101520 (11%)] Loss: -1025.121216\n",
      "Train Epoch: 185 [22528/101520 (22%)] Loss: -1016.297729\n",
      "Train Epoch: 185 [33792/101520 (33%)] Loss: -1011.290833\n",
      "Train Epoch: 185 [45056/101520 (44%)] Loss: -1018.299011\n",
      "Train Epoch: 185 [56320/101520 (55%)] Loss: -1020.438110\n",
      "Train Epoch: 185 [67584/101520 (67%)] Loss: -1017.936035\n",
      "Train Epoch: 185 [78848/101520 (78%)] Loss: -1019.161804\n",
      "Train Epoch: 185 [90112/101520 (89%)] Loss: -1012.627686\n",
      "Train Epoch: 185 [101376/101520 (100%)] Loss: -1013.373901\n",
      "    epoch          : 185\n",
      "    loss           : -1017.9373033993209\n",
      "    ess            : 1.9636165586548235\n",
      "    log_marginal   : 1017.967900721871\n",
      "    log_joint      : 1225.7665868404522\n",
      "    val_loss       : -1016.3304602581521\n",
      "    val_ess        : 1.9647158332492993\n",
      "    val_log_marginal: 1016.358666461447\n",
      "    val_log_joint  : 1223.8576023267663\n",
      "Train Epoch: 186 [0/101520 (0%)] Loss: -1021.747925\n",
      "Train Epoch: 186 [11264/101520 (11%)] Loss: -1016.972229\n",
      "Train Epoch: 186 [22528/101520 (22%)] Loss: -1019.284851\n",
      "Train Epoch: 186 [33792/101520 (33%)] Loss: -1021.643372\n",
      "Train Epoch: 186 [45056/101520 (44%)] Loss: -1017.382080\n",
      "Train Epoch: 186 [56320/101520 (55%)] Loss: -1020.486084\n",
      "Train Epoch: 186 [67584/101520 (67%)] Loss: -1019.370605\n",
      "Train Epoch: 186 [78848/101520 (78%)] Loss: -1018.078247\n",
      "Train Epoch: 186 [90112/101520 (89%)] Loss: -1021.606628\n",
      "Train Epoch: 186 [101376/101520 (100%)] Loss: -1017.009888\n",
      "    epoch          : 186\n",
      "    loss           : -1019.7534222626806\n",
      "    ess            : 1.9628817507969074\n",
      "    log_marginal   : 1019.7843989846695\n",
      "    log_joint      : 1227.4775304746388\n",
      "    val_loss       : -1022.901123046875\n",
      "    val_ess        : 1.964616060256958\n",
      "    val_log_marginal: 1022.9317998471467\n",
      "    val_log_joint  : 1230.4319166100543\n",
      "Train Epoch: 187 [0/101520 (0%)] Loss: -1018.565674\n",
      "Train Epoch: 187 [11264/101520 (11%)] Loss: -1016.495361\n",
      "Train Epoch: 187 [22528/101520 (22%)] Loss: -1012.903015\n",
      "Train Epoch: 187 [33792/101520 (33%)] Loss: -1023.048462\n",
      "Train Epoch: 187 [45056/101520 (44%)] Loss: -1007.458984\n",
      "Train Epoch: 187 [56320/101520 (55%)] Loss: -1017.806641\n",
      "Train Epoch: 187 [67584/101520 (67%)] Loss: -1018.107605\n",
      "Train Epoch: 187 [78848/101520 (78%)] Loss: -1017.155518\n",
      "Train Epoch: 187 [90112/101520 (89%)] Loss: -1015.560791\n",
      "Train Epoch: 187 [101376/101520 (100%)] Loss: -1021.982849\n",
      "    epoch          : 187\n",
      "    loss           : -1018.7382297228329\n",
      "    ess            : 1.9636823412161977\n",
      "    log_marginal   : 1018.7689948153856\n",
      "    log_joint      : 1226.5538808544677\n",
      "    val_loss       : -1019.7706006920856\n",
      "    val_ess        : 1.962372873140418\n",
      "    val_log_marginal: 1019.7989555027174\n",
      "    val_log_joint  : 1227.2790155825408\n",
      "Train Epoch: 188 [0/101520 (0%)] Loss: -1016.329285\n",
      "Train Epoch: 188 [11264/101520 (11%)] Loss: -1015.835388\n",
      "Train Epoch: 188 [22528/101520 (22%)] Loss: -1023.317810\n",
      "Train Epoch: 188 [33792/101520 (33%)] Loss: -1020.963501\n",
      "Train Epoch: 188 [45056/101520 (44%)] Loss: -1021.821106\n",
      "Train Epoch: 188 [56320/101520 (55%)] Loss: -1016.507385\n",
      "Train Epoch: 188 [67584/101520 (67%)] Loss: -1022.446411\n",
      "Train Epoch: 188 [78848/101520 (78%)] Loss: -1020.306702\n",
      "Train Epoch: 188 [90112/101520 (89%)] Loss: -1022.033936\n",
      "Train Epoch: 188 [101376/101520 (100%)] Loss: -1044.906006\n",
      "    epoch          : 188\n",
      "    loss           : -1020.4031399674152\n",
      "    ess            : 1.9629817326464245\n",
      "    log_marginal   : 1020.4345016096106\n",
      "    log_joint      : 1228.2047370642274\n",
      "    val_loss       : -1021.0073905613111\n",
      "    val_ess        : 1.962278962135315\n",
      "    val_log_marginal: 1021.0401903235394\n",
      "    val_log_joint  : 1228.93090289572\n",
      "Train Epoch: 189 [0/101520 (0%)] Loss: -1021.329102\n",
      "Train Epoch: 189 [11264/101520 (11%)] Loss: -1014.851318\n",
      "Train Epoch: 189 [22528/101520 (22%)] Loss: -1027.158447\n",
      "Train Epoch: 189 [33792/101520 (33%)] Loss: -1019.128967\n",
      "Train Epoch: 189 [45056/101520 (44%)] Loss: -1018.232910\n",
      "Train Epoch: 189 [56320/101520 (55%)] Loss: -1023.947021\n",
      "Train Epoch: 189 [67584/101520 (67%)] Loss: -1018.657288\n",
      "Train Epoch: 189 [78848/101520 (78%)] Loss: -1017.572144\n",
      "Train Epoch: 189 [90112/101520 (89%)] Loss: -1018.141907\n",
      "Train Epoch: 189 [101376/101520 (100%)] Loss: -1022.384460\n",
      "    epoch          : 189\n",
      "    loss           : -1019.5064678863065\n",
      "    ess            : 1.963237981700418\n",
      "    log_marginal   : 1019.5375099373822\n",
      "    log_joint      : 1227.3452154571687\n",
      "    val_loss       : -1018.9144207498301\n",
      "    val_ess        : 1.962705855784209\n",
      "    val_log_marginal: 1018.9430037788723\n",
      "    val_log_joint  : 1226.5894350798233\n",
      "Train Epoch: 190 [0/101520 (0%)] Loss: -1013.928223\n",
      "Train Epoch: 190 [11264/101520 (11%)] Loss: -1021.669983\n",
      "Train Epoch: 190 [22528/101520 (22%)] Loss: -1026.231201\n",
      "Train Epoch: 190 [33792/101520 (33%)] Loss: -1021.246948\n",
      "Train Epoch: 190 [45056/101520 (44%)] Loss: -1020.860901\n",
      "Train Epoch: 190 [56320/101520 (55%)] Loss: -1017.293579\n",
      "Train Epoch: 190 [67584/101520 (67%)] Loss: -1020.613098\n",
      "Train Epoch: 190 [78848/101520 (78%)] Loss: -1023.164307\n",
      "Train Epoch: 190 [90112/101520 (89%)] Loss: -1028.707764\n",
      "Train Epoch: 190 [101376/101520 (100%)] Loss: -1012.441406\n",
      "    epoch          : 190\n",
      "    loss           : -1021.0532665156838\n",
      "    ess            : 1.9629637931459514\n",
      "    log_marginal   : 1021.0841202472322\n",
      "    log_joint      : 1228.8476807867462\n",
      "    val_loss       : -1022.3706266983696\n",
      "    val_ess        : 1.9627195026563562\n",
      "    val_log_marginal: 1022.4021606445312\n",
      "    val_log_joint  : 1230.113382090693\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch190.pth ...\n",
      "Train Epoch: 191 [0/101520 (0%)] Loss: -1021.427368\n",
      "Train Epoch: 191 [11264/101520 (11%)] Loss: -1022.731873\n",
      "Train Epoch: 191 [22528/101520 (22%)] Loss: -1008.823486\n",
      "Train Epoch: 191 [33792/101520 (33%)] Loss: -1019.866943\n",
      "Train Epoch: 191 [45056/101520 (44%)] Loss: -1016.567627\n",
      "Train Epoch: 191 [56320/101520 (55%)] Loss: -1019.062866\n",
      "Train Epoch: 191 [67584/101520 (67%)] Loss: -1021.215332\n",
      "Train Epoch: 191 [78848/101520 (78%)] Loss: -1023.283447\n",
      "Train Epoch: 191 [90112/101520 (89%)] Loss: -1021.717041\n",
      "Train Epoch: 191 [101376/101520 (100%)] Loss: -1034.911743\n",
      "    epoch          : 191\n",
      "    loss           : -1020.417647011915\n",
      "    ess            : 1.9633369529666613\n",
      "    log_marginal   : 1020.4480741107883\n",
      "    log_joint      : 1228.2464752964038\n",
      "    val_loss       : -1020.1086213485054\n",
      "    val_ess        : 1.9643578892168791\n",
      "    val_log_marginal: 1020.1370716924253\n",
      "    val_log_joint  : 1227.7802044412365\n",
      "Train Epoch: 192 [0/101520 (0%)] Loss: -1025.722412\n",
      "Train Epoch: 192 [11264/101520 (11%)] Loss: -1016.127197\n",
      "Train Epoch: 192 [22528/101520 (22%)] Loss: -1020.950684\n",
      "Train Epoch: 192 [33792/101520 (33%)] Loss: -1021.547363\n",
      "Train Epoch: 192 [45056/101520 (44%)] Loss: -1017.163147\n",
      "Train Epoch: 192 [56320/101520 (55%)] Loss: -1025.436646\n",
      "Train Epoch: 192 [67584/101520 (67%)] Loss: -1024.152344\n",
      "Train Epoch: 192 [78848/101520 (78%)] Loss: -1020.586792\n",
      "Train Epoch: 192 [90112/101520 (89%)] Loss: -1027.216553\n",
      "Train Epoch: 192 [101376/101520 (100%)] Loss: -1020.237610\n",
      "    epoch          : 192\n",
      "    loss           : -1021.581450342533\n",
      "    ess            : 1.9627369576363107\n",
      "    log_marginal   : 1021.6123540677017\n",
      "    log_joint      : 1229.4047225875472\n",
      "    val_loss       : -1023.1470098080842\n",
      "    val_ess        : 1.9630400408869204\n",
      "    val_log_marginal: 1023.179127568784\n",
      "    val_log_joint  : 1230.7220512058425\n",
      "Train Epoch: 193 [0/101520 (0%)] Loss: -1025.480225\n",
      "Train Epoch: 193 [11264/101520 (11%)] Loss: -1025.270264\n",
      "Train Epoch: 193 [22528/101520 (22%)] Loss: -1027.774902\n",
      "Train Epoch: 193 [33792/101520 (33%)] Loss: -1028.532959\n",
      "Train Epoch: 193 [45056/101520 (44%)] Loss: -1022.333374\n",
      "Train Epoch: 193 [56320/101520 (55%)] Loss: -1024.816406\n",
      "Train Epoch: 193 [67584/101520 (67%)] Loss: -1013.544861\n",
      "Train Epoch: 193 [78848/101520 (78%)] Loss: -1009.696350\n",
      "Train Epoch: 193 [90112/101520 (89%)] Loss: -1021.152405\n",
      "Train Epoch: 193 [101376/101520 (100%)] Loss: -1008.254333\n",
      "    epoch          : 193\n",
      "    loss           : -1020.9114275601642\n",
      "    ess            : 1.9630575347785375\n",
      "    log_marginal   : 1020.94276159852\n",
      "    log_joint      : 1228.6877950543735\n",
      "    val_loss       : -1020.526391070822\n",
      "    val_ess        : 1.9610968413560286\n",
      "    val_log_marginal: 1020.5577286430027\n",
      "    val_log_joint  : 1227.9727889351223\n",
      "Train Epoch: 194 [0/101520 (0%)] Loss: -1021.121582\n",
      "Train Epoch: 194 [11264/101520 (11%)] Loss: -1016.695312\n",
      "Train Epoch: 194 [22528/101520 (22%)] Loss: -1027.388672\n",
      "Train Epoch: 194 [33792/101520 (33%)] Loss: -1019.183594\n",
      "Train Epoch: 194 [45056/101520 (44%)] Loss: -1020.871826\n",
      "Train Epoch: 194 [56320/101520 (55%)] Loss: -1032.258179\n",
      "Train Epoch: 194 [67584/101520 (67%)] Loss: -1015.478394\n",
      "Train Epoch: 194 [78848/101520 (78%)] Loss: -1020.655640\n",
      "Train Epoch: 194 [90112/101520 (89%)] Loss: -1028.850098\n",
      "Train Epoch: 194 [101376/101520 (100%)] Loss: -1030.669312\n",
      "    epoch          : 194\n",
      "    loss           : -1022.2113046310654\n",
      "    ess            : 1.9632978199714393\n",
      "    log_marginal   : 1022.2422571230175\n",
      "    log_joint      : 1230.0204654866127\n",
      "    val_loss       : -1022.8142195991848\n",
      "    val_ess        : 1.9653944813686868\n",
      "    val_log_marginal: 1022.8436385444973\n",
      "    val_log_joint  : 1230.5812563688858\n",
      "Train Epoch: 195 [0/101520 (0%)] Loss: -1030.321289\n",
      "Train Epoch: 195 [11264/101520 (11%)] Loss: -1021.595459\n",
      "Train Epoch: 195 [22528/101520 (22%)] Loss: -1022.663635\n",
      "Train Epoch: 195 [33792/101520 (33%)] Loss: -1031.178711\n",
      "Train Epoch: 195 [45056/101520 (44%)] Loss: -1025.254395\n",
      "Train Epoch: 195 [56320/101520 (55%)] Loss: -1021.273376\n",
      "Train Epoch: 195 [67584/101520 (67%)] Loss: -1021.268799\n",
      "Train Epoch: 195 [78848/101520 (78%)] Loss: -1018.538330\n",
      "Train Epoch: 195 [90112/101520 (89%)] Loss: -1022.028687\n",
      "Train Epoch: 195 [101376/101520 (100%)] Loss: -1013.496521\n",
      "    epoch          : 195\n",
      "    loss           : -1021.3415754308653\n",
      "    ess            : 1.963209888443875\n",
      "    log_marginal   : 1021.3721782741834\n",
      "    log_joint      : 1229.1413586487124\n",
      "    val_loss       : -1021.30456808339\n",
      "    val_ess        : 1.963367197824561\n",
      "    val_log_marginal: 1021.3350909689199\n",
      "    val_log_joint  : 1229.0041185461957\n",
      "Train Epoch: 196 [0/101520 (0%)] Loss: -1023.372437\n",
      "Train Epoch: 196 [11264/101520 (11%)] Loss: -1026.181030\n",
      "Train Epoch: 196 [22528/101520 (22%)] Loss: -1026.534180\n",
      "Train Epoch: 196 [33792/101520 (33%)] Loss: -1018.292480\n",
      "Train Epoch: 196 [45056/101520 (44%)] Loss: -1019.188782\n",
      "Train Epoch: 196 [56320/101520 (55%)] Loss: -1022.168884\n",
      "Train Epoch: 196 [67584/101520 (67%)] Loss: -1016.506104\n",
      "Train Epoch: 196 [78848/101520 (78%)] Loss: -1019.030334\n",
      "Train Epoch: 196 [90112/101520 (89%)] Loss: -1027.841064\n",
      "Train Epoch: 196 [101376/101520 (100%)] Loss: -1021.091064\n",
      "    epoch          : 196\n",
      "    loss           : -1022.4844989105685\n",
      "    ess            : 1.9627104380621982\n",
      "    log_marginal   : 1022.5164328723697\n",
      "    log_joint      : 1230.2964842523163\n",
      "    val_loss       : -1024.0343893299932\n",
      "    val_ess        : 1.9602470190628716\n",
      "    val_log_marginal: 1024.0745610776155\n",
      "    val_log_joint  : 1231.7034275220788\n",
      "Train Epoch: 197 [0/101520 (0%)] Loss: -1032.431030\n",
      "Train Epoch: 197 [11264/101520 (11%)] Loss: -1023.015320\n",
      "Train Epoch: 197 [22528/101520 (22%)] Loss: -1016.717041\n",
      "Train Epoch: 197 [33792/101520 (33%)] Loss: -1027.461426\n",
      "Train Epoch: 197 [45056/101520 (44%)] Loss: -1017.497070\n",
      "Train Epoch: 197 [56320/101520 (55%)] Loss: -1027.681641\n",
      "Train Epoch: 197 [67584/101520 (67%)] Loss: -1025.984985\n",
      "Train Epoch: 197 [78848/101520 (78%)] Loss: -1025.135498\n",
      "Train Epoch: 197 [90112/101520 (89%)] Loss: -1016.862671\n",
      "Train Epoch: 197 [101376/101520 (100%)] Loss: -1029.915527\n",
      "    epoch          : 197\n",
      "    loss           : -1022.1690434594849\n",
      "    ess            : 1.9630981659769413\n",
      "    log_marginal   : 1022.1999428907232\n",
      "    log_joint      : 1230.0059654964275\n",
      "    val_loss       : -1020.2548642365829\n",
      "    val_ess        : 1.962781512218973\n",
      "    val_log_marginal: 1020.2857321034307\n",
      "    val_log_joint  : 1227.892482591712\n",
      "Train Epoch: 198 [0/101520 (0%)] Loss: -1022.623657\n",
      "Train Epoch: 198 [11264/101520 (11%)] Loss: -1017.195801\n",
      "Train Epoch: 198 [22528/101520 (22%)] Loss: -1028.462524\n",
      "Train Epoch: 198 [33792/101520 (33%)] Loss: -1027.149902\n",
      "Train Epoch: 198 [45056/101520 (44%)] Loss: -1024.150269\n",
      "Train Epoch: 198 [56320/101520 (55%)] Loss: -1021.988953\n",
      "Train Epoch: 198 [67584/101520 (67%)] Loss: -1024.000000\n",
      "Train Epoch: 198 [78848/101520 (78%)] Loss: -1030.099854\n",
      "Train Epoch: 198 [90112/101520 (89%)] Loss: -1031.314697\n",
      "Train Epoch: 198 [101376/101520 (100%)] Loss: -1019.106567\n",
      "    epoch          : 198\n",
      "    loss           : -1022.8736897377513\n",
      "    ess            : 1.9632314701176168\n",
      "    log_marginal   : 1022.9052952138622\n",
      "    log_joint      : 1230.671507562225\n",
      "    val_loss       : -1026.8349290930707\n",
      "    val_ess        : 1.9600992150928662\n",
      "    val_log_marginal: 1026.8709132982337\n",
      "    val_log_joint  : 1234.412592348845\n",
      "Train Epoch: 199 [0/101520 (0%)] Loss: -1026.490479\n",
      "Train Epoch: 199 [11264/101520 (11%)] Loss: -1028.317383\n",
      "Train Epoch: 199 [22528/101520 (22%)] Loss: -1019.226318\n",
      "Train Epoch: 199 [33792/101520 (33%)] Loss: -1024.676880\n",
      "Train Epoch: 199 [45056/101520 (44%)] Loss: -1016.203857\n",
      "Train Epoch: 199 [56320/101520 (55%)] Loss: -1024.822144\n",
      "Train Epoch: 199 [67584/101520 (67%)] Loss: -1027.061768\n",
      "Train Epoch: 199 [78848/101520 (78%)] Loss: -1023.477844\n",
      "Train Epoch: 199 [90112/101520 (89%)] Loss: -1022.958801\n",
      "Train Epoch: 199 [101376/101520 (100%)] Loss: -1019.874817\n",
      "    epoch          : 199\n",
      "    loss           : -1023.3543317785216\n",
      "    ess            : 1.961418064395387\n",
      "    log_marginal   : 1023.3866574081344\n",
      "    log_joint      : 1231.1927171256673\n",
      "    val_loss       : -1025.61450726053\n",
      "    val_ess        : 1.9623816894448323\n",
      "    val_log_marginal: 1025.646311884341\n",
      "    val_log_joint  : 1233.4077042289402\n",
      "Train Epoch: 200 [0/101520 (0%)] Loss: -1019.184021\n",
      "Train Epoch: 200 [11264/101520 (11%)] Loss: -1029.596680\n",
      "Train Epoch: 200 [22528/101520 (22%)] Loss: -1019.269470\n",
      "Train Epoch: 200 [33792/101520 (33%)] Loss: -1022.063232\n",
      "Train Epoch: 200 [45056/101520 (44%)] Loss: -1025.550537\n",
      "Train Epoch: 200 [56320/101520 (55%)] Loss: -1016.457886\n",
      "Train Epoch: 200 [67584/101520 (67%)] Loss: -1019.222412\n",
      "Train Epoch: 200 [78848/101520 (78%)] Loss: -1025.334229\n",
      "Train Epoch: 200 [90112/101520 (89%)] Loss: -1024.068726\n",
      "Train Epoch: 200 [101376/101520 (100%)] Loss: -997.684387\n",
      "    epoch          : 200\n",
      "    loss           : -1022.1412396454931\n",
      "    ess            : 1.9619302719681706\n",
      "    log_marginal   : 1022.1743719206384\n",
      "    log_joint      : 1229.9732555600267\n",
      "    val_loss       : -1021.7469933551291\n",
      "    val_ess        : 1.9673127402429995\n",
      "    val_log_marginal: 1021.7736789869225\n",
      "    val_log_joint  : 1229.5621390964675\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [0/101520 (0%)] Loss: -1022.619995\n",
      "Train Epoch: 201 [11264/101520 (11%)] Loss: -1027.762817\n",
      "Train Epoch: 201 [22528/101520 (22%)] Loss: -1026.697510\n",
      "Train Epoch: 201 [33792/101520 (33%)] Loss: -1025.123901\n",
      "Train Epoch: 201 [45056/101520 (44%)] Loss: -1017.015869\n",
      "Train Epoch: 201 [56320/101520 (55%)] Loss: -1023.049927\n",
      "Train Epoch: 201 [67584/101520 (67%)] Loss: -1024.212646\n",
      "Train Epoch: 201 [78848/101520 (78%)] Loss: -1026.604248\n",
      "Train Epoch: 201 [90112/101520 (89%)] Loss: -1027.171265\n",
      "Train Epoch: 201 [101376/101520 (100%)] Loss: -1036.648804\n",
      "    epoch          : 201\n",
      "    loss           : -1022.7104007586762\n",
      "    ess            : 1.9626255694346213\n",
      "    log_marginal   : 1022.7424773403148\n",
      "    log_joint      : 1230.4656503955323\n",
      "    val_loss       : -1024.3507425059443\n",
      "    val_ess        : 1.9628551576448523\n",
      "    val_log_marginal: 1024.3828257685122\n",
      "    val_log_joint  : 1231.7513905400815\n",
      "Train Epoch: 202 [0/101520 (0%)] Loss: -1028.791016\n",
      "Train Epoch: 202 [11264/101520 (11%)] Loss: -1023.038696\n",
      "Train Epoch: 202 [22528/101520 (22%)] Loss: -1021.105225\n",
      "Train Epoch: 202 [33792/101520 (33%)] Loss: -1022.226196\n",
      "Train Epoch: 202 [45056/101520 (44%)] Loss: -1029.963013\n",
      "Train Epoch: 202 [56320/101520 (55%)] Loss: -1022.872559\n",
      "Train Epoch: 202 [67584/101520 (67%)] Loss: -1022.544312\n",
      "Train Epoch: 202 [78848/101520 (78%)] Loss: -1018.721802\n",
      "Train Epoch: 202 [90112/101520 (89%)] Loss: -1029.744873\n",
      "Train Epoch: 202 [101376/101520 (100%)] Loss: -1030.302734\n",
      "    epoch          : 202\n",
      "    loss           : -1023.732050450004\n",
      "    ess            : 1.9617203120610223\n",
      "    log_marginal   : 1023.7647024183417\n",
      "    log_joint      : 1231.5793407957758\n",
      "    val_loss       : -1025.6759272036345\n",
      "    val_ess        : 1.962749719619751\n",
      "    val_log_marginal: 1025.707126783288\n",
      "    val_log_joint  : 1233.4672161599865\n",
      "Train Epoch: 203 [0/101520 (0%)] Loss: -1022.304565\n",
      "Train Epoch: 203 [11264/101520 (11%)] Loss: -1022.464355\n",
      "Train Epoch: 203 [22528/101520 (22%)] Loss: -1027.785156\n",
      "Train Epoch: 203 [33792/101520 (33%)] Loss: -1023.263306\n",
      "Train Epoch: 203 [45056/101520 (44%)] Loss: -1019.546143\n",
      "Train Epoch: 203 [56320/101520 (55%)] Loss: -1026.901611\n",
      "Train Epoch: 203 [67584/101520 (67%)] Loss: -1019.529663\n",
      "Train Epoch: 203 [78848/101520 (78%)] Loss: -1019.765991\n",
      "Train Epoch: 203 [90112/101520 (89%)] Loss: -1015.658325\n",
      "Train Epoch: 203 [101376/101520 (100%)] Loss: -1018.549133\n",
      "    epoch          : 203\n",
      "    loss           : -1021.4040591752708\n",
      "    ess            : 1.9626391742696714\n",
      "    log_marginal   : 1021.4359784150244\n",
      "    log_joint      : 1229.300642003965\n",
      "    val_loss       : -1020.6582668138587\n",
      "    val_ess        : 1.9634738486746084\n",
      "    val_log_marginal: 1020.6886304772419\n",
      "    val_log_joint  : 1228.6160570227582\n",
      "Train Epoch: 204 [0/101520 (0%)] Loss: -1019.115601\n",
      "Train Epoch: 204 [11264/101520 (11%)] Loss: -1020.966675\n",
      "Train Epoch: 204 [22528/101520 (22%)] Loss: -1021.254150\n",
      "Train Epoch: 204 [33792/101520 (33%)] Loss: -1022.341064\n",
      "Train Epoch: 204 [45056/101520 (44%)] Loss: -1020.353027\n",
      "Train Epoch: 204 [56320/101520 (55%)] Loss: -1019.320435\n",
      "Train Epoch: 204 [67584/101520 (67%)] Loss: -1026.246704\n",
      "Train Epoch: 204 [78848/101520 (78%)] Loss: -1022.501953\n",
      "Train Epoch: 204 [90112/101520 (89%)] Loss: -1023.007019\n",
      "Train Epoch: 204 [101376/101520 (100%)] Loss: -1019.075500\n",
      "    epoch          : 204\n",
      "    loss           : -1022.3372158644786\n",
      "    ess            : 1.962805380174263\n",
      "    log_marginal   : 1022.3682842925565\n",
      "    log_joint      : 1230.3581070636385\n",
      "    val_loss       : -1023.1527709960938\n",
      "    val_ess        : 1.9628048461416494\n",
      "    val_log_marginal: 1023.1820254118546\n",
      "    val_log_joint  : 1231.2084589419158\n",
      "Train Epoch: 205 [0/101520 (0%)] Loss: -1027.065308\n",
      "Train Epoch: 205 [11264/101520 (11%)] Loss: -1024.038574\n",
      "Train Epoch: 205 [22528/101520 (22%)] Loss: -1032.088013\n",
      "Train Epoch: 205 [33792/101520 (33%)] Loss: -1022.202454\n",
      "Train Epoch: 205 [45056/101520 (44%)] Loss: -1017.984985\n",
      "Train Epoch: 205 [56320/101520 (55%)] Loss: -1031.376221\n",
      "Train Epoch: 205 [67584/101520 (67%)] Loss: -1026.311035\n",
      "Train Epoch: 205 [78848/101520 (78%)] Loss: -1027.670166\n",
      "Train Epoch: 205 [90112/101520 (89%)] Loss: -1021.543884\n",
      "Train Epoch: 205 [101376/101520 (100%)] Loss: -1035.540161\n",
      "    epoch          : 205\n",
      "    loss           : -1023.7016546354821\n",
      "    ess            : 1.9626243360078515\n",
      "    log_marginal   : 1023.7335073193115\n",
      "    log_joint      : 1231.565529674741\n",
      "    val_loss       : -1023.0380063264266\n",
      "    val_ess        : 1.9616835169170215\n",
      "    val_log_marginal: 1023.0715491253396\n",
      "    val_log_joint  : 1230.8552405315897\n",
      "Train Epoch: 206 [0/101520 (0%)] Loss: -1025.440430\n",
      "Train Epoch: 206 [11264/101520 (11%)] Loss: -1019.885010\n",
      "Train Epoch: 206 [22528/101520 (22%)] Loss: -1025.609497\n",
      "Train Epoch: 206 [33792/101520 (33%)] Loss: -1028.694092\n",
      "Train Epoch: 206 [45056/101520 (44%)] Loss: -1031.005371\n",
      "Train Epoch: 206 [56320/101520 (55%)] Loss: -1024.919434\n",
      "Train Epoch: 206 [67584/101520 (67%)] Loss: -1029.852661\n",
      "Train Epoch: 206 [78848/101520 (78%)] Loss: -1027.254150\n",
      "Train Epoch: 206 [90112/101520 (89%)] Loss: -1025.603760\n",
      "Train Epoch: 206 [101376/101520 (100%)] Loss: -1023.177734\n",
      "    epoch          : 206\n",
      "    loss           : -1026.0024625691935\n",
      "    ess            : 1.9628269157217975\n",
      "    log_marginal   : 1026.034413400008\n",
      "    log_joint      : 1233.7760702928706\n",
      "    val_loss       : -1027.897877568784\n",
      "    val_ess        : 1.967659001765044\n",
      "    val_log_marginal: 1027.925263778023\n",
      "    val_log_joint  : 1235.84351116678\n",
      "Train Epoch: 207 [0/101520 (0%)] Loss: -1029.226074\n",
      "Train Epoch: 207 [11264/101520 (11%)] Loss: -1025.494629\n",
      "Train Epoch: 207 [22528/101520 (22%)] Loss: -1028.346680\n",
      "Train Epoch: 207 [33792/101520 (33%)] Loss: -1027.151123\n",
      "Train Epoch: 207 [45056/101520 (44%)] Loss: -1026.570679\n",
      "Train Epoch: 207 [56320/101520 (55%)] Loss: -1028.097412\n",
      "Train Epoch: 207 [67584/101520 (67%)] Loss: -1021.338318\n",
      "Train Epoch: 207 [78848/101520 (78%)] Loss: -1029.295776\n",
      "Train Epoch: 207 [90112/101520 (89%)] Loss: -1028.470947\n",
      "Train Epoch: 207 [101376/101520 (100%)] Loss: -1033.403931\n",
      "    epoch          : 207\n",
      "    loss           : -1024.859467626217\n",
      "    ess            : 1.9628683412494372\n",
      "    log_marginal   : 1024.8909675943194\n",
      "    log_joint      : 1232.763433255143\n",
      "    val_loss       : -1023.710597826087\n",
      "    val_ess        : 1.9627449978952822\n",
      "    val_log_marginal: 1023.7403723675271\n",
      "    val_log_joint  : 1231.7236965013587\n",
      "Train Epoch: 208 [0/101520 (0%)] Loss: -1014.775818\n",
      "Train Epoch: 208 [11264/101520 (11%)] Loss: -1017.349487\n",
      "Train Epoch: 208 [22528/101520 (22%)] Loss: -1025.818726\n",
      "Train Epoch: 208 [33792/101520 (33%)] Loss: -1025.085815\n",
      "Train Epoch: 208 [45056/101520 (44%)] Loss: -1021.182739\n",
      "Train Epoch: 208 [56320/101520 (55%)] Loss: -1033.256836\n",
      "Train Epoch: 208 [67584/101520 (67%)] Loss: -1025.738892\n",
      "Train Epoch: 208 [78848/101520 (78%)] Loss: -1023.012756\n",
      "Train Epoch: 208 [90112/101520 (89%)] Loss: -1021.704834\n",
      "Train Epoch: 208 [101376/101520 (100%)] Loss: -1036.471802\n",
      "    epoch          : 208\n",
      "    loss           : -1026.2438793086526\n",
      "    ess            : 1.9613400302340638\n",
      "    log_marginal   : 1026.2770993026656\n",
      "    log_joint      : 1234.1865847793656\n",
      "    val_loss       : -1027.901611328125\n",
      "    val_ess        : 1.9613303516222083\n",
      "    val_log_marginal: 1027.9332832668138\n",
      "    val_log_joint  : 1236.006443189538\n",
      "Train Epoch: 209 [0/101520 (0%)] Loss: -1032.351074\n",
      "Train Epoch: 209 [11264/101520 (11%)] Loss: -1024.402344\n",
      "Train Epoch: 209 [22528/101520 (22%)] Loss: -1021.137207\n",
      "Train Epoch: 209 [33792/101520 (33%)] Loss: -1017.169556\n",
      "Train Epoch: 209 [45056/101520 (44%)] Loss: -1018.622925\n",
      "Train Epoch: 209 [56320/101520 (55%)] Loss: -1021.007874\n",
      "Train Epoch: 209 [67584/101520 (67%)] Loss: -1025.677612\n",
      "Train Epoch: 209 [78848/101520 (78%)] Loss: -1026.744263\n",
      "Train Epoch: 209 [90112/101520 (89%)] Loss: -1018.897888\n",
      "Train Epoch: 209 [101376/101520 (100%)] Loss: -1009.949646\n",
      "    epoch          : 209\n",
      "    loss           : -1024.614507473893\n",
      "    ess            : 1.9623309835117666\n",
      "    log_marginal   : 1024.6461660107177\n",
      "    log_joint      : 1232.549017057946\n",
      "    val_loss       : -1022.3630636463995\n",
      "    val_ess        : 1.9637110077816506\n",
      "    val_log_marginal: 1022.3922357973845\n",
      "    val_log_joint  : 1230.1504171620245\n",
      "Train Epoch: 210 [0/101520 (0%)] Loss: -1030.355103\n",
      "Train Epoch: 210 [11264/101520 (11%)] Loss: -1025.108154\n",
      "Train Epoch: 210 [22528/101520 (22%)] Loss: -1027.263306\n",
      "Train Epoch: 210 [33792/101520 (33%)] Loss: -1025.327881\n",
      "Train Epoch: 210 [45056/101520 (44%)] Loss: -1029.148438\n",
      "Train Epoch: 210 [56320/101520 (55%)] Loss: -1029.164307\n",
      "Train Epoch: 210 [67584/101520 (67%)] Loss: -1033.382812\n",
      "Train Epoch: 210 [78848/101520 (78%)] Loss: -1031.248047\n",
      "Train Epoch: 210 [90112/101520 (89%)] Loss: -1027.496094\n",
      "Train Epoch: 210 [101376/101520 (100%)] Loss: -1028.506348\n",
      "    epoch          : 210\n",
      "    loss           : -1026.53527954715\n",
      "    ess            : 1.9621530711351327\n",
      "    log_marginal   : 1026.5680100330756\n",
      "    log_joint      : 1234.386659861809\n",
      "    val_loss       : -1029.1296121348505\n",
      "    val_ess        : 1.9615156028581702\n",
      "    val_log_marginal: 1029.1631788170855\n",
      "    val_log_joint  : 1236.9693762737772\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch210.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 211 [0/101520 (0%)] Loss: -1032.094238\n",
      "Train Epoch: 211 [11264/101520 (11%)] Loss: -1021.384521\n",
      "Train Epoch: 211 [22528/101520 (22%)] Loss: -1028.952759\n",
      "Train Epoch: 211 [33792/101520 (33%)] Loss: -1033.534058\n",
      "Train Epoch: 211 [45056/101520 (44%)] Loss: -1022.888306\n",
      "Train Epoch: 211 [56320/101520 (55%)] Loss: -1027.433105\n",
      "Train Epoch: 211 [67584/101520 (67%)] Loss: -1022.386230\n",
      "Train Epoch: 211 [78848/101520 (78%)] Loss: -1030.113525\n",
      "Train Epoch: 211 [90112/101520 (89%)] Loss: -1022.524719\n",
      "Train Epoch: 211 [101376/101520 (100%)] Loss: -1019.258484\n",
      "    epoch          : 211\n",
      "    loss           : -1025.555458260541\n",
      "    ess            : 1.9607172599389924\n",
      "    log_marginal   : 1025.5898520311518\n",
      "    log_joint      : 1233.380301777442\n",
      "    val_loss       : -1026.4908606487772\n",
      "    val_ess        : 1.9619738433672034\n",
      "    val_log_marginal: 1026.5230951723845\n",
      "    val_log_joint  : 1234.1759829313858\n",
      "Train Epoch: 212 [0/101520 (0%)] Loss: -1031.506714\n",
      "Train Epoch: 212 [11264/101520 (11%)] Loss: -1026.641846\n",
      "Train Epoch: 212 [22528/101520 (22%)] Loss: -1025.980347\n",
      "Train Epoch: 212 [33792/101520 (33%)] Loss: -1032.009277\n",
      "Train Epoch: 212 [45056/101520 (44%)] Loss: -1026.894531\n",
      "Train Epoch: 212 [56320/101520 (55%)] Loss: -1023.278442\n",
      "Train Epoch: 212 [67584/101520 (67%)] Loss: -1019.121460\n",
      "Train Epoch: 212 [78848/101520 (78%)] Loss: -1021.365540\n",
      "Train Epoch: 212 [90112/101520 (89%)] Loss: -1026.031494\n",
      "Train Epoch: 212 [101376/101520 (100%)] Loss: -1028.175537\n",
      "    epoch          : 212\n",
      "    loss           : -1027.424059077124\n",
      "    ess            : 1.9612020877138454\n",
      "    log_marginal   : 1027.4576676718555\n",
      "    log_joint      : 1235.253786633362\n",
      "    val_loss       : -1027.682760487432\n",
      "    val_ess        : 1.9612401361050813\n",
      "    val_log_marginal: 1027.7154142960258\n",
      "    val_log_joint  : 1235.6045346467392\n",
      "Train Epoch: 213 [0/101520 (0%)] Loss: -1029.727417\n",
      "Train Epoch: 213 [11264/101520 (11%)] Loss: -1028.471558\n",
      "Train Epoch: 213 [22528/101520 (22%)] Loss: -1026.861084\n",
      "Train Epoch: 213 [33792/101520 (33%)] Loss: -1029.602417\n",
      "Train Epoch: 213 [45056/101520 (44%)] Loss: -1020.467712\n",
      "Train Epoch: 213 [56320/101520 (55%)] Loss: -1023.901489\n",
      "Train Epoch: 213 [67584/101520 (67%)] Loss: -1030.697876\n",
      "Train Epoch: 213 [78848/101520 (78%)] Loss: -1023.298401\n",
      "Train Epoch: 213 [90112/101520 (89%)] Loss: -1024.888184\n",
      "Train Epoch: 213 [101376/101520 (100%)] Loss: -1020.734619\n",
      "    epoch          : 213\n",
      "    loss           : -1026.0725662001414\n",
      "    ess            : 1.961634092594511\n",
      "    log_marginal   : 1026.1057984625274\n",
      "    log_joint      : 1233.9856067446608\n",
      "    val_loss       : -1025.8691963527513\n",
      "    val_ess        : 1.9638994310213171\n",
      "    val_log_marginal: 1025.908696713655\n",
      "    val_log_joint  : 1233.6313582710598\n",
      "Train Epoch: 214 [0/101520 (0%)] Loss: -1029.626099\n",
      "Train Epoch: 214 [11264/101520 (11%)] Loss: -1030.900269\n",
      "Train Epoch: 214 [22528/101520 (22%)] Loss: -1028.252441\n",
      "Train Epoch: 214 [33792/101520 (33%)] Loss: -1027.416382\n",
      "Train Epoch: 214 [45056/101520 (44%)] Loss: -1024.953491\n",
      "Train Epoch: 214 [56320/101520 (55%)] Loss: -1028.328003\n",
      "Train Epoch: 214 [67584/101520 (67%)] Loss: -1034.190186\n",
      "Train Epoch: 214 [78848/101520 (78%)] Loss: -1028.222168\n",
      "Train Epoch: 214 [90112/101520 (89%)] Loss: -1030.713989\n",
      "Train Epoch: 214 [101376/101520 (100%)] Loss: -1039.884766\n",
      "    epoch          : 214\n",
      "    loss           : -1027.9320068359375\n",
      "    ess            : 1.9617294372625687\n",
      "    log_marginal   : 1027.96442386493\n",
      "    log_joint      : 1235.803220815994\n",
      "    val_loss       : -1028.1402959408967\n",
      "    val_ess        : 1.9628309633420862\n",
      "    val_log_marginal: 1028.1708055579145\n",
      "    val_log_joint  : 1236.0028925356658\n",
      "Train Epoch: 215 [0/101520 (0%)] Loss: -1026.387695\n",
      "Train Epoch: 215 [11264/101520 (11%)] Loss: -1027.107422\n",
      "Train Epoch: 215 [22528/101520 (22%)] Loss: -1022.409363\n",
      "Train Epoch: 215 [33792/101520 (33%)] Loss: -1016.580444\n",
      "Train Epoch: 215 [45056/101520 (44%)] Loss: -1029.542480\n",
      "Train Epoch: 215 [56320/101520 (55%)] Loss: -1026.891357\n",
      "Train Epoch: 215 [67584/101520 (67%)] Loss: -1025.346191\n",
      "Train Epoch: 215 [78848/101520 (78%)] Loss: -1025.782959\n",
      "Train Epoch: 215 [90112/101520 (89%)] Loss: -1025.100220\n",
      "Train Epoch: 215 [101376/101520 (100%)] Loss: -1026.809937\n",
      "    epoch          : 215\n",
      "    loss           : -1026.9062656421756\n",
      "    ess            : 1.9623139635402354\n",
      "    log_marginal   : 1026.9391439619974\n",
      "    log_joint      : 1234.6929250745918\n",
      "    val_loss       : -1027.980731466542\n",
      "    val_ess        : 1.9632725197335947\n",
      "    val_log_marginal: 1028.0112198539402\n",
      "    val_log_joint  : 1236.0757685122283\n",
      "Train Epoch: 216 [0/101520 (0%)] Loss: -1029.384888\n",
      "Train Epoch: 216 [11264/101520 (11%)] Loss: -1025.892822\n",
      "Train Epoch: 216 [22528/101520 (22%)] Loss: -1027.008545\n",
      "Train Epoch: 216 [33792/101520 (33%)] Loss: -1025.826660\n",
      "Train Epoch: 216 [45056/101520 (44%)] Loss: -1032.399048\n",
      "Train Epoch: 216 [56320/101520 (55%)] Loss: -1020.665527\n",
      "Train Epoch: 216 [67584/101520 (67%)] Loss: -1022.036438\n",
      "Train Epoch: 216 [78848/101520 (78%)] Loss: -1029.772217\n",
      "Train Epoch: 216 [90112/101520 (89%)] Loss: -1032.418213\n",
      "Train Epoch: 216 [101376/101520 (100%)] Loss: -1021.131531\n",
      "    epoch          : 216\n",
      "    loss           : -1028.4969522294089\n",
      "    ess            : 1.9612598227496123\n",
      "    log_marginal   : 1028.5302522745565\n",
      "    log_joint      : 1236.3514404296875\n",
      "    val_loss       : -1028.3307495117188\n",
      "    val_ess        : 1.9590648464534595\n",
      "    val_log_marginal: 1028.3663489300272\n",
      "    val_log_joint  : 1236.026468028193\n",
      "Train Epoch: 217 [0/101520 (0%)] Loss: -1029.564819\n",
      "Train Epoch: 217 [11264/101520 (11%)] Loss: -1034.874512\n",
      "Train Epoch: 217 [22528/101520 (22%)] Loss: -1023.638245\n",
      "Train Epoch: 217 [33792/101520 (33%)] Loss: -1027.696411\n",
      "Train Epoch: 217 [45056/101520 (44%)] Loss: -1025.693604\n",
      "Train Epoch: 217 [56320/101520 (55%)] Loss: -1030.395508\n",
      "Train Epoch: 217 [67584/101520 (67%)] Loss: -1029.306152\n",
      "Train Epoch: 217 [78848/101520 (78%)] Loss: -1018.633545\n",
      "Train Epoch: 217 [90112/101520 (89%)] Loss: -1031.202393\n",
      "Train Epoch: 217 [101376/101520 (100%)] Loss: -1021.410828\n",
      "    epoch          : 217\n",
      "    loss           : -1027.3577929932867\n",
      "    ess            : 1.9626595189223937\n",
      "    log_marginal   : 1027.390008514251\n",
      "    log_joint      : 1235.260752615617\n",
      "    val_loss       : -1026.1258969514267\n",
      "    val_ess        : 1.9638041931649912\n",
      "    val_log_marginal: 1026.1550372579823\n",
      "    val_log_joint  : 1234.0102698284647\n",
      "Train Epoch: 218 [0/101520 (0%)] Loss: -1028.850342\n",
      "Train Epoch: 218 [11264/101520 (11%)] Loss: -1024.414185\n",
      "Train Epoch: 218 [22528/101520 (22%)] Loss: -1030.066406\n",
      "Train Epoch: 218 [33792/101520 (33%)] Loss: -1032.464844\n",
      "Train Epoch: 218 [45056/101520 (44%)] Loss: -1026.816895\n",
      "Train Epoch: 218 [56320/101520 (55%)] Loss: -1029.812012\n",
      "Train Epoch: 218 [67584/101520 (67%)] Loss: -1028.380371\n",
      "Train Epoch: 218 [78848/101520 (78%)] Loss: -1034.142334\n",
      "Train Epoch: 218 [90112/101520 (89%)] Loss: -1028.226074\n",
      "Train Epoch: 218 [101376/101520 (100%)] Loss: -1036.876099\n",
      "    epoch          : 218\n",
      "    loss           : -1029.0658925118757\n",
      "    ess            : 1.9619084416921415\n",
      "    log_marginal   : 1029.098937681572\n",
      "    log_joint      : 1236.9321350404366\n",
      "    val_loss       : -1032.0773368503737\n",
      "    val_ess        : 1.9633312743643057\n",
      "    val_log_marginal: 1032.1084541652513\n",
      "    val_log_joint  : 1239.6490053923233\n",
      "Train Epoch: 219 [0/101520 (0%)] Loss: -1026.113037\n",
      "Train Epoch: 219 [11264/101520 (11%)] Loss: -1027.443115\n",
      "Train Epoch: 219 [22528/101520 (22%)] Loss: -1030.798218\n",
      "Train Epoch: 219 [33792/101520 (33%)] Loss: -1028.339722\n",
      "Train Epoch: 219 [45056/101520 (44%)] Loss: -1038.863281\n",
      "Train Epoch: 219 [56320/101520 (55%)] Loss: -1030.443359\n",
      "Train Epoch: 219 [67584/101520 (67%)] Loss: -1031.405151\n",
      "Train Epoch: 219 [78848/101520 (78%)] Loss: -1024.518921\n",
      "Train Epoch: 219 [90112/101520 (89%)] Loss: -1028.059326\n",
      "Train Epoch: 219 [101376/101520 (100%)] Loss: -1026.549805\n",
      "    epoch          : 219\n",
      "    loss           : -1027.8013600105016\n",
      "    ess            : 1.961362476923957\n",
      "    log_marginal   : 1027.8351382154915\n",
      "    log_joint      : 1235.7305337723776\n",
      "    val_loss       : -1026.7418345575747\n",
      "    val_ess        : 1.9622715245122495\n",
      "    val_log_marginal: 1026.773341966712\n",
      "    val_log_joint  : 1234.7200768512228\n",
      "Train Epoch: 220 [0/101520 (0%)] Loss: -1032.110962\n",
      "Train Epoch: 220 [11264/101520 (11%)] Loss: -1034.725708\n",
      "Train Epoch: 220 [22528/101520 (22%)] Loss: -1024.550293\n",
      "Train Epoch: 220 [33792/101520 (33%)] Loss: -1029.758545\n",
      "Train Epoch: 220 [45056/101520 (44%)] Loss: -1030.317505\n",
      "Train Epoch: 220 [56320/101520 (55%)] Loss: -1032.994995\n",
      "Train Epoch: 220 [67584/101520 (67%)] Loss: -1033.414795\n",
      "Train Epoch: 220 [78848/101520 (78%)] Loss: -1025.058594\n",
      "Train Epoch: 220 [90112/101520 (89%)] Loss: -1034.020020\n",
      "Train Epoch: 220 [101376/101520 (100%)] Loss: -1031.011597\n",
      "    epoch          : 220\n",
      "    loss           : -1029.4163744749137\n",
      "    ess            : 1.9622611220757566\n",
      "    log_marginal   : 1029.448245868012\n",
      "    log_joint      : 1237.329749946019\n",
      "    val_loss       : -1029.6531690514605\n",
      "    val_ess        : 1.9637228146843289\n",
      "    val_log_marginal: 1029.6854433806045\n",
      "    val_log_joint  : 1237.3827381963315\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch220.pth ...\n",
      "Train Epoch: 221 [0/101520 (0%)] Loss: -1032.544434\n",
      "Train Epoch: 221 [11264/101520 (11%)] Loss: -1039.546143\n",
      "Train Epoch: 221 [22528/101520 (22%)] Loss: -1031.727783\n",
      "Train Epoch: 221 [33792/101520 (33%)] Loss: -1026.382324\n",
      "Train Epoch: 221 [45056/101520 (44%)] Loss: -1022.469788\n",
      "Train Epoch: 221 [56320/101520 (55%)] Loss: -1029.404907\n",
      "Train Epoch: 221 [67584/101520 (67%)] Loss: -1029.338257\n",
      "Train Epoch: 221 [78848/101520 (78%)] Loss: -1031.686035\n",
      "Train Epoch: 221 [90112/101520 (89%)] Loss: -1019.772583\n",
      "Train Epoch: 221 [101376/101520 (100%)] Loss: -1033.401489\n",
      "    epoch          : 221\n",
      "    loss           : -1028.5249768741166\n",
      "    ess            : 1.9617701553220126\n",
      "    log_marginal   : 1028.5583520630496\n",
      "    log_joint      : 1236.361142872566\n",
      "    val_loss       : -1029.1196129840353\n",
      "    val_ess        : 1.9636024651320085\n",
      "    val_log_marginal: 1029.148633873981\n",
      "    val_log_joint  : 1236.8579313858695\n",
      "Train Epoch: 222 [0/101520 (0%)] Loss: -1032.357544\n",
      "Train Epoch: 222 [11264/101520 (11%)] Loss: -1030.443726\n",
      "Train Epoch: 222 [22528/101520 (22%)] Loss: -1033.648315\n",
      "Train Epoch: 222 [33792/101520 (33%)] Loss: -1030.520996\n",
      "Train Epoch: 222 [45056/101520 (44%)] Loss: -1031.128784\n",
      "Train Epoch: 222 [56320/101520 (55%)] Loss: -1034.671753\n",
      "Train Epoch: 222 [67584/101520 (67%)] Loss: -1027.735229\n",
      "Train Epoch: 222 [78848/101520 (78%)] Loss: -1034.548950\n",
      "Train Epoch: 222 [90112/101520 (89%)] Loss: -1031.810059\n",
      "Train Epoch: 222 [101376/101520 (100%)] Loss: -1025.577148\n",
      "    epoch          : 222\n",
      "    loss           : -1030.0803489493364\n",
      "    ess            : 1.9620194614832125\n",
      "    log_marginal   : 1030.1132140806571\n",
      "    log_joint      : 1237.9474067112908\n",
      "    val_loss       : -1033.5778171705163\n",
      "    val_ess        : 1.9610140790110049\n",
      "    val_log_marginal: 1033.6141516644022\n",
      "    val_log_joint  : 1241.2017928413723\n",
      "Train Epoch: 223 [0/101520 (0%)] Loss: -1038.196533\n",
      "Train Epoch: 223 [11264/101520 (11%)] Loss: -1034.533936\n",
      "Train Epoch: 223 [22528/101520 (22%)] Loss: -1025.827393\n",
      "Train Epoch: 223 [33792/101520 (33%)] Loss: -1024.817627\n",
      "Train Epoch: 223 [45056/101520 (44%)] Loss: -1027.517822\n",
      "Train Epoch: 223 [56320/101520 (55%)] Loss: -1028.461060\n",
      "Train Epoch: 223 [67584/101520 (67%)] Loss: -1035.406006\n",
      "Train Epoch: 223 [78848/101520 (78%)] Loss: -1025.103271\n",
      "Train Epoch: 223 [90112/101520 (89%)] Loss: -1029.624023\n",
      "Train Epoch: 223 [101376/101520 (100%)] Loss: -1032.039795\n",
      "    epoch          : 223\n",
      "    loss           : -1029.044521619327\n",
      "    ess            : 1.9607998432226517\n",
      "    log_marginal   : 1029.0788549682004\n",
      "    log_joint      : 1236.968826677332\n",
      "    val_loss       : -1029.2106057871943\n",
      "    val_ess        : 1.9617449770803037\n",
      "    val_log_marginal: 1029.2436629585598\n",
      "    val_log_joint  : 1236.999755859375\n",
      "Train Epoch: 224 [0/101520 (0%)] Loss: -1028.363159\n",
      "Train Epoch: 224 [11264/101520 (11%)] Loss: -1028.896484\n",
      "Train Epoch: 224 [22528/101520 (22%)] Loss: -1036.455566\n",
      "Train Epoch: 224 [33792/101520 (33%)] Loss: -1031.491699\n",
      "Train Epoch: 224 [45056/101520 (44%)] Loss: -1021.039185\n",
      "Train Epoch: 224 [56320/101520 (55%)] Loss: -1031.280518\n",
      "Train Epoch: 224 [67584/101520 (67%)] Loss: -1036.256348\n",
      "Train Epoch: 224 [78848/101520 (78%)] Loss: -1029.317139\n",
      "Train Epoch: 224 [90112/101520 (89%)] Loss: -1031.333740\n",
      "Train Epoch: 224 [101376/101520 (100%)] Loss: -1038.632690\n",
      "    epoch          : 224\n",
      "    loss           : -1030.6373018044324\n",
      "    ess            : 1.9611435260005932\n",
      "    log_marginal   : 1030.671515843377\n",
      "    log_joint      : 1238.5514308603565\n",
      "    val_loss       : -1032.0965894616168\n",
      "    val_ess        : 1.9616308056789895\n",
      "    val_log_marginal: 1032.1304029381793\n",
      "    val_log_joint  : 1239.7704918902853\n",
      "Train Epoch: 225 [0/101520 (0%)] Loss: -1037.747314\n",
      "Train Epoch: 225 [11264/101520 (11%)] Loss: -1028.732422\n",
      "Train Epoch: 225 [22528/101520 (22%)] Loss: -1030.963257\n",
      "Train Epoch: 225 [33792/101520 (33%)] Loss: -1027.980103\n",
      "Train Epoch: 225 [45056/101520 (44%)] Loss: -1034.410889\n",
      "Train Epoch: 225 [56320/101520 (55%)] Loss: -1024.585205\n",
      "Train Epoch: 225 [67584/101520 (67%)] Loss: -1025.117432\n",
      "Train Epoch: 225 [78848/101520 (78%)] Loss: -1035.303467\n",
      "Train Epoch: 225 [90112/101520 (89%)] Loss: -1024.765869\n",
      "Train Epoch: 225 [101376/101520 (100%)] Loss: -1027.211792\n",
      "    epoch          : 225\n",
      "    loss           : -1029.761799107844\n",
      "    ess            : 1.9622255797362207\n",
      "    log_marginal   : 1029.7939719962114\n",
      "    log_joint      : 1237.700325357255\n",
      "    val_loss       : -1030.7609704059103\n",
      "    val_ess        : 1.9621498429256936\n",
      "    val_log_marginal: 1030.7921514096467\n",
      "    val_log_joint  : 1238.6490000849185\n",
      "Train Epoch: 226 [0/101520 (0%)] Loss: -1027.613281\n",
      "Train Epoch: 226 [11264/101520 (11%)] Loss: -1026.951782\n",
      "Train Epoch: 226 [22528/101520 (22%)] Loss: -1033.444702\n",
      "Train Epoch: 226 [33792/101520 (33%)] Loss: -1032.073853\n",
      "Train Epoch: 226 [45056/101520 (44%)] Loss: -1033.578369\n",
      "Train Epoch: 226 [56320/101520 (55%)] Loss: -1026.012939\n",
      "Train Epoch: 226 [67584/101520 (67%)] Loss: -1032.318115\n",
      "Train Epoch: 226 [78848/101520 (78%)] Loss: -1030.236084\n",
      "Train Epoch: 226 [90112/101520 (89%)] Loss: -1026.649658\n",
      "Train Epoch: 226 [101376/101520 (100%)] Loss: -1021.469543\n",
      "    epoch          : 226\n",
      "    loss           : -1031.2082844643137\n",
      "    ess            : 1.9620833432854121\n",
      "    log_marginal   : 1031.2412863879947\n",
      "    log_joint      : 1239.0745696254712\n",
      "    val_loss       : -1031.6173652980638\n",
      "    val_ess        : 1.9633397641389265\n",
      "    val_log_marginal: 1031.6487612516983\n",
      "    val_log_joint  : 1239.415283203125\n",
      "Train Epoch: 227 [0/101520 (0%)] Loss: -1033.244629\n",
      "Train Epoch: 227 [11264/101520 (11%)] Loss: -1035.554932\n",
      "Train Epoch: 227 [22528/101520 (22%)] Loss: -1038.480469\n",
      "Train Epoch: 227 [33792/101520 (33%)] Loss: -1033.725464\n",
      "Train Epoch: 227 [45056/101520 (44%)] Loss: -1031.068604\n",
      "Train Epoch: 227 [56320/101520 (55%)] Loss: -1025.586426\n",
      "Train Epoch: 227 [67584/101520 (67%)] Loss: -1028.751099\n",
      "Train Epoch: 227 [78848/101520 (78%)] Loss: -1026.432373\n",
      "Train Epoch: 227 [90112/101520 (89%)] Loss: -1026.149170\n",
      "Train Epoch: 227 [101376/101520 (100%)] Loss: -1021.488403\n",
      "    epoch          : 227\n",
      "    loss           : -1030.19242391155\n",
      "    ess            : 1.9611806090752684\n",
      "    log_marginal   : 1030.2264401229781\n",
      "    log_joint      : 1238.0823569753063\n",
      "    val_loss       : -1029.8987400220788\n",
      "    val_ess        : 1.9609413768934167\n",
      "    val_log_marginal: 1029.932914402174\n",
      "    val_log_joint  : 1237.8384001358695\n",
      "Train Epoch: 228 [0/101520 (0%)] Loss: -1027.889648\n",
      "Train Epoch: 228 [11264/101520 (11%)] Loss: -1042.501465\n",
      "Train Epoch: 228 [22528/101520 (22%)] Loss: -1030.876465\n",
      "Train Epoch: 228 [33792/101520 (33%)] Loss: -1034.644287\n",
      "Train Epoch: 228 [45056/101520 (44%)] Loss: -1042.348633\n",
      "Train Epoch: 228 [56320/101520 (55%)] Loss: -1032.305176\n",
      "Train Epoch: 228 [67584/101520 (67%)] Loss: -1030.114380\n",
      "Train Epoch: 228 [78848/101520 (78%)] Loss: -1027.676025\n",
      "Train Epoch: 228 [90112/101520 (89%)] Loss: -1029.477173\n",
      "Train Epoch: 228 [101376/101520 (100%)] Loss: -1034.787842\n",
      "    epoch          : 228\n",
      "    loss           : -1031.5318689394237\n",
      "    ess            : 1.9621524319576857\n",
      "    log_marginal   : 1031.5639749651577\n",
      "    log_joint      : 1239.469356057632\n",
      "    val_loss       : -1033.421392026155\n",
      "    val_ess        : 1.9633783672166907\n",
      "    val_log_marginal: 1033.4500307829483\n",
      "    val_log_joint  : 1241.0469493036685\n",
      "Train Epoch: 229 [0/101520 (0%)] Loss: -1028.452393\n",
      "Train Epoch: 229 [11264/101520 (11%)] Loss: -1029.972412\n",
      "Train Epoch: 229 [22528/101520 (22%)] Loss: -1030.463623\n",
      "Train Epoch: 229 [33792/101520 (33%)] Loss: -1035.497192\n",
      "Train Epoch: 229 [45056/101520 (44%)] Loss: -1026.841187\n",
      "Train Epoch: 229 [56320/101520 (55%)] Loss: -1026.498047\n",
      "Train Epoch: 229 [67584/101520 (67%)] Loss: -1034.986694\n",
      "Train Epoch: 229 [78848/101520 (78%)] Loss: -1024.314819\n",
      "Train Epoch: 229 [90112/101520 (89%)] Loss: -1020.291016\n",
      "Train Epoch: 229 [101376/101520 (100%)] Loss: -1029.707642\n",
      "    epoch          : 229\n",
      "    loss           : -1030.7311545520572\n",
      "    ess            : 1.9612927993937352\n",
      "    log_marginal   : 1030.7643021626689\n",
      "    log_joint      : 1238.6180843180748\n",
      "    val_loss       : -1031.058004628057\n",
      "    val_ess        : 1.95790331778319\n",
      "    val_log_marginal: 1031.0944373089335\n",
      "    val_log_joint  : 1239.1146240234375\n",
      "Train Epoch: 230 [0/101520 (0%)] Loss: -1037.402588\n",
      "Train Epoch: 230 [11264/101520 (11%)] Loss: -1034.372070\n",
      "Train Epoch: 230 [22528/101520 (22%)] Loss: -1032.139648\n",
      "Train Epoch: 230 [33792/101520 (33%)] Loss: -1034.822510\n",
      "Train Epoch: 230 [45056/101520 (44%)] Loss: -1029.256592\n",
      "Train Epoch: 230 [56320/101520 (55%)] Loss: -1029.387817\n",
      "Train Epoch: 230 [67584/101520 (67%)] Loss: -1029.221680\n",
      "Train Epoch: 230 [78848/101520 (78%)] Loss: -1037.258789\n",
      "Train Epoch: 230 [90112/101520 (89%)] Loss: -1028.779541\n",
      "Train Epoch: 230 [101376/101520 (100%)] Loss: -1029.451416\n",
      "    epoch          : 230\n",
      "    loss           : -1032.0686804996662\n",
      "    ess            : 1.9614404733456559\n",
      "    log_marginal   : 1032.1020308451436\n",
      "    log_joint      : 1239.9682629455874\n",
      "    val_loss       : -1033.7804061226223\n",
      "    val_ess        : 1.9628316319507102\n",
      "    val_log_marginal: 1033.8125106148098\n",
      "    val_log_joint  : 1241.6258014181385\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch230.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 231 [0/101520 (0%)] Loss: -1030.082764\n",
      "Train Epoch: 231 [11264/101520 (11%)] Loss: -1027.597046\n",
      "Train Epoch: 231 [22528/101520 (22%)] Loss: -1029.820068\n",
      "Train Epoch: 231 [33792/101520 (33%)] Loss: -1033.954834\n",
      "Train Epoch: 231 [45056/101520 (44%)] Loss: -1029.691895\n",
      "Train Epoch: 231 [56320/101520 (55%)] Loss: -1036.710693\n",
      "Train Epoch: 231 [67584/101520 (67%)] Loss: -1031.892212\n",
      "Train Epoch: 231 [78848/101520 (78%)] Loss: -1028.882080\n",
      "Train Epoch: 231 [90112/101520 (89%)] Loss: -1030.051514\n",
      "Train Epoch: 231 [101376/101520 (100%)] Loss: -1024.716431\n",
      "    epoch          : 231\n",
      "    loss           : -1031.4278561386031\n",
      "    ess            : 1.9608837138468296\n",
      "    log_marginal   : 1031.462197461919\n",
      "    log_joint      : 1239.3422287217336\n",
      "    val_loss       : -1032.0036196501358\n",
      "    val_ess        : 1.9565333749936975\n",
      "    val_log_marginal: 1032.045654296875\n",
      "    val_log_joint  : 1239.6160517153533\n",
      "Train Epoch: 232 [0/101520 (0%)] Loss: -1032.450928\n",
      "Train Epoch: 232 [11264/101520 (11%)] Loss: -1029.312500\n",
      "Train Epoch: 232 [22528/101520 (22%)] Loss: -1034.511475\n",
      "Train Epoch: 232 [33792/101520 (33%)] Loss: -1035.519531\n",
      "Train Epoch: 232 [45056/101520 (44%)] Loss: -1029.556030\n",
      "Train Epoch: 232 [56320/101520 (55%)] Loss: -1032.098877\n",
      "Train Epoch: 232 [67584/101520 (67%)] Loss: -1032.637695\n",
      "Train Epoch: 232 [78848/101520 (78%)] Loss: -1029.458008\n",
      "Train Epoch: 232 [90112/101520 (89%)] Loss: -1029.602417\n",
      "Train Epoch: 232 [101376/101520 (100%)] Loss: -1037.881714\n",
      "    epoch          : 232\n",
      "    loss           : -1032.6571403771788\n",
      "    ess            : 1.9609236339827878\n",
      "    log_marginal   : 1032.6913311062146\n",
      "    log_joint      : 1240.5258960819724\n",
      "    val_loss       : -1034.284519361413\n",
      "    val_ess        : 1.9638322332630986\n",
      "    val_log_marginal: 1034.3160028872283\n",
      "    val_log_joint  : 1242.171392026155\n",
      "Train Epoch: 233 [0/101520 (0%)] Loss: -1033.577271\n",
      "Train Epoch: 233 [11264/101520 (11%)] Loss: -1033.808838\n",
      "Train Epoch: 233 [22528/101520 (22%)] Loss: -1034.209839\n",
      "Train Epoch: 233 [33792/101520 (33%)] Loss: -1030.879028\n",
      "Train Epoch: 233 [45056/101520 (44%)] Loss: -1030.251709\n",
      "Train Epoch: 233 [56320/101520 (55%)] Loss: -1028.594849\n",
      "Train Epoch: 233 [67584/101520 (67%)] Loss: -1030.179443\n",
      "Train Epoch: 233 [78848/101520 (78%)] Loss: -1035.875488\n",
      "Train Epoch: 233 [90112/101520 (89%)] Loss: -1026.028076\n",
      "Train Epoch: 233 [101376/101520 (100%)] Loss: -1025.588501\n",
      "    epoch          : 233\n",
      "    loss           : -1031.8749773035097\n",
      "    ess            : 1.961471067601113\n",
      "    log_marginal   : 1031.9077246584484\n",
      "    log_joint      : 1239.7608832737908\n",
      "    val_loss       : -1033.1585374915082\n",
      "    val_ess        : 1.9592748051104338\n",
      "    val_log_marginal: 1033.194192637568\n",
      "    val_log_joint  : 1240.6602783203125\n",
      "Train Epoch: 234 [0/101520 (0%)] Loss: -1028.319458\n",
      "Train Epoch: 234 [11264/101520 (11%)] Loss: -1032.886475\n",
      "Train Epoch: 234 [22528/101520 (22%)] Loss: -1041.048584\n",
      "Train Epoch: 234 [33792/101520 (33%)] Loss: -1036.235840\n",
      "Train Epoch: 234 [45056/101520 (44%)] Loss: -1034.469360\n",
      "Train Epoch: 234 [56320/101520 (55%)] Loss: -1033.234619\n",
      "Train Epoch: 234 [67584/101520 (67%)] Loss: -1040.570068\n",
      "Train Epoch: 234 [78848/101520 (78%)] Loss: -1028.005493\n",
      "Train Epoch: 234 [90112/101520 (89%)] Loss: -1034.126465\n",
      "Train Epoch: 234 [101376/101520 (100%)] Loss: -1031.268433\n",
      "    epoch          : 234\n",
      "    loss           : -1032.9046075715491\n",
      "    ess            : 1.9621983467034958\n",
      "    log_marginal   : 1032.937164153286\n",
      "    log_joint      : 1240.83304891155\n",
      "    val_loss       : -1034.3962296195652\n",
      "    val_ess        : 1.9617501652759055\n",
      "    val_log_marginal: 1034.4299687924592\n",
      "    val_log_joint  : 1242.1570938773777\n",
      "Train Epoch: 235 [0/101520 (0%)] Loss: -1030.279297\n",
      "Train Epoch: 235 [11264/101520 (11%)] Loss: -1033.527588\n",
      "Train Epoch: 235 [22528/101520 (22%)] Loss: -1035.642334\n",
      "Train Epoch: 235 [33792/101520 (33%)] Loss: -1034.268433\n",
      "Train Epoch: 235 [45056/101520 (44%)] Loss: -1033.493896\n",
      "Train Epoch: 235 [56320/101520 (55%)] Loss: -1038.188110\n",
      "Train Epoch: 235 [67584/101520 (67%)] Loss: -1029.685547\n",
      "Train Epoch: 235 [78848/101520 (78%)] Loss: -1029.410889\n",
      "Train Epoch: 235 [90112/101520 (89%)] Loss: -1029.930664\n",
      "Train Epoch: 235 [101376/101520 (100%)] Loss: -1036.175171\n",
      "    epoch          : 235\n",
      "    loss           : -1032.295934015782\n",
      "    ess            : 1.9622782918077017\n",
      "    log_marginal   : 1032.3279317731235\n",
      "    log_joint      : 1240.1521732196136\n",
      "    val_loss       : -1032.955025050951\n",
      "    val_ess        : 1.9619481200757234\n",
      "    val_log_marginal: 1032.986840289572\n",
      "    val_log_joint  : 1240.6621465268342\n",
      "Train Epoch: 236 [0/101520 (0%)] Loss: -1031.310059\n",
      "Train Epoch: 236 [11264/101520 (11%)] Loss: -1025.484741\n",
      "Train Epoch: 236 [22528/101520 (22%)] Loss: -1033.469482\n",
      "Train Epoch: 236 [33792/101520 (33%)] Loss: -1035.264648\n",
      "Train Epoch: 236 [45056/101520 (44%)] Loss: -1034.240356\n",
      "Train Epoch: 236 [56320/101520 (55%)] Loss: -1037.538940\n",
      "Train Epoch: 236 [67584/101520 (67%)] Loss: -1032.434082\n",
      "Train Epoch: 236 [78848/101520 (78%)] Loss: -1033.784424\n",
      "Train Epoch: 236 [90112/101520 (89%)] Loss: -1033.984253\n",
      "Train Epoch: 236 [101376/101520 (100%)] Loss: -1037.052856\n",
      "    epoch          : 236\n",
      "    loss           : -1033.3048384009894\n",
      "    ess            : 1.9616137407532888\n",
      "    log_marginal   : 1033.3382126697943\n",
      "    log_joint      : 1241.1584055531564\n",
      "    val_loss       : -1034.2659328294837\n",
      "    val_ess        : 1.9558583446170972\n",
      "    val_log_marginal: 1034.3055579144022\n",
      "    val_log_joint  : 1242.2915410580842\n",
      "Train Epoch: 237 [0/101520 (0%)] Loss: -1034.978027\n",
      "Train Epoch: 237 [11264/101520 (11%)] Loss: -1027.594360\n",
      "Train Epoch: 237 [22528/101520 (22%)] Loss: -1031.565796\n",
      "Train Epoch: 237 [33792/101520 (33%)] Loss: -1032.442139\n",
      "Train Epoch: 237 [45056/101520 (44%)] Loss: -1035.062622\n",
      "Train Epoch: 237 [56320/101520 (55%)] Loss: -1035.155029\n",
      "Train Epoch: 237 [67584/101520 (67%)] Loss: -1035.663574\n",
      "Train Epoch: 237 [78848/101520 (78%)] Loss: -1037.223877\n",
      "Train Epoch: 237 [90112/101520 (89%)] Loss: -1026.823853\n",
      "Train Epoch: 237 [101376/101520 (100%)] Loss: -1042.953735\n",
      "    epoch          : 237\n",
      "    loss           : -1033.0523503749214\n",
      "    ess            : 1.9613226981618297\n",
      "    log_marginal   : 1033.085811135757\n",
      "    log_joint      : 1241.001840869386\n",
      "    val_loss       : -1033.9218006963315\n",
      "    val_ess        : 1.9621520457060442\n",
      "    val_log_marginal: 1033.9536212423573\n",
      "    val_log_joint  : 1241.945020592731\n",
      "Train Epoch: 238 [0/101520 (0%)] Loss: -1031.403809\n",
      "Train Epoch: 238 [11264/101520 (11%)] Loss: -1029.518066\n",
      "Train Epoch: 238 [22528/101520 (22%)] Loss: -1033.431641\n",
      "Train Epoch: 238 [33792/101520 (33%)] Loss: -1035.815674\n",
      "Train Epoch: 238 [45056/101520 (44%)] Loss: -1034.886353\n",
      "Train Epoch: 238 [56320/101520 (55%)] Loss: -1037.779785\n",
      "Train Epoch: 238 [67584/101520 (67%)] Loss: -1041.121582\n",
      "Train Epoch: 238 [78848/101520 (78%)] Loss: -1031.107788\n",
      "Train Epoch: 238 [90112/101520 (89%)] Loss: -1028.673584\n",
      "Train Epoch: 238 [101376/101520 (100%)] Loss: -1034.082642\n",
      "    epoch          : 238\n",
      "    loss           : -1033.530526779405\n",
      "    ess            : 1.960186056755296\n",
      "    log_marginal   : 1033.5652294063088\n",
      "    log_joint      : 1241.4005679029915\n",
      "    val_loss       : -1034.9307277513587\n",
      "    val_ess        : 1.9601483085881108\n",
      "    val_log_marginal: 1034.9655549422555\n",
      "    val_log_joint  : 1242.8541206691575\n",
      "Train Epoch: 239 [0/101520 (0%)] Loss: -1032.171387\n",
      "Train Epoch: 239 [11264/101520 (11%)] Loss: -1028.914062\n",
      "Train Epoch: 239 [22528/101520 (22%)] Loss: -1032.436401\n",
      "Train Epoch: 239 [33792/101520 (33%)] Loss: -1035.281372\n",
      "Train Epoch: 239 [45056/101520 (44%)] Loss: -1031.958740\n",
      "Train Epoch: 239 [56320/101520 (55%)] Loss: -1034.683838\n",
      "Train Epoch: 239 [67584/101520 (67%)] Loss: -1033.686646\n",
      "Train Epoch: 239 [78848/101520 (78%)] Loss: -1037.378174\n",
      "Train Epoch: 239 [90112/101520 (89%)] Loss: -1034.456299\n",
      "Train Epoch: 239 [101376/101520 (100%)] Loss: -1034.876099\n",
      "    epoch          : 239\n",
      "    loss           : -1034.0433239194017\n",
      "    ess            : 1.9609613652205347\n",
      "    log_marginal   : 1034.077059491795\n",
      "    log_joint      : 1241.942580946726\n",
      "    val_loss       : -1032.6129973038383\n",
      "    val_ess        : 1.9613688303076702\n",
      "    val_log_marginal: 1032.6462773862092\n",
      "    val_log_joint  : 1240.5826787533967\n",
      "Train Epoch: 240 [0/101520 (0%)] Loss: -1041.101562\n",
      "Train Epoch: 240 [11264/101520 (11%)] Loss: -1031.619873\n",
      "Train Epoch: 240 [22528/101520 (22%)] Loss: -1033.577393\n",
      "Train Epoch: 240 [33792/101520 (33%)] Loss: -1036.703735\n",
      "Train Epoch: 240 [45056/101520 (44%)] Loss: -1030.609985\n",
      "Train Epoch: 240 [56320/101520 (55%)] Loss: -1027.125488\n",
      "Train Epoch: 240 [67584/101520 (67%)] Loss: -1035.090332\n",
      "Train Epoch: 240 [78848/101520 (78%)] Loss: -1037.065430\n",
      "Train Epoch: 240 [90112/101520 (89%)] Loss: -1028.288574\n",
      "Train Epoch: 240 [101376/101520 (100%)] Loss: -1045.454468\n",
      "    epoch          : 240\n",
      "    loss           : -1033.24583366049\n",
      "    ess            : 1.9613247085456273\n",
      "    log_marginal   : 1033.2793235587114\n",
      "    log_joint      : 1241.208294892431\n",
      "    val_loss       : -1032.0098823879075\n",
      "    val_ess        : 1.9610437206600024\n",
      "    val_log_marginal: 1032.0438816236413\n",
      "    val_log_joint  : 1240.000297214674\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch240.pth ...\n",
      "Train Epoch: 241 [0/101520 (0%)] Loss: -1030.502686\n",
      "Train Epoch: 241 [11264/101520 (11%)] Loss: -1030.393433\n",
      "Train Epoch: 241 [22528/101520 (22%)] Loss: -1033.289307\n",
      "Train Epoch: 241 [33792/101520 (33%)] Loss: -1034.111816\n",
      "Train Epoch: 241 [45056/101520 (44%)] Loss: -1039.994385\n",
      "Train Epoch: 241 [56320/101520 (55%)] Loss: -1034.979736\n",
      "Train Epoch: 241 [67584/101520 (67%)] Loss: -1036.765991\n",
      "Train Epoch: 241 [78848/101520 (78%)] Loss: -1042.869141\n",
      "Train Epoch: 241 [90112/101520 (89%)] Loss: -1030.080078\n",
      "Train Epoch: 241 [101376/101520 (100%)] Loss: -1023.375000\n",
      "    epoch          : 241\n",
      "    loss           : -1033.8400400439698\n",
      "    ess            : 1.96109205394534\n",
      "    log_marginal   : 1033.8741574694764\n",
      "    log_joint      : 1241.738215614204\n",
      "    val_loss       : -1035.0603717306385\n",
      "    val_ess        : 1.9618111278699792\n",
      "    val_log_marginal: 1035.0931024966033\n",
      "    val_log_joint  : 1242.938773777174\n",
      "Train Epoch: 242 [0/101520 (0%)] Loss: -1034.142090\n",
      "Train Epoch: 242 [11264/101520 (11%)] Loss: -1035.709351\n",
      "Train Epoch: 242 [22528/101520 (22%)] Loss: -1038.707520\n",
      "Train Epoch: 242 [33792/101520 (33%)] Loss: -1029.966797\n",
      "Train Epoch: 242 [45056/101520 (44%)] Loss: -1040.293335\n",
      "Train Epoch: 242 [56320/101520 (55%)] Loss: -1034.386963\n",
      "Train Epoch: 242 [67584/101520 (67%)] Loss: -1037.050659\n",
      "Train Epoch: 242 [78848/101520 (78%)] Loss: -1029.521362\n",
      "Train Epoch: 242 [90112/101520 (89%)] Loss: -1036.169434\n",
      "Train Epoch: 242 [101376/101520 (100%)] Loss: -1029.302612\n",
      "    epoch          : 242\n",
      "    loss           : -1034.283835559634\n",
      "    ess            : 1.960586595774895\n",
      "    log_marginal   : 1034.3185722312735\n",
      "    log_joint      : 1242.247180727858\n",
      "    val_loss       : -1036.073098887568\n",
      "    val_ess        : 1.962130323700283\n",
      "    val_log_marginal: 1036.1053360648777\n",
      "    val_log_joint  : 1244.1632080078125\n",
      "Train Epoch: 243 [0/101520 (0%)] Loss: -1043.950928\n",
      "Train Epoch: 243 [11264/101520 (11%)] Loss: -1041.749756\n",
      "Train Epoch: 243 [22528/101520 (22%)] Loss: -1030.061035\n",
      "Train Epoch: 243 [33792/101520 (33%)] Loss: -1035.628418\n",
      "Train Epoch: 243 [45056/101520 (44%)] Loss: -1036.815918\n",
      "Train Epoch: 243 [56320/101520 (55%)] Loss: -1040.979492\n",
      "Train Epoch: 243 [67584/101520 (67%)] Loss: -1037.328247\n",
      "Train Epoch: 243 [78848/101520 (78%)] Loss: -1035.130859\n",
      "Train Epoch: 243 [90112/101520 (89%)] Loss: -1033.478027\n",
      "Train Epoch: 243 [101376/101520 (100%)] Loss: -1041.646729\n",
      "    epoch          : 243\n",
      "    loss           : -1032.778583162394\n",
      "    ess            : 1.9610492062928089\n",
      "    log_marginal   : 1032.8121905302883\n",
      "    log_joint      : 1240.78211246663\n",
      "    val_loss       : -1034.1685419497283\n",
      "    val_ess        : 1.9612724107244741\n",
      "    val_log_marginal: 1034.2046269955842\n",
      "    val_log_joint  : 1241.9338219684103\n",
      "Train Epoch: 244 [0/101520 (0%)] Loss: -1037.402954\n",
      "Train Epoch: 244 [11264/101520 (11%)] Loss: -1030.836426\n",
      "Train Epoch: 244 [22528/101520 (22%)] Loss: -1032.587646\n",
      "Train Epoch: 244 [33792/101520 (33%)] Loss: -1041.067627\n",
      "Train Epoch: 244 [45056/101520 (44%)] Loss: -1032.225098\n",
      "Train Epoch: 244 [56320/101520 (55%)] Loss: -1033.689453\n",
      "Train Epoch: 244 [67584/101520 (67%)] Loss: -1033.902466\n",
      "Train Epoch: 244 [78848/101520 (78%)] Loss: -1027.784546\n",
      "Train Epoch: 244 [90112/101520 (89%)] Loss: -1036.366577\n",
      "Train Epoch: 244 [101376/101520 (100%)] Loss: -1044.063110\n",
      "    epoch          : 244\n",
      "    loss           : -1033.8175337134894\n",
      "    ess            : 1.9616850668461479\n",
      "    log_marginal   : 1033.8511956756438\n",
      "    log_joint      : 1241.8573458110866\n",
      "    val_loss       : -1032.9913170855978\n",
      "    val_ess        : 1.962569641030353\n",
      "    val_log_marginal: 1033.0235489555027\n",
      "    val_log_joint  : 1241.0182680876358\n",
      "Train Epoch: 245 [0/101520 (0%)] Loss: -1030.605591\n",
      "Train Epoch: 245 [11264/101520 (11%)] Loss: -1034.917725\n",
      "Train Epoch: 245 [22528/101520 (22%)] Loss: -1034.250000\n",
      "Train Epoch: 245 [33792/101520 (33%)] Loss: -1037.272827\n",
      "Train Epoch: 245 [45056/101520 (44%)] Loss: -1036.510376\n",
      "Train Epoch: 245 [56320/101520 (55%)] Loss: -1039.259033\n",
      "Train Epoch: 245 [67584/101520 (67%)] Loss: -1030.526123\n",
      "Train Epoch: 245 [78848/101520 (78%)] Loss: -1026.841553\n",
      "Train Epoch: 245 [90112/101520 (89%)] Loss: -1027.537598\n",
      "Train Epoch: 245 [101376/101520 (100%)] Loss: -1035.185303\n",
      "    epoch          : 245\n",
      "    loss           : -1033.6954241421954\n",
      "    ess            : 1.9614124837233193\n",
      "    log_marginal   : 1033.728726027599\n",
      "    log_joint      : 1241.7152050535883\n",
      "    val_loss       : -1032.2812128481658\n",
      "    val_ess        : 1.9588954396869824\n",
      "    val_log_marginal: 1032.3201506241508\n",
      "    val_log_joint  : 1240.5134118121603\n",
      "Train Epoch: 246 [0/101520 (0%)] Loss: -1032.354858\n",
      "Train Epoch: 246 [11264/101520 (11%)] Loss: -1033.702881\n",
      "Train Epoch: 246 [22528/101520 (22%)] Loss: -1036.853394\n",
      "Train Epoch: 246 [33792/101520 (33%)] Loss: -1044.096436\n",
      "Train Epoch: 246 [45056/101520 (44%)] Loss: -1035.896484\n",
      "Train Epoch: 246 [56320/101520 (55%)] Loss: -1034.213135\n",
      "Train Epoch: 246 [67584/101520 (67%)] Loss: -1029.231079\n",
      "Train Epoch: 246 [78848/101520 (78%)] Loss: -1032.729858\n",
      "Train Epoch: 246 [90112/101520 (89%)] Loss: -1034.670898\n",
      "Train Epoch: 246 [101376/101520 (100%)] Loss: -1043.206787\n",
      "    epoch          : 246\n",
      "    loss           : -1035.825478711919\n",
      "    ess            : 1.9605482091855764\n",
      "    log_marginal   : 1035.8601031279445\n",
      "    log_joint      : 1243.707239812343\n",
      "    val_loss       : -1039.09838336447\n",
      "    val_ess        : 1.9600207701973293\n",
      "    val_log_marginal: 1039.132515285326\n",
      "    val_log_joint  : 1247.0307245669158\n",
      "Train Epoch: 247 [0/101520 (0%)] Loss: -1037.581787\n",
      "Train Epoch: 247 [11264/101520 (11%)] Loss: -1036.073120\n",
      "Train Epoch: 247 [22528/101520 (22%)] Loss: -1047.589966\n",
      "Train Epoch: 247 [33792/101520 (33%)] Loss: -1034.228638\n",
      "Train Epoch: 247 [45056/101520 (44%)] Loss: -1036.702515\n",
      "Train Epoch: 247 [56320/101520 (55%)] Loss: -1040.492188\n",
      "Train Epoch: 247 [67584/101520 (67%)] Loss: -1038.996338\n",
      "Train Epoch: 247 [78848/101520 (78%)] Loss: -1038.312744\n",
      "Train Epoch: 247 [90112/101520 (89%)] Loss: -1035.371948\n",
      "Train Epoch: 247 [101376/101520 (100%)] Loss: -1032.584229\n",
      "    epoch          : 247\n",
      "    loss           : -1034.602724314934\n",
      "    ess            : 1.9619888498555476\n",
      "    log_marginal   : 1034.636112692368\n",
      "    log_joint      : 1242.6269236809046\n",
      "    val_loss       : -1035.3816289487092\n",
      "    val_ess        : 1.9588531048401543\n",
      "    val_log_marginal: 1035.4173053243885\n",
      "    val_log_joint  : 1243.083108653193\n",
      "Train Epoch: 248 [0/101520 (0%)] Loss: -1033.661377\n",
      "Train Epoch: 248 [11264/101520 (11%)] Loss: -1029.893311\n",
      "Train Epoch: 248 [22528/101520 (22%)] Loss: -1031.633545\n",
      "Train Epoch: 248 [33792/101520 (33%)] Loss: -1039.097656\n",
      "Train Epoch: 248 [45056/101520 (44%)] Loss: -1035.268921\n",
      "Train Epoch: 248 [56320/101520 (55%)] Loss: -1042.754395\n",
      "Train Epoch: 248 [67584/101520 (67%)] Loss: -1036.891357\n",
      "Train Epoch: 248 [78848/101520 (78%)] Loss: -1040.295044\n",
      "Train Epoch: 248 [90112/101520 (89%)] Loss: -1036.129639\n",
      "Train Epoch: 248 [101376/101520 (100%)] Loss: -1035.588257\n",
      "    epoch          : 248\n",
      "    loss           : -1036.1693060026696\n",
      "    ess            : 1.9608679302972765\n",
      "    log_marginal   : 1036.2038193899184\n",
      "    log_joint      : 1244.1508298327576\n",
      "    val_loss       : -1038.3464090098505\n",
      "    val_ess        : 1.9549779529156892\n",
      "    val_log_marginal: 1038.38501507303\n",
      "    val_log_joint  : 1246.2046747622283\n",
      "Train Epoch: 249 [0/101520 (0%)] Loss: -1038.564697\n",
      "Train Epoch: 249 [11264/101520 (11%)] Loss: -1033.370361\n",
      "Train Epoch: 249 [22528/101520 (22%)] Loss: -1032.635132\n",
      "Train Epoch: 249 [33792/101520 (33%)] Loss: -1036.159180\n",
      "Train Epoch: 249 [45056/101520 (44%)] Loss: -1028.348877\n",
      "Train Epoch: 249 [56320/101520 (55%)] Loss: -1034.495850\n",
      "Train Epoch: 249 [67584/101520 (67%)] Loss: -1037.649048\n",
      "Train Epoch: 249 [78848/101520 (78%)] Loss: -1038.000366\n",
      "Train Epoch: 249 [90112/101520 (89%)] Loss: -1033.569824\n",
      "Train Epoch: 249 [101376/101520 (100%)] Loss: -1041.698853\n",
      "    epoch          : 249\n",
      "    loss           : -1035.2325016194252\n",
      "    ess            : 1.9607800299198783\n",
      "    log_marginal   : 1035.266426738183\n",
      "    log_joint      : 1243.1596158281643\n",
      "    val_loss       : -1034.8809867527175\n",
      "    val_ess        : 1.9612434739651887\n",
      "    val_log_marginal: 1034.912300441576\n",
      "    val_log_joint  : 1242.7862442680027\n",
      "Train Epoch: 250 [0/101520 (0%)] Loss: -1032.541138\n",
      "Train Epoch: 250 [11264/101520 (11%)] Loss: -1030.765625\n",
      "Train Epoch: 250 [22528/101520 (22%)] Loss: -1040.056641\n",
      "Train Epoch: 250 [33792/101520 (33%)] Loss: -1041.291748\n",
      "Train Epoch: 250 [45056/101520 (44%)] Loss: -1038.208740\n",
      "Train Epoch: 250 [56320/101520 (55%)] Loss: -1037.334595\n",
      "Train Epoch: 250 [67584/101520 (67%)] Loss: -1033.887207\n",
      "Train Epoch: 250 [78848/101520 (78%)] Loss: -1037.241577\n",
      "Train Epoch: 250 [90112/101520 (89%)] Loss: -1030.631836\n",
      "Train Epoch: 250 [101376/101520 (100%)] Loss: -1036.041748\n",
      "    epoch          : 250\n",
      "    loss           : -1036.739865710388\n",
      "    ess            : 1.9610690789007061\n",
      "    log_marginal   : 1036.77391657997\n",
      "    log_joint      : 1244.7181151116913\n",
      "    val_loss       : -1038.0410846212635\n",
      "    val_ess        : 1.9593166683031165\n",
      "    val_log_marginal: 1038.0785071331522\n",
      "    val_log_joint  : 1245.9471064028533\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [0/101520 (0%)] Loss: -1045.123169\n",
      "Train Epoch: 251 [11264/101520 (11%)] Loss: -1040.381348\n",
      "Train Epoch: 251 [22528/101520 (22%)] Loss: -1039.055542\n",
      "Train Epoch: 251 [33792/101520 (33%)] Loss: -1039.210205\n",
      "Train Epoch: 251 [45056/101520 (44%)] Loss: -1037.950439\n",
      "Train Epoch: 251 [56320/101520 (55%)] Loss: -1026.447021\n",
      "Train Epoch: 251 [67584/101520 (67%)] Loss: -1030.302002\n",
      "Train Epoch: 251 [78848/101520 (78%)] Loss: -1032.569092\n",
      "Train Epoch: 251 [90112/101520 (89%)] Loss: -1038.962891\n",
      "Train Epoch: 251 [101376/101520 (100%)] Loss: -1029.748901\n",
      "    epoch          : 251\n",
      "    loss           : -1035.5008225944173\n",
      "    ess            : 1.960967174726515\n",
      "    log_marginal   : 1035.534929898516\n",
      "    log_joint      : 1243.5063249597597\n",
      "    val_loss       : -1035.236333432405\n",
      "    val_ess        : 1.9602464230164238\n",
      "    val_log_marginal: 1035.273389733356\n",
      "    val_log_joint  : 1243.2099609375\n",
      "Train Epoch: 252 [0/101520 (0%)] Loss: -1041.209717\n",
      "Train Epoch: 252 [11264/101520 (11%)] Loss: -1037.313721\n",
      "Train Epoch: 252 [22528/101520 (22%)] Loss: -1032.190918\n",
      "Train Epoch: 252 [33792/101520 (33%)] Loss: -1034.893555\n",
      "Train Epoch: 252 [45056/101520 (44%)] Loss: -1038.246826\n",
      "Train Epoch: 252 [56320/101520 (55%)] Loss: -1037.875000\n",
      "Train Epoch: 252 [67584/101520 (67%)] Loss: -1038.233276\n",
      "Train Epoch: 252 [78848/101520 (78%)] Loss: -1038.224731\n",
      "Train Epoch: 252 [90112/101520 (89%)] Loss: -1030.100342\n",
      "Train Epoch: 252 [101376/101520 (100%)] Loss: -1029.931274\n",
      "    epoch          : 252\n",
      "    loss           : -1037.1050166604507\n",
      "    ess            : 1.961833451860514\n",
      "    log_marginal   : 1037.1376302901224\n",
      "    log_joint      : 1245.0743334592887\n",
      "    val_loss       : -1038.3349025560462\n",
      "    val_ess        : 1.9572097166724827\n",
      "    val_log_marginal: 1038.3752812924592\n",
      "    val_log_joint  : 1246.1133237092392\n",
      "Train Epoch: 253 [0/101520 (0%)] Loss: -1035.272705\n",
      "Train Epoch: 253 [11264/101520 (11%)] Loss: -1040.248291\n",
      "Train Epoch: 253 [22528/101520 (22%)] Loss: -1041.584106\n",
      "Train Epoch: 253 [33792/101520 (33%)] Loss: -1038.614502\n",
      "Train Epoch: 253 [45056/101520 (44%)] Loss: -1032.071289\n",
      "Train Epoch: 253 [56320/101520 (55%)] Loss: -1033.237183\n",
      "Train Epoch: 253 [67584/101520 (67%)] Loss: -1044.589111\n",
      "Train Epoch: 253 [78848/101520 (78%)] Loss: -1033.396973\n",
      "Train Epoch: 253 [90112/101520 (89%)] Loss: -1032.789673\n",
      "Train Epoch: 253 [101376/101520 (100%)] Loss: -1027.452637\n",
      "    epoch          : 253\n",
      "    loss           : -1036.1141780680748\n",
      "    ess            : 1.960869110409339\n",
      "    log_marginal   : 1036.1480019727544\n",
      "    log_joint      : 1244.0939303450848\n",
      "    val_loss       : -1035.534715735394\n",
      "    val_ess        : 1.9621411561965942\n",
      "    val_log_marginal: 1035.56945535411\n",
      "    val_log_joint  : 1243.6698794157608\n",
      "Train Epoch: 254 [0/101520 (0%)] Loss: -1031.568848\n",
      "Train Epoch: 254 [11264/101520 (11%)] Loss: -1038.691406\n",
      "Train Epoch: 254 [22528/101520 (22%)] Loss: -1040.583496\n",
      "Train Epoch: 254 [33792/101520 (33%)] Loss: -1043.702393\n",
      "Train Epoch: 254 [45056/101520 (44%)] Loss: -1031.491577\n",
      "Train Epoch: 254 [56320/101520 (55%)] Loss: -1043.855713\n",
      "Train Epoch: 254 [67584/101520 (67%)] Loss: -1028.431885\n",
      "Train Epoch: 254 [78848/101520 (78%)] Loss: -1042.065430\n",
      "Train Epoch: 254 [90112/101520 (89%)] Loss: -1039.887207\n",
      "Train Epoch: 254 [101376/101520 (100%)] Loss: -1039.635376\n",
      "    epoch          : 254\n",
      "    loss           : -1037.686138824003\n",
      "    ess            : 1.961095833299148\n",
      "    log_marginal   : 1037.7199657957758\n",
      "    log_joint      : 1245.6504182288395\n",
      "    val_loss       : -1039.9084578804348\n",
      "    val_ess        : 1.9614886511927065\n",
      "    val_log_marginal: 1039.9412204908288\n",
      "    val_log_joint  : 1248.0440833050272\n",
      "Train Epoch: 255 [0/101520 (0%)] Loss: -1041.600098\n",
      "Train Epoch: 255 [11264/101520 (11%)] Loss: -1036.255493\n",
      "Train Epoch: 255 [22528/101520 (22%)] Loss: -1038.404297\n",
      "Train Epoch: 255 [33792/101520 (33%)] Loss: -1039.536377\n",
      "Train Epoch: 255 [45056/101520 (44%)] Loss: -1036.300903\n",
      "Train Epoch: 255 [56320/101520 (55%)] Loss: -1038.606201\n",
      "Train Epoch: 255 [67584/101520 (67%)] Loss: -1039.677490\n",
      "Train Epoch: 255 [78848/101520 (78%)] Loss: -1032.640625\n",
      "Train Epoch: 255 [90112/101520 (89%)] Loss: -1036.350220\n",
      "Train Epoch: 255 [101376/101520 (100%)] Loss: -1041.819214\n",
      "    epoch          : 255\n",
      "    loss           : -1036.8032202025754\n",
      "    ess            : 1.962361516065933\n",
      "    log_marginal   : 1036.835336963136\n",
      "    log_joint      : 1244.7123127846262\n",
      "    val_loss       : -1035.7229322350543\n",
      "    val_ess        : 1.9609350328860076\n",
      "    val_log_marginal: 1035.7545245626698\n",
      "    val_log_joint  : 1243.9695726477582\n",
      "Train Epoch: 256 [0/101520 (0%)] Loss: -1038.723633\n",
      "Train Epoch: 256 [11264/101520 (11%)] Loss: -1028.966064\n",
      "Train Epoch: 256 [22528/101520 (22%)] Loss: -1043.354980\n",
      "Train Epoch: 256 [33792/101520 (33%)] Loss: -1039.198975\n",
      "Train Epoch: 256 [45056/101520 (44%)] Loss: -1037.788574\n",
      "Train Epoch: 256 [56320/101520 (55%)] Loss: -1038.874634\n",
      "Train Epoch: 256 [67584/101520 (67%)] Loss: -1045.134644\n",
      "Train Epoch: 256 [78848/101520 (78%)] Loss: -1041.426025\n",
      "Train Epoch: 256 [90112/101520 (89%)] Loss: -1041.541992\n",
      "Train Epoch: 256 [101376/101520 (100%)] Loss: -1045.643188\n",
      "    epoch          : 256\n",
      "    loss           : -1038.2487032329616\n",
      "    ess            : 1.9612418922347639\n",
      "    log_marginal   : 1038.283017872566\n",
      "    log_joint      : 1246.1412052940484\n",
      "    val_loss       : -1039.8876687754755\n",
      "    val_ess        : 1.9641132406566455\n",
      "    val_log_marginal: 1039.9186162533967\n",
      "    val_log_joint  : 1247.6999352496603\n",
      "Train Epoch: 257 [0/101520 (0%)] Loss: -1033.079590\n",
      "Train Epoch: 257 [11264/101520 (11%)] Loss: -1027.968018\n",
      "Train Epoch: 257 [22528/101520 (22%)] Loss: -1034.173828\n",
      "Train Epoch: 257 [33792/101520 (33%)] Loss: -1040.328125\n",
      "Train Epoch: 257 [45056/101520 (44%)] Loss: -1039.971802\n",
      "Train Epoch: 257 [56320/101520 (55%)] Loss: -1029.983643\n",
      "Train Epoch: 257 [67584/101520 (67%)] Loss: -1044.473999\n",
      "Train Epoch: 257 [78848/101520 (78%)] Loss: -1031.756958\n",
      "Train Epoch: 257 [90112/101520 (89%)] Loss: -1037.310303\n",
      "Train Epoch: 257 [101376/101520 (100%)] Loss: -1038.349365\n",
      "    epoch          : 257\n",
      "    loss           : -1037.427151627277\n",
      "    ess            : 1.9613377167351882\n",
      "    log_marginal   : 1037.460693359375\n",
      "    log_joint      : 1245.384257714353\n",
      "    val_loss       : -1036.589623492697\n",
      "    val_ess        : 1.958483892938365\n",
      "    val_log_marginal: 1036.6267487899117\n",
      "    val_log_joint  : 1244.6527471127717\n",
      "Train Epoch: 258 [0/101520 (0%)] Loss: -1042.870605\n",
      "Train Epoch: 258 [11264/101520 (11%)] Loss: -1044.623413\n",
      "Train Epoch: 258 [22528/101520 (22%)] Loss: -1039.493896\n",
      "Train Epoch: 258 [33792/101520 (33%)] Loss: -1029.084473\n",
      "Train Epoch: 258 [45056/101520 (44%)] Loss: -1033.369019\n",
      "Train Epoch: 258 [56320/101520 (55%)] Loss: -1045.723755\n",
      "Train Epoch: 258 [67584/101520 (67%)] Loss: -1036.801147\n",
      "Train Epoch: 258 [78848/101520 (78%)] Loss: -1043.416504\n",
      "Train Epoch: 258 [90112/101520 (89%)] Loss: -1036.640381\n",
      "Train Epoch: 258 [101376/101520 (100%)] Loss: -1045.683716\n",
      "    epoch          : 258\n",
      "    loss           : -1038.806355385325\n",
      "    ess            : 1.9617839392705179\n",
      "    log_marginal   : 1038.8395652579302\n",
      "    log_joint      : 1246.6530240312893\n",
      "    val_loss       : -1040.3149679432745\n",
      "    val_ess        : 1.9613564429075823\n",
      "    val_log_marginal: 1040.3485160495925\n",
      "    val_log_joint  : 1248.1169592815897\n",
      "Train Epoch: 259 [0/101520 (0%)] Loss: -1035.456055\n",
      "Train Epoch: 259 [11264/101520 (11%)] Loss: -1039.467773\n",
      "Train Epoch: 259 [22528/101520 (22%)] Loss: -1038.581299\n",
      "Train Epoch: 259 [33792/101520 (33%)] Loss: -1045.379395\n",
      "Train Epoch: 259 [45056/101520 (44%)] Loss: -1041.273682\n",
      "Train Epoch: 259 [56320/101520 (55%)] Loss: -1042.866211\n",
      "Train Epoch: 259 [67584/101520 (67%)] Loss: -1038.594971\n",
      "Train Epoch: 259 [78848/101520 (78%)] Loss: -1030.970215\n",
      "Train Epoch: 259 [90112/101520 (89%)] Loss: -1038.204712\n",
      "Train Epoch: 259 [101376/101520 (100%)] Loss: -1046.771729\n",
      "    epoch          : 259\n",
      "    loss           : -1037.9163217209093\n",
      "    ess            : 1.9614977039883483\n",
      "    log_marginal   : 1037.949712552018\n",
      "    log_joint      : 1245.8745497507066\n",
      "    val_loss       : -1038.9263862941575\n",
      "    val_ess        : 1.9615028474641882\n",
      "    val_log_marginal: 1038.9606827445652\n",
      "    val_log_joint  : 1247.098149838655\n",
      "Train Epoch: 260 [0/101520 (0%)] Loss: -1038.244751\n",
      "Train Epoch: 260 [11264/101520 (11%)] Loss: -1035.688599\n",
      "Train Epoch: 260 [22528/101520 (22%)] Loss: -1044.444824\n",
      "Train Epoch: 260 [33792/101520 (33%)] Loss: -1026.752930\n",
      "Train Epoch: 260 [45056/101520 (44%)] Loss: -1037.132690\n",
      "Train Epoch: 260 [56320/101520 (55%)] Loss: -1034.430298\n",
      "Train Epoch: 260 [67584/101520 (67%)] Loss: -1042.858154\n",
      "Train Epoch: 260 [78848/101520 (78%)] Loss: -1037.135864\n",
      "Train Epoch: 260 [90112/101520 (89%)] Loss: -1038.276855\n",
      "Train Epoch: 260 [101376/101520 (100%)] Loss: -1042.092407\n",
      "    epoch          : 260\n",
      "    loss           : -1039.0286343828518\n",
      "    ess            : 1.9608654191146544\n",
      "    log_marginal   : 1039.062758249254\n",
      "    log_joint      : 1246.953150150165\n",
      "    val_loss       : -1039.811451787534\n",
      "    val_ess        : 1.9618227585502293\n",
      "    val_log_marginal: 1039.8449839716372\n",
      "    val_log_joint  : 1247.4569994055707\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch260.pth ...\n",
      "Train Epoch: 261 [0/101520 (0%)] Loss: -1037.650391\n",
      "Train Epoch: 261 [11264/101520 (11%)] Loss: -1032.045166\n",
      "Train Epoch: 261 [22528/101520 (22%)] Loss: -1035.533936\n",
      "Train Epoch: 261 [33792/101520 (33%)] Loss: -1039.053345\n",
      "Train Epoch: 261 [45056/101520 (44%)] Loss: -1040.887939\n",
      "Train Epoch: 261 [56320/101520 (55%)] Loss: -1041.763550\n",
      "Train Epoch: 261 [67584/101520 (67%)] Loss: -1044.449463\n",
      "Train Epoch: 261 [78848/101520 (78%)] Loss: -1043.148193\n",
      "Train Epoch: 261 [90112/101520 (89%)] Loss: -1033.949097\n",
      "Train Epoch: 261 [101376/101520 (100%)] Loss: -1039.462769\n",
      "    epoch          : 261\n",
      "    loss           : -1038.4796884814698\n",
      "    ess            : 1.9617694993713992\n",
      "    log_marginal   : 1038.5127873252984\n",
      "    log_joint      : 1246.372343283802\n",
      "    val_loss       : -1038.0189633576767\n",
      "    val_ess        : 1.960032406060592\n",
      "    val_log_marginal: 1038.0565132472825\n",
      "    val_log_joint  : 1246.1491115404212\n",
      "Train Epoch: 262 [0/101520 (0%)] Loss: -1039.926147\n",
      "Train Epoch: 262 [11264/101520 (11%)] Loss: -1031.656006\n",
      "Train Epoch: 262 [22528/101520 (22%)] Loss: -1040.847656\n",
      "Train Epoch: 262 [33792/101520 (33%)] Loss: -1040.135010\n",
      "Train Epoch: 262 [45056/101520 (44%)] Loss: -1035.006714\n",
      "Train Epoch: 262 [56320/101520 (55%)] Loss: -1031.218994\n",
      "Train Epoch: 262 [67584/101520 (67%)] Loss: -1038.422241\n",
      "Train Epoch: 262 [78848/101520 (78%)] Loss: -1036.349976\n",
      "Train Epoch: 262 [90112/101520 (89%)] Loss: -1037.684082\n",
      "Train Epoch: 262 [101376/101520 (100%)] Loss: -1038.943237\n",
      "    epoch          : 262\n",
      "    loss           : -1039.4170621172268\n",
      "    ess            : 1.961388234517083\n",
      "    log_marginal   : 1039.4511731018374\n",
      "    log_joint      : 1247.360561965099\n",
      "    val_loss       : -1041.5419231912365\n",
      "    val_ess        : 1.9622495589048967\n",
      "    val_log_marginal: 1041.574558423913\n",
      "    val_log_joint  : 1249.479099439538\n",
      "Train Epoch: 263 [0/101520 (0%)] Loss: -1044.389404\n",
      "Train Epoch: 263 [11264/101520 (11%)] Loss: -1042.393555\n",
      "Train Epoch: 263 [22528/101520 (22%)] Loss: -1031.607422\n",
      "Train Epoch: 263 [33792/101520 (33%)] Loss: -1037.833984\n",
      "Train Epoch: 263 [45056/101520 (44%)] Loss: -1037.111816\n",
      "Train Epoch: 263 [56320/101520 (55%)] Loss: -1038.963013\n",
      "Train Epoch: 263 [67584/101520 (67%)] Loss: -1037.975586\n",
      "Train Epoch: 263 [78848/101520 (78%)] Loss: -1036.818604\n",
      "Train Epoch: 263 [90112/101520 (89%)] Loss: -1037.640991\n",
      "Train Epoch: 263 [101376/101520 (100%)] Loss: -1027.023682\n",
      "    epoch          : 263\n",
      "    loss           : -1039.0899112260522\n",
      "    ess            : 1.9609081811042288\n",
      "    log_marginal   : 1039.1238019933653\n",
      "    log_joint      : 1247.016532246192\n",
      "    val_loss       : -1039.05640179178\n",
      "    val_ess        : 1.9588635952576348\n",
      "    val_log_marginal: 1039.0923966117527\n",
      "    val_log_joint  : 1246.8562701681385\n",
      "Train Epoch: 264 [0/101520 (0%)] Loss: -1037.723389\n",
      "Train Epoch: 264 [11264/101520 (11%)] Loss: -1047.209717\n",
      "Train Epoch: 264 [22528/101520 (22%)] Loss: -1038.375732\n",
      "Train Epoch: 264 [33792/101520 (33%)] Loss: -1044.106201\n",
      "Train Epoch: 264 [45056/101520 (44%)] Loss: -1035.779297\n",
      "Train Epoch: 264 [56320/101520 (55%)] Loss: -1041.658081\n",
      "Train Epoch: 264 [67584/101520 (67%)] Loss: -1032.277832\n",
      "Train Epoch: 264 [78848/101520 (78%)] Loss: -1039.287231\n",
      "Train Epoch: 264 [90112/101520 (89%)] Loss: -1039.444824\n",
      "Train Epoch: 264 [101376/101520 (100%)] Loss: -1039.994141\n",
      "    epoch          : 264\n",
      "    loss           : -1039.8018676144393\n",
      "    ess            : 1.9613205200463684\n",
      "    log_marginal   : 1039.8358240175487\n",
      "    log_joint      : 1247.7425003435144\n",
      "    val_loss       : -1040.01941183339\n",
      "    val_ess        : 1.9633195503898289\n",
      "    val_log_marginal: 1040.0491359544837\n",
      "    val_log_joint  : 1247.4609587296195\n",
      "Train Epoch: 265 [0/101520 (0%)] Loss: -1041.820557\n",
      "Train Epoch: 265 [11264/101520 (11%)] Loss: -1039.266357\n",
      "Train Epoch: 265 [22528/101520 (22%)] Loss: -1038.447021\n",
      "Train Epoch: 265 [33792/101520 (33%)] Loss: -1039.668457\n",
      "Train Epoch: 265 [45056/101520 (44%)] Loss: -1040.182983\n",
      "Train Epoch: 265 [56320/101520 (55%)] Loss: -1042.279663\n",
      "Train Epoch: 265 [67584/101520 (67%)] Loss: -1041.675293\n",
      "Train Epoch: 265 [78848/101520 (78%)] Loss: -1039.597900\n",
      "Train Epoch: 265 [90112/101520 (89%)] Loss: -1045.105957\n",
      "Train Epoch: 265 [101376/101520 (100%)] Loss: -1052.458130\n",
      "    epoch          : 265\n",
      "    loss           : -1039.737294259383\n",
      "    ess            : 1.9602893105703383\n",
      "    log_marginal   : 1039.771787403816\n",
      "    log_joint      : 1247.6458611416458\n",
      "    val_loss       : -1041.4477751358695\n",
      "    val_ess        : 1.963312055753625\n",
      "    val_log_marginal: 1041.480813731318\n",
      "    val_log_joint  : 1249.3815493376358\n",
      "Train Epoch: 266 [0/101520 (0%)] Loss: -1048.251221\n",
      "Train Epoch: 266 [11264/101520 (11%)] Loss: -1043.095215\n",
      "Train Epoch: 266 [22528/101520 (22%)] Loss: -1040.674072\n",
      "Train Epoch: 266 [33792/101520 (33%)] Loss: -1042.195679\n",
      "Train Epoch: 266 [45056/101520 (44%)] Loss: -1041.419434\n",
      "Train Epoch: 266 [56320/101520 (55%)] Loss: -1036.816650\n",
      "Train Epoch: 266 [67584/101520 (67%)] Loss: -1039.301514\n",
      "Train Epoch: 266 [78848/101520 (78%)] Loss: -1043.704590\n",
      "Train Epoch: 266 [90112/101520 (89%)] Loss: -1040.994019\n",
      "Train Epoch: 266 [101376/101520 (100%)] Loss: -1039.138550\n",
      "    epoch          : 266\n",
      "    loss           : -1040.2919124430748\n",
      "    ess            : 1.9607519606250015\n",
      "    log_marginal   : 1040.3260234276854\n",
      "    log_joint      : 1248.1484976150282\n",
      "    val_loss       : -1041.4352178158967\n",
      "    val_ess        : 1.9601371651110442\n",
      "    val_log_marginal: 1041.4710109544837\n",
      "    val_log_joint  : 1249.2637833305027\n",
      "Train Epoch: 267 [0/101520 (0%)] Loss: -1041.639160\n",
      "Train Epoch: 267 [11264/101520 (11%)] Loss: -1044.412598\n",
      "Train Epoch: 267 [22528/101520 (22%)] Loss: -1044.557495\n",
      "Train Epoch: 267 [33792/101520 (33%)] Loss: -1044.038452\n",
      "Train Epoch: 267 [45056/101520 (44%)] Loss: -1036.817871\n",
      "Train Epoch: 267 [56320/101520 (55%)] Loss: -1044.328369\n",
      "Train Epoch: 267 [67584/101520 (67%)] Loss: -1042.112183\n",
      "Train Epoch: 267 [78848/101520 (78%)] Loss: -1036.879517\n",
      "Train Epoch: 267 [90112/101520 (89%)] Loss: -1041.908813\n",
      "Train Epoch: 267 [101376/101520 (100%)] Loss: -1037.263306\n",
      "    epoch          : 267\n",
      "    loss           : -1039.8802005633636\n",
      "    ess            : 1.9612016713798945\n",
      "    log_marginal   : 1039.914761183849\n",
      "    log_joint      : 1247.8193721292007\n",
      "    val_loss       : -1040.7652694038723\n",
      "    val_ess        : 1.9625457006952036\n",
      "    val_log_marginal: 1040.7973738960598\n",
      "    val_log_joint  : 1248.4947509765625\n",
      "Train Epoch: 268 [0/101520 (0%)] Loss: -1035.433960\n",
      "Train Epoch: 268 [11264/101520 (11%)] Loss: -1051.068115\n",
      "Train Epoch: 268 [22528/101520 (22%)] Loss: -1041.535767\n",
      "Train Epoch: 268 [33792/101520 (33%)] Loss: -1035.950195\n",
      "Train Epoch: 268 [45056/101520 (44%)] Loss: -1040.014404\n",
      "Train Epoch: 268 [56320/101520 (55%)] Loss: -1037.694092\n",
      "Train Epoch: 268 [67584/101520 (67%)] Loss: -1042.989258\n",
      "Train Epoch: 268 [78848/101520 (78%)] Loss: -1045.530151\n",
      "Train Epoch: 268 [90112/101520 (89%)] Loss: -1036.537598\n",
      "Train Epoch: 268 [101376/101520 (100%)] Loss: -1032.485596\n",
      "    epoch          : 268\n",
      "    loss           : -1040.579513166418\n",
      "    ess            : 1.960954626600946\n",
      "    log_marginal   : 1040.6136210839354\n",
      "    log_joint      : 1248.4665398525833\n",
      "    val_loss       : -1041.6635476817255\n",
      "    val_ess        : 1.9648561633151511\n",
      "    val_log_marginal: 1041.693168308424\n",
      "    val_log_joint  : 1249.5795155400815\n",
      "Train Epoch: 269 [0/101520 (0%)] Loss: -1037.261963\n",
      "Train Epoch: 269 [11264/101520 (11%)] Loss: -1040.616943\n",
      "Train Epoch: 269 [22528/101520 (22%)] Loss: -1036.446045\n",
      "Train Epoch: 269 [33792/101520 (33%)] Loss: -1043.819336\n",
      "Train Epoch: 269 [45056/101520 (44%)] Loss: -1046.787231\n",
      "Train Epoch: 269 [56320/101520 (55%)] Loss: -1035.364014\n",
      "Train Epoch: 269 [67584/101520 (67%)] Loss: -1040.579468\n",
      "Train Epoch: 269 [78848/101520 (78%)] Loss: -1047.076172\n",
      "Train Epoch: 269 [90112/101520 (89%)] Loss: -1034.280762\n",
      "Train Epoch: 269 [101376/101520 (100%)] Loss: -1044.740723\n",
      "    epoch          : 269\n",
      "    loss           : -1040.240234375\n",
      "    ess            : 1.9609768558387182\n",
      "    log_marginal   : 1040.2741275959877\n",
      "    log_joint      : 1248.1961860081658\n",
      "    val_loss       : -1039.3642153532608\n",
      "    val_ess        : 1.9588287135829097\n",
      "    val_log_marginal: 1039.4022800611413\n",
      "    val_log_joint  : 1247.4415230129075\n",
      "Train Epoch: 270 [0/101520 (0%)] Loss: -1039.366943\n",
      "Train Epoch: 270 [11264/101520 (11%)] Loss: -1041.965332\n",
      "Train Epoch: 270 [22528/101520 (22%)] Loss: -1038.059204\n",
      "Train Epoch: 270 [33792/101520 (33%)] Loss: -1041.973877\n",
      "Train Epoch: 270 [45056/101520 (44%)] Loss: -1038.438110\n",
      "Train Epoch: 270 [56320/101520 (55%)] Loss: -1043.911865\n",
      "Train Epoch: 270 [67584/101520 (67%)] Loss: -1046.817505\n",
      "Train Epoch: 270 [78848/101520 (78%)] Loss: -1034.035522\n",
      "Train Epoch: 270 [90112/101520 (89%)] Loss: -1041.646484\n",
      "Train Epoch: 270 [101376/101520 (100%)] Loss: -1047.556274\n",
      "    epoch          : 270\n",
      "    loss           : -1040.2720603751177\n",
      "    ess            : 1.9611692051192624\n",
      "    log_marginal   : 1040.3065903246702\n",
      "    log_joint      : 1248.253975566308\n",
      "    val_loss       : -1041.3143204398777\n",
      "    val_ess        : 1.9611732596936433\n",
      "    val_log_marginal: 1041.3482666015625\n",
      "    val_log_joint  : 1249.140280018682\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch270.pth ...\n",
      "Train Epoch: 271 [0/101520 (0%)] Loss: -1053.630737\n",
      "Train Epoch: 271 [11264/101520 (11%)] Loss: -1039.966309\n",
      "Train Epoch: 271 [22528/101520 (22%)] Loss: -1049.077637\n",
      "Train Epoch: 271 [33792/101520 (33%)] Loss: -1036.130859\n",
      "Train Epoch: 271 [45056/101520 (44%)] Loss: -1036.588257\n",
      "Train Epoch: 271 [56320/101520 (55%)] Loss: -1039.827148\n",
      "Train Epoch: 271 [67584/101520 (67%)] Loss: -1038.178467\n",
      "Train Epoch: 271 [78848/101520 (78%)] Loss: -1041.229980\n",
      "Train Epoch: 271 [90112/101520 (89%)] Loss: -1043.099365\n",
      "Train Epoch: 271 [101376/101520 (100%)] Loss: -1023.312195\n",
      "    epoch          : 271\n",
      "    loss           : -1041.0321728883675\n",
      "    ess            : 1.9610577492258656\n",
      "    log_marginal   : 1041.0662679240932\n",
      "    log_joint      : 1249.0007434634108\n",
      "    val_loss       : -1040.1533203125\n",
      "    val_ess        : 1.957798905994581\n",
      "    val_log_marginal: 1040.1920404848845\n",
      "    val_log_joint  : 1248.1094015370245\n",
      "Train Epoch: 272 [0/101520 (0%)] Loss: -1044.784180\n",
      "Train Epoch: 272 [11264/101520 (11%)] Loss: -1041.057007\n",
      "Train Epoch: 272 [22528/101520 (22%)] Loss: -1040.587158\n",
      "Train Epoch: 272 [33792/101520 (33%)] Loss: -1039.589355\n",
      "Train Epoch: 272 [45056/101520 (44%)] Loss: -1043.041504\n",
      "Train Epoch: 272 [56320/101520 (55%)] Loss: -1036.487915\n",
      "Train Epoch: 272 [67584/101520 (67%)] Loss: -1041.886475\n",
      "Train Epoch: 272 [78848/101520 (78%)] Loss: -1035.818848\n",
      "Train Epoch: 272 [90112/101520 (89%)] Loss: -1025.326294\n",
      "Train Epoch: 272 [101376/101520 (100%)] Loss: -1040.416260\n",
      "    epoch          : 272\n",
      "    loss           : -1039.3705735709798\n",
      "    ess            : 1.961187145817819\n",
      "    log_marginal   : 1039.4045845683495\n",
      "    log_joint      : 1247.4149519570508\n",
      "    val_loss       : -1036.8942977241848\n",
      "    val_ess        : 1.9559506126072095\n",
      "    val_log_marginal: 1036.9332646908967\n",
      "    val_log_joint  : 1244.8025592306385\n",
      "Train Epoch: 273 [0/101520 (0%)] Loss: -1036.026001\n",
      "Train Epoch: 273 [11264/101520 (11%)] Loss: -1035.581421\n",
      "Train Epoch: 273 [22528/101520 (22%)] Loss: -1036.255859\n",
      "Train Epoch: 273 [33792/101520 (33%)] Loss: -1041.097656\n",
      "Train Epoch: 273 [45056/101520 (44%)] Loss: -1043.618774\n",
      "Train Epoch: 273 [56320/101520 (55%)] Loss: -1043.847290\n",
      "Train Epoch: 273 [67584/101520 (67%)] Loss: -1038.456421\n",
      "Train Epoch: 273 [78848/101520 (78%)] Loss: -1046.803223\n",
      "Train Epoch: 273 [90112/101520 (89%)] Loss: -1044.584106\n",
      "Train Epoch: 273 [101376/101520 (100%)] Loss: -1048.228882\n",
      "    epoch          : 273\n",
      "    loss           : -1040.527940606352\n",
      "    ess            : 1.9614526872059808\n",
      "    log_marginal   : 1040.5615111691268\n",
      "    log_joint      : 1248.5172904316505\n",
      "    val_loss       : -1039.6324064835258\n",
      "    val_ess        : 1.962646888649982\n",
      "    val_log_marginal: 1039.666867463485\n",
      "    val_log_joint  : 1247.7889935037365\n",
      "Train Epoch: 274 [0/101520 (0%)] Loss: -1043.383057\n",
      "Train Epoch: 274 [11264/101520 (11%)] Loss: -1044.793335\n",
      "Train Epoch: 274 [22528/101520 (22%)] Loss: -1044.336548\n",
      "Train Epoch: 274 [33792/101520 (33%)] Loss: -1042.249390\n",
      "Train Epoch: 274 [45056/101520 (44%)] Loss: -1037.777222\n",
      "Train Epoch: 274 [56320/101520 (55%)] Loss: -1042.297485\n",
      "Train Epoch: 274 [67584/101520 (67%)] Loss: -1037.975098\n",
      "Train Epoch: 274 [78848/101520 (78%)] Loss: -1040.437012\n",
      "Train Epoch: 274 [90112/101520 (89%)] Loss: -1036.830688\n",
      "Train Epoch: 274 [101376/101520 (100%)] Loss: -1045.650879\n",
      "    epoch          : 274\n",
      "    loss           : -1040.8262528462626\n",
      "    ess            : 1.9618086203857883\n",
      "    log_marginal   : 1040.8592510894316\n",
      "    log_joint      : 1248.8016351287688\n",
      "    val_loss       : -1042.6683774201767\n",
      "    val_ess        : 1.9652363424715789\n",
      "    val_log_marginal: 1042.6995849609375\n",
      "    val_log_joint  : 1250.4360192340353\n",
      "Train Epoch: 275 [0/101520 (0%)] Loss: -1045.686890\n",
      "Train Epoch: 275 [11264/101520 (11%)] Loss: -1042.513916\n",
      "Train Epoch: 275 [22528/101520 (22%)] Loss: -1039.354736\n",
      "Train Epoch: 275 [33792/101520 (33%)] Loss: -1043.736450\n",
      "Train Epoch: 275 [45056/101520 (44%)] Loss: -1042.442627\n",
      "Train Epoch: 275 [56320/101520 (55%)] Loss: -1047.323242\n",
      "Train Epoch: 275 [67584/101520 (67%)] Loss: -1044.011719\n",
      "Train Epoch: 275 [78848/101520 (78%)] Loss: -1048.064575\n",
      "Train Epoch: 275 [90112/101520 (89%)] Loss: -1046.939453\n",
      "Train Epoch: 275 [101376/101520 (100%)] Loss: -1047.724731\n",
      "    epoch          : 275\n",
      "    loss           : -1041.7483032839982\n",
      "    ess            : 1.961225350897516\n",
      "    log_marginal   : 1041.7824731567996\n",
      "    log_joint      : 1249.6851972263662\n",
      "    val_loss       : -1043.75365680197\n",
      "    val_ess        : 1.9606957642928413\n",
      "    val_log_marginal: 1043.7884680706522\n",
      "    val_log_joint  : 1251.1963209069293\n",
      "Train Epoch: 276 [0/101520 (0%)] Loss: -1046.883057\n",
      "Train Epoch: 276 [11264/101520 (11%)] Loss: -1039.565796\n",
      "Train Epoch: 276 [22528/101520 (22%)] Loss: -1037.365479\n",
      "Train Epoch: 276 [33792/101520 (33%)] Loss: -1039.501953\n",
      "Train Epoch: 276 [45056/101520 (44%)] Loss: -1043.091064\n",
      "Train Epoch: 276 [56320/101520 (55%)] Loss: -1047.190796\n",
      "Train Epoch: 276 [67584/101520 (67%)] Loss: -1039.803467\n",
      "Train Epoch: 276 [78848/101520 (78%)] Loss: -1042.776611\n",
      "Train Epoch: 276 [90112/101520 (89%)] Loss: -1040.986328\n",
      "Train Epoch: 276 [101376/101520 (100%)] Loss: -1040.646973\n",
      "    epoch          : 276\n",
      "    loss           : -1042.0978764672975\n",
      "    ess            : 1.9617491984487179\n",
      "    log_marginal   : 1042.1313899821373\n",
      "    log_joint      : 1250.0192723873272\n",
      "    val_loss       : -1042.460056470788\n",
      "    val_ess        : 1.9619866972384246\n",
      "    val_log_marginal: 1042.4940663213315\n",
      "    val_log_joint  : 1250.3748460852582\n",
      "Train Epoch: 277 [0/101520 (0%)] Loss: -1053.270508\n",
      "Train Epoch: 277 [11264/101520 (11%)] Loss: -1043.015503\n",
      "Train Epoch: 277 [22528/101520 (22%)] Loss: -1040.628296\n",
      "Train Epoch: 277 [33792/101520 (33%)] Loss: -1036.956299\n",
      "Train Epoch: 277 [45056/101520 (44%)] Loss: -1045.960938\n",
      "Train Epoch: 277 [56320/101520 (55%)] Loss: -1040.954346\n",
      "Train Epoch: 277 [67584/101520 (67%)] Loss: -1037.463623\n",
      "Train Epoch: 277 [78848/101520 (78%)] Loss: -1039.446289\n",
      "Train Epoch: 277 [90112/101520 (89%)] Loss: -1038.614380\n",
      "Train Epoch: 277 [101376/101520 (100%)] Loss: -1044.002808\n",
      "    epoch          : 277\n",
      "    loss           : -1042.4726157643688\n",
      "    ess            : 1.9613486863859935\n",
      "    log_marginal   : 1042.5061734453518\n",
      "    log_joint      : 1250.3899821372488\n",
      "    val_loss       : -1043.8125371518342\n",
      "    val_ess        : 1.9626651017562202\n",
      "    val_log_marginal: 1043.8457402768342\n",
      "    val_log_joint  : 1251.7697010869565\n",
      "Train Epoch: 278 [0/101520 (0%)] Loss: -1040.534424\n",
      "Train Epoch: 278 [11264/101520 (11%)] Loss: -1048.740967\n",
      "Train Epoch: 278 [22528/101520 (22%)] Loss: -1044.383667\n",
      "Train Epoch: 278 [33792/101520 (33%)] Loss: -1040.379639\n",
      "Train Epoch: 278 [45056/101520 (44%)] Loss: -1038.519165\n",
      "Train Epoch: 278 [56320/101520 (55%)] Loss: -1037.651611\n",
      "Train Epoch: 278 [67584/101520 (67%)] Loss: -1043.524170\n",
      "Train Epoch: 278 [78848/101520 (78%)] Loss: -1041.549072\n",
      "Train Epoch: 278 [90112/101520 (89%)] Loss: -1046.139648\n",
      "Train Epoch: 278 [101376/101520 (100%)] Loss: -1047.427368\n",
      "    epoch          : 278\n",
      "    loss           : -1042.023909832365\n",
      "    ess            : 1.9621368970104198\n",
      "    log_marginal   : 1042.0573405356863\n",
      "    log_joint      : 1250.0215886561714\n",
      "    val_loss       : -1042.9472921620245\n",
      "    val_ess        : 1.9614870237267537\n",
      "    val_log_marginal: 1042.98267132303\n",
      "    val_log_joint  : 1250.763236667799\n",
      "Train Epoch: 279 [0/101520 (0%)] Loss: -1036.210449\n",
      "Train Epoch: 279 [11264/101520 (11%)] Loss: -1047.155029\n",
      "Train Epoch: 279 [22528/101520 (22%)] Loss: -1042.064209\n",
      "Train Epoch: 279 [33792/101520 (33%)] Loss: -1040.634644\n",
      "Train Epoch: 279 [45056/101520 (44%)] Loss: -1049.799683\n",
      "Train Epoch: 279 [56320/101520 (55%)] Loss: -1041.573364\n",
      "Train Epoch: 279 [67584/101520 (67%)] Loss: -1037.414795\n",
      "Train Epoch: 279 [78848/101520 (78%)] Loss: -1049.994263\n",
      "Train Epoch: 279 [90112/101520 (89%)] Loss: -1044.624634\n",
      "Train Epoch: 279 [101376/101520 (100%)] Loss: -1039.223267\n",
      "    epoch          : 279\n",
      "    loss           : -1042.852473426704\n",
      "    ess            : 1.9603318353394168\n",
      "    log_marginal   : 1042.887301497723\n",
      "    log_joint      : 1250.8377029188914\n",
      "    val_loss       : -1043.9986625339675\n",
      "    val_ess        : 1.9605303328970205\n",
      "    val_log_marginal: 1044.0337497877038\n",
      "    val_log_joint  : 1252.0171269955842\n",
      "Train Epoch: 280 [0/101520 (0%)] Loss: -1043.720581\n",
      "Train Epoch: 280 [11264/101520 (11%)] Loss: -1049.944824\n",
      "Train Epoch: 280 [22528/101520 (22%)] Loss: -1044.193237\n",
      "Train Epoch: 280 [33792/101520 (33%)] Loss: -1045.378662\n",
      "Train Epoch: 280 [45056/101520 (44%)] Loss: -1044.082764\n",
      "Train Epoch: 280 [56320/101520 (55%)] Loss: -1044.747925\n",
      "Train Epoch: 280 [67584/101520 (67%)] Loss: -1040.983521\n",
      "Train Epoch: 280 [78848/101520 (78%)] Loss: -1034.717896\n",
      "Train Epoch: 280 [90112/101520 (89%)] Loss: -1039.665527\n",
      "Train Epoch: 280 [101376/101520 (100%)] Loss: -1063.525269\n",
      "    epoch          : 280\n",
      "    loss           : -1041.9192912806218\n",
      "    ess            : 1.9610973009512054\n",
      "    log_marginal   : 1041.9530372811323\n",
      "    log_joint      : 1249.9890038571766\n",
      "    val_loss       : -1040.2467890200408\n",
      "    val_ess        : 1.9597351188245027\n",
      "    val_log_marginal: 1040.2799921450408\n",
      "    val_log_joint  : 1248.514064622962\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch280.pth ...\n",
      "Train Epoch: 281 [0/101520 (0%)] Loss: -1032.482178\n",
      "Train Epoch: 281 [11264/101520 (11%)] Loss: -1044.298950\n",
      "Train Epoch: 281 [22528/101520 (22%)] Loss: -1038.611206\n",
      "Train Epoch: 281 [33792/101520 (33%)] Loss: -1052.303589\n",
      "Train Epoch: 281 [45056/101520 (44%)] Loss: -1042.471924\n",
      "Train Epoch: 281 [56320/101520 (55%)] Loss: -1044.231201\n",
      "Train Epoch: 281 [67584/101520 (67%)] Loss: -1048.985962\n",
      "Train Epoch: 281 [78848/101520 (78%)] Loss: -1050.265259\n",
      "Train Epoch: 281 [90112/101520 (89%)] Loss: -1037.717651\n",
      "Train Epoch: 281 [101376/101520 (100%)] Loss: -1042.194946\n",
      "    epoch          : 281\n",
      "    loss           : -1042.744395193742\n",
      "    ess            : 1.9617517240083397\n",
      "    log_marginal   : 1042.7774510982647\n",
      "    log_joint      : 1250.6907517322943\n",
      "    val_loss       : -1043.6578316066575\n",
      "    val_ess        : 1.9601075597431348\n",
      "    val_log_marginal: 1043.6921068274457\n",
      "    val_log_joint  : 1251.936666737432\n",
      "Train Epoch: 282 [0/101520 (0%)] Loss: -1047.255615\n",
      "Train Epoch: 282 [11264/101520 (11%)] Loss: -1044.886108\n",
      "Train Epoch: 282 [22528/101520 (22%)] Loss: -1038.203979\n",
      "Train Epoch: 282 [33792/101520 (33%)] Loss: -1043.193481\n",
      "Train Epoch: 282 [45056/101520 (44%)] Loss: -1047.505981\n",
      "Train Epoch: 282 [56320/101520 (55%)] Loss: -1048.540039\n",
      "Train Epoch: 282 [67584/101520 (67%)] Loss: -1046.136963\n",
      "Train Epoch: 282 [78848/101520 (78%)] Loss: -1041.340210\n",
      "Train Epoch: 282 [90112/101520 (89%)] Loss: -1042.817627\n",
      "Train Epoch: 282 [101376/101520 (100%)] Loss: -1045.950439\n",
      "    epoch          : 282\n",
      "    loss           : -1043.2320378749214\n",
      "    ess            : 1.960656319431324\n",
      "    log_marginal   : 1043.26693464883\n",
      "    log_joint      : 1251.2123747399105\n",
      "    val_loss       : -1043.6323613705842\n",
      "    val_ess        : 1.961891277976658\n",
      "    val_log_marginal: 1043.6685313349185\n",
      "    val_log_joint  : 1251.4743864639945\n",
      "Train Epoch: 283 [0/101520 (0%)] Loss: -1039.381348\n",
      "Train Epoch: 283 [11264/101520 (11%)] Loss: -1042.605469\n",
      "Train Epoch: 283 [22528/101520 (22%)] Loss: -1050.182617\n",
      "Train Epoch: 283 [33792/101520 (33%)] Loss: -1042.313965\n",
      "Train Epoch: 283 [45056/101520 (44%)] Loss: -1041.838257\n",
      "Train Epoch: 283 [56320/101520 (55%)] Loss: -1039.886963\n",
      "Train Epoch: 283 [67584/101520 (67%)] Loss: -1045.656250\n",
      "Train Epoch: 283 [78848/101520 (78%)] Loss: -1038.569214\n",
      "Train Epoch: 283 [90112/101520 (89%)] Loss: -1040.255981\n",
      "Train Epoch: 283 [101376/101520 (100%)] Loss: -1034.316040\n",
      "    epoch          : 283\n",
      "    loss           : -1042.0936960191582\n",
      "    ess            : 1.9599786553550604\n",
      "    log_marginal   : 1042.1283793233747\n",
      "    log_joint      : 1250.084745627552\n",
      "    val_loss       : -1042.6190132472825\n",
      "    val_ess        : 1.9596386474111807\n",
      "    val_log_marginal: 1042.6540102751358\n",
      "    val_log_joint  : 1250.6515847911005\n",
      "Train Epoch: 284 [0/101520 (0%)] Loss: -1032.144653\n",
      "Train Epoch: 284 [11264/101520 (11%)] Loss: -1038.249756\n",
      "Train Epoch: 284 [22528/101520 (22%)] Loss: -1037.326782\n",
      "Train Epoch: 284 [33792/101520 (33%)] Loss: -1040.366943\n",
      "Train Epoch: 284 [45056/101520 (44%)] Loss: -1038.990723\n",
      "Train Epoch: 284 [56320/101520 (55%)] Loss: -1046.168213\n",
      "Train Epoch: 284 [67584/101520 (67%)] Loss: -1042.692383\n",
      "Train Epoch: 284 [78848/101520 (78%)] Loss: -1049.039551\n",
      "Train Epoch: 284 [90112/101520 (89%)] Loss: -1041.098755\n",
      "Train Epoch: 284 [101376/101520 (100%)] Loss: -1029.635864\n",
      "    epoch          : 284\n",
      "    loss           : -1043.073794877709\n",
      "    ess            : 1.9611008568624755\n",
      "    log_marginal   : 1043.1087272298994\n",
      "    log_joint      : 1251.141769025793\n",
      "    val_loss       : -1043.1967401919158\n",
      "    val_ess        : 1.962743614030921\n",
      "    val_log_marginal: 1043.2296779466712\n",
      "    val_log_joint  : 1251.028421153193\n",
      "Train Epoch: 285 [0/101520 (0%)] Loss: -1050.599854\n",
      "Train Epoch: 285 [11264/101520 (11%)] Loss: -1047.808472\n",
      "Train Epoch: 285 [22528/101520 (22%)] Loss: -1045.626099\n",
      "Train Epoch: 285 [33792/101520 (33%)] Loss: -1044.422241\n",
      "Train Epoch: 285 [45056/101520 (44%)] Loss: -1034.421143\n",
      "Train Epoch: 285 [56320/101520 (55%)] Loss: -1045.158447\n",
      "Train Epoch: 285 [67584/101520 (67%)] Loss: -1035.607910\n",
      "Train Epoch: 285 [78848/101520 (78%)] Loss: -1042.837524\n",
      "Train Epoch: 285 [90112/101520 (89%)] Loss: -1044.102295\n",
      "Train Epoch: 285 [101376/101520 (100%)] Loss: -1049.644165\n",
      "    epoch          : 285\n",
      "    loss           : -1042.6769829371467\n",
      "    ess            : 1.9616399919567395\n",
      "    log_marginal   : 1042.710338803392\n",
      "    log_joint      : 1250.7025109679255\n",
      "    val_loss       : -1042.7986264436142\n",
      "    val_ess        : 1.960417374320652\n",
      "    val_log_marginal: 1042.8308954653533\n",
      "    val_log_joint  : 1250.8624745244565\n",
      "Train Epoch: 286 [0/101520 (0%)] Loss: -1049.322754\n",
      "Train Epoch: 286 [11264/101520 (11%)] Loss: -1047.806519\n",
      "Train Epoch: 286 [22528/101520 (22%)] Loss: -1044.629028\n",
      "Train Epoch: 286 [33792/101520 (33%)] Loss: -1043.232178\n",
      "Train Epoch: 286 [45056/101520 (44%)] Loss: -1046.608398\n",
      "Train Epoch: 286 [56320/101520 (55%)] Loss: -1041.051392\n",
      "Train Epoch: 286 [67584/101520 (67%)] Loss: -1040.370972\n",
      "Train Epoch: 286 [78848/101520 (78%)] Loss: -1045.003174\n",
      "Train Epoch: 286 [90112/101520 (89%)] Loss: -1048.957397\n",
      "Train Epoch: 286 [101376/101520 (100%)] Loss: -1048.043213\n",
      "    epoch          : 286\n",
      "    loss           : -1044.198117663513\n",
      "    ess            : 1.9614110130760538\n",
      "    log_marginal   : 1044.2327525076555\n",
      "    log_joint      : 1252.211445410647\n",
      "    val_loss       : -1045.6948666779892\n",
      "    val_ess        : 1.9607098828191343\n",
      "    val_log_marginal: 1045.7315355383832\n",
      "    val_log_joint  : 1253.313131581182\n",
      "Train Epoch: 287 [0/101520 (0%)] Loss: -1040.745117\n",
      "Train Epoch: 287 [11264/101520 (11%)] Loss: -1036.861450\n",
      "Train Epoch: 287 [22528/101520 (22%)] Loss: -1051.512939\n",
      "Train Epoch: 287 [33792/101520 (33%)] Loss: -1041.623047\n",
      "Train Epoch: 287 [45056/101520 (44%)] Loss: -1040.179565\n",
      "Train Epoch: 287 [56320/101520 (55%)] Loss: -1041.281860\n",
      "Train Epoch: 287 [67584/101520 (67%)] Loss: -1052.473145\n",
      "Train Epoch: 287 [78848/101520 (78%)] Loss: -1047.603516\n",
      "Train Epoch: 287 [90112/101520 (89%)] Loss: -1039.289185\n",
      "Train Epoch: 287 [101376/101520 (100%)] Loss: -1035.031860\n",
      "    epoch          : 287\n",
      "    loss           : -1043.5932150989322\n",
      "    ess            : 1.961642436645738\n",
      "    log_marginal   : 1043.626680153698\n",
      "    log_joint      : 1251.557701225856\n",
      "    val_loss       : -1044.4847730553668\n",
      "    val_ess        : 1.963137097980665\n",
      "    val_log_marginal: 1044.5150677224865\n",
      "    val_log_joint  : 1252.4081765879755\n",
      "Train Epoch: 288 [0/101520 (0%)] Loss: -1047.772461\n",
      "Train Epoch: 288 [11264/101520 (11%)] Loss: -1045.007202\n",
      "Train Epoch: 288 [22528/101520 (22%)] Loss: -1049.021484\n",
      "Train Epoch: 288 [33792/101520 (33%)] Loss: -1046.339722\n",
      "Train Epoch: 288 [45056/101520 (44%)] Loss: -1044.030518\n",
      "Train Epoch: 288 [56320/101520 (55%)] Loss: -1048.747192\n",
      "Train Epoch: 288 [67584/101520 (67%)] Loss: -1047.080078\n",
      "Train Epoch: 288 [78848/101520 (78%)] Loss: -1039.487549\n",
      "Train Epoch: 288 [90112/101520 (89%)] Loss: -1041.406372\n",
      "Train Epoch: 288 [101376/101520 (100%)] Loss: -1049.476562\n",
      "    epoch          : 288\n",
      "    loss           : -1044.8538370563756\n",
      "    ess            : 1.9609013394494752\n",
      "    log_marginal   : 1044.8885461241755\n",
      "    log_joint      : 1252.7365575435772\n",
      "    val_loss       : -1046.632223378057\n",
      "    val_ess        : 1.9622887165650078\n",
      "    val_log_marginal: 1046.6646622367527\n",
      "    val_log_joint  : 1254.6327435037365\n",
      "Train Epoch: 289 [0/101520 (0%)] Loss: -1033.558105\n",
      "Train Epoch: 289 [11264/101520 (11%)] Loss: -1044.351562\n",
      "Train Epoch: 289 [22528/101520 (22%)] Loss: -1045.056763\n",
      "Train Epoch: 289 [33792/101520 (33%)] Loss: -1039.539917\n",
      "Train Epoch: 289 [45056/101520 (44%)] Loss: -1035.975464\n",
      "Train Epoch: 289 [56320/101520 (55%)] Loss: -1048.111328\n",
      "Train Epoch: 289 [67584/101520 (67%)] Loss: -1046.992188\n",
      "Train Epoch: 289 [78848/101520 (78%)] Loss: -1050.497314\n",
      "Train Epoch: 289 [90112/101520 (89%)] Loss: -1040.015137\n",
      "Train Epoch: 289 [101376/101520 (100%)] Loss: -1036.743774\n",
      "    epoch          : 289\n",
      "    loss           : -1043.9456805511934\n",
      "    ess            : 1.9611774203765333\n",
      "    log_marginal   : 1043.9804601621388\n",
      "    log_joint      : 1251.9670066641802\n",
      "    val_loss       : -1043.354200280231\n",
      "    val_ess        : 1.9617694927298504\n",
      "    val_log_marginal: 1043.3868355129075\n",
      "    val_log_joint  : 1251.3747877038043\n",
      "Train Epoch: 290 [0/101520 (0%)] Loss: -1039.595703\n",
      "Train Epoch: 290 [11264/101520 (11%)] Loss: -1041.669678\n",
      "Train Epoch: 290 [22528/101520 (22%)] Loss: -1042.797119\n",
      "Train Epoch: 290 [33792/101520 (33%)] Loss: -1052.132812\n",
      "Train Epoch: 290 [45056/101520 (44%)] Loss: -1053.510376\n",
      "Train Epoch: 290 [56320/101520 (55%)] Loss: -1047.615601\n",
      "Train Epoch: 290 [67584/101520 (67%)] Loss: -1036.442993\n",
      "Train Epoch: 290 [78848/101520 (78%)] Loss: -1050.310303\n",
      "Train Epoch: 290 [90112/101520 (89%)] Loss: -1045.451538\n",
      "Train Epoch: 290 [101376/101520 (100%)] Loss: -1044.170532\n",
      "    epoch          : 290\n",
      "    loss           : -1045.0556198963568\n",
      "    ess            : 1.9613997702622534\n",
      "    log_marginal   : 1045.0901621142823\n",
      "    log_joint      : 1252.9810901430983\n",
      "    val_loss       : -1045.7388969089675\n",
      "    val_ess        : 1.9602281736290974\n",
      "    val_log_marginal: 1045.774360988451\n",
      "    val_log_joint  : 1253.589742909307\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch290.pth ...\n",
      "Train Epoch: 291 [0/101520 (0%)] Loss: -1046.577393\n",
      "Train Epoch: 291 [11264/101520 (11%)] Loss: -1053.000610\n",
      "Train Epoch: 291 [22528/101520 (22%)] Loss: -1038.103516\n",
      "Train Epoch: 291 [33792/101520 (33%)] Loss: -1044.217163\n",
      "Train Epoch: 291 [45056/101520 (44%)] Loss: -1047.720825\n",
      "Train Epoch: 291 [56320/101520 (55%)] Loss: -1043.775146\n",
      "Train Epoch: 291 [67584/101520 (67%)] Loss: -1041.832031\n",
      "Train Epoch: 291 [78848/101520 (78%)] Loss: -1044.466797\n",
      "Train Epoch: 291 [90112/101520 (89%)] Loss: -1035.720459\n",
      "Train Epoch: 291 [101376/101520 (100%)] Loss: -1029.329346\n",
      "    epoch          : 291\n",
      "    loss           : -1044.5490213518765\n",
      "    ess            : 1.9616959011135389\n",
      "    log_marginal   : 1044.582230611063\n",
      "    log_joint      : 1252.5203201063914\n",
      "    val_loss       : -1045.3715236497962\n",
      "    val_ess        : 1.9628043433894282\n",
      "    val_log_marginal: 1045.4063083814538\n",
      "    val_log_joint  : 1253.082954738451\n",
      "Train Epoch: 292 [0/101520 (0%)] Loss: -1040.469727\n",
      "Train Epoch: 292 [11264/101520 (11%)] Loss: -1048.380859\n",
      "Train Epoch: 292 [22528/101520 (22%)] Loss: -1042.071045\n",
      "Train Epoch: 292 [33792/101520 (33%)] Loss: -1047.787842\n",
      "Train Epoch: 292 [45056/101520 (44%)] Loss: -1046.902710\n",
      "Train Epoch: 292 [56320/101520 (55%)] Loss: -1040.226318\n",
      "Train Epoch: 292 [67584/101520 (67%)] Loss: -1044.727905\n",
      "Train Epoch: 292 [78848/101520 (78%)] Loss: -1046.652954\n",
      "Train Epoch: 292 [90112/101520 (89%)] Loss: -1040.474243\n",
      "Train Epoch: 292 [101376/101520 (100%)] Loss: -1055.356079\n",
      "    epoch          : 292\n",
      "    loss           : -1045.5703646405857\n",
      "    ess            : 1.9614592041801568\n",
      "    log_marginal   : 1045.6041100276775\n",
      "    log_joint      : 1253.4829346929962\n",
      "    val_loss       : -1046.9159307065217\n",
      "    val_ess        : 1.959911144298056\n",
      "    val_log_marginal: 1046.9506942085598\n",
      "    val_log_joint  : 1254.691698157269\n",
      "Train Epoch: 293 [0/101520 (0%)] Loss: -1039.718018\n",
      "Train Epoch: 293 [11264/101520 (11%)] Loss: -1046.769897\n",
      "Train Epoch: 293 [22528/101520 (22%)] Loss: -1050.650146\n",
      "Train Epoch: 293 [33792/101520 (33%)] Loss: -1050.377197\n",
      "Train Epoch: 293 [45056/101520 (44%)] Loss: -1049.416260\n",
      "Train Epoch: 293 [56320/101520 (55%)] Loss: -1048.649658\n",
      "Train Epoch: 293 [67584/101520 (67%)] Loss: -1044.931152\n",
      "Train Epoch: 293 [78848/101520 (78%)] Loss: -1046.115967\n",
      "Train Epoch: 293 [90112/101520 (89%)] Loss: -1045.278076\n",
      "Train Epoch: 293 [101376/101520 (100%)] Loss: -1045.453735\n",
      "    epoch          : 293\n",
      "    loss           : -1045.340247992894\n",
      "    ess            : 1.9617373356268035\n",
      "    log_marginal   : 1045.3729438206658\n",
      "    log_joint      : 1253.3354553529366\n",
      "    val_loss       : -1044.8064124065897\n",
      "    val_ess        : 1.9641097473061604\n",
      "    val_log_marginal: 1044.836574388587\n",
      "    val_log_joint  : 1252.6984969429348\n",
      "Train Epoch: 294 [0/101520 (0%)] Loss: -1038.985107\n",
      "Train Epoch: 294 [11264/101520 (11%)] Loss: -1044.778198\n",
      "Train Epoch: 294 [22528/101520 (22%)] Loss: -1050.222168\n",
      "Train Epoch: 294 [33792/101520 (33%)] Loss: -1044.234009\n",
      "Train Epoch: 294 [45056/101520 (44%)] Loss: -1052.720825\n",
      "Train Epoch: 294 [56320/101520 (55%)] Loss: -1039.854248\n",
      "Train Epoch: 294 [67584/101520 (67%)] Loss: -1046.207275\n",
      "Train Epoch: 294 [78848/101520 (78%)] Loss: -1045.587280\n",
      "Train Epoch: 294 [90112/101520 (89%)] Loss: -1040.535156\n",
      "Train Epoch: 294 [101376/101520 (100%)] Loss: -1063.407837\n",
      "    epoch          : 294\n",
      "    loss           : -1045.941994518491\n",
      "    ess            : 1.9607959278863878\n",
      "    log_marginal   : 1045.9767894649026\n",
      "    log_joint      : 1253.8589701436872\n",
      "    val_loss       : -1046.6816618546195\n",
      "    val_ess        : 1.9617319988167805\n",
      "    val_log_marginal: 1046.7155124830163\n",
      "    val_log_joint  : 1254.7090746008832\n",
      "Train Epoch: 295 [0/101520 (0%)] Loss: -1042.854370\n",
      "Train Epoch: 295 [11264/101520 (11%)] Loss: -1045.041870\n",
      "Train Epoch: 295 [22528/101520 (22%)] Loss: -1046.546631\n",
      "Train Epoch: 295 [33792/101520 (33%)] Loss: -1042.804199\n",
      "Train Epoch: 295 [45056/101520 (44%)] Loss: -1045.885010\n",
      "Train Epoch: 295 [56320/101520 (55%)] Loss: -1051.194580\n",
      "Train Epoch: 295 [67584/101520 (67%)] Loss: -1043.323364\n",
      "Train Epoch: 295 [78848/101520 (78%)] Loss: -1045.391602\n",
      "Train Epoch: 295 [90112/101520 (89%)] Loss: -1046.305786\n",
      "Train Epoch: 295 [101376/101520 (100%)] Loss: -1056.314697\n",
      "    epoch          : 295\n",
      "    loss           : -1045.9297917811714\n",
      "    ess            : 1.9620245761008719\n",
      "    log_marginal   : 1045.9629881585663\n",
      "    log_joint      : 1253.8879026480056\n",
      "    val_loss       : -1045.8790707795517\n",
      "    val_ess        : 1.9627471177474312\n",
      "    val_log_marginal: 1045.9119342306385\n",
      "    val_log_joint  : 1253.6196660580842\n",
      "Train Epoch: 296 [0/101520 (0%)] Loss: -1042.836548\n",
      "Train Epoch: 296 [11264/101520 (11%)] Loss: -1049.876221\n",
      "Train Epoch: 296 [22528/101520 (22%)] Loss: -1040.755981\n",
      "Train Epoch: 296 [33792/101520 (33%)] Loss: -1045.172363\n",
      "Train Epoch: 296 [45056/101520 (44%)] Loss: -1041.261841\n",
      "Train Epoch: 296 [56320/101520 (55%)] Loss: -1055.505859\n",
      "Train Epoch: 296 [67584/101520 (67%)] Loss: -1038.321899\n",
      "Train Epoch: 296 [78848/101520 (78%)] Loss: -1044.777100\n",
      "Train Epoch: 296 [90112/101520 (89%)] Loss: -1054.258789\n",
      "Train Epoch: 296 [101376/101520 (100%)] Loss: -1046.296021\n",
      "    epoch          : 296\n",
      "    loss           : -1045.92795336546\n",
      "    ess            : 1.9616836327404232\n",
      "    log_marginal   : 1045.9613245671717\n",
      "    log_joint      : 1253.9302990784\n",
      "    val_loss       : -1047.17041015625\n",
      "    val_ess        : 1.9607937595118647\n",
      "    val_log_marginal: 1047.2048021399457\n",
      "    val_log_joint  : 1255.4113344938858\n",
      "Train Epoch: 297 [0/101520 (0%)] Loss: -1045.208740\n",
      "Train Epoch: 297 [11264/101520 (11%)] Loss: -1047.627197\n",
      "Train Epoch: 297 [22528/101520 (22%)] Loss: -1041.850952\n",
      "Train Epoch: 297 [33792/101520 (33%)] Loss: -1037.880127\n",
      "Train Epoch: 297 [45056/101520 (44%)] Loss: -1041.920654\n",
      "Train Epoch: 297 [56320/101520 (55%)] Loss: -1043.475830\n",
      "Train Epoch: 297 [67584/101520 (67%)] Loss: -1045.506226\n",
      "Train Epoch: 297 [78848/101520 (78%)] Loss: -1054.865723\n",
      "Train Epoch: 297 [90112/101520 (89%)] Loss: -1049.343872\n",
      "Train Epoch: 297 [101376/101520 (100%)] Loss: -1027.349854\n",
      "    epoch          : 297\n",
      "    loss           : -1046.3383028423366\n",
      "    ess            : 1.962141135829178\n",
      "    log_marginal   : 1046.371816970595\n",
      "    log_joint      : 1254.2567052793263\n",
      "    val_loss       : -1046.6209345278533\n",
      "    val_ess        : 1.9586639559787253\n",
      "    val_log_marginal: 1046.6562128481658\n",
      "    val_log_joint  : 1254.6138438349185\n",
      "Train Epoch: 298 [0/101520 (0%)] Loss: -1053.504761\n",
      "Train Epoch: 298 [11264/101520 (11%)] Loss: -1044.436768\n",
      "Train Epoch: 298 [22528/101520 (22%)] Loss: -1052.933960\n",
      "Train Epoch: 298 [33792/101520 (33%)] Loss: -1045.648193\n",
      "Train Epoch: 298 [45056/101520 (44%)] Loss: -1057.187134\n",
      "Train Epoch: 298 [56320/101520 (55%)] Loss: -1046.500244\n",
      "Train Epoch: 298 [67584/101520 (67%)] Loss: -1055.493896\n",
      "Train Epoch: 298 [78848/101520 (78%)] Loss: -1046.451416\n",
      "Train Epoch: 298 [90112/101520 (89%)] Loss: -1041.459717\n",
      "Train Epoch: 298 [101376/101520 (100%)] Loss: -1050.676392\n",
      "    epoch          : 298\n",
      "    loss           : -1046.3943337291928\n",
      "    ess            : 1.9608588308545214\n",
      "    log_marginal   : 1046.428391959799\n",
      "    log_joint      : 1254.3960647966394\n",
      "    val_loss       : -1046.6405453889267\n",
      "    val_ess        : 1.9636963346730107\n",
      "    val_log_marginal: 1046.670707370924\n",
      "    val_log_joint  : 1254.405082370924\n",
      "Train Epoch: 299 [0/101520 (0%)] Loss: -1044.170898\n",
      "Train Epoch: 299 [11264/101520 (11%)] Loss: -1041.112549\n",
      "Train Epoch: 299 [22528/101520 (22%)] Loss: -1046.305542\n",
      "Train Epoch: 299 [33792/101520 (33%)] Loss: -1048.470947\n",
      "Train Epoch: 299 [45056/101520 (44%)] Loss: -1054.861328\n",
      "Train Epoch: 299 [56320/101520 (55%)] Loss: -1047.442871\n",
      "Train Epoch: 299 [67584/101520 (67%)] Loss: -1042.784668\n",
      "Train Epoch: 299 [78848/101520 (78%)] Loss: -1042.158691\n",
      "Train Epoch: 299 [90112/101520 (89%)] Loss: -1040.313477\n",
      "Train Epoch: 299 [101376/101520 (100%)] Loss: -1061.587158\n",
      "    epoch          : 299\n",
      "    loss           : -1046.057052228918\n",
      "    ess            : 1.9616789620126311\n",
      "    log_marginal   : 1046.090768785333\n",
      "    log_joint      : 1254.08909537924\n",
      "    val_loss       : -1044.0951723845108\n",
      "    val_ess        : 1.962505578994751\n",
      "    val_log_marginal: 1044.126411769701\n",
      "    val_log_joint  : 1252.0343389096467\n",
      "Train Epoch: 300 [0/101520 (0%)] Loss: -1043.994629\n",
      "Train Epoch: 300 [11264/101520 (11%)] Loss: -1052.789062\n",
      "Train Epoch: 300 [22528/101520 (22%)] Loss: -1049.714844\n",
      "Train Epoch: 300 [33792/101520 (33%)] Loss: -1040.675781\n",
      "Train Epoch: 300 [45056/101520 (44%)] Loss: -1049.113037\n",
      "Train Epoch: 300 [56320/101520 (55%)] Loss: -1044.599365\n",
      "Train Epoch: 300 [67584/101520 (67%)] Loss: -1042.018066\n",
      "Train Epoch: 300 [78848/101520 (78%)] Loss: -1044.626221\n",
      "Train Epoch: 300 [90112/101520 (89%)] Loss: -1040.620361\n",
      "Train Epoch: 300 [101376/101520 (100%)] Loss: -1043.287598\n",
      "    epoch          : 300\n",
      "    loss           : -1046.8896116323806\n",
      "    ess            : 1.9618499931977622\n",
      "    log_marginal   : 1046.92315275106\n",
      "    log_joint      : 1254.873141341473\n",
      "    val_loss       : -1046.651510487432\n",
      "    val_ess        : 1.9629115643708601\n",
      "    val_log_marginal: 1046.682325280231\n",
      "    val_log_joint  : 1254.6011856742527\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [0/101520 (0%)] Loss: -1052.187988\n",
      "Train Epoch: 301 [11264/101520 (11%)] Loss: -1049.527344\n",
      "Train Epoch: 301 [22528/101520 (22%)] Loss: -1040.694946\n",
      "Train Epoch: 301 [33792/101520 (33%)] Loss: -1042.348877\n",
      "Train Epoch: 301 [45056/101520 (44%)] Loss: -1048.806396\n",
      "Train Epoch: 301 [56320/101520 (55%)] Loss: -1040.170898\n",
      "Train Epoch: 301 [67584/101520 (67%)] Loss: -1038.545166\n",
      "Train Epoch: 301 [78848/101520 (78%)] Loss: -1043.362183\n",
      "Train Epoch: 301 [90112/101520 (89%)] Loss: -1040.729248\n",
      "Train Epoch: 301 [101376/101520 (100%)] Loss: -1047.592407\n",
      "    epoch          : 301\n",
      "    loss           : -1045.3847680786746\n",
      "    ess            : 1.9620191332083852\n",
      "    log_marginal   : 1045.417271906407\n",
      "    log_joint      : 1253.4217676517353\n",
      "    val_loss       : -1043.8677076256793\n",
      "    val_ess        : 1.9614648818969727\n",
      "    val_log_marginal: 1043.900634765625\n",
      "    val_log_joint  : 1251.9770826256793\n",
      "Train Epoch: 302 [0/101520 (0%)] Loss: -1047.664307\n",
      "Train Epoch: 302 [11264/101520 (11%)] Loss: -1043.454468\n",
      "Train Epoch: 302 [22528/101520 (22%)] Loss: -1040.280029\n",
      "Train Epoch: 302 [33792/101520 (33%)] Loss: -1049.696289\n",
      "Train Epoch: 302 [45056/101520 (44%)] Loss: -1043.739868\n",
      "Train Epoch: 302 [56320/101520 (55%)] Loss: -1049.183228\n",
      "Train Epoch: 302 [67584/101520 (67%)] Loss: -1042.021606\n",
      "Train Epoch: 302 [78848/101520 (78%)] Loss: -1050.118408\n",
      "Train Epoch: 302 [90112/101520 (89%)] Loss: -1046.566528\n",
      "Train Epoch: 302 [101376/101520 (100%)] Loss: -1048.745239\n",
      "    epoch          : 302\n",
      "    loss           : -1046.7311895169205\n",
      "    ess            : 1.9616906810645482\n",
      "    log_marginal   : 1046.7640221370525\n",
      "    log_joint      : 1254.737204700259\n",
      "    val_loss       : -1046.0621444038723\n",
      "    val_ess        : 1.9651129764059316\n",
      "    val_log_marginal: 1046.0907460088315\n",
      "    val_log_joint  : 1253.916652513587\n",
      "Train Epoch: 303 [0/101520 (0%)] Loss: -1055.520508\n",
      "Train Epoch: 303 [11264/101520 (11%)] Loss: -1051.761963\n",
      "Train Epoch: 303 [22528/101520 (22%)] Loss: -1044.549927\n",
      "Train Epoch: 303 [33792/101520 (33%)] Loss: -1040.244141\n",
      "Train Epoch: 303 [45056/101520 (44%)] Loss: -1049.706665\n",
      "Train Epoch: 303 [56320/101520 (55%)] Loss: -1045.898926\n",
      "Train Epoch: 303 [67584/101520 (67%)] Loss: -1047.176636\n",
      "Train Epoch: 303 [78848/101520 (78%)] Loss: -1048.402832\n",
      "Train Epoch: 303 [90112/101520 (89%)] Loss: -1044.592529\n",
      "Train Epoch: 303 [101376/101520 (100%)] Loss: -1057.826660\n",
      "    epoch          : 303\n",
      "    loss           : -1046.558233673249\n",
      "    ess            : 1.961340499882722\n",
      "    log_marginal   : 1046.5915177695117\n",
      "    log_joint      : 1254.5592593092415\n",
      "    val_loss       : -1045.7962115743885\n",
      "    val_ess        : 1.9596671384313833\n",
      "    val_log_marginal: 1045.8308901579483\n",
      "    val_log_joint  : 1253.9586606233017\n",
      "Train Epoch: 304 [0/101520 (0%)] Loss: -1043.822998\n",
      "Train Epoch: 304 [11264/101520 (11%)] Loss: -1046.697632\n",
      "Train Epoch: 304 [22528/101520 (22%)] Loss: -1047.366089\n",
      "Train Epoch: 304 [33792/101520 (33%)] Loss: -1054.445312\n",
      "Train Epoch: 304 [45056/101520 (44%)] Loss: -1055.437012\n",
      "Train Epoch: 304 [56320/101520 (55%)] Loss: -1054.034668\n",
      "Train Epoch: 304 [67584/101520 (67%)] Loss: -1052.149048\n",
      "Train Epoch: 304 [78848/101520 (78%)] Loss: -1054.006958\n",
      "Train Epoch: 304 [90112/101520 (89%)] Loss: -1045.403320\n",
      "Train Epoch: 304 [101376/101520 (100%)] Loss: -1053.642944\n",
      "    epoch          : 304\n",
      "    loss           : -1047.79612662924\n",
      "    ess            : 1.961706109981441\n",
      "    log_marginal   : 1047.8297321568782\n",
      "    log_joint      : 1255.8107719996467\n",
      "    val_loss       : -1047.2153771441915\n",
      "    val_ess        : 1.9623596616413281\n",
      "    val_log_marginal: 1047.2477815047555\n",
      "    val_log_joint  : 1255.3368450662365\n",
      "Train Epoch: 305 [0/101520 (0%)] Loss: -1050.798828\n",
      "Train Epoch: 305 [11264/101520 (11%)] Loss: -1054.835205\n",
      "Train Epoch: 305 [22528/101520 (22%)] Loss: -1041.074219\n",
      "Train Epoch: 305 [33792/101520 (33%)] Loss: -1047.696777\n",
      "Train Epoch: 305 [45056/101520 (44%)] Loss: -1047.363892\n",
      "Train Epoch: 305 [56320/101520 (55%)] Loss: -1047.814941\n",
      "Train Epoch: 305 [67584/101520 (67%)] Loss: -1047.906616\n",
      "Train Epoch: 305 [78848/101520 (78%)] Loss: -1045.344238\n",
      "Train Epoch: 305 [90112/101520 (89%)] Loss: -1044.181641\n",
      "Train Epoch: 305 [101376/101520 (100%)] Loss: -1045.671021\n",
      "    epoch          : 305\n",
      "    loss           : -1047.393046776853\n",
      "    ess            : 1.9616387567328448\n",
      "    log_marginal   : 1047.426659665515\n",
      "    log_joint      : 1255.403113590413\n",
      "    val_loss       : -1046.499368418818\n",
      "    val_ess        : 1.9594442740730618\n",
      "    val_log_marginal: 1046.536281419837\n",
      "    val_log_joint  : 1254.1912045686142\n",
      "Train Epoch: 306 [0/101520 (0%)] Loss: -1049.598389\n",
      "Train Epoch: 306 [11264/101520 (11%)] Loss: -1049.363281\n",
      "Train Epoch: 306 [22528/101520 (22%)] Loss: -1052.156982\n",
      "Train Epoch: 306 [33792/101520 (33%)] Loss: -1052.007324\n",
      "Train Epoch: 306 [45056/101520 (44%)] Loss: -1046.118530\n",
      "Train Epoch: 306 [56320/101520 (55%)] Loss: -1047.117432\n",
      "Train Epoch: 306 [67584/101520 (67%)] Loss: -1049.732300\n",
      "Train Epoch: 306 [78848/101520 (78%)] Loss: -1050.437256\n",
      "Train Epoch: 306 [90112/101520 (89%)] Loss: -1050.433105\n",
      "Train Epoch: 306 [101376/101520 (100%)] Loss: -1044.713013\n",
      "    epoch          : 306\n",
      "    loss           : -1048.351829950534\n",
      "    ess            : 1.9614294989025174\n",
      "    log_marginal   : 1048.3858887945587\n",
      "    log_joint      : 1256.3098224275675\n",
      "    val_loss       : -1048.8352369225543\n",
      "    val_ess        : 1.9640423163123752\n",
      "    val_log_marginal: 1048.8659986413043\n",
      "    val_log_joint  : 1256.5229704483695\n",
      "Train Epoch: 307 [0/101520 (0%)] Loss: -1048.688354\n",
      "Train Epoch: 307 [11264/101520 (11%)] Loss: -1047.365234\n",
      "Train Epoch: 307 [22528/101520 (22%)] Loss: -1049.672119\n",
      "Train Epoch: 307 [33792/101520 (33%)] Loss: -1031.922852\n",
      "Train Epoch: 307 [45056/101520 (44%)] Loss: -1044.426392\n",
      "Train Epoch: 307 [56320/101520 (55%)] Loss: -1042.569580\n",
      "Train Epoch: 307 [67584/101520 (67%)] Loss: -1043.127441\n",
      "Train Epoch: 307 [78848/101520 (78%)] Loss: -1041.792725\n",
      "Train Epoch: 307 [90112/101520 (89%)] Loss: -1043.825195\n",
      "Train Epoch: 307 [101376/101520 (100%)] Loss: -1050.445312\n",
      "    epoch          : 307\n",
      "    loss           : -1047.8357301644944\n",
      "    ess            : 1.961197772816797\n",
      "    log_marginal   : 1047.8698454430355\n",
      "    log_joint      : 1255.7613691013662\n",
      "    val_loss       : -1047.187107252038\n",
      "    val_ess        : 1.9626883786657583\n",
      "    val_log_marginal: 1047.2185695482337\n",
      "    val_log_joint  : 1254.9408118206522\n",
      "Train Epoch: 308 [0/101520 (0%)] Loss: -1043.969971\n",
      "Train Epoch: 308 [11264/101520 (11%)] Loss: -1045.199707\n",
      "Train Epoch: 308 [22528/101520 (22%)] Loss: -1043.004883\n",
      "Train Epoch: 308 [33792/101520 (33%)] Loss: -1050.107666\n",
      "Train Epoch: 308 [45056/101520 (44%)] Loss: -1049.148926\n",
      "Train Epoch: 308 [56320/101520 (55%)] Loss: -1038.936401\n",
      "Train Epoch: 308 [67584/101520 (67%)] Loss: -1050.811768\n",
      "Train Epoch: 308 [78848/101520 (78%)] Loss: -1046.903564\n",
      "Train Epoch: 308 [90112/101520 (89%)] Loss: -1056.187744\n",
      "Train Epoch: 308 [101376/101520 (100%)] Loss: -1048.603149\n",
      "    epoch          : 308\n",
      "    loss           : -1048.719272019276\n",
      "    ess            : 1.9614396304940458\n",
      "    log_marginal   : 1048.7526413807318\n",
      "    log_joint      : 1256.6799089441347\n",
      "    val_loss       : -1050.8466956097147\n",
      "    val_ess        : 1.9637226902920266\n",
      "    val_log_marginal: 1050.8777014690897\n",
      "    val_log_joint  : 1258.7029710852582\n",
      "Train Epoch: 309 [0/101520 (0%)] Loss: -1044.314941\n",
      "Train Epoch: 309 [11264/101520 (11%)] Loss: -1044.976807\n",
      "Train Epoch: 309 [22528/101520 (22%)] Loss: -1045.856812\n",
      "Train Epoch: 309 [33792/101520 (33%)] Loss: -1047.099854\n",
      "Train Epoch: 309 [45056/101520 (44%)] Loss: -1049.554810\n",
      "Train Epoch: 309 [56320/101520 (55%)] Loss: -1049.855713\n",
      "Train Epoch: 309 [67584/101520 (67%)] Loss: -1050.437622\n",
      "Train Epoch: 309 [78848/101520 (78%)] Loss: -1049.727539\n",
      "Train Epoch: 309 [90112/101520 (89%)] Loss: -1044.629272\n",
      "Train Epoch: 309 [101376/101520 (100%)] Loss: -1042.408691\n",
      "    epoch          : 309\n",
      "    loss           : -1048.113450553549\n",
      "    ess            : 1.9619798684239986\n",
      "    log_marginal   : 1048.1468542664495\n",
      "    log_joint      : 1256.0876710211212\n",
      "    val_loss       : -1047.6557882557745\n",
      "    val_ess        : 1.9627052649207737\n",
      "    val_log_marginal: 1047.6862899116848\n",
      "    val_log_joint  : 1255.288521144701\n",
      "Train Epoch: 310 [0/101520 (0%)] Loss: -1044.154541\n",
      "Train Epoch: 310 [11264/101520 (11%)] Loss: -1044.199097\n",
      "Train Epoch: 310 [22528/101520 (22%)] Loss: -1051.597900\n",
      "Train Epoch: 310 [33792/101520 (33%)] Loss: -1044.648193\n",
      "Train Epoch: 310 [45056/101520 (44%)] Loss: -1053.967529\n",
      "Train Epoch: 310 [56320/101520 (55%)] Loss: -1056.748291\n",
      "Train Epoch: 310 [67584/101520 (67%)] Loss: -1044.141113\n",
      "Train Epoch: 310 [78848/101520 (78%)] Loss: -1049.315674\n",
      "Train Epoch: 310 [90112/101520 (89%)] Loss: -1043.765381\n",
      "Train Epoch: 310 [101376/101520 (100%)] Loss: -1040.909790\n",
      "    epoch          : 310\n",
      "    loss           : -1048.854923420815\n",
      "    ess            : 1.9615488196138162\n",
      "    log_marginal   : 1048.8893576770572\n",
      "    log_joint      : 1256.8655668479114\n",
      "    val_loss       : -1050.0798923658288\n",
      "    val_ess        : 1.9624526086060896\n",
      "    val_log_marginal: 1050.1128141983695\n",
      "    val_log_joint  : 1258.1222773012908\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch310.pth ...\n",
      "Train Epoch: 311 [0/101520 (0%)] Loss: -1050.312622\n",
      "Train Epoch: 311 [11264/101520 (11%)] Loss: -1043.240234\n",
      "Train Epoch: 311 [22528/101520 (22%)] Loss: -1053.899902\n",
      "Train Epoch: 311 [33792/101520 (33%)] Loss: -1045.854980\n",
      "Train Epoch: 311 [45056/101520 (44%)] Loss: -1047.759033\n",
      "Train Epoch: 311 [56320/101520 (55%)] Loss: -1055.068481\n",
      "Train Epoch: 311 [67584/101520 (67%)] Loss: -1050.743408\n",
      "Train Epoch: 311 [78848/101520 (78%)] Loss: -1047.279907\n",
      "Train Epoch: 311 [90112/101520 (89%)] Loss: -1044.087646\n",
      "Train Epoch: 311 [101376/101520 (100%)] Loss: -1045.888062\n",
      "    epoch          : 311\n",
      "    loss           : -1048.8664826819645\n",
      "    ess            : 1.9614251588457194\n",
      "    log_marginal   : 1048.8996858069645\n",
      "    log_joint      : 1256.8350885285804\n",
      "    val_loss       : -1048.2344068444293\n",
      "    val_ess        : 1.9625499714975772\n",
      "    val_log_marginal: 1048.2660336701767\n",
      "    val_log_joint  : 1256.4237644361413\n",
      "Train Epoch: 312 [0/101520 (0%)] Loss: -1048.646973\n",
      "Train Epoch: 312 [11264/101520 (11%)] Loss: -1049.527100\n",
      "Train Epoch: 312 [22528/101520 (22%)] Loss: -1054.378784\n",
      "Train Epoch: 312 [33792/101520 (33%)] Loss: -1055.818848\n",
      "Train Epoch: 312 [45056/101520 (44%)] Loss: -1050.481934\n",
      "Train Epoch: 312 [56320/101520 (55%)] Loss: -1041.628418\n",
      "Train Epoch: 312 [67584/101520 (67%)] Loss: -1056.952148\n",
      "Train Epoch: 312 [78848/101520 (78%)] Loss: -1047.244751\n",
      "Train Epoch: 312 [90112/101520 (89%)] Loss: -1042.483887\n",
      "Train Epoch: 312 [101376/101520 (100%)] Loss: -1041.460693\n",
      "    epoch          : 312\n",
      "    loss           : -1049.085941180512\n",
      "    ess            : 1.9624966442884513\n",
      "    log_marginal   : 1049.1182020944566\n",
      "    log_joint      : 1257.055221787649\n",
      "    val_loss       : -1050.5006581182065\n",
      "    val_ess        : 1.9595011006230894\n",
      "    val_log_marginal: 1050.53857421875\n",
      "    val_log_joint  : 1258.4467136548913\n",
      "Train Epoch: 313 [0/101520 (0%)] Loss: -1049.739136\n",
      "Train Epoch: 313 [11264/101520 (11%)] Loss: -1053.044678\n",
      "Train Epoch: 313 [22528/101520 (22%)] Loss: -1056.662964\n",
      "Train Epoch: 313 [33792/101520 (33%)] Loss: -1049.261108\n",
      "Train Epoch: 313 [45056/101520 (44%)] Loss: -1046.184814\n",
      "Train Epoch: 313 [56320/101520 (55%)] Loss: -1057.409302\n",
      "Train Epoch: 313 [67584/101520 (67%)] Loss: -1045.609741\n",
      "Train Epoch: 313 [78848/101520 (78%)] Loss: -1046.550903\n",
      "Train Epoch: 313 [90112/101520 (89%)] Loss: -1044.626343\n",
      "Train Epoch: 313 [101376/101520 (100%)] Loss: -1042.385620\n",
      "    epoch          : 313\n",
      "    loss           : -1049.6846561911118\n",
      "    ess            : 1.961286324951517\n",
      "    log_marginal   : 1049.718550638937\n",
      "    log_joint      : 1257.6052755231235\n",
      "    val_loss       : -1047.92150613536\n",
      "    val_ess        : 1.9578550121058589\n",
      "    val_log_marginal: 1047.9580927309783\n",
      "    val_log_joint  : 1256.1088018002717\n",
      "Train Epoch: 314 [0/101520 (0%)] Loss: -1050.544067\n",
      "Train Epoch: 314 [11264/101520 (11%)] Loss: -1060.771362\n",
      "Train Epoch: 314 [22528/101520 (22%)] Loss: -1050.098755\n",
      "Train Epoch: 314 [33792/101520 (33%)] Loss: -1037.634766\n",
      "Train Epoch: 314 [45056/101520 (44%)] Loss: -1048.795532\n",
      "Train Epoch: 314 [56320/101520 (55%)] Loss: -1055.612305\n",
      "Train Epoch: 314 [67584/101520 (67%)] Loss: -1048.241699\n",
      "Train Epoch: 314 [78848/101520 (78%)] Loss: -1049.859741\n",
      "Train Epoch: 314 [90112/101520 (89%)] Loss: -1049.710449\n",
      "Train Epoch: 314 [101376/101520 (100%)] Loss: -1056.278687\n",
      "    epoch          : 314\n",
      "    loss           : -1049.2952040475816\n",
      "    ess            : 1.961313129669458\n",
      "    log_marginal   : 1049.3290003484217\n",
      "    log_joint      : 1257.3024963685616\n",
      "    val_loss       : -1049.3753821331522\n",
      "    val_ess        : 1.962705487790315\n",
      "    val_log_marginal: 1049.407614003057\n",
      "    val_log_joint  : 1257.3986232591712\n",
      "Train Epoch: 315 [0/101520 (0%)] Loss: -1051.827759\n",
      "Train Epoch: 315 [11264/101520 (11%)] Loss: -1047.293213\n",
      "Train Epoch: 315 [22528/101520 (22%)] Loss: -1044.146362\n",
      "Train Epoch: 315 [33792/101520 (33%)] Loss: -1050.552490\n",
      "Train Epoch: 315 [45056/101520 (44%)] Loss: -1048.872070\n",
      "Train Epoch: 315 [56320/101520 (55%)] Loss: -1049.149292\n",
      "Train Epoch: 315 [67584/101520 (67%)] Loss: -1048.100830\n",
      "Train Epoch: 315 [78848/101520 (78%)] Loss: -1053.845093\n",
      "Train Epoch: 315 [90112/101520 (89%)] Loss: -1045.920166\n",
      "Train Epoch: 315 [101376/101520 (100%)] Loss: -1054.443481\n",
      "    epoch          : 315\n",
      "    loss           : -1049.6986586987673\n",
      "    ess            : 1.9615373761210608\n",
      "    log_marginal   : 1049.7315906927215\n",
      "    log_joint      : 1257.6733564060537\n",
      "    val_loss       : -1051.3918881623642\n",
      "    val_ess        : 1.960447301035342\n",
      "    val_log_marginal: 1051.4250382133152\n",
      "    val_log_joint  : 1259.271144701087\n",
      "Train Epoch: 316 [0/101520 (0%)] Loss: -1049.008301\n",
      "Train Epoch: 316 [11264/101520 (11%)] Loss: -1060.389648\n",
      "Train Epoch: 316 [22528/101520 (22%)] Loss: -1050.785278\n",
      "Train Epoch: 316 [33792/101520 (33%)] Loss: -1051.512695\n",
      "Train Epoch: 316 [45056/101520 (44%)] Loss: -1054.721191\n",
      "Train Epoch: 316 [56320/101520 (55%)] Loss: -1055.533691\n",
      "Train Epoch: 316 [67584/101520 (67%)] Loss: -1046.417847\n",
      "Train Epoch: 316 [78848/101520 (78%)] Loss: -1051.170410\n",
      "Train Epoch: 316 [90112/101520 (89%)] Loss: -1050.582886\n",
      "Train Epoch: 316 [101376/101520 (100%)] Loss: -1054.468628\n",
      "    epoch          : 316\n",
      "    loss           : -1050.2221655150754\n",
      "    ess            : 1.9616389903590907\n",
      "    log_marginal   : 1050.2566261483198\n",
      "    log_joint      : 1258.2012829037767\n",
      "    val_loss       : -1051.888480808424\n",
      "    val_ess        : 1.963453919991203\n",
      "    val_log_marginal: 1051.9184835682745\n",
      "    val_log_joint  : 1259.5784009850543\n",
      "Train Epoch: 317 [0/101520 (0%)] Loss: -1050.702637\n",
      "Train Epoch: 317 [11264/101520 (11%)] Loss: -1042.690674\n",
      "Train Epoch: 317 [22528/101520 (22%)] Loss: -1054.672485\n",
      "Train Epoch: 317 [33792/101520 (33%)] Loss: -1051.748779\n",
      "Train Epoch: 317 [45056/101520 (44%)] Loss: -1045.719238\n",
      "Train Epoch: 317 [56320/101520 (55%)] Loss: -1043.682129\n",
      "Train Epoch: 317 [67584/101520 (67%)] Loss: -1046.586426\n",
      "Train Epoch: 317 [78848/101520 (78%)] Loss: -1047.208740\n",
      "Train Epoch: 317 [90112/101520 (89%)] Loss: -1053.459473\n",
      "Train Epoch: 317 [101376/101520 (100%)] Loss: -1047.780396\n",
      "    epoch          : 317\n",
      "    loss           : -1048.9175418106156\n",
      "    ess            : 1.961506724357605\n",
      "    log_marginal   : 1048.951220335074\n",
      "    log_joint      : 1256.8980620877826\n",
      "    val_loss       : -1047.7983663807745\n",
      "    val_ess        : 1.9623853486517202\n",
      "    val_log_marginal: 1047.831059994905\n",
      "    val_log_joint  : 1255.9669667119565\n",
      "Train Epoch: 318 [0/101520 (0%)] Loss: -1051.995850\n",
      "Train Epoch: 318 [11264/101520 (11%)] Loss: -1047.625732\n",
      "Train Epoch: 318 [22528/101520 (22%)] Loss: -1047.532471\n",
      "Train Epoch: 318 [33792/101520 (33%)] Loss: -1052.345459\n",
      "Train Epoch: 318 [45056/101520 (44%)] Loss: -1053.047241\n",
      "Train Epoch: 318 [56320/101520 (55%)] Loss: -1053.821411\n",
      "Train Epoch: 318 [67584/101520 (67%)] Loss: -1052.945435\n",
      "Train Epoch: 318 [78848/101520 (78%)] Loss: -1053.300293\n",
      "Train Epoch: 318 [90112/101520 (89%)] Loss: -1044.636719\n",
      "Train Epoch: 318 [101376/101520 (100%)] Loss: -1055.171387\n",
      "    epoch          : 318\n",
      "    loss           : -1049.7997212625628\n",
      "    ess            : 1.960870858412891\n",
      "    log_marginal   : 1049.8339279404836\n",
      "    log_joint      : 1257.850241809634\n",
      "    val_loss       : -1048.7737718665082\n",
      "    val_ess        : 1.9634141351865686\n",
      "    val_log_marginal: 1048.8055950662365\n",
      "    val_log_joint  : 1256.979635487432\n",
      "Train Epoch: 319 [0/101520 (0%)] Loss: -1049.896606\n",
      "Train Epoch: 319 [11264/101520 (11%)] Loss: -1049.385498\n",
      "Train Epoch: 319 [22528/101520 (22%)] Loss: -1057.460449\n",
      "Train Epoch: 319 [33792/101520 (33%)] Loss: -1048.255127\n",
      "Train Epoch: 319 [45056/101520 (44%)] Loss: -1053.520020\n",
      "Train Epoch: 319 [56320/101520 (55%)] Loss: -1046.022217\n",
      "Train Epoch: 319 [67584/101520 (67%)] Loss: -1052.862061\n",
      "Train Epoch: 319 [78848/101520 (78%)] Loss: -1049.291260\n",
      "Train Epoch: 319 [90112/101520 (89%)] Loss: -1047.910156\n",
      "Train Epoch: 319 [101376/101520 (100%)] Loss: -1056.983276\n",
      "    epoch          : 319\n",
      "    loss           : -1049.7279377846262\n",
      "    ess            : 1.9615596377070825\n",
      "    log_marginal   : 1049.7618714912453\n",
      "    log_joint      : 1257.7827375402403\n",
      "    val_loss       : -1049.1782810377038\n",
      "    val_ess        : 1.9652045965194702\n",
      "    val_log_marginal: 1049.2072647758152\n",
      "    val_log_joint  : 1257.1566958220108\n",
      "Train Epoch: 320 [0/101520 (0%)] Loss: -1039.947266\n",
      "Train Epoch: 320 [11264/101520 (11%)] Loss: -1050.979492\n",
      "Train Epoch: 320 [22528/101520 (22%)] Loss: -1049.491699\n",
      "Train Epoch: 320 [33792/101520 (33%)] Loss: -1048.081299\n",
      "Train Epoch: 320 [45056/101520 (44%)] Loss: -1051.037598\n",
      "Train Epoch: 320 [56320/101520 (55%)] Loss: -1060.788574\n",
      "Train Epoch: 320 [67584/101520 (67%)] Loss: -1051.243408\n",
      "Train Epoch: 320 [78848/101520 (78%)] Loss: -1046.526245\n",
      "Train Epoch: 320 [90112/101520 (89%)] Loss: -1051.032471\n",
      "Train Epoch: 320 [101376/101520 (100%)] Loss: -1046.332520\n",
      "    epoch          : 320\n",
      "    loss           : -1050.7316704371467\n",
      "    ess            : 1.9622644521483226\n",
      "    log_marginal   : 1050.764286520493\n",
      "    log_joint      : 1258.6845617246388\n",
      "    val_loss       : -1052.7149605129075\n",
      "    val_ess        : 1.9652344454889712\n",
      "    val_log_marginal: 1052.7454303243885\n",
      "    val_log_joint  : 1260.4705863620925\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch320.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 321 [0/101520 (0%)] Loss: -1051.542969\n",
      "Train Epoch: 321 [11264/101520 (11%)] Loss: -1050.459106\n",
      "Train Epoch: 321 [22528/101520 (22%)] Loss: -1049.203857\n",
      "Train Epoch: 321 [33792/101520 (33%)] Loss: -1061.976318\n",
      "Train Epoch: 321 [45056/101520 (44%)] Loss: -1050.224365\n",
      "Train Epoch: 321 [56320/101520 (55%)] Loss: -1055.645508\n",
      "Train Epoch: 321 [67584/101520 (67%)] Loss: -1049.989380\n",
      "Train Epoch: 321 [78848/101520 (78%)] Loss: -1048.654053\n",
      "Train Epoch: 321 [90112/101520 (89%)] Loss: -1041.336914\n",
      "Train Epoch: 321 [101376/101520 (100%)] Loss: -1057.041870\n",
      "    epoch          : 321\n",
      "    loss           : -1051.005392563403\n",
      "    ess            : 1.961521591972466\n",
      "    log_marginal   : 1051.0393606214668\n",
      "    log_joint      : 1258.9121921865185\n",
      "    val_loss       : -1051.7256708559783\n",
      "    val_ess        : 1.959720948468084\n",
      "    val_log_marginal: 1051.7599938434103\n",
      "    val_log_joint  : 1259.6166779891305\n",
      "Train Epoch: 322 [0/101520 (0%)] Loss: -1044.336548\n",
      "Train Epoch: 322 [11264/101520 (11%)] Loss: -1050.676270\n",
      "Train Epoch: 322 [22528/101520 (22%)] Loss: -1050.601562\n",
      "Train Epoch: 322 [33792/101520 (33%)] Loss: -1049.737061\n",
      "Train Epoch: 322 [45056/101520 (44%)] Loss: -1053.456909\n",
      "Train Epoch: 322 [56320/101520 (55%)] Loss: -1048.800293\n",
      "Train Epoch: 322 [67584/101520 (67%)] Loss: -1045.436523\n",
      "Train Epoch: 322 [78848/101520 (78%)] Loss: -1049.374268\n",
      "Train Epoch: 322 [90112/101520 (89%)] Loss: -1052.470825\n",
      "Train Epoch: 322 [101376/101520 (100%)] Loss: -1057.976074\n",
      "    epoch          : 322\n",
      "    loss           : -1051.3383476218985\n",
      "    ess            : 1.9624378591326612\n",
      "    log_marginal   : 1051.3699736475346\n",
      "    log_joint      : 1259.2855169401696\n",
      "    val_loss       : -1052.0011463994565\n",
      "    val_ess        : 1.9584607611531797\n",
      "    val_log_marginal: 1052.0358674422555\n",
      "    val_log_joint  : 1259.9108196756115\n",
      "Train Epoch: 323 [0/101520 (0%)] Loss: -1059.985596\n",
      "Train Epoch: 323 [11264/101520 (11%)] Loss: -1055.032471\n",
      "Train Epoch: 323 [22528/101520 (22%)] Loss: -1056.212891\n",
      "Train Epoch: 323 [33792/101520 (33%)] Loss: -1053.073120\n",
      "Train Epoch: 323 [45056/101520 (44%)] Loss: -1050.250732\n",
      "Train Epoch: 323 [56320/101520 (55%)] Loss: -1057.148926\n",
      "Train Epoch: 323 [67584/101520 (67%)] Loss: -1054.552368\n",
      "Train Epoch: 323 [78848/101520 (78%)] Loss: -1057.807739\n",
      "Train Epoch: 323 [90112/101520 (89%)] Loss: -1047.773926\n",
      "Train Epoch: 323 [101376/101520 (100%)] Loss: -1050.133911\n",
      "    epoch          : 323\n",
      "    loss           : -1051.3569734659627\n",
      "    ess            : 1.962335037226653\n",
      "    log_marginal   : 1051.3891024948964\n",
      "    log_joint      : 1259.3211970497016\n",
      "    val_loss       : -1053.0400762143342\n",
      "    val_ess        : 1.9634876769521963\n",
      "    val_log_marginal: 1053.070307192595\n",
      "    val_log_joint  : 1260.906303074049\n",
      "Train Epoch: 324 [0/101520 (0%)] Loss: -1055.077148\n",
      "Train Epoch: 324 [11264/101520 (11%)] Loss: -1056.199707\n",
      "Train Epoch: 324 [22528/101520 (22%)] Loss: -1053.509766\n",
      "Train Epoch: 324 [33792/101520 (33%)] Loss: -1055.396240\n",
      "Train Epoch: 324 [45056/101520 (44%)] Loss: -1051.652954\n",
      "Train Epoch: 324 [56320/101520 (55%)] Loss: -1051.535034\n",
      "Train Epoch: 324 [67584/101520 (67%)] Loss: -1057.895630\n",
      "Train Epoch: 324 [78848/101520 (78%)] Loss: -1057.790771\n",
      "Train Epoch: 324 [90112/101520 (89%)] Loss: -1049.947754\n",
      "Train Epoch: 324 [101376/101520 (100%)] Loss: -1054.168823\n",
      "    epoch          : 324\n",
      "    loss           : -1051.7537694576397\n",
      "    ess            : 1.9624167890404935\n",
      "    log_marginal   : 1051.7866842758715\n",
      "    log_joint      : 1259.691090952811\n",
      "    val_loss       : -1051.53344195822\n",
      "    val_ess        : 1.9656857977742734\n",
      "    val_log_marginal: 1051.5626273777175\n",
      "    val_log_joint  : 1259.407614003057\n",
      "Train Epoch: 325 [0/101520 (0%)] Loss: -1047.166748\n",
      "Train Epoch: 325 [11264/101520 (11%)] Loss: -1048.849854\n",
      "Train Epoch: 325 [22528/101520 (22%)] Loss: -1055.039185\n",
      "Train Epoch: 325 [33792/101520 (33%)] Loss: -1052.021362\n",
      "Train Epoch: 325 [45056/101520 (44%)] Loss: -1055.825684\n",
      "Train Epoch: 325 [56320/101520 (55%)] Loss: -1043.732056\n",
      "Train Epoch: 325 [67584/101520 (67%)] Loss: -1055.409668\n",
      "Train Epoch: 325 [78848/101520 (78%)] Loss: -1049.343994\n",
      "Train Epoch: 325 [90112/101520 (89%)] Loss: -1044.764404\n",
      "Train Epoch: 325 [101376/101520 (100%)] Loss: -1061.086182\n",
      "    epoch          : 325\n",
      "    loss           : -1051.728481886974\n",
      "    ess            : 1.9618103186688831\n",
      "    log_marginal   : 1051.7615758234533\n",
      "    log_joint      : 1259.7286996505968\n",
      "    val_loss       : -1050.3839429772418\n",
      "    val_ess        : 1.9592258100924285\n",
      "    val_log_marginal: 1050.42199441661\n",
      "    val_log_joint  : 1258.6164179262908\n",
      "Train Epoch: 326 [0/101520 (0%)] Loss: -1058.896729\n",
      "Train Epoch: 326 [11264/101520 (11%)] Loss: -1050.964233\n",
      "Train Epoch: 326 [22528/101520 (22%)] Loss: -1047.825562\n",
      "Train Epoch: 326 [33792/101520 (33%)] Loss: -1051.063232\n",
      "Train Epoch: 326 [45056/101520 (44%)] Loss: -1050.271973\n",
      "Train Epoch: 326 [56320/101520 (55%)] Loss: -1054.219360\n",
      "Train Epoch: 326 [67584/101520 (67%)] Loss: -1056.361450\n",
      "Train Epoch: 326 [78848/101520 (78%)] Loss: -1058.796265\n",
      "Train Epoch: 326 [90112/101520 (89%)] Loss: -1048.389771\n",
      "Train Epoch: 326 [101376/101520 (100%)] Loss: -1059.648315\n",
      "    epoch          : 326\n",
      "    loss           : -1052.0006134186558\n",
      "    ess            : 1.9612597742272382\n",
      "    log_marginal   : 1052.0346655150754\n",
      "    log_joint      : 1259.9992577634266\n",
      "    val_loss       : -1054.1677617612092\n",
      "    val_ess        : 1.9651538392771846\n",
      "    val_log_marginal: 1054.1975734544837\n",
      "    val_log_joint  : 1261.93140179178\n",
      "Train Epoch: 327 [0/101520 (0%)] Loss: -1061.051514\n",
      "Train Epoch: 327 [11264/101520 (11%)] Loss: -1051.995605\n",
      "Train Epoch: 327 [22528/101520 (22%)] Loss: -1051.335449\n",
      "Train Epoch: 327 [33792/101520 (33%)] Loss: -1051.025024\n",
      "Train Epoch: 327 [45056/101520 (44%)] Loss: -1052.670776\n",
      "Train Epoch: 327 [56320/101520 (55%)] Loss: -1046.429443\n",
      "Train Epoch: 327 [67584/101520 (67%)] Loss: -1060.829224\n",
      "Train Epoch: 327 [78848/101520 (78%)] Loss: -1046.991089\n",
      "Train Epoch: 327 [90112/101520 (89%)] Loss: -1050.421387\n",
      "Train Epoch: 327 [101376/101520 (100%)] Loss: -1058.176880\n",
      "    epoch          : 327\n",
      "    loss           : -1052.3654472312735\n",
      "    ess            : 1.9617108316277738\n",
      "    log_marginal   : 1052.3986589441347\n",
      "    log_joint      : 1260.339360989518\n",
      "    val_loss       : -1051.8038489300272\n",
      "    val_ess        : 1.9652778635854307\n",
      "    val_log_marginal: 1051.833204186481\n",
      "    val_log_joint  : 1259.734566066576\n",
      "Train Epoch: 328 [0/101520 (0%)] Loss: -1048.561157\n",
      "Train Epoch: 328 [11264/101520 (11%)] Loss: -1059.027344\n",
      "Train Epoch: 328 [22528/101520 (22%)] Loss: -1052.861206\n",
      "Train Epoch: 328 [33792/101520 (33%)] Loss: -1053.684814\n",
      "Train Epoch: 328 [45056/101520 (44%)] Loss: -1055.193726\n",
      "Train Epoch: 328 [56320/101520 (55%)] Loss: -1057.240112\n",
      "Train Epoch: 328 [67584/101520 (67%)] Loss: -1055.372314\n",
      "Train Epoch: 328 [78848/101520 (78%)] Loss: -1049.743286\n",
      "Train Epoch: 328 [90112/101520 (89%)] Loss: -1051.196655\n",
      "Train Epoch: 328 [101376/101520 (100%)] Loss: -1042.936401\n",
      "    epoch          : 328\n",
      "    loss           : -1052.0382110749058\n",
      "    ess            : 1.9626185175162465\n",
      "    log_marginal   : 1052.0706241166772\n",
      "    log_joint      : 1259.9941553470478\n",
      "    val_loss       : -1051.12890625\n",
      "    val_ess        : 1.9617962837219238\n",
      "    val_log_marginal: 1051.1642270295517\n",
      "    val_log_joint  : 1259.0567786175272\n",
      "Train Epoch: 329 [0/101520 (0%)] Loss: -1051.531494\n",
      "Train Epoch: 329 [11264/101520 (11%)] Loss: -1055.725220\n",
      "Train Epoch: 329 [22528/101520 (22%)] Loss: -1050.303467\n",
      "Train Epoch: 329 [33792/101520 (33%)] Loss: -1054.979614\n",
      "Train Epoch: 329 [45056/101520 (44%)] Loss: -1045.867188\n",
      "Train Epoch: 329 [56320/101520 (55%)] Loss: -1047.890137\n",
      "Train Epoch: 329 [67584/101520 (67%)] Loss: -1056.243408\n",
      "Train Epoch: 329 [78848/101520 (78%)] Loss: -1053.756104\n",
      "Train Epoch: 329 [90112/101520 (89%)] Loss: -1050.366455\n",
      "Train Epoch: 329 [101376/101520 (100%)] Loss: -1047.044922\n",
      "    epoch          : 329\n",
      "    loss           : -1052.4950435772614\n",
      "    ess            : 1.961134570926877\n",
      "    log_marginal   : 1052.52907788454\n",
      "    log_joint      : 1260.4897350522142\n",
      "    val_loss       : -1052.4506729789402\n",
      "    val_ess        : 1.9611946551696113\n",
      "    val_log_marginal: 1052.4841096297555\n",
      "    val_log_joint  : 1260.2357761548913\n",
      "Train Epoch: 330 [0/101520 (0%)] Loss: -1055.359131\n",
      "Train Epoch: 330 [11264/101520 (11%)] Loss: -1057.910889\n",
      "Train Epoch: 330 [22528/101520 (22%)] Loss: -1056.089355\n",
      "Train Epoch: 330 [33792/101520 (33%)] Loss: -1049.732910\n",
      "Train Epoch: 330 [45056/101520 (44%)] Loss: -1048.957764\n",
      "Train Epoch: 330 [56320/101520 (55%)] Loss: -1055.325806\n",
      "Train Epoch: 330 [67584/101520 (67%)] Loss: -1054.580688\n",
      "Train Epoch: 330 [78848/101520 (78%)] Loss: -1053.375488\n",
      "Train Epoch: 330 [90112/101520 (89%)] Loss: -1049.794800\n",
      "Train Epoch: 330 [101376/101520 (100%)] Loss: -1045.914673\n",
      "    epoch          : 330\n",
      "    loss           : -1052.6205140693703\n",
      "    ess            : 1.9610588886030955\n",
      "    log_marginal   : 1052.6546753543107\n",
      "    log_joint      : 1260.658876658684\n",
      "    val_loss       : -1052.2224545686142\n",
      "    val_ess        : 1.9639466741810674\n",
      "    val_log_marginal: 1052.2524095618207\n",
      "    val_log_joint  : 1260.079690684443\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch330.pth ...\n",
      "Train Epoch: 331 [0/101520 (0%)] Loss: -1049.603760\n",
      "Train Epoch: 331 [11264/101520 (11%)] Loss: -1051.536499\n",
      "Train Epoch: 331 [22528/101520 (22%)] Loss: -1052.846069\n",
      "Train Epoch: 331 [33792/101520 (33%)] Loss: -1052.799927\n",
      "Train Epoch: 331 [45056/101520 (44%)] Loss: -1047.815186\n",
      "Train Epoch: 331 [56320/101520 (55%)] Loss: -1046.614746\n",
      "Train Epoch: 331 [67584/101520 (67%)] Loss: -1058.925293\n",
      "Train Epoch: 331 [78848/101520 (78%)] Loss: -1046.604736\n",
      "Train Epoch: 331 [90112/101520 (89%)] Loss: -1048.849365\n",
      "Train Epoch: 331 [101376/101520 (100%)] Loss: -1050.312744\n",
      "    epoch          : 331\n",
      "    loss           : -1051.9755748959642\n",
      "    ess            : 1.9619674443000525\n",
      "    log_marginal   : 1052.008535720595\n",
      "    log_joint      : 1259.9994841149105\n",
      "    val_loss       : -1052.087991465693\n",
      "    val_ess        : 1.9630736319915107\n",
      "    val_log_marginal: 1052.1196395210598\n",
      "    val_log_joint  : 1259.9682457965353\n",
      "Train Epoch: 332 [0/101520 (0%)] Loss: -1054.789551\n",
      "Train Epoch: 332 [11264/101520 (11%)] Loss: -1051.020264\n",
      "Train Epoch: 332 [22528/101520 (22%)] Loss: -1053.037842\n",
      "Train Epoch: 332 [33792/101520 (33%)] Loss: -1049.611816\n",
      "Train Epoch: 332 [45056/101520 (44%)] Loss: -1051.968262\n",
      "Train Epoch: 332 [56320/101520 (55%)] Loss: -1052.361816\n",
      "Train Epoch: 332 [67584/101520 (67%)] Loss: -1052.207520\n",
      "Train Epoch: 332 [78848/101520 (78%)] Loss: -1054.003662\n",
      "Train Epoch: 332 [90112/101520 (89%)] Loss: -1059.171143\n",
      "Train Epoch: 332 [101376/101520 (100%)] Loss: -1046.166260\n",
      "    epoch          : 332\n",
      "    loss           : -1052.852273452222\n",
      "    ess            : 1.9614418080104656\n",
      "    log_marginal   : 1052.8861599256045\n",
      "    log_joint      : 1260.888878597087\n",
      "    val_loss       : -1051.6495361328125\n",
      "    val_ess        : 1.9633696701215662\n",
      "    val_log_marginal: 1051.6798201851223\n",
      "    val_log_joint  : 1259.8464992357337\n",
      "Train Epoch: 333 [0/101520 (0%)] Loss: -1058.534912\n",
      "Train Epoch: 333 [11264/101520 (11%)] Loss: -1049.662598\n",
      "Train Epoch: 333 [22528/101520 (22%)] Loss: -1052.691650\n",
      "Train Epoch: 333 [33792/101520 (33%)] Loss: -1054.771606\n",
      "Train Epoch: 333 [45056/101520 (44%)] Loss: -1054.603271\n",
      "Train Epoch: 333 [56320/101520 (55%)] Loss: -1049.957764\n",
      "Train Epoch: 333 [67584/101520 (67%)] Loss: -1054.561768\n",
      "Train Epoch: 333 [78848/101520 (78%)] Loss: -1045.683960\n",
      "Train Epoch: 333 [90112/101520 (89%)] Loss: -1051.476807\n",
      "Train Epoch: 333 [101376/101520 (100%)] Loss: -1047.218628\n",
      "    epoch          : 333\n",
      "    loss           : -1052.054425570234\n",
      "    ess            : 1.9622017127185611\n",
      "    log_marginal   : 1052.087561219182\n",
      "    log_joint      : 1260.0975544225032\n",
      "    val_loss       : -1049.946235988451\n",
      "    val_ess        : 1.9649833337120388\n",
      "    val_log_marginal: 1049.9741953974185\n",
      "    val_log_joint  : 1257.9013088060462\n",
      "Train Epoch: 334 [0/101520 (0%)] Loss: -1056.321533\n",
      "Train Epoch: 334 [11264/101520 (11%)] Loss: -1053.203247\n",
      "Train Epoch: 334 [22528/101520 (22%)] Loss: -1049.916504\n",
      "Train Epoch: 334 [33792/101520 (33%)] Loss: -1052.620117\n",
      "Train Epoch: 334 [45056/101520 (44%)] Loss: -1049.674805\n",
      "Train Epoch: 334 [56320/101520 (55%)] Loss: -1051.437744\n",
      "Train Epoch: 334 [67584/101520 (67%)] Loss: -1052.774658\n",
      "Train Epoch: 334 [78848/101520 (78%)] Loss: -1050.133179\n",
      "Train Epoch: 334 [90112/101520 (89%)] Loss: -1055.366455\n",
      "Train Epoch: 334 [101376/101520 (100%)] Loss: -1054.316772\n",
      "    epoch          : 334\n",
      "    loss           : -1053.5204851160097\n",
      "    ess            : 1.9616927633333445\n",
      "    log_marginal   : 1053.5540894168107\n",
      "    log_joint      : 1261.5483478181925\n",
      "    val_loss       : -1052.7488960597825\n",
      "    val_ess        : 1.9634402472039927\n",
      "    val_log_marginal: 1052.7800452190897\n",
      "    val_log_joint  : 1260.7664264181385\n",
      "Train Epoch: 335 [0/101520 (0%)] Loss: -1052.689941\n",
      "Train Epoch: 335 [11264/101520 (11%)] Loss: -1055.043091\n",
      "Train Epoch: 335 [22528/101520 (22%)] Loss: -1054.209595\n",
      "Train Epoch: 335 [33792/101520 (33%)] Loss: -1055.449097\n",
      "Train Epoch: 335 [45056/101520 (44%)] Loss: -1057.050049\n",
      "Train Epoch: 335 [56320/101520 (55%)] Loss: -1048.095947\n",
      "Train Epoch: 335 [67584/101520 (67%)] Loss: -1049.175537\n",
      "Train Epoch: 335 [78848/101520 (78%)] Loss: -1049.019287\n",
      "Train Epoch: 335 [90112/101520 (89%)] Loss: -1049.612305\n",
      "Train Epoch: 335 [101376/101520 (100%)] Loss: -1042.239380\n",
      "    epoch          : 335\n",
      "    loss           : -1053.005832997998\n",
      "    ess            : 1.9610164009745996\n",
      "    log_marginal   : 1053.0404267430904\n",
      "    log_joint      : 1261.016029242894\n",
      "    val_loss       : -1051.4734152088995\n",
      "    val_ess        : 1.9642985178076702\n",
      "    val_log_marginal: 1051.5041822350543\n",
      "    val_log_joint  : 1259.767965565557\n",
      "Train Epoch: 336 [0/101520 (0%)] Loss: -1052.798096\n",
      "Train Epoch: 336 [11264/101520 (11%)] Loss: -1049.473389\n",
      "Train Epoch: 336 [22528/101520 (22%)] Loss: -1059.617798\n",
      "Train Epoch: 336 [33792/101520 (33%)] Loss: -1055.096191\n",
      "Train Epoch: 336 [45056/101520 (44%)] Loss: -1053.253296\n",
      "Train Epoch: 336 [56320/101520 (55%)] Loss: -1053.697388\n",
      "Train Epoch: 336 [67584/101520 (67%)] Loss: -1055.618896\n",
      "Train Epoch: 336 [78848/101520 (78%)] Loss: -1056.647217\n",
      "Train Epoch: 336 [90112/101520 (89%)] Loss: -1052.529541\n",
      "Train Epoch: 336 [101376/101520 (100%)] Loss: -1051.970825\n",
      "    epoch          : 336\n",
      "    loss           : -1054.1774933014683\n",
      "    ess            : 1.9623066450483235\n",
      "    log_marginal   : 1054.2103044519472\n",
      "    log_joint      : 1262.173213479507\n",
      "    val_loss       : -1053.8074951171875\n",
      "    val_ess        : 1.9613052140111509\n",
      "    val_log_marginal: 1053.8416111158288\n",
      "    val_log_joint  : 1261.6310292119565\n",
      "Train Epoch: 337 [0/101520 (0%)] Loss: -1060.267212\n",
      "Train Epoch: 337 [11264/101520 (11%)] Loss: -1060.395508\n",
      "Train Epoch: 337 [22528/101520 (22%)] Loss: -1056.040894\n",
      "Train Epoch: 337 [33792/101520 (33%)] Loss: -1058.232544\n",
      "Train Epoch: 337 [45056/101520 (44%)] Loss: -1046.158691\n",
      "Train Epoch: 337 [56320/101520 (55%)] Loss: -1057.692383\n",
      "Train Epoch: 337 [67584/101520 (67%)] Loss: -1050.324219\n",
      "Train Epoch: 337 [78848/101520 (78%)] Loss: -1051.379395\n",
      "Train Epoch: 337 [90112/101520 (89%)] Loss: -1058.766479\n",
      "Train Epoch: 337 [101376/101520 (100%)] Loss: -1056.650513\n",
      "    epoch          : 337\n",
      "    loss           : -1053.7303534272928\n",
      "    ess            : 1.9631774036129515\n",
      "    log_marginal   : 1053.7628707502356\n",
      "    log_joint      : 1261.640902878651\n",
      "    val_loss       : -1054.4495053498642\n",
      "    val_ess        : 1.962952214738597\n",
      "    val_log_marginal: 1054.4801927649457\n",
      "    val_log_joint  : 1262.3607708474865\n",
      "Train Epoch: 338 [0/101520 (0%)] Loss: -1058.590576\n",
      "Train Epoch: 338 [11264/101520 (11%)] Loss: -1056.461670\n",
      "Train Epoch: 338 [22528/101520 (22%)] Loss: -1047.461304\n",
      "Train Epoch: 338 [33792/101520 (33%)] Loss: -1055.875732\n",
      "Train Epoch: 338 [45056/101520 (44%)] Loss: -1054.065918\n",
      "Train Epoch: 338 [56320/101520 (55%)] Loss: -1058.564941\n",
      "Train Epoch: 338 [67584/101520 (67%)] Loss: -1053.447021\n",
      "Train Epoch: 338 [78848/101520 (78%)] Loss: -1056.590088\n",
      "Train Epoch: 338 [90112/101520 (89%)] Loss: -1062.494019\n",
      "Train Epoch: 338 [101376/101520 (100%)] Loss: -1065.527222\n",
      "    epoch          : 338\n",
      "    loss           : -1054.5864030847597\n",
      "    ess            : 1.9616100123180218\n",
      "    log_marginal   : 1054.6205527147456\n",
      "    log_joint      : 1262.58075349894\n",
      "    val_loss       : -1054.3947488536005\n",
      "    val_ess        : 1.957757778789686\n",
      "    val_log_marginal: 1054.4302129330842\n",
      "    val_log_joint  : 1262.1417607846467\n",
      "Train Epoch: 339 [0/101520 (0%)] Loss: -1059.189941\n",
      "Train Epoch: 339 [11264/101520 (11%)] Loss: -1057.478271\n",
      "Train Epoch: 339 [22528/101520 (22%)] Loss: -1051.551025\n",
      "Train Epoch: 339 [33792/101520 (33%)] Loss: -1052.191772\n",
      "Train Epoch: 339 [45056/101520 (44%)] Loss: -1053.110840\n",
      "Train Epoch: 339 [56320/101520 (55%)] Loss: -1051.347534\n",
      "Train Epoch: 339 [67584/101520 (67%)] Loss: -1047.412720\n",
      "Train Epoch: 339 [78848/101520 (78%)] Loss: -1053.826660\n",
      "Train Epoch: 339 [90112/101520 (89%)] Loss: -1056.618164\n",
      "Train Epoch: 339 [101376/101520 (100%)] Loss: -1058.359985\n",
      "    epoch          : 339\n",
      "    loss           : -1054.1669357529836\n",
      "    ess            : 1.9609811611511\n",
      "    log_marginal   : 1054.2021214470792\n",
      "    log_joint      : 1262.19538549682\n",
      "    val_loss       : -1052.8396792204483\n",
      "    val_ess        : 1.9603892098302427\n",
      "    val_log_marginal: 1052.8735670006793\n",
      "    val_log_joint  : 1261.189798106318\n",
      "Train Epoch: 340 [0/101520 (0%)] Loss: -1060.086304\n",
      "Train Epoch: 340 [11264/101520 (11%)] Loss: -1055.315552\n",
      "Train Epoch: 340 [22528/101520 (22%)] Loss: -1058.351318\n",
      "Train Epoch: 340 [33792/101520 (33%)] Loss: -1054.474365\n",
      "Train Epoch: 340 [45056/101520 (44%)] Loss: -1052.510986\n",
      "Train Epoch: 340 [56320/101520 (55%)] Loss: -1053.157959\n",
      "Train Epoch: 340 [67584/101520 (67%)] Loss: -1054.775146\n",
      "Train Epoch: 340 [78848/101520 (78%)] Loss: -1059.367432\n",
      "Train Epoch: 340 [90112/101520 (89%)] Loss: -1059.120361\n",
      "Train Epoch: 340 [101376/101520 (100%)] Loss: -1057.928345\n",
      "    epoch          : 340\n",
      "    loss           : -1055.041377542007\n",
      "    ess            : 1.9617555824356463\n",
      "    log_marginal   : 1055.0749910440877\n",
      "    log_joint      : 1263.048061965099\n",
      "    val_loss       : -1054.2869554602582\n",
      "    val_ess        : 1.9604870962060017\n",
      "    val_log_marginal: 1054.3193200152853\n",
      "    val_log_joint  : 1262.3702445652175\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch340.pth ...\n",
      "Train Epoch: 341 [0/101520 (0%)] Loss: -1059.068604\n",
      "Train Epoch: 341 [11264/101520 (11%)] Loss: -1058.928955\n",
      "Train Epoch: 341 [22528/101520 (22%)] Loss: -1051.706299\n",
      "Train Epoch: 341 [33792/101520 (33%)] Loss: -1053.816284\n",
      "Train Epoch: 341 [45056/101520 (44%)] Loss: -1054.555420\n",
      "Train Epoch: 341 [56320/101520 (55%)] Loss: -1051.903076\n",
      "Train Epoch: 341 [67584/101520 (67%)] Loss: -1054.708740\n",
      "Train Epoch: 341 [78848/101520 (78%)] Loss: -1051.631592\n",
      "Train Epoch: 341 [90112/101520 (89%)] Loss: -1054.237061\n",
      "Train Epoch: 341 [101376/101520 (100%)] Loss: -1069.995728\n",
      "    epoch          : 341\n",
      "    loss           : -1054.8454583709563\n",
      "    ess            : 1.962814920511677\n",
      "    log_marginal   : 1054.8780867226758\n",
      "    log_joint      : 1262.8070675643844\n",
      "    val_loss       : -1053.8277481742527\n",
      "    val_ess        : 1.9603067118188608\n",
      "    val_log_marginal: 1053.8626762058425\n",
      "    val_log_joint  : 1262.1219535495925\n",
      "Train Epoch: 342 [0/101520 (0%)] Loss: -1054.509155\n",
      "Train Epoch: 342 [11264/101520 (11%)] Loss: -1057.080078\n",
      "Train Epoch: 342 [22528/101520 (22%)] Loss: -1062.662598\n",
      "Train Epoch: 342 [33792/101520 (33%)] Loss: -1052.736694\n",
      "Train Epoch: 342 [45056/101520 (44%)] Loss: -1049.860352\n",
      "Train Epoch: 342 [56320/101520 (55%)] Loss: -1055.635986\n",
      "Train Epoch: 342 [67584/101520 (67%)] Loss: -1054.390869\n",
      "Train Epoch: 342 [78848/101520 (78%)] Loss: -1055.511719\n",
      "Train Epoch: 342 [90112/101520 (89%)] Loss: -1055.261475\n",
      "Train Epoch: 342 [101376/101520 (100%)] Loss: -1057.758301\n",
      "    epoch          : 342\n",
      "    loss           : -1055.3801435154287\n",
      "    ess            : 1.9619411397818944\n",
      "    log_marginal   : 1055.4136564168498\n",
      "    log_joint      : 1263.4005807847832\n",
      "    val_loss       : -1055.2948953379755\n",
      "    val_ess        : 1.963713324588278\n",
      "    val_log_marginal: 1055.3250201681385\n",
      "    val_log_joint  : 1263.2831235139267\n",
      "Train Epoch: 343 [0/101520 (0%)] Loss: -1049.764771\n",
      "Train Epoch: 343 [11264/101520 (11%)] Loss: -1049.529053\n",
      "Train Epoch: 343 [22528/101520 (22%)] Loss: -1064.137451\n",
      "Train Epoch: 343 [33792/101520 (33%)] Loss: -1054.504883\n",
      "Train Epoch: 343 [45056/101520 (44%)] Loss: -1049.971680\n",
      "Train Epoch: 343 [56320/101520 (55%)] Loss: -1050.807373\n",
      "Train Epoch: 343 [67584/101520 (67%)] Loss: -1056.367676\n",
      "Train Epoch: 343 [78848/101520 (78%)] Loss: -1057.206909\n",
      "Train Epoch: 343 [90112/101520 (89%)] Loss: -1051.167480\n",
      "Train Epoch: 343 [101376/101520 (100%)] Loss: -1047.323975\n",
      "    epoch          : 343\n",
      "    loss           : -1055.3994097685695\n",
      "    ess            : 1.962607365157736\n",
      "    log_marginal   : 1055.4314983118718\n",
      "    log_joint      : 1263.330765767313\n",
      "    val_loss       : -1053.8854608950408\n",
      "    val_ess        : 1.964402587517448\n",
      "    val_log_marginal: 1053.9168170431385\n",
      "    val_log_joint  : 1261.9855585512908\n",
      "Train Epoch: 344 [0/101520 (0%)] Loss: -1054.907959\n",
      "Train Epoch: 344 [11264/101520 (11%)] Loss: -1055.320557\n",
      "Train Epoch: 344 [22528/101520 (22%)] Loss: -1055.058350\n",
      "Train Epoch: 344 [33792/101520 (33%)] Loss: -1061.511719\n",
      "Train Epoch: 344 [45056/101520 (44%)] Loss: -1053.913086\n",
      "Train Epoch: 344 [56320/101520 (55%)] Loss: -1058.855835\n",
      "Train Epoch: 344 [67584/101520 (67%)] Loss: -1054.748779\n",
      "Train Epoch: 344 [78848/101520 (78%)] Loss: -1055.728882\n",
      "Train Epoch: 344 [90112/101520 (89%)] Loss: -1054.228271\n",
      "Train Epoch: 344 [101376/101520 (100%)] Loss: -1045.114868\n",
      "    epoch          : 344\n",
      "    loss           : -1055.6043523280464\n",
      "    ess            : 1.9622060515772757\n",
      "    log_marginal   : 1055.6376407182397\n",
      "    log_joint      : 1263.5953884412295\n",
      "    val_loss       : -1054.3852751358695\n",
      "    val_ess        : 1.9615783795066501\n",
      "    val_log_marginal: 1054.4175972316575\n",
      "    val_log_joint  : 1262.4031663977582\n",
      "Train Epoch: 345 [0/101520 (0%)] Loss: -1059.688965\n",
      "Train Epoch: 345 [11264/101520 (11%)] Loss: -1049.469604\n",
      "Train Epoch: 345 [22528/101520 (22%)] Loss: -1052.835205\n",
      "Train Epoch: 345 [33792/101520 (33%)] Loss: -1047.929932\n",
      "Train Epoch: 345 [45056/101520 (44%)] Loss: -1057.967285\n",
      "Train Epoch: 345 [56320/101520 (55%)] Loss: -1052.727661\n",
      "Train Epoch: 345 [67584/101520 (67%)] Loss: -1053.672363\n",
      "Train Epoch: 345 [78848/101520 (78%)] Loss: -1057.257690\n",
      "Train Epoch: 345 [90112/101520 (89%)] Loss: -1053.019287\n",
      "Train Epoch: 345 [101376/101520 (100%)] Loss: -1048.291016\n",
      "    epoch          : 345\n",
      "    loss           : -1055.9879236269237\n",
      "    ess            : 1.9616150035330997\n",
      "    log_marginal   : 1056.0211132567133\n",
      "    log_joint      : 1263.9129197010443\n",
      "    val_loss       : -1057.188036047894\n",
      "    val_ess        : 1.9581059061962625\n",
      "    val_log_marginal: 1057.225639011549\n",
      "    val_log_joint  : 1265.3135508661685\n",
      "Train Epoch: 346 [0/101520 (0%)] Loss: -1056.855713\n",
      "Train Epoch: 346 [11264/101520 (11%)] Loss: -1063.657471\n",
      "Train Epoch: 346 [22528/101520 (22%)] Loss: -1057.962158\n",
      "Train Epoch: 346 [33792/101520 (33%)] Loss: -1055.362549\n",
      "Train Epoch: 346 [45056/101520 (44%)] Loss: -1050.946045\n",
      "Train Epoch: 346 [56320/101520 (55%)] Loss: -1056.495239\n",
      "Train Epoch: 346 [67584/101520 (67%)] Loss: -1059.649780\n",
      "Train Epoch: 346 [78848/101520 (78%)] Loss: -1053.313843\n",
      "Train Epoch: 346 [90112/101520 (89%)] Loss: -1057.201172\n",
      "Train Epoch: 346 [101376/101520 (100%)] Loss: -1052.352417\n",
      "    epoch          : 346\n",
      "    loss           : -1055.9375226964903\n",
      "    ess            : 1.9627054827896195\n",
      "    log_marginal   : 1055.969471993758\n",
      "    log_joint      : 1263.8880474148084\n",
      "    val_loss       : -1054.5618365743885\n",
      "    val_ess        : 1.9608754748883455\n",
      "    val_log_marginal: 1054.5955757472825\n",
      "    val_log_joint  : 1262.7243440047555\n",
      "Train Epoch: 347 [0/101520 (0%)] Loss: -1063.994019\n",
      "Train Epoch: 347 [11264/101520 (11%)] Loss: -1063.864014\n",
      "Train Epoch: 347 [22528/101520 (22%)] Loss: -1054.382080\n",
      "Train Epoch: 347 [33792/101520 (33%)] Loss: -1054.114014\n",
      "Train Epoch: 347 [45056/101520 (44%)] Loss: -1059.668701\n",
      "Train Epoch: 347 [56320/101520 (55%)] Loss: -1058.800537\n",
      "Train Epoch: 347 [67584/101520 (67%)] Loss: -1058.256836\n",
      "Train Epoch: 347 [78848/101520 (78%)] Loss: -1058.326294\n",
      "Train Epoch: 347 [90112/101520 (89%)] Loss: -1058.794189\n",
      "Train Epoch: 347 [101376/101520 (100%)] Loss: -1053.075073\n",
      "    epoch          : 347\n",
      "    loss           : -1056.256103515625\n",
      "    ess            : 1.9613773553215679\n",
      "    log_marginal   : 1056.289666717376\n",
      "    log_joint      : 1264.253416128494\n",
      "    val_loss       : -1055.0111269743545\n",
      "    val_ess        : 1.960240213767342\n",
      "    val_log_marginal: 1055.0446883491848\n",
      "    val_log_joint  : 1262.9649605129075\n",
      "Train Epoch: 348 [0/101520 (0%)] Loss: -1064.074829\n",
      "Train Epoch: 348 [11264/101520 (11%)] Loss: -1059.530884\n",
      "Train Epoch: 348 [22528/101520 (22%)] Loss: -1057.960205\n",
      "Train Epoch: 348 [33792/101520 (33%)] Loss: -1063.414795\n",
      "Train Epoch: 348 [45056/101520 (44%)] Loss: -1045.663330\n",
      "Train Epoch: 348 [56320/101520 (55%)] Loss: -1056.881348\n",
      "Train Epoch: 348 [67584/101520 (67%)] Loss: -1049.064209\n",
      "Train Epoch: 348 [78848/101520 (78%)] Loss: -1049.541260\n",
      "Train Epoch: 348 [90112/101520 (89%)] Loss: -1060.435303\n",
      "Train Epoch: 348 [101376/101520 (100%)] Loss: -1048.020630\n",
      "    epoch          : 348\n",
      "    loss           : -1056.5719705706265\n",
      "    ess            : 1.9610687266642124\n",
      "    log_marginal   : 1056.605868085545\n",
      "    log_joint      : 1264.5559364203832\n",
      "    val_loss       : -1055.0274976647418\n",
      "    val_ess        : 1.9635745286941528\n",
      "    val_log_marginal: 1055.05932086447\n",
      "    val_log_joint  : 1262.84008258322\n",
      "Train Epoch: 349 [0/101520 (0%)] Loss: -1053.658691\n",
      "Train Epoch: 349 [11264/101520 (11%)] Loss: -1059.914551\n",
      "Train Epoch: 349 [22528/101520 (22%)] Loss: -1057.308838\n",
      "Train Epoch: 349 [33792/101520 (33%)] Loss: -1053.321533\n",
      "Train Epoch: 349 [45056/101520 (44%)] Loss: -1060.909546\n",
      "Train Epoch: 349 [56320/101520 (55%)] Loss: -1064.208496\n",
      "Train Epoch: 349 [67584/101520 (67%)] Loss: -1055.598877\n",
      "Train Epoch: 349 [78848/101520 (78%)] Loss: -1059.447510\n",
      "Train Epoch: 349 [90112/101520 (89%)] Loss: -1061.479980\n",
      "Train Epoch: 349 [101376/101520 (100%)] Loss: -1059.934448\n",
      "    epoch          : 349\n",
      "    loss           : -1055.981130015311\n",
      "    ess            : 1.9614885027085118\n",
      "    log_marginal   : 1056.0137706354035\n",
      "    log_joint      : 1264.0339662178078\n",
      "    val_loss       : -1054.7581362516983\n",
      "    val_ess        : 1.9610265389732693\n",
      "    val_log_marginal: 1054.791848887568\n",
      "    val_log_joint  : 1262.753757642663\n",
      "Train Epoch: 350 [0/101520 (0%)] Loss: -1054.046875\n",
      "Train Epoch: 350 [11264/101520 (11%)] Loss: -1052.888428\n",
      "Train Epoch: 350 [22528/101520 (22%)] Loss: -1061.430176\n",
      "Train Epoch: 350 [33792/101520 (33%)] Loss: -1067.810059\n",
      "Train Epoch: 350 [45056/101520 (44%)] Loss: -1058.213745\n",
      "Train Epoch: 350 [56320/101520 (55%)] Loss: -1052.531738\n",
      "Train Epoch: 350 [67584/101520 (67%)] Loss: -1057.152222\n",
      "Train Epoch: 350 [78848/101520 (78%)] Loss: -1057.310791\n",
      "Train Epoch: 350 [90112/101520 (89%)] Loss: -1053.973877\n",
      "Train Epoch: 350 [101376/101520 (100%)] Loss: -1070.276489\n",
      "    epoch          : 350\n",
      "    loss           : -1056.895299250157\n",
      "    ess            : 1.9624109651575137\n",
      "    log_marginal   : 1056.928297493326\n",
      "    log_joint      : 1264.848032275636\n",
      "    val_loss       : -1057.226413892663\n",
      "    val_ess        : 1.9636610124422156\n",
      "    val_log_marginal: 1057.2661557404892\n",
      "    val_log_joint  : 1265.0938773777175\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch350.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 351 [0/101520 (0%)] Loss: -1057.005127\n",
      "Train Epoch: 351 [11264/101520 (11%)] Loss: -1056.842896\n",
      "Train Epoch: 351 [22528/101520 (22%)] Loss: -1053.371582\n",
      "Train Epoch: 351 [33792/101520 (33%)] Loss: -1054.193970\n",
      "Train Epoch: 351 [45056/101520 (44%)] Loss: -1055.457031\n",
      "Train Epoch: 351 [56320/101520 (55%)] Loss: -1059.162842\n",
      "Train Epoch: 351 [67584/101520 (67%)] Loss: -1054.906738\n",
      "Train Epoch: 351 [78848/101520 (78%)] Loss: -1064.602539\n",
      "Train Epoch: 351 [90112/101520 (89%)] Loss: -1061.772095\n",
      "Train Epoch: 351 [101376/101520 (100%)] Loss: -1054.403931\n",
      "    epoch          : 351\n",
      "    loss           : -1056.4645100502512\n",
      "    ess            : 1.9617399115059244\n",
      "    log_marginal   : 1056.4978984276854\n",
      "    log_joint      : 1264.4665570283055\n",
      "    val_loss       : -1054.4027630349865\n",
      "    val_ess        : 1.9601549221121746\n",
      "    val_log_marginal: 1054.4359130859375\n",
      "    val_log_joint  : 1262.417135487432\n",
      "Train Epoch: 352 [0/101520 (0%)] Loss: -1059.799805\n",
      "Train Epoch: 352 [11264/101520 (11%)] Loss: -1060.619507\n",
      "Train Epoch: 352 [22528/101520 (22%)] Loss: -1057.042480\n",
      "Train Epoch: 352 [33792/101520 (33%)] Loss: -1053.821899\n",
      "Train Epoch: 352 [45056/101520 (44%)] Loss: -1057.220093\n",
      "Train Epoch: 352 [56320/101520 (55%)] Loss: -1054.974121\n",
      "Train Epoch: 352 [67584/101520 (67%)] Loss: -1058.605469\n",
      "Train Epoch: 352 [78848/101520 (78%)] Loss: -1058.348145\n",
      "Train Epoch: 352 [90112/101520 (89%)] Loss: -1050.817505\n",
      "Train Epoch: 352 [101376/101520 (100%)] Loss: -1054.546143\n",
      "    epoch          : 352\n",
      "    loss           : -1057.0275553794363\n",
      "    ess            : 1.9619253664160494\n",
      "    log_marginal   : 1057.061145571608\n",
      "    log_joint      : 1264.9808791270807\n",
      "    val_loss       : -1057.8167353091033\n",
      "    val_ess        : 1.9586485106012095\n",
      "    val_log_marginal: 1057.859815514606\n",
      "    val_log_joint  : 1265.550972316576\n",
      "Train Epoch: 353 [0/101520 (0%)] Loss: -1059.656738\n",
      "Train Epoch: 353 [11264/101520 (11%)] Loss: -1055.246582\n",
      "Train Epoch: 353 [22528/101520 (22%)] Loss: -1056.505371\n",
      "Train Epoch: 353 [33792/101520 (33%)] Loss: -1061.473267\n",
      "Train Epoch: 353 [45056/101520 (44%)] Loss: -1061.235596\n",
      "Train Epoch: 353 [56320/101520 (55%)] Loss: -1058.881714\n",
      "Train Epoch: 353 [67584/101520 (67%)] Loss: -1061.480225\n",
      "Train Epoch: 353 [78848/101520 (78%)] Loss: -1063.353271\n",
      "Train Epoch: 353 [90112/101520 (89%)] Loss: -1058.809814\n",
      "Train Epoch: 353 [101376/101520 (100%)] Loss: -1076.127075\n",
      "    epoch          : 353\n",
      "    loss           : -1057.589243213136\n",
      "    ess            : 1.9612417796149326\n",
      "    log_marginal   : 1057.6233738271435\n",
      "    log_joint      : 1265.6319929726758\n",
      "    val_loss       : -1056.6904562245245\n",
      "    val_ess        : 1.9632790969765705\n",
      "    val_log_marginal: 1056.7239565641983\n",
      "    val_log_joint  : 1264.5455640709918\n",
      "Train Epoch: 354 [0/101520 (0%)] Loss: -1063.627441\n",
      "Train Epoch: 354 [11264/101520 (11%)] Loss: -1055.307861\n",
      "Train Epoch: 354 [22528/101520 (22%)] Loss: -1058.555176\n",
      "Train Epoch: 354 [33792/101520 (33%)] Loss: -1053.421143\n",
      "Train Epoch: 354 [45056/101520 (44%)] Loss: -1061.974731\n",
      "Train Epoch: 354 [56320/101520 (55%)] Loss: -1055.478760\n",
      "Train Epoch: 354 [67584/101520 (67%)] Loss: -1055.461792\n",
      "Train Epoch: 354 [78848/101520 (78%)] Loss: -1059.281738\n",
      "Train Epoch: 354 [90112/101520 (89%)] Loss: -1050.890991\n",
      "Train Epoch: 354 [101376/101520 (100%)] Loss: -1069.816040\n",
      "    epoch          : 354\n",
      "    loss           : -1057.2178415269707\n",
      "    ess            : 1.9629397350340034\n",
      "    log_marginal   : 1057.2499815974404\n",
      "    log_joint      : 1265.2272188579616\n",
      "    val_loss       : -1055.6935929008152\n",
      "    val_ess        : 1.9649074025776074\n",
      "    val_log_marginal: 1055.7244342306385\n",
      "    val_log_joint  : 1263.901754628057\n",
      "Train Epoch: 355 [0/101520 (0%)] Loss: -1064.580566\n",
      "Train Epoch: 355 [11264/101520 (11%)] Loss: -1058.092896\n",
      "Train Epoch: 355 [22528/101520 (22%)] Loss: -1054.234375\n",
      "Train Epoch: 355 [33792/101520 (33%)] Loss: -1057.981201\n",
      "Train Epoch: 355 [45056/101520 (44%)] Loss: -1059.270752\n",
      "Train Epoch: 355 [56320/101520 (55%)] Loss: -1052.039307\n",
      "Train Epoch: 355 [67584/101520 (67%)] Loss: -1049.984375\n",
      "Train Epoch: 355 [78848/101520 (78%)] Loss: -1059.100098\n",
      "Train Epoch: 355 [90112/101520 (89%)] Loss: -1057.068970\n",
      "Train Epoch: 355 [101376/101520 (100%)] Loss: -1059.143188\n",
      "    epoch          : 355\n",
      "    loss           : -1057.685885482098\n",
      "    ess            : 1.962345598331049\n",
      "    log_marginal   : 1057.718566587822\n",
      "    log_joint      : 1265.673794386974\n",
      "    val_loss       : -1058.1384383491848\n",
      "    val_ess        : 1.9624400346175483\n",
      "    val_log_marginal: 1058.1700121008832\n",
      "    val_log_joint  : 1266.3966595193615\n",
      "Train Epoch: 356 [0/101520 (0%)] Loss: -1053.172607\n",
      "Train Epoch: 356 [11264/101520 (11%)] Loss: -1062.273682\n",
      "Train Epoch: 356 [22528/101520 (22%)] Loss: -1059.456787\n",
      "Train Epoch: 356 [33792/101520 (33%)] Loss: -1051.684814\n",
      "Train Epoch: 356 [45056/101520 (44%)] Loss: -1055.451416\n",
      "Train Epoch: 356 [56320/101520 (55%)] Loss: -1062.517212\n",
      "Train Epoch: 356 [67584/101520 (67%)] Loss: -1060.716675\n",
      "Train Epoch: 356 [78848/101520 (78%)] Loss: -1058.311157\n",
      "Train Epoch: 356 [90112/101520 (89%)] Loss: -1057.581055\n",
      "Train Epoch: 356 [101376/101520 (100%)] Loss: -1056.623291\n",
      "    epoch          : 356\n",
      "    loss           : -1058.005192588921\n",
      "    ess            : 1.9629619828420668\n",
      "    log_marginal   : 1058.037603177018\n",
      "    log_joint      : 1266.007445062225\n",
      "    val_loss       : -1057.922660495924\n",
      "    val_ess        : 1.9623439985772837\n",
      "    val_log_marginal: 1057.9575832201087\n",
      "    val_log_joint  : 1265.832859205163\n",
      "Train Epoch: 357 [0/101520 (0%)] Loss: -1060.243896\n",
      "Train Epoch: 357 [11264/101520 (11%)] Loss: -1059.792236\n",
      "Train Epoch: 357 [22528/101520 (22%)] Loss: -1047.390381\n",
      "Train Epoch: 357 [33792/101520 (33%)] Loss: -1060.579102\n",
      "Train Epoch: 357 [45056/101520 (44%)] Loss: -1060.484863\n",
      "Train Epoch: 357 [56320/101520 (55%)] Loss: -1065.404053\n",
      "Train Epoch: 357 [67584/101520 (67%)] Loss: -1061.235962\n",
      "Train Epoch: 357 [78848/101520 (78%)] Loss: -1056.042358\n",
      "Train Epoch: 357 [90112/101520 (89%)] Loss: -1047.203369\n",
      "Train Epoch: 357 [101376/101520 (100%)] Loss: -1044.129517\n",
      "    epoch          : 357\n",
      "    loss           : -1057.443922493326\n",
      "    ess            : 1.9632126296584929\n",
      "    log_marginal   : 1057.4752657329616\n",
      "    log_joint      : 1265.4001777687265\n",
      "    val_loss       : -1055.5885487432065\n",
      "    val_ess        : 1.955701910931131\n",
      "    val_log_marginal: 1055.6284763502038\n",
      "    val_log_joint  : 1263.6559315557065\n",
      "Train Epoch: 358 [0/101520 (0%)] Loss: -1062.176025\n",
      "Train Epoch: 358 [11264/101520 (11%)] Loss: -1057.075317\n",
      "Train Epoch: 358 [22528/101520 (22%)] Loss: -1059.561157\n",
      "Train Epoch: 358 [33792/101520 (33%)] Loss: -1057.976074\n",
      "Train Epoch: 358 [45056/101520 (44%)] Loss: -1056.050171\n",
      "Train Epoch: 358 [56320/101520 (55%)] Loss: -1058.380127\n",
      "Train Epoch: 358 [67584/101520 (67%)] Loss: -1056.347290\n",
      "Train Epoch: 358 [78848/101520 (78%)] Loss: -1052.101685\n",
      "Train Epoch: 358 [90112/101520 (89%)] Loss: -1059.506592\n",
      "Train Epoch: 358 [101376/101520 (100%)] Loss: -1058.470093\n",
      "    epoch          : 358\n",
      "    loss           : -1058.4352328046482\n",
      "    ess            : 1.9616549440364741\n",
      "    log_marginal   : 1058.4686371309674\n",
      "    log_joint      : 1266.3831879122174\n",
      "    val_loss       : -1059.810005519701\n",
      "    val_ess        : 1.9611040509265403\n",
      "    val_log_marginal: 1059.844381581182\n",
      "    val_log_joint  : 1267.7963230298913\n",
      "Train Epoch: 359 [0/101520 (0%)] Loss: -1062.432129\n",
      "Train Epoch: 359 [11264/101520 (11%)] Loss: -1062.598145\n",
      "Train Epoch: 359 [22528/101520 (22%)] Loss: -1055.971313\n",
      "Train Epoch: 359 [33792/101520 (33%)] Loss: -1063.941162\n",
      "Train Epoch: 359 [45056/101520 (44%)] Loss: -1061.402344\n",
      "Train Epoch: 359 [56320/101520 (55%)] Loss: -1058.401489\n",
      "Train Epoch: 359 [67584/101520 (67%)] Loss: -1061.331543\n",
      "Train Epoch: 359 [78848/101520 (78%)] Loss: -1056.300415\n",
      "Train Epoch: 359 [90112/101520 (89%)] Loss: -1058.772583\n",
      "Train Epoch: 359 [101376/101520 (100%)] Loss: -1057.038574\n",
      "    epoch          : 359\n",
      "    loss           : -1057.8278931277482\n",
      "    ess            : 1.9624422824562495\n",
      "    log_marginal   : 1057.8605282270728\n",
      "    log_joint      : 1265.841542306258\n",
      "    val_loss       : -1055.5802904211957\n",
      "    val_ess        : 1.9637070479600325\n",
      "    val_log_marginal: 1055.6124267578125\n",
      "    val_log_joint  : 1263.705078125\n",
      "Train Epoch: 360 [0/101520 (0%)] Loss: -1059.317627\n",
      "Train Epoch: 360 [11264/101520 (11%)] Loss: -1054.965820\n",
      "Train Epoch: 360 [22528/101520 (22%)] Loss: -1054.213135\n",
      "Train Epoch: 360 [33792/101520 (33%)] Loss: -1056.033203\n",
      "Train Epoch: 360 [45056/101520 (44%)] Loss: -1058.961914\n",
      "Train Epoch: 360 [56320/101520 (55%)] Loss: -1061.109863\n",
      "Train Epoch: 360 [67584/101520 (67%)] Loss: -1059.771362\n",
      "Train Epoch: 360 [78848/101520 (78%)] Loss: -1059.395630\n",
      "Train Epoch: 360 [90112/101520 (89%)] Loss: -1058.837646\n",
      "Train Epoch: 360 [101376/101520 (100%)] Loss: -1064.671143\n",
      "    epoch          : 360\n",
      "    loss           : -1058.7803488879947\n",
      "    ess            : 1.9622185985047613\n",
      "    log_marginal   : 1058.8130937892588\n",
      "    log_joint      : 1266.7776651813756\n",
      "    val_loss       : -1059.2511782438858\n",
      "    val_ess        : 1.9636830557947573\n",
      "    val_log_marginal: 1059.2819452700408\n",
      "    val_log_joint  : 1267.2025518002717\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch360.pth ...\n",
      "Train Epoch: 361 [0/101520 (0%)] Loss: -1064.671143\n",
      "Train Epoch: 361 [11264/101520 (11%)] Loss: -1061.545288\n",
      "Train Epoch: 361 [22528/101520 (22%)] Loss: -1053.534790\n",
      "Train Epoch: 361 [33792/101520 (33%)] Loss: -1055.190186\n",
      "Train Epoch: 361 [45056/101520 (44%)] Loss: -1057.287109\n",
      "Train Epoch: 361 [56320/101520 (55%)] Loss: -1058.168945\n",
      "Train Epoch: 361 [67584/101520 (67%)] Loss: -1056.934326\n",
      "Train Epoch: 361 [78848/101520 (78%)] Loss: -1054.827393\n",
      "Train Epoch: 361 [90112/101520 (89%)] Loss: -1063.231323\n",
      "Train Epoch: 361 [101376/101520 (100%)] Loss: -1059.577271\n",
      "    epoch          : 361\n",
      "    loss           : -1058.8985221517744\n",
      "    ess            : 1.9619824287280365\n",
      "    log_marginal   : 1058.9315860307397\n",
      "    log_joint      : 1266.8973535892353\n",
      "    val_loss       : -1058.982963230299\n",
      "    val_ess        : 1.9594273256218953\n",
      "    val_log_marginal: 1059.0213145380435\n",
      "    val_log_joint  : 1266.9649764351223\n",
      "Train Epoch: 362 [0/101520 (0%)] Loss: -1060.674194\n",
      "Train Epoch: 362 [11264/101520 (11%)] Loss: -1059.274658\n",
      "Train Epoch: 362 [22528/101520 (22%)] Loss: -1061.818359\n",
      "Train Epoch: 362 [33792/101520 (33%)] Loss: -1055.532715\n",
      "Train Epoch: 362 [45056/101520 (44%)] Loss: -1053.932495\n",
      "Train Epoch: 362 [56320/101520 (55%)] Loss: -1059.697510\n",
      "Train Epoch: 362 [67584/101520 (67%)] Loss: -1053.169678\n",
      "Train Epoch: 362 [78848/101520 (78%)] Loss: -1058.146729\n",
      "Train Epoch: 362 [90112/101520 (89%)] Loss: -1045.009644\n",
      "Train Epoch: 362 [101376/101520 (100%)] Loss: -1059.744629\n",
      "    epoch          : 362\n",
      "    loss           : -1059.1720375559437\n",
      "    ess            : 1.96146174112157\n",
      "    log_marginal   : 1059.205523466944\n",
      "    log_joint      : 1267.1944831579774\n",
      "    val_loss       : -1061.230665123981\n",
      "    val_ess        : 1.963108197502468\n",
      "    val_log_marginal: 1061.2606731912365\n",
      "    val_log_joint  : 1268.835353685462\n",
      "Train Epoch: 363 [0/101520 (0%)] Loss: -1058.866943\n",
      "Train Epoch: 363 [11264/101520 (11%)] Loss: -1065.706177\n",
      "Train Epoch: 363 [22528/101520 (22%)] Loss: -1057.393921\n",
      "Train Epoch: 363 [33792/101520 (33%)] Loss: -1060.203491\n",
      "Train Epoch: 363 [45056/101520 (44%)] Loss: -1065.134155\n",
      "Train Epoch: 363 [56320/101520 (55%)] Loss: -1064.515869\n",
      "Train Epoch: 363 [67584/101520 (67%)] Loss: -1060.812012\n",
      "Train Epoch: 363 [78848/101520 (78%)] Loss: -1061.346191\n",
      "Train Epoch: 363 [90112/101520 (89%)] Loss: -1054.956543\n",
      "Train Epoch: 363 [101376/101520 (100%)] Loss: -1056.449829\n",
      "    epoch          : 363\n",
      "    loss           : -1059.5674232981312\n",
      "    ess            : 1.9623974322074622\n",
      "    log_marginal   : 1059.6005847106626\n",
      "    log_joint      : 1267.5557511679492\n",
      "    val_loss       : -1061.9580927309783\n",
      "    val_ess        : 1.9643900394439697\n",
      "    val_log_marginal: 1061.9878991168478\n",
      "    val_log_joint  : 1269.9033840013587\n",
      "Train Epoch: 364 [0/101520 (0%)] Loss: -1061.852539\n",
      "Train Epoch: 364 [11264/101520 (11%)] Loss: -1059.492188\n",
      "Train Epoch: 364 [22528/101520 (22%)] Loss: -1059.400391\n",
      "Train Epoch: 364 [33792/101520 (33%)] Loss: -1055.168213\n",
      "Train Epoch: 364 [45056/101520 (44%)] Loss: -1065.175537\n",
      "Train Epoch: 364 [56320/101520 (55%)] Loss: -1051.875488\n",
      "Train Epoch: 364 [67584/101520 (67%)] Loss: -1054.089355\n",
      "Train Epoch: 364 [78848/101520 (78%)] Loss: -1065.007812\n",
      "Train Epoch: 364 [90112/101520 (89%)] Loss: -1054.948120\n",
      "Train Epoch: 364 [101376/101520 (100%)] Loss: -1051.834229\n",
      "    epoch          : 364\n",
      "    loss           : -1059.7437756409\n",
      "    ess            : 1.9622368914398116\n",
      "    log_marginal   : 1059.776251251374\n",
      "    log_joint      : 1267.7389265664258\n",
      "    val_loss       : -1059.53148883322\n",
      "    val_ess        : 1.9643964145494544\n",
      "    val_log_marginal: 1059.561327063519\n",
      "    val_log_joint  : 1267.558397376019\n",
      "Train Epoch: 365 [0/101520 (0%)] Loss: -1065.046875\n",
      "Train Epoch: 365 [11264/101520 (11%)] Loss: -1057.998779\n",
      "Train Epoch: 365 [22528/101520 (22%)] Loss: -1060.818359\n",
      "Train Epoch: 365 [33792/101520 (33%)] Loss: -1061.088379\n",
      "Train Epoch: 365 [45056/101520 (44%)] Loss: -1058.753418\n",
      "Train Epoch: 365 [56320/101520 (55%)] Loss: -1055.039795\n",
      "Train Epoch: 365 [67584/101520 (67%)] Loss: -1055.188965\n",
      "Train Epoch: 365 [78848/101520 (78%)] Loss: -1060.092529\n",
      "Train Epoch: 365 [90112/101520 (89%)] Loss: -1057.387573\n",
      "Train Epoch: 365 [101376/101520 (100%)] Loss: -1060.231323\n",
      "    epoch          : 365\n",
      "    loss           : -1059.6922018539965\n",
      "    ess            : 1.9622156074897728\n",
      "    log_marginal   : 1059.724644953282\n",
      "    log_joint      : 1267.7510796168342\n",
      "    val_loss       : -1059.424852454144\n",
      "    val_ess        : 1.9614249208699102\n",
      "    val_log_marginal: 1059.4564368206522\n",
      "    val_log_joint  : 1267.4135423743207\n",
      "Train Epoch: 366 [0/101520 (0%)] Loss: -1059.511719\n",
      "Train Epoch: 366 [11264/101520 (11%)] Loss: -1061.062500\n",
      "Train Epoch: 366 [22528/101520 (22%)] Loss: -1061.277100\n",
      "Train Epoch: 366 [33792/101520 (33%)] Loss: -1051.518555\n",
      "Train Epoch: 366 [45056/101520 (44%)] Loss: -1060.401489\n",
      "Train Epoch: 366 [56320/101520 (55%)] Loss: -1061.851685\n",
      "Train Epoch: 366 [67584/101520 (67%)] Loss: -1061.883789\n",
      "Train Epoch: 366 [78848/101520 (78%)] Loss: -1064.247559\n",
      "Train Epoch: 366 [90112/101520 (89%)] Loss: -1061.366455\n",
      "Train Epoch: 366 [101376/101520 (100%)] Loss: -1057.017212\n",
      "    epoch          : 366\n",
      "    loss           : -1060.2943464883008\n",
      "    ess            : 1.9622832362975307\n",
      "    log_marginal   : 1060.3269110444803\n",
      "    log_joint      : 1268.2795183191347\n",
      "    val_loss       : -1059.019626783288\n",
      "    val_ess        : 1.966225909150165\n",
      "    val_log_marginal: 1059.0475968070652\n",
      "    val_log_joint  : 1267.10131305197\n",
      "Train Epoch: 367 [0/101520 (0%)] Loss: -1062.262573\n",
      "Train Epoch: 367 [11264/101520 (11%)] Loss: -1064.090576\n",
      "Train Epoch: 367 [22528/101520 (22%)] Loss: -1059.356689\n",
      "Train Epoch: 367 [33792/101520 (33%)] Loss: -1054.728271\n",
      "Train Epoch: 367 [45056/101520 (44%)] Loss: -1060.498657\n",
      "Train Epoch: 367 [56320/101520 (55%)] Loss: -1055.302002\n",
      "Train Epoch: 367 [67584/101520 (67%)] Loss: -1064.313843\n",
      "Train Epoch: 367 [78848/101520 (78%)] Loss: -1060.643555\n",
      "Train Epoch: 367 [90112/101520 (89%)] Loss: -1061.409912\n",
      "Train Epoch: 367 [101376/101520 (100%)] Loss: -1059.175903\n",
      "    epoch          : 367\n",
      "    loss           : -1059.7679161186793\n",
      "    ess            : 1.9625054729643778\n",
      "    log_marginal   : 1059.7998985405543\n",
      "    log_joint      : 1267.7387284322\n",
      "    val_loss       : -1058.7798753821332\n",
      "    val_ess        : 1.958872297535772\n",
      "    val_log_marginal: 1058.818263841712\n",
      "    val_log_joint  : 1266.8263523267663\n",
      "Train Epoch: 368 [0/101520 (0%)] Loss: -1057.619751\n",
      "Train Epoch: 368 [11264/101520 (11%)] Loss: -1064.731445\n",
      "Train Epoch: 368 [22528/101520 (22%)] Loss: -1054.410400\n",
      "Train Epoch: 368 [33792/101520 (33%)] Loss: -1056.885376\n",
      "Train Epoch: 368 [45056/101520 (44%)] Loss: -1063.590820\n",
      "Train Epoch: 368 [56320/101520 (55%)] Loss: -1058.569580\n",
      "Train Epoch: 368 [67584/101520 (67%)] Loss: -1059.513184\n",
      "Train Epoch: 368 [78848/101520 (78%)] Loss: -1056.503662\n",
      "Train Epoch: 368 [90112/101520 (89%)] Loss: -1057.111694\n",
      "Train Epoch: 368 [101376/101520 (100%)] Loss: -1050.048462\n",
      "    epoch          : 368\n",
      "    loss           : -1060.3834823531722\n",
      "    ess            : 1.9628167463906447\n",
      "    log_marginal   : 1060.4160395483275\n",
      "    log_joint      : 1268.3785860454616\n",
      "    val_loss       : -1060.7419698963995\n",
      "    val_ess        : 1.9573304394017095\n",
      "    val_log_marginal: 1060.7784105383832\n",
      "    val_log_joint  : 1268.6314538043478\n",
      "Train Epoch: 369 [0/101520 (0%)] Loss: -1059.252441\n",
      "Train Epoch: 369 [11264/101520 (11%)] Loss: -1059.044067\n",
      "Train Epoch: 369 [22528/101520 (22%)] Loss: -1060.565308\n",
      "Train Epoch: 369 [33792/101520 (33%)] Loss: -1064.423218\n",
      "Train Epoch: 369 [45056/101520 (44%)] Loss: -1058.054321\n",
      "Train Epoch: 369 [56320/101520 (55%)] Loss: -1056.735107\n",
      "Train Epoch: 369 [67584/101520 (67%)] Loss: -1063.693604\n",
      "Train Epoch: 369 [78848/101520 (78%)] Loss: -1055.398071\n",
      "Train Epoch: 369 [90112/101520 (89%)] Loss: -1061.389893\n",
      "Train Epoch: 369 [101376/101520 (100%)] Loss: -1068.202026\n",
      "    epoch          : 369\n",
      "    loss           : -1060.5362229850423\n",
      "    ess            : 1.962167653007124\n",
      "    log_marginal   : 1060.5700511836526\n",
      "    log_joint      : 1268.571449164769\n",
      "    val_loss       : -1059.577631411345\n",
      "    val_ess        : 1.9646400835203088\n",
      "    val_log_marginal: 1059.60595703125\n",
      "    val_log_joint  : 1267.6054369055707\n",
      "Train Epoch: 370 [0/101520 (0%)] Loss: -1063.464844\n",
      "Train Epoch: 370 [11264/101520 (11%)] Loss: -1060.644409\n",
      "Train Epoch: 370 [22528/101520 (22%)] Loss: -1049.652954\n",
      "Train Epoch: 370 [33792/101520 (33%)] Loss: -1056.993042\n",
      "Train Epoch: 370 [45056/101520 (44%)] Loss: -1057.788086\n",
      "Train Epoch: 370 [56320/101520 (55%)] Loss: -1058.164062\n",
      "Train Epoch: 370 [67584/101520 (67%)] Loss: -1055.475098\n",
      "Train Epoch: 370 [78848/101520 (78%)] Loss: -1053.444946\n",
      "Train Epoch: 370 [90112/101520 (89%)] Loss: -1059.174316\n",
      "Train Epoch: 370 [101376/101520 (100%)] Loss: -1028.327515\n",
      "    epoch          : 370\n",
      "    loss           : -1060.1207324464117\n",
      "    ess            : 1.962073194920717\n",
      "    log_marginal   : 1060.1534178460663\n",
      "    log_joint      : 1268.1393533831265\n",
      "    val_loss       : -1061.203761888587\n",
      "    val_ess        : 1.960027347440305\n",
      "    val_log_marginal: 1061.2365881878397\n",
      "    val_log_joint  : 1269.1293308423913\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch370.pth ...\n",
      "Train Epoch: 371 [0/101520 (0%)] Loss: -1057.883789\n",
      "Train Epoch: 371 [11264/101520 (11%)] Loss: -1057.350342\n",
      "Train Epoch: 371 [22528/101520 (22%)] Loss: -1059.804321\n",
      "Train Epoch: 371 [33792/101520 (33%)] Loss: -1060.648682\n",
      "Train Epoch: 371 [45056/101520 (44%)] Loss: -1069.016724\n",
      "Train Epoch: 371 [56320/101520 (55%)] Loss: -1064.907227\n",
      "Train Epoch: 371 [67584/101520 (67%)] Loss: -1068.932373\n",
      "Train Epoch: 371 [78848/101520 (78%)] Loss: -1062.833740\n",
      "Train Epoch: 371 [90112/101520 (89%)] Loss: -1064.711792\n",
      "Train Epoch: 371 [101376/101520 (100%)] Loss: -1049.244873\n",
      "    epoch          : 371\n",
      "    loss           : -1061.0524479084877\n",
      "    ess            : 1.9624101858043193\n",
      "    log_marginal   : 1061.0852198001728\n",
      "    log_joint      : 1269.0598665937107\n",
      "    val_loss       : -1061.657279636549\n",
      "    val_ess        : 1.9620508318362029\n",
      "    val_log_marginal: 1061.6886517068615\n",
      "    val_log_joint  : 1269.69605022928\n",
      "Train Epoch: 372 [0/101520 (0%)] Loss: -1059.309937\n",
      "Train Epoch: 372 [11264/101520 (11%)] Loss: -1064.146606\n",
      "Train Epoch: 372 [22528/101520 (22%)] Loss: -1061.586426\n",
      "Train Epoch: 372 [33792/101520 (33%)] Loss: -1057.582764\n",
      "Train Epoch: 372 [45056/101520 (44%)] Loss: -1067.046387\n",
      "Train Epoch: 372 [56320/101520 (55%)] Loss: -1061.325684\n",
      "Train Epoch: 372 [67584/101520 (67%)] Loss: -1058.716919\n",
      "Train Epoch: 372 [78848/101520 (78%)] Loss: -1058.496338\n",
      "Train Epoch: 372 [90112/101520 (89%)] Loss: -1061.683594\n",
      "Train Epoch: 372 [101376/101520 (100%)] Loss: -1047.421875\n",
      "    epoch          : 372\n",
      "    loss           : -1060.7066742403424\n",
      "    ess            : 1.9616162249790363\n",
      "    log_marginal   : 1060.740198796718\n",
      "    log_joint      : 1268.7297786540123\n",
      "    val_loss       : -1058.9269382642663\n",
      "    val_ess        : 1.9587151745091314\n",
      "    val_log_marginal: 1058.9629118546195\n",
      "    val_log_joint  : 1266.8797978940217\n",
      "Train Epoch: 373 [0/101520 (0%)] Loss: -1061.736572\n",
      "Train Epoch: 373 [11264/101520 (11%)] Loss: -1060.747803\n",
      "Train Epoch: 373 [22528/101520 (22%)] Loss: -1059.209595\n",
      "Train Epoch: 373 [33792/101520 (33%)] Loss: -1061.376099\n",
      "Train Epoch: 373 [45056/101520 (44%)] Loss: -1065.800293\n",
      "Train Epoch: 373 [56320/101520 (55%)] Loss: -1062.250977\n",
      "Train Epoch: 373 [67584/101520 (67%)] Loss: -1051.951172\n",
      "Train Epoch: 373 [78848/101520 (78%)] Loss: -1068.959351\n",
      "Train Epoch: 373 [90112/101520 (89%)] Loss: -1061.923340\n",
      "Train Epoch: 373 [101376/101520 (100%)] Loss: -1064.428833\n",
      "    epoch          : 373\n",
      "    loss           : -1061.2176090413002\n",
      "    ess            : 1.962274406423521\n",
      "    log_marginal   : 1061.2501055080088\n",
      "    log_joint      : 1269.2164269835505\n",
      "    val_loss       : -1060.6554963485055\n",
      "    val_ess        : 1.96432452098183\n",
      "    val_log_marginal: 1060.6858971637228\n",
      "    val_log_joint  : 1268.6557776409647\n",
      "Train Epoch: 374 [0/101520 (0%)] Loss: -1071.512939\n",
      "Train Epoch: 374 [11264/101520 (11%)] Loss: -1063.247314\n",
      "Train Epoch: 374 [22528/101520 (22%)] Loss: -1055.646973\n",
      "Train Epoch: 374 [33792/101520 (33%)] Loss: -1058.054932\n",
      "Train Epoch: 374 [45056/101520 (44%)] Loss: -1067.358154\n",
      "Train Epoch: 374 [56320/101520 (55%)] Loss: -1073.620605\n",
      "Train Epoch: 374 [67584/101520 (67%)] Loss: -1062.354492\n",
      "Train Epoch: 374 [78848/101520 (78%)] Loss: -1067.433838\n",
      "Train Epoch: 374 [90112/101520 (89%)] Loss: -1066.518555\n",
      "Train Epoch: 374 [101376/101520 (100%)] Loss: -1067.512085\n",
      "    epoch          : 374\n",
      "    loss           : -1061.806879244857\n",
      "    ess            : 1.9622249926753978\n",
      "    log_marginal   : 1061.8399713410804\n",
      "    log_joint      : 1269.841668670501\n",
      "    val_loss       : -1061.2457753057065\n",
      "    val_ess        : 1.9658750347469165\n",
      "    val_log_marginal: 1061.2728112262228\n",
      "    val_log_joint  : 1269.2794136379075\n",
      "Train Epoch: 375 [0/101520 (0%)] Loss: -1062.538086\n",
      "Train Epoch: 375 [11264/101520 (11%)] Loss: -1059.359619\n",
      "Train Epoch: 375 [22528/101520 (22%)] Loss: -1074.726074\n",
      "Train Epoch: 375 [33792/101520 (33%)] Loss: -1065.810547\n",
      "Train Epoch: 375 [45056/101520 (44%)] Loss: -1067.912476\n",
      "Train Epoch: 375 [56320/101520 (55%)] Loss: -1061.059570\n",
      "Train Epoch: 375 [67584/101520 (67%)] Loss: -1052.074463\n",
      "Train Epoch: 375 [78848/101520 (78%)] Loss: -1062.412720\n",
      "Train Epoch: 375 [90112/101520 (89%)] Loss: -1058.317383\n",
      "Train Epoch: 375 [101376/101520 (100%)] Loss: -1058.205322\n",
      "    epoch          : 375\n",
      "    loss           : -1061.295433466159\n",
      "    ess            : 1.9632918666954615\n",
      "    log_marginal   : 1061.326969932671\n",
      "    log_joint      : 1269.4168842258166\n",
      "    val_loss       : -1060.6698847231658\n",
      "    val_ess        : 1.9592937127403591\n",
      "    val_log_marginal: 1060.7022598930027\n",
      "    val_log_joint  : 1268.7198380180027\n",
      "Train Epoch: 376 [0/101520 (0%)] Loss: -1058.534180\n",
      "Train Epoch: 376 [11264/101520 (11%)] Loss: -1060.892334\n",
      "Train Epoch: 376 [22528/101520 (22%)] Loss: -1071.582764\n",
      "Train Epoch: 376 [33792/101520 (33%)] Loss: -1067.657227\n",
      "Train Epoch: 376 [45056/101520 (44%)] Loss: -1069.029541\n",
      "Train Epoch: 376 [56320/101520 (55%)] Loss: -1070.228394\n",
      "Train Epoch: 376 [67584/101520 (67%)] Loss: -1058.491211\n",
      "Train Epoch: 376 [78848/101520 (78%)] Loss: -1055.105103\n",
      "Train Epoch: 376 [90112/101520 (89%)] Loss: -1076.125732\n",
      "Train Epoch: 376 [101376/101520 (100%)] Loss: -1064.064087\n",
      "    epoch          : 376\n",
      "    loss           : -1062.0921146258636\n",
      "    ess            : 1.9620848265125523\n",
      "    log_marginal   : 1062.1255030032978\n",
      "    log_joint      : 1270.1134027068938\n",
      "    val_loss       : -1061.8770274286685\n",
      "    val_ess        : 1.9621561817500903\n",
      "    val_log_marginal: 1061.9130540930707\n",
      "    val_log_joint  : 1269.5340204653533\n",
      "Train Epoch: 377 [0/101520 (0%)] Loss: -1070.536865\n",
      "Train Epoch: 377 [11264/101520 (11%)] Loss: -1061.187134\n",
      "Train Epoch: 377 [22528/101520 (22%)] Loss: -1063.229004\n",
      "Train Epoch: 377 [33792/101520 (33%)] Loss: -1061.376099\n",
      "Train Epoch: 377 [45056/101520 (44%)] Loss: -1067.354004\n",
      "Train Epoch: 377 [56320/101520 (55%)] Loss: -1064.674438\n",
      "Train Epoch: 377 [67584/101520 (67%)] Loss: -1060.388428\n",
      "Train Epoch: 377 [78848/101520 (78%)] Loss: -1061.743530\n",
      "Train Epoch: 377 [90112/101520 (89%)] Loss: -1060.633301\n",
      "Train Epoch: 377 [101376/101520 (100%)] Loss: -1065.970459\n",
      "    epoch          : 377\n",
      "    loss           : -1062.1034579636464\n",
      "    ess            : 1.9627840554893916\n",
      "    log_marginal   : 1062.1359985964982\n",
      "    log_joint      : 1270.026197883951\n",
      "    val_loss       : -1060.9406048318615\n",
      "    val_ess        : 1.963381782821987\n",
      "    val_log_marginal: 1060.9725341796875\n",
      "    val_log_joint  : 1269.2822371773098\n",
      "Train Epoch: 378 [0/101520 (0%)] Loss: -1065.441895\n",
      "Train Epoch: 378 [11264/101520 (11%)] Loss: -1056.449707\n",
      "Train Epoch: 378 [22528/101520 (22%)] Loss: -1068.423584\n",
      "Train Epoch: 378 [33792/101520 (33%)] Loss: -1065.584473\n",
      "Train Epoch: 378 [45056/101520 (44%)] Loss: -1064.246094\n",
      "Train Epoch: 378 [56320/101520 (55%)] Loss: -1058.757812\n",
      "Train Epoch: 378 [67584/101520 (67%)] Loss: -1060.124023\n",
      "Train Epoch: 378 [78848/101520 (78%)] Loss: -1058.369629\n",
      "Train Epoch: 378 [90112/101520 (89%)] Loss: -1059.301025\n",
      "Train Epoch: 378 [101376/101520 (100%)] Loss: -1066.742432\n",
      "    epoch          : 378\n",
      "    loss           : -1062.3826186597048\n",
      "    ess            : 1.9629830175907768\n",
      "    log_marginal   : 1062.4141060527245\n",
      "    log_joint      : 1270.344313118326\n",
      "    val_loss       : -1063.033351732337\n",
      "    val_ess        : 1.9638249148493228\n",
      "    val_log_marginal: 1063.0642992102582\n",
      "    val_log_joint  : 1270.878762950068\n",
      "Train Epoch: 379 [0/101520 (0%)] Loss: -1065.899902\n",
      "Train Epoch: 379 [11264/101520 (11%)] Loss: -1056.980713\n",
      "Train Epoch: 379 [22528/101520 (22%)] Loss: -1062.038086\n",
      "Train Epoch: 379 [33792/101520 (33%)] Loss: -1061.358765\n",
      "Train Epoch: 379 [45056/101520 (44%)] Loss: -1059.066650\n",
      "Train Epoch: 379 [56320/101520 (55%)] Loss: -1067.886597\n",
      "Train Epoch: 379 [67584/101520 (67%)] Loss: -1063.658936\n",
      "Train Epoch: 379 [78848/101520 (78%)] Loss: -1058.826904\n",
      "Train Epoch: 379 [90112/101520 (89%)] Loss: -1063.589355\n",
      "Train Epoch: 379 [101376/101520 (100%)] Loss: -1064.635620\n",
      "    epoch          : 379\n",
      "    loss           : -1062.8638008156015\n",
      "    ess            : 1.9627960758592615\n",
      "    log_marginal   : 1062.896534061911\n",
      "    log_joint      : 1270.8651172120367\n",
      "    val_loss       : -1061.4665951936142\n",
      "    val_ess        : 1.960594882135806\n",
      "    val_log_marginal: 1061.4979778787365\n",
      "    val_log_joint  : 1269.3785878057065\n",
      "Train Epoch: 380 [0/101520 (0%)] Loss: -1065.393188\n",
      "Train Epoch: 380 [11264/101520 (11%)] Loss: -1066.435425\n",
      "Train Epoch: 380 [22528/101520 (22%)] Loss: -1071.583252\n",
      "Train Epoch: 380 [33792/101520 (33%)] Loss: -1070.078857\n",
      "Train Epoch: 380 [45056/101520 (44%)] Loss: -1058.985229\n",
      "Train Epoch: 380 [56320/101520 (55%)] Loss: -1054.221436\n",
      "Train Epoch: 380 [67584/101520 (67%)] Loss: -1050.735962\n",
      "Train Epoch: 380 [78848/101520 (78%)] Loss: -1065.196899\n",
      "Train Epoch: 380 [90112/101520 (89%)] Loss: -1065.452637\n",
      "Train Epoch: 380 [101376/101520 (100%)] Loss: -1067.706665\n",
      "    epoch          : 380\n",
      "    loss           : -1062.497268446726\n",
      "    ess            : 1.9622700909274307\n",
      "    log_marginal   : 1062.5294827408527\n",
      "    log_joint      : 1270.5454862201634\n",
      "    val_loss       : -1061.338283372962\n",
      "    val_ess        : 1.9639761085095613\n",
      "    val_log_marginal: 1061.368503736413\n",
      "    val_log_joint  : 1269.4470798658288\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch380.pth ...\n",
      "Train Epoch: 381 [0/101520 (0%)] Loss: -1065.419434\n",
      "Train Epoch: 381 [11264/101520 (11%)] Loss: -1060.560547\n",
      "Train Epoch: 381 [22528/101520 (22%)] Loss: -1067.082520\n",
      "Train Epoch: 381 [33792/101520 (33%)] Loss: -1065.349365\n",
      "Train Epoch: 381 [45056/101520 (44%)] Loss: -1062.114502\n",
      "Train Epoch: 381 [56320/101520 (55%)] Loss: -1064.284180\n",
      "Train Epoch: 381 [67584/101520 (67%)] Loss: -1057.549805\n",
      "Train Epoch: 381 [78848/101520 (78%)] Loss: -1060.790771\n",
      "Train Epoch: 381 [90112/101520 (89%)] Loss: -1058.858276\n",
      "Train Epoch: 381 [101376/101520 (100%)] Loss: -1054.696167\n",
      "    epoch          : 381\n",
      "    loss           : -1062.912294627434\n",
      "    ess            : 1.9628521233946834\n",
      "    log_marginal   : 1062.9448377139604\n",
      "    log_joint      : 1270.9169449542635\n",
      "    val_loss       : -1062.8621720023777\n",
      "    val_ess        : 1.957428419071695\n",
      "    val_log_marginal: 1062.9000562584918\n",
      "    val_log_joint  : 1271.0465512483017\n",
      "Train Epoch: 382 [0/101520 (0%)] Loss: -1071.773438\n",
      "Train Epoch: 382 [11264/101520 (11%)] Loss: -1062.964111\n",
      "Train Epoch: 382 [22528/101520 (22%)] Loss: -1070.022217\n",
      "Train Epoch: 382 [33792/101520 (33%)] Loss: -1062.932861\n",
      "Train Epoch: 382 [45056/101520 (44%)] Loss: -1058.747437\n",
      "Train Epoch: 382 [56320/101520 (55%)] Loss: -1060.911499\n",
      "Train Epoch: 382 [67584/101520 (67%)] Loss: -1063.964355\n",
      "Train Epoch: 382 [78848/101520 (78%)] Loss: -1073.727051\n",
      "Train Epoch: 382 [90112/101520 (89%)] Loss: -1060.209351\n",
      "Train Epoch: 382 [101376/101520 (100%)] Loss: -1074.510254\n",
      "    epoch          : 382\n",
      "    loss           : -1063.5459027314305\n",
      "    ess            : 1.9622097578480016\n",
      "    log_marginal   : 1063.5788261375235\n",
      "    log_joint      : 1271.5383509343592\n",
      "    val_loss       : -1063.9962742017663\n",
      "    val_ess        : 1.9619765696318254\n",
      "    val_log_marginal: 1064.0290261973505\n",
      "    val_log_joint  : 1271.9341669497283\n",
      "Train Epoch: 383 [0/101520 (0%)] Loss: -1068.661743\n",
      "Train Epoch: 383 [11264/101520 (11%)] Loss: -1055.166748\n",
      "Train Epoch: 383 [22528/101520 (22%)] Loss: -1056.763428\n",
      "Train Epoch: 383 [33792/101520 (33%)] Loss: -1061.525513\n",
      "Train Epoch: 383 [45056/101520 (44%)] Loss: -1055.474365\n",
      "Train Epoch: 383 [56320/101520 (55%)] Loss: -1063.877686\n",
      "Train Epoch: 383 [67584/101520 (67%)] Loss: -1063.375244\n",
      "Train Epoch: 383 [78848/101520 (78%)] Loss: -1054.574341\n",
      "Train Epoch: 383 [90112/101520 (89%)] Loss: -1063.195312\n",
      "Train Epoch: 383 [101376/101520 (100%)] Loss: -1071.642334\n",
      "    epoch          : 383\n",
      "    loss           : -1062.4692425751805\n",
      "    ess            : 1.9613927944221687\n",
      "    log_marginal   : 1062.50285669068\n",
      "    log_joint      : 1270.591507954813\n",
      "    val_loss       : -1062.0799719769022\n",
      "    val_ess        : 1.9610701073770938\n",
      "    val_log_marginal: 1062.1153564453125\n",
      "    val_log_joint  : 1270.3536801545517\n",
      "Train Epoch: 384 [0/101520 (0%)] Loss: -1059.475830\n",
      "Train Epoch: 384 [11264/101520 (11%)] Loss: -1059.761475\n",
      "Train Epoch: 384 [22528/101520 (22%)] Loss: -1061.887451\n",
      "Train Epoch: 384 [33792/101520 (33%)] Loss: -1061.303589\n",
      "Train Epoch: 384 [45056/101520 (44%)] Loss: -1059.706421\n",
      "Train Epoch: 384 [56320/101520 (55%)] Loss: -1060.027100\n",
      "Train Epoch: 384 [67584/101520 (67%)] Loss: -1072.787720\n",
      "Train Epoch: 384 [78848/101520 (78%)] Loss: -1059.373779\n",
      "Train Epoch: 384 [90112/101520 (89%)] Loss: -1056.277954\n",
      "Train Epoch: 384 [101376/101520 (100%)] Loss: -1056.150146\n",
      "    epoch          : 384\n",
      "    loss           : -1063.5414977720634\n",
      "    ess            : 1.9616371968283726\n",
      "    log_marginal   : 1063.57437394492\n",
      "    log_joint      : 1271.5318566710505\n",
      "    val_loss       : -1064.0762461786685\n",
      "    val_ess        : 1.9639024941817573\n",
      "    val_log_marginal: 1064.1061215608017\n",
      "    val_log_joint  : 1272.301614512568\n",
      "Train Epoch: 385 [0/101520 (0%)] Loss: -1058.459717\n",
      "Train Epoch: 385 [11264/101520 (11%)] Loss: -1059.900757\n",
      "Train Epoch: 385 [22528/101520 (22%)] Loss: -1068.439209\n",
      "Train Epoch: 385 [33792/101520 (33%)] Loss: -1067.028320\n",
      "Train Epoch: 385 [45056/101520 (44%)] Loss: -1061.190430\n",
      "Train Epoch: 385 [56320/101520 (55%)] Loss: -1060.625488\n",
      "Train Epoch: 385 [67584/101520 (67%)] Loss: -1069.093994\n",
      "Train Epoch: 385 [78848/101520 (78%)] Loss: -1068.702881\n",
      "Train Epoch: 385 [90112/101520 (89%)] Loss: -1063.402954\n",
      "Train Epoch: 385 [101376/101520 (100%)] Loss: -1059.787598\n",
      "    epoch          : 385\n",
      "    loss           : -1063.1907535725502\n",
      "    ess            : 1.9627590862350848\n",
      "    log_marginal   : 1063.22242805826\n",
      "    log_joint      : 1271.2238603908213\n",
      "    val_loss       : -1063.0447573454483\n",
      "    val_ess        : 1.9676333717677905\n",
      "    val_log_marginal: 1063.0711351477582\n",
      "    val_log_joint  : 1270.8389414911685\n",
      "Train Epoch: 386 [0/101520 (0%)] Loss: -1064.147217\n",
      "Train Epoch: 386 [11264/101520 (11%)] Loss: -1059.059570\n",
      "Train Epoch: 386 [22528/101520 (22%)] Loss: -1056.860474\n",
      "Train Epoch: 386 [33792/101520 (33%)] Loss: -1057.581421\n",
      "Train Epoch: 386 [45056/101520 (44%)] Loss: -1068.018555\n",
      "Train Epoch: 386 [56320/101520 (55%)] Loss: -1063.552734\n",
      "Train Epoch: 386 [67584/101520 (67%)] Loss: -1070.707397\n",
      "Train Epoch: 386 [78848/101520 (78%)] Loss: -1065.700439\n",
      "Train Epoch: 386 [90112/101520 (89%)] Loss: -1058.727783\n",
      "Train Epoch: 386 [101376/101520 (100%)] Loss: -1053.112915\n",
      "    epoch          : 386\n",
      "    loss           : -1064.033425795972\n",
      "    ess            : 1.9625723972991482\n",
      "    log_marginal   : 1064.0656265948885\n",
      "    log_joint      : 1272.068519477269\n",
      "    val_loss       : -1065.22827679178\n",
      "    val_ess        : 1.9613706858261772\n",
      "    val_log_marginal: 1065.2605192764945\n",
      "    val_log_joint  : 1273.2711977751358\n",
      "Train Epoch: 387 [0/101520 (0%)] Loss: -1060.424194\n",
      "Train Epoch: 387 [11264/101520 (11%)] Loss: -1071.046509\n",
      "Train Epoch: 387 [22528/101520 (22%)] Loss: -1062.228271\n",
      "Train Epoch: 387 [33792/101520 (33%)] Loss: -1061.283081\n",
      "Train Epoch: 387 [45056/101520 (44%)] Loss: -1072.286865\n",
      "Train Epoch: 387 [56320/101520 (55%)] Loss: -1066.772583\n",
      "Train Epoch: 387 [67584/101520 (67%)] Loss: -1061.419434\n",
      "Train Epoch: 387 [78848/101520 (78%)] Loss: -1054.792236\n",
      "Train Epoch: 387 [90112/101520 (89%)] Loss: -1068.308594\n",
      "Train Epoch: 387 [101376/101520 (100%)] Loss: -1060.437988\n",
      "    epoch          : 387\n",
      "    loss           : -1064.2686589686714\n",
      "    ess            : 1.9620886286299433\n",
      "    log_marginal   : 1064.301151754868\n",
      "    log_joint      : 1272.2747195449906\n",
      "    val_loss       : -1063.3645125679348\n",
      "    val_ess        : 1.9646701087122378\n",
      "    val_log_marginal: 1063.3948443868885\n",
      "    val_log_joint  : 1271.1788860818615\n",
      "Train Epoch: 388 [0/101520 (0%)] Loss: -1063.445801\n",
      "Train Epoch: 388 [11264/101520 (11%)] Loss: -1063.312988\n",
      "Train Epoch: 388 [22528/101520 (22%)] Loss: -1064.435791\n",
      "Train Epoch: 388 [33792/101520 (33%)] Loss: -1061.375854\n",
      "Train Epoch: 388 [45056/101520 (44%)] Loss: -1065.831299\n",
      "Train Epoch: 388 [56320/101520 (55%)] Loss: -1072.314209\n",
      "Train Epoch: 388 [67584/101520 (67%)] Loss: -1064.542603\n",
      "Train Epoch: 388 [78848/101520 (78%)] Loss: -1061.236694\n",
      "Train Epoch: 388 [90112/101520 (89%)] Loss: -1067.097412\n",
      "Train Epoch: 388 [101376/101520 (100%)] Loss: -1060.291626\n",
      "    epoch          : 388\n",
      "    loss           : -1064.6646728515625\n",
      "    ess            : 1.9623257323126098\n",
      "    log_marginal   : 1064.6978637081893\n",
      "    log_joint      : 1272.6299196176194\n",
      "    val_loss       : -1065.8160081946332\n",
      "    val_ess        : 1.9597365856170654\n",
      "    val_log_marginal: 1065.8477199388587\n",
      "    val_log_joint  : 1274.0319611922555\n",
      "Train Epoch: 389 [0/101520 (0%)] Loss: -1063.068115\n",
      "Train Epoch: 389 [11264/101520 (11%)] Loss: -1060.541260\n",
      "Train Epoch: 389 [22528/101520 (22%)] Loss: -1064.954102\n",
      "Train Epoch: 389 [33792/101520 (33%)] Loss: -1066.702759\n",
      "Train Epoch: 389 [45056/101520 (44%)] Loss: -1060.079346\n",
      "Train Epoch: 389 [56320/101520 (55%)] Loss: -1074.562866\n",
      "Train Epoch: 389 [67584/101520 (67%)] Loss: -1064.724854\n",
      "Train Epoch: 389 [78848/101520 (78%)] Loss: -1060.493652\n",
      "Train Epoch: 389 [90112/101520 (89%)] Loss: -1061.808594\n",
      "Train Epoch: 389 [101376/101520 (100%)] Loss: -1076.661255\n",
      "    epoch          : 389\n",
      "    loss           : -1064.7469175712547\n",
      "    ess            : 1.9617700582772644\n",
      "    log_marginal   : 1064.7799869709877\n",
      "    log_joint      : 1272.7810340766332\n",
      "    val_loss       : -1065.339647376019\n",
      "    val_ess        : 1.9600021372670713\n",
      "    val_log_marginal: 1065.376953125\n",
      "    val_log_joint  : 1273.4205481487772\n",
      "Train Epoch: 390 [0/101520 (0%)] Loss: -1074.975098\n",
      "Train Epoch: 390 [11264/101520 (11%)] Loss: -1066.315918\n",
      "Train Epoch: 390 [22528/101520 (22%)] Loss: -1069.168335\n",
      "Train Epoch: 390 [33792/101520 (33%)] Loss: -1069.428223\n",
      "Train Epoch: 390 [45056/101520 (44%)] Loss: -1067.551758\n",
      "Train Epoch: 390 [56320/101520 (55%)] Loss: -1065.421265\n",
      "Train Epoch: 390 [67584/101520 (67%)] Loss: -1059.055054\n",
      "Train Epoch: 390 [78848/101520 (78%)] Loss: -1061.426758\n",
      "Train Epoch: 390 [90112/101520 (89%)] Loss: -1064.142334\n",
      "Train Epoch: 390 [101376/101520 (100%)] Loss: -1061.788940\n",
      "    epoch          : 390\n",
      "    loss           : -1065.11399772299\n",
      "    ess            : 1.9616447405599469\n",
      "    log_marginal   : 1065.146682509226\n",
      "    log_joint      : 1273.1624645444017\n",
      "    val_loss       : -1064.5238886294158\n",
      "    val_ess        : 1.9622501186702563\n",
      "    val_log_marginal: 1064.554878566576\n",
      "    val_log_joint  : 1272.8645656419837\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch390.pth ...\n",
      "Train Epoch: 391 [0/101520 (0%)] Loss: -1064.561646\n",
      "Train Epoch: 391 [11264/101520 (11%)] Loss: -1063.147705\n",
      "Train Epoch: 391 [22528/101520 (22%)] Loss: -1064.793945\n",
      "Train Epoch: 391 [33792/101520 (33%)] Loss: -1065.443359\n",
      "Train Epoch: 391 [45056/101520 (44%)] Loss: -1059.507812\n",
      "Train Epoch: 391 [56320/101520 (55%)] Loss: -1062.649536\n",
      "Train Epoch: 391 [67584/101520 (67%)] Loss: -1060.952393\n",
      "Train Epoch: 391 [78848/101520 (78%)] Loss: -1068.037354\n",
      "Train Epoch: 391 [90112/101520 (89%)] Loss: -1063.088013\n",
      "Train Epoch: 391 [101376/101520 (100%)] Loss: -1059.158325\n",
      "    epoch          : 391\n",
      "    loss           : -1064.9088067289572\n",
      "    ess            : 1.962673877351847\n",
      "    log_marginal   : 1064.941113649301\n",
      "    log_joint      : 1272.994727053235\n",
      "    val_loss       : -1062.516548488451\n",
      "    val_ess        : 1.9609143474827642\n",
      "    val_log_marginal: 1062.5500275985055\n",
      "    val_log_joint  : 1270.7723017153533\n",
      "Train Epoch: 392 [0/101520 (0%)] Loss: -1064.766846\n",
      "Train Epoch: 392 [11264/101520 (11%)] Loss: -1069.305786\n",
      "Train Epoch: 392 [22528/101520 (22%)] Loss: -1068.845337\n",
      "Train Epoch: 392 [33792/101520 (33%)] Loss: -1065.864624\n",
      "Train Epoch: 392 [45056/101520 (44%)] Loss: -1069.624878\n",
      "Train Epoch: 392 [56320/101520 (55%)] Loss: -1062.901611\n",
      "Train Epoch: 392 [67584/101520 (67%)] Loss: -1063.240234\n",
      "Train Epoch: 392 [78848/101520 (78%)] Loss: -1070.665771\n",
      "Train Epoch: 392 [90112/101520 (89%)] Loss: -1059.209351\n",
      "Train Epoch: 392 [101376/101520 (100%)] Loss: -1060.459961\n",
      "    epoch          : 392\n",
      "    loss           : -1065.5049367933418\n",
      "    ess            : 1.9627866325665957\n",
      "    log_marginal   : 1065.5368946784704\n",
      "    log_joint      : 1273.480458321883\n",
      "    val_loss       : -1065.2764521059783\n",
      "    val_ess        : 1.9627469467080159\n",
      "    val_log_marginal: 1065.30689007303\n",
      "    val_log_joint  : 1273.4090151579483\n",
      "Train Epoch: 393 [0/101520 (0%)] Loss: -1065.582764\n",
      "Train Epoch: 393 [11264/101520 (11%)] Loss: -1063.237549\n",
      "Train Epoch: 393 [22528/101520 (22%)] Loss: -1070.871460\n",
      "Train Epoch: 393 [33792/101520 (33%)] Loss: -1068.456787\n",
      "Train Epoch: 393 [45056/101520 (44%)] Loss: -1066.055298\n",
      "Train Epoch: 393 [56320/101520 (55%)] Loss: -1070.838379\n",
      "Train Epoch: 393 [67584/101520 (67%)] Loss: -1066.032837\n",
      "Train Epoch: 393 [78848/101520 (78%)] Loss: -1060.971436\n",
      "Train Epoch: 393 [90112/101520 (89%)] Loss: -1062.369141\n",
      "Train Epoch: 393 [101376/101520 (100%)] Loss: -1050.349854\n",
      "    epoch          : 393\n",
      "    loss           : -1065.4176896445117\n",
      "    ess            : 1.9633179399835405\n",
      "    log_marginal   : 1065.4490997467808\n",
      "    log_joint      : 1273.459982407153\n",
      "    val_loss       : -1063.4684156334918\n",
      "    val_ess        : 1.9572126865386963\n",
      "    val_log_marginal: 1063.5059177564538\n",
      "    val_log_joint  : 1271.2367102581522\n",
      "Train Epoch: 394 [0/101520 (0%)] Loss: -1068.353149\n",
      "Train Epoch: 394 [11264/101520 (11%)] Loss: -1062.882446\n",
      "Train Epoch: 394 [22528/101520 (22%)] Loss: -1058.893311\n",
      "Train Epoch: 394 [33792/101520 (33%)] Loss: -1075.246094\n",
      "Train Epoch: 394 [45056/101520 (44%)] Loss: -1061.521973\n",
      "Train Epoch: 394 [56320/101520 (55%)] Loss: -1064.232300\n",
      "Train Epoch: 394 [67584/101520 (67%)] Loss: -1070.504150\n",
      "Train Epoch: 394 [78848/101520 (78%)] Loss: -1062.903442\n",
      "Train Epoch: 394 [90112/101520 (89%)] Loss: -1061.836914\n",
      "Train Epoch: 394 [101376/101520 (100%)] Loss: -1081.452881\n",
      "    epoch          : 394\n",
      "    loss           : -1065.688978952379\n",
      "    ess            : 1.9629128302761059\n",
      "    log_marginal   : 1065.721331879122\n",
      "    log_joint      : 1273.66105981568\n",
      "    val_loss       : -1065.8512387483017\n",
      "    val_ess        : 1.9616621784541919\n",
      "    val_log_marginal: 1065.8834334663723\n",
      "    val_log_joint  : 1273.5559825067935\n",
      "Train Epoch: 395 [0/101520 (0%)] Loss: -1073.891602\n",
      "Train Epoch: 395 [11264/101520 (11%)] Loss: -1068.378052\n",
      "Train Epoch: 395 [22528/101520 (22%)] Loss: -1070.066895\n",
      "Train Epoch: 395 [33792/101520 (33%)] Loss: -1067.057129\n",
      "Train Epoch: 395 [45056/101520 (44%)] Loss: -1063.534790\n",
      "Train Epoch: 395 [56320/101520 (55%)] Loss: -1068.536499\n",
      "Train Epoch: 395 [67584/101520 (67%)] Loss: -1068.900513\n",
      "Train Epoch: 395 [78848/101520 (78%)] Loss: -1069.761963\n",
      "Train Epoch: 395 [90112/101520 (89%)] Loss: -1067.703613\n",
      "Train Epoch: 395 [101376/101520 (100%)] Loss: -1067.078613\n",
      "    epoch          : 395\n",
      "    loss           : -1066.139512871977\n",
      "    ess            : 1.9630171396025462\n",
      "    log_marginal   : 1066.1702776578204\n",
      "    log_joint      : 1274.1780527392823\n",
      "    val_loss       : -1066.2450906504755\n",
      "    val_ess        : 1.9642589921536653\n",
      "    val_log_marginal: 1066.2753534731658\n",
      "    val_log_joint  : 1274.0916748046875\n",
      "Train Epoch: 396 [0/101520 (0%)] Loss: -1066.580811\n",
      "Train Epoch: 396 [11264/101520 (11%)] Loss: -1071.390747\n",
      "Train Epoch: 396 [22528/101520 (22%)] Loss: -1064.757690\n",
      "Train Epoch: 396 [33792/101520 (33%)] Loss: -1064.928223\n",
      "Train Epoch: 396 [45056/101520 (44%)] Loss: -1068.527466\n",
      "Train Epoch: 396 [56320/101520 (55%)] Loss: -1067.964844\n",
      "Train Epoch: 396 [67584/101520 (67%)] Loss: -1067.394043\n",
      "Train Epoch: 396 [78848/101520 (78%)] Loss: -1068.907959\n",
      "Train Epoch: 396 [90112/101520 (89%)] Loss: -1065.501831\n",
      "Train Epoch: 396 [101376/101520 (100%)] Loss: -1052.839966\n",
      "    epoch          : 396\n",
      "    loss           : -1065.729205107569\n",
      "    ess            : 1.9634442263512155\n",
      "    log_marginal   : 1065.7599318614557\n",
      "    log_joint      : 1273.758728334053\n",
      "    val_loss       : -1065.500679347826\n",
      "    val_ess        : 1.9614988824595576\n",
      "    val_log_marginal: 1065.5339673913043\n",
      "    val_log_joint  : 1273.4596796450408\n",
      "Train Epoch: 397 [0/101520 (0%)] Loss: -1065.957764\n",
      "Train Epoch: 397 [11264/101520 (11%)] Loss: -1068.964600\n",
      "Train Epoch: 397 [22528/101520 (22%)] Loss: -1064.782471\n",
      "Train Epoch: 397 [33792/101520 (33%)] Loss: -1064.899170\n",
      "Train Epoch: 397 [45056/101520 (44%)] Loss: -1060.881470\n",
      "Train Epoch: 397 [56320/101520 (55%)] Loss: -1067.072021\n",
      "Train Epoch: 397 [67584/101520 (67%)] Loss: -1071.372192\n",
      "Train Epoch: 397 [78848/101520 (78%)] Loss: -1070.609131\n",
      "Train Epoch: 397 [90112/101520 (89%)] Loss: -1064.081055\n",
      "Train Epoch: 397 [101376/101520 (100%)] Loss: -1069.413696\n",
      "    epoch          : 397\n",
      "    loss           : -1066.3085851621388\n",
      "    ess            : 1.962870593646064\n",
      "    log_marginal   : 1066.340490906682\n",
      "    log_joint      : 1274.3231612162374\n",
      "    val_loss       : -1064.7578814962635\n",
      "    val_ess        : 1.960437992344732\n",
      "    val_log_marginal: 1064.793021824049\n",
      "    val_log_joint  : 1272.7942106827445\n",
      "Train Epoch: 398 [0/101520 (0%)] Loss: -1064.061768\n",
      "Train Epoch: 398 [11264/101520 (11%)] Loss: -1064.480225\n",
      "Train Epoch: 398 [22528/101520 (22%)] Loss: -1061.687866\n",
      "Train Epoch: 398 [33792/101520 (33%)] Loss: -1066.112305\n",
      "Train Epoch: 398 [45056/101520 (44%)] Loss: -1067.476318\n",
      "Train Epoch: 398 [56320/101520 (55%)] Loss: -1074.299927\n",
      "Train Epoch: 398 [67584/101520 (67%)] Loss: -1059.391235\n",
      "Train Epoch: 398 [78848/101520 (78%)] Loss: -1062.890991\n",
      "Train Epoch: 398 [90112/101520 (89%)] Loss: -1068.531128\n",
      "Train Epoch: 398 [101376/101520 (100%)] Loss: -1067.395386\n",
      "    epoch          : 398\n",
      "    loss           : -1066.6592925565326\n",
      "    ess            : 1.9626935756386226\n",
      "    log_marginal   : 1066.690295962233\n",
      "    log_joint      : 1274.655494268216\n",
      "    val_loss       : -1066.6904137652853\n",
      "    val_ess        : 1.961967364601467\n",
      "    val_log_marginal: 1066.7232241423233\n",
      "    val_log_joint  : 1274.787454356318\n",
      "Train Epoch: 399 [0/101520 (0%)] Loss: -1071.811768\n",
      "Train Epoch: 399 [11264/101520 (11%)] Loss: -1065.618652\n",
      "Train Epoch: 399 [22528/101520 (22%)] Loss: -1062.675415\n",
      "Train Epoch: 399 [33792/101520 (33%)] Loss: -1073.192139\n",
      "Train Epoch: 399 [45056/101520 (44%)] Loss: -1065.020752\n",
      "Train Epoch: 399 [56320/101520 (55%)] Loss: -1070.368286\n",
      "Train Epoch: 399 [67584/101520 (67%)] Loss: -1064.469360\n",
      "Train Epoch: 399 [78848/101520 (78%)] Loss: -1064.363037\n",
      "Train Epoch: 399 [90112/101520 (89%)] Loss: -1071.044434\n",
      "Train Epoch: 399 [101376/101520 (100%)] Loss: -1058.840088\n",
      "    epoch          : 399\n",
      "    loss           : -1066.5005852013976\n",
      "    ess            : 1.962943323293523\n",
      "    log_marginal   : 1066.532330230253\n",
      "    log_joint      : 1274.536585515468\n",
      "    val_loss       : -1065.1651239809783\n",
      "    val_ess        : 1.9643025139103765\n",
      "    val_log_marginal: 1065.1931311565897\n",
      "    val_log_joint  : 1273.301073157269\n",
      "Train Epoch: 400 [0/101520 (0%)] Loss: -1068.384766\n",
      "Train Epoch: 400 [11264/101520 (11%)] Loss: -1074.874756\n",
      "Train Epoch: 400 [22528/101520 (22%)] Loss: -1066.672485\n",
      "Train Epoch: 400 [33792/101520 (33%)] Loss: -1069.445312\n",
      "Train Epoch: 400 [45056/101520 (44%)] Loss: -1069.521973\n",
      "Train Epoch: 400 [56320/101520 (55%)] Loss: -1067.654907\n",
      "Train Epoch: 400 [67584/101520 (67%)] Loss: -1064.202637\n",
      "Train Epoch: 400 [78848/101520 (78%)] Loss: -1072.701660\n",
      "Train Epoch: 400 [90112/101520 (89%)] Loss: -1069.499512\n",
      "Train Epoch: 400 [101376/101520 (100%)] Loss: -1068.967041\n",
      "    epoch          : 400\n",
      "    loss           : -1067.222886281996\n",
      "    ess            : 1.9616888252334979\n",
      "    log_marginal   : 1067.256479541261\n",
      "    log_joint      : 1275.2270777716708\n",
      "    val_loss       : -1067.9018448539402\n",
      "    val_ess        : 1.9656812263571697\n",
      "    val_log_marginal: 1067.930074940557\n",
      "    val_log_joint  : 1275.863382090693\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch400.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 401 [0/101520 (0%)] Loss: -1067.988037\n",
      "Train Epoch: 401 [11264/101520 (11%)] Loss: -1061.658447\n",
      "Train Epoch: 401 [22528/101520 (22%)] Loss: -1063.772095\n",
      "Train Epoch: 401 [33792/101520 (33%)] Loss: -1066.755859\n",
      "Train Epoch: 401 [45056/101520 (44%)] Loss: -1075.442261\n",
      "Train Epoch: 401 [56320/101520 (55%)] Loss: -1064.943848\n",
      "Train Epoch: 401 [67584/101520 (67%)] Loss: -1065.712891\n",
      "Train Epoch: 401 [78848/101520 (78%)] Loss: -1071.062256\n",
      "Train Epoch: 401 [90112/101520 (89%)] Loss: -1064.153809\n",
      "Train Epoch: 401 [101376/101520 (100%)] Loss: -1065.391479\n",
      "    epoch          : 401\n",
      "    loss           : -1066.9142471390153\n",
      "    ess            : 1.9619367715701386\n",
      "    log_marginal   : 1066.9477269158292\n",
      "    log_joint      : 1274.972041604507\n",
      "    val_loss       : -1066.041259765625\n",
      "    val_ess        : 1.964969894160395\n",
      "    val_log_marginal: 1066.070408033288\n",
      "    val_log_joint  : 1274.2705396569293\n",
      "Train Epoch: 402 [0/101520 (0%)] Loss: -1069.261475\n",
      "Train Epoch: 402 [11264/101520 (11%)] Loss: -1074.753906\n",
      "Train Epoch: 402 [22528/101520 (22%)] Loss: -1073.332520\n",
      "Train Epoch: 402 [33792/101520 (33%)] Loss: -1066.145386\n",
      "Train Epoch: 402 [45056/101520 (44%)] Loss: -1068.956299\n",
      "Train Epoch: 402 [56320/101520 (55%)] Loss: -1067.150391\n",
      "Train Epoch: 402 [67584/101520 (67%)] Loss: -1071.076538\n",
      "Train Epoch: 402 [78848/101520 (78%)] Loss: -1061.501709\n",
      "Train Epoch: 402 [90112/101520 (89%)] Loss: -1067.708984\n",
      "Train Epoch: 402 [101376/101520 (100%)] Loss: -1063.637207\n",
      "    epoch          : 402\n",
      "    loss           : -1067.4024118394707\n",
      "    ess            : 1.9626574624123885\n",
      "    log_marginal   : 1067.4349598333465\n",
      "    log_joint      : 1275.4436108766488\n",
      "    val_loss       : -1067.6846817680027\n",
      "    val_ess        : 1.9610104509021924\n",
      "    val_log_marginal: 1067.7180016559103\n",
      "    val_log_joint  : 1275.5199346127717\n",
      "Train Epoch: 403 [0/101520 (0%)] Loss: -1068.961426\n",
      "Train Epoch: 403 [11264/101520 (11%)] Loss: -1069.403809\n",
      "Train Epoch: 403 [22528/101520 (22%)] Loss: -1068.547119\n",
      "Train Epoch: 403 [33792/101520 (33%)] Loss: -1066.058350\n",
      "Train Epoch: 403 [45056/101520 (44%)] Loss: -1063.171509\n",
      "Train Epoch: 403 [56320/101520 (55%)] Loss: -1064.488770\n",
      "Train Epoch: 403 [67584/101520 (67%)] Loss: -1066.084473\n",
      "Train Epoch: 403 [78848/101520 (78%)] Loss: -1065.538940\n",
      "Train Epoch: 403 [90112/101520 (89%)] Loss: -1069.101318\n",
      "Train Epoch: 403 [101376/101520 (100%)] Loss: -1076.139526\n",
      "    epoch          : 403\n",
      "    loss           : -1067.8960543685223\n",
      "    ess            : 1.9629055980461925\n",
      "    log_marginal   : 1067.9277889692603\n",
      "    log_joint      : 1275.857541491638\n",
      "    val_loss       : -1066.0756252122962\n",
      "    val_ess        : 1.9589275691820227\n",
      "    val_log_marginal: 1066.109857973845\n",
      "    val_log_joint  : 1274.101175059443\n",
      "Train Epoch: 404 [0/101520 (0%)] Loss: -1076.140503\n",
      "Train Epoch: 404 [11264/101520 (11%)] Loss: -1070.932861\n",
      "Train Epoch: 404 [22528/101520 (22%)] Loss: -1059.390137\n",
      "Train Epoch: 404 [33792/101520 (33%)] Loss: -1075.207764\n",
      "Train Epoch: 404 [45056/101520 (44%)] Loss: -1068.674194\n",
      "Train Epoch: 404 [56320/101520 (55%)] Loss: -1080.366211\n",
      "Train Epoch: 404 [67584/101520 (67%)] Loss: -1061.335693\n",
      "Train Epoch: 404 [78848/101520 (78%)] Loss: -1064.997314\n",
      "Train Epoch: 404 [90112/101520 (89%)] Loss: -1067.369385\n",
      "Train Epoch: 404 [101376/101520 (100%)] Loss: -1068.450928\n",
      "    epoch          : 404\n",
      "    loss           : -1067.6639974776224\n",
      "    ess            : 1.9628420163638627\n",
      "    log_marginal   : 1067.6966706089038\n",
      "    log_joint      : 1275.647213116363\n",
      "    val_loss       : -1066.6516219429348\n",
      "    val_ess        : 1.965349943741508\n",
      "    val_log_marginal: 1066.6802129330842\n",
      "    val_log_joint  : 1274.5705778702445\n",
      "Train Epoch: 405 [0/101520 (0%)] Loss: -1063.825928\n",
      "Train Epoch: 405 [11264/101520 (11%)] Loss: -1068.847046\n",
      "Train Epoch: 405 [22528/101520 (22%)] Loss: -1064.523804\n",
      "Train Epoch: 405 [33792/101520 (33%)] Loss: -1069.109619\n",
      "Train Epoch: 405 [45056/101520 (44%)] Loss: -1064.490479\n",
      "Train Epoch: 405 [56320/101520 (55%)] Loss: -1067.274414\n",
      "Train Epoch: 405 [67584/101520 (67%)] Loss: -1071.628662\n",
      "Train Epoch: 405 [78848/101520 (78%)] Loss: -1074.477905\n",
      "Train Epoch: 405 [90112/101520 (89%)] Loss: -1074.187012\n",
      "Train Epoch: 405 [101376/101520 (100%)] Loss: -1066.488892\n",
      "    epoch          : 405\n",
      "    loss           : -1068.1365715295226\n",
      "    ess            : 1.9628729083430227\n",
      "    log_marginal   : 1068.1685238938835\n",
      "    log_joint      : 1276.1442245406722\n",
      "    val_loss       : -1068.3727390455163\n",
      "    val_ess        : 1.962012819621874\n",
      "    val_log_marginal: 1068.4043287194293\n",
      "    val_log_joint  : 1276.253614342731\n",
      "Train Epoch: 406 [0/101520 (0%)] Loss: -1067.052734\n",
      "Train Epoch: 406 [11264/101520 (11%)] Loss: -1067.724609\n",
      "Train Epoch: 406 [22528/101520 (22%)] Loss: -1081.916016\n",
      "Train Epoch: 406 [33792/101520 (33%)] Loss: -1070.514893\n",
      "Train Epoch: 406 [45056/101520 (44%)] Loss: -1074.396729\n",
      "Train Epoch: 406 [56320/101520 (55%)] Loss: -1063.341064\n",
      "Train Epoch: 406 [67584/101520 (67%)] Loss: -1073.030884\n",
      "Train Epoch: 406 [78848/101520 (78%)] Loss: -1067.868652\n",
      "Train Epoch: 406 [90112/101520 (89%)] Loss: -1067.123901\n",
      "Train Epoch: 406 [101376/101520 (100%)] Loss: -1068.149780\n",
      "    epoch          : 406\n",
      "    loss           : -1068.303363129122\n",
      "    ess            : 1.9627201958517333\n",
      "    log_marginal   : 1068.335632630928\n",
      "    log_joint      : 1276.3384568104193\n",
      "    val_loss       : -1067.9827774711277\n",
      "    val_ess        : 1.9613094381664111\n",
      "    val_log_marginal: 1068.0112570057745\n",
      "    val_log_joint  : 1276.2225394870925\n",
      "Train Epoch: 407 [0/101520 (0%)] Loss: -1069.840576\n",
      "Train Epoch: 407 [11264/101520 (11%)] Loss: -1077.205811\n",
      "Train Epoch: 407 [22528/101520 (22%)] Loss: -1068.326416\n",
      "Train Epoch: 407 [33792/101520 (33%)] Loss: -1070.885986\n",
      "Train Epoch: 407 [45056/101520 (44%)] Loss: -1068.595703\n",
      "Train Epoch: 407 [56320/101520 (55%)] Loss: -1072.833740\n",
      "Train Epoch: 407 [67584/101520 (67%)] Loss: -1060.107422\n",
      "Train Epoch: 407 [78848/101520 (78%)] Loss: -1073.271240\n",
      "Train Epoch: 407 [90112/101520 (89%)] Loss: -1078.104980\n",
      "Train Epoch: 407 [101376/101520 (100%)] Loss: -1071.391235\n",
      "    epoch          : 407\n",
      "    loss           : -1068.1689072805434\n",
      "    ess            : 1.9635195055199628\n",
      "    log_marginal   : 1068.2000088332286\n",
      "    log_joint      : 1276.2072901126728\n",
      "    val_loss       : -1067.7082360309103\n",
      "    val_ess        : 1.9608143982679949\n",
      "    val_log_marginal: 1067.7423573369565\n",
      "    val_log_joint  : 1275.5249607252038\n",
      "Train Epoch: 408 [0/101520 (0%)] Loss: -1073.014648\n",
      "Train Epoch: 408 [11264/101520 (11%)] Loss: -1075.091797\n",
      "Train Epoch: 408 [22528/101520 (22%)] Loss: -1070.314453\n",
      "Train Epoch: 408 [33792/101520 (33%)] Loss: -1065.711182\n",
      "Train Epoch: 408 [45056/101520 (44%)] Loss: -1067.204224\n",
      "Train Epoch: 408 [56320/101520 (55%)] Loss: -1066.543457\n",
      "Train Epoch: 408 [67584/101520 (67%)] Loss: -1070.592041\n",
      "Train Epoch: 408 [78848/101520 (78%)] Loss: -1067.497925\n",
      "Train Epoch: 408 [90112/101520 (89%)] Loss: -1073.568237\n",
      "Train Epoch: 408 [101376/101520 (100%)] Loss: -1061.491089\n",
      "    epoch          : 408\n",
      "    loss           : -1068.8670464137092\n",
      "    ess            : 1.9624846293099563\n",
      "    log_marginal   : 1068.8994251040358\n",
      "    log_joint      : 1276.856625044166\n",
      "    val_loss       : -1067.780862559443\n",
      "    val_ess        : 1.9637583546016528\n",
      "    val_log_marginal: 1067.8119055706522\n",
      "    val_log_joint  : 1275.92260211447\n",
      "Train Epoch: 409 [0/101520 (0%)] Loss: -1071.974243\n",
      "Train Epoch: 409 [11264/101520 (11%)] Loss: -1067.796265\n",
      "Train Epoch: 409 [22528/101520 (22%)] Loss: -1071.517456\n",
      "Train Epoch: 409 [33792/101520 (33%)] Loss: -1067.568848\n",
      "Train Epoch: 409 [45056/101520 (44%)] Loss: -1063.320557\n",
      "Train Epoch: 409 [56320/101520 (55%)] Loss: -1066.307617\n",
      "Train Epoch: 409 [67584/101520 (67%)] Loss: -1068.276489\n",
      "Train Epoch: 409 [78848/101520 (78%)] Loss: -1064.287109\n",
      "Train Epoch: 409 [90112/101520 (89%)] Loss: -1061.625977\n",
      "Train Epoch: 409 [101376/101520 (100%)] Loss: -1054.706665\n",
      "    epoch          : 409\n",
      "    loss           : -1068.594590996977\n",
      "    ess            : 1.9632514253932627\n",
      "    log_marginal   : 1068.625234939345\n",
      "    log_joint      : 1276.595175584956\n",
      "    val_loss       : -1068.237405528193\n",
      "    val_ess        : 1.963013514228489\n",
      "    val_log_marginal: 1068.2682256283967\n",
      "    val_log_joint  : 1276.3427999745245\n",
      "Train Epoch: 410 [0/101520 (0%)] Loss: -1068.887939\n",
      "Train Epoch: 410 [11264/101520 (11%)] Loss: -1066.168701\n",
      "Train Epoch: 410 [22528/101520 (22%)] Loss: -1067.606079\n",
      "Train Epoch: 410 [33792/101520 (33%)] Loss: -1066.964355\n",
      "Train Epoch: 410 [45056/101520 (44%)] Loss: -1071.287598\n",
      "Train Epoch: 410 [56320/101520 (55%)] Loss: -1075.637329\n",
      "Train Epoch: 410 [67584/101520 (67%)] Loss: -1064.751343\n",
      "Train Epoch: 410 [78848/101520 (78%)] Loss: -1074.004639\n",
      "Train Epoch: 410 [90112/101520 (89%)] Loss: -1065.661133\n",
      "Train Epoch: 410 [101376/101520 (100%)] Loss: -1054.011230\n",
      "    epoch          : 410\n",
      "    loss           : -1068.959820464628\n",
      "    ess            : 1.962121786783688\n",
      "    log_marginal   : 1068.9924377748116\n",
      "    log_joint      : 1277.0079744425252\n",
      "    val_loss       : -1068.7448358950408\n",
      "    val_ess        : 1.963865243870279\n",
      "    val_log_marginal: 1068.7765211022418\n",
      "    val_log_joint  : 1276.92894977072\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch410.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 411 [0/101520 (0%)] Loss: -1067.113281\n",
      "Train Epoch: 411 [11264/101520 (11%)] Loss: -1070.257324\n",
      "Train Epoch: 411 [22528/101520 (22%)] Loss: -1072.059204\n",
      "Train Epoch: 411 [33792/101520 (33%)] Loss: -1074.026733\n",
      "Train Epoch: 411 [45056/101520 (44%)] Loss: -1068.604492\n",
      "Train Epoch: 411 [56320/101520 (55%)] Loss: -1065.106689\n",
      "Train Epoch: 411 [67584/101520 (67%)] Loss: -1077.989380\n",
      "Train Epoch: 411 [78848/101520 (78%)] Loss: -1070.437866\n",
      "Train Epoch: 411 [90112/101520 (89%)] Loss: -1073.426392\n",
      "Train Epoch: 411 [101376/101520 (100%)] Loss: -1094.931030\n",
      "    epoch          : 411\n",
      "    loss           : -1069.5179191857726\n",
      "    ess            : 1.9630075603274244\n",
      "    log_marginal   : 1069.5491201122802\n",
      "    log_joint      : 1277.549082693742\n",
      "    val_loss       : -1069.4209408967392\n",
      "    val_ess        : 1.9607184451559316\n",
      "    val_log_marginal: 1069.4543191661005\n",
      "    val_log_joint  : 1277.3026972231658\n",
      "Train Epoch: 412 [0/101520 (0%)] Loss: -1066.467773\n",
      "Train Epoch: 412 [11264/101520 (11%)] Loss: -1070.542847\n",
      "Train Epoch: 412 [22528/101520 (22%)] Loss: -1076.681152\n",
      "Train Epoch: 412 [33792/101520 (33%)] Loss: -1065.022949\n",
      "Train Epoch: 412 [45056/101520 (44%)] Loss: -1068.577881\n",
      "Train Epoch: 412 [56320/101520 (55%)] Loss: -1064.677979\n",
      "Train Epoch: 412 [67584/101520 (67%)] Loss: -1069.134644\n",
      "Train Epoch: 412 [78848/101520 (78%)] Loss: -1054.532837\n",
      "Train Epoch: 412 [90112/101520 (89%)] Loss: -1075.528809\n",
      "Train Epoch: 412 [101376/101520 (100%)] Loss: -1075.560547\n",
      "    epoch          : 412\n",
      "    loss           : -1069.30547696981\n",
      "    ess            : 1.96311466777744\n",
      "    log_marginal   : 1069.33635278443\n",
      "    log_joint      : 1277.2406238958465\n",
      "    val_loss       : -1068.8008077870245\n",
      "    val_ess        : 1.965271892754928\n",
      "    val_log_marginal: 1068.8298499065897\n",
      "    val_log_joint  : 1276.4184358016305\n",
      "Train Epoch: 413 [0/101520 (0%)] Loss: -1069.265015\n",
      "Train Epoch: 413 [11264/101520 (11%)] Loss: -1060.613159\n",
      "Train Epoch: 413 [22528/101520 (22%)] Loss: -1068.846313\n",
      "Train Epoch: 413 [33792/101520 (33%)] Loss: -1070.931396\n",
      "Train Epoch: 413 [45056/101520 (44%)] Loss: -1075.329834\n",
      "Train Epoch: 413 [56320/101520 (55%)] Loss: -1066.411621\n",
      "Train Epoch: 413 [67584/101520 (67%)] Loss: -1073.352051\n",
      "Train Epoch: 413 [78848/101520 (78%)] Loss: -1066.481079\n",
      "Train Epoch: 413 [90112/101520 (89%)] Loss: -1067.349365\n",
      "Train Epoch: 413 [101376/101520 (100%)] Loss: -1059.708740\n",
      "    epoch          : 413\n",
      "    loss           : -1069.751206594496\n",
      "    ess            : 1.9625222706914547\n",
      "    log_marginal   : 1069.7833147671954\n",
      "    log_joint      : 1277.7335634471185\n",
      "    val_loss       : -1069.8903702445652\n",
      "    val_ess        : 1.9615242533061816\n",
      "    val_log_marginal: 1069.9219121518342\n",
      "    val_log_joint  : 1277.8723409901495\n",
      "Train Epoch: 414 [0/101520 (0%)] Loss: -1069.280273\n",
      "Train Epoch: 414 [11264/101520 (11%)] Loss: -1073.729126\n",
      "Train Epoch: 414 [22528/101520 (22%)] Loss: -1066.637695\n",
      "Train Epoch: 414 [33792/101520 (33%)] Loss: -1076.090210\n",
      "Train Epoch: 414 [45056/101520 (44%)] Loss: -1070.809326\n",
      "Train Epoch: 414 [56320/101520 (55%)] Loss: -1071.660278\n",
      "Train Epoch: 414 [67584/101520 (67%)] Loss: -1075.604736\n",
      "Train Epoch: 414 [78848/101520 (78%)] Loss: -1067.304688\n",
      "Train Epoch: 414 [90112/101520 (89%)] Loss: -1068.532349\n",
      "Train Epoch: 414 [101376/101520 (100%)] Loss: -1081.607178\n",
      "    epoch          : 414\n",
      "    loss           : -1069.9512632743797\n",
      "    ess            : 1.9636040189158377\n",
      "    log_marginal   : 1069.981915804609\n",
      "    log_joint      : 1277.9610595703125\n",
      "    val_loss       : -1069.4431736158288\n",
      "    val_ess        : 1.9623925996863323\n",
      "    val_log_marginal: 1069.4760423743207\n",
      "    val_log_joint  : 1277.325678286345\n",
      "Train Epoch: 415 [0/101520 (0%)] Loss: -1068.352539\n",
      "Train Epoch: 415 [11264/101520 (11%)] Loss: -1063.510254\n",
      "Train Epoch: 415 [22528/101520 (22%)] Loss: -1070.481201\n",
      "Train Epoch: 415 [33792/101520 (33%)] Loss: -1071.163330\n",
      "Train Epoch: 415 [45056/101520 (44%)] Loss: -1065.732910\n",
      "Train Epoch: 415 [56320/101520 (55%)] Loss: -1067.085938\n",
      "Train Epoch: 415 [67584/101520 (67%)] Loss: -1062.128174\n",
      "Train Epoch: 415 [78848/101520 (78%)] Loss: -1071.334717\n",
      "Train Epoch: 415 [90112/101520 (89%)] Loss: -1066.111084\n",
      "Train Epoch: 415 [101376/101520 (100%)] Loss: -1081.626587\n",
      "    epoch          : 415\n",
      "    loss           : -1069.9461510433025\n",
      "    ess            : 1.962408564797598\n",
      "    log_marginal   : 1069.9787977975816\n",
      "    log_joint      : 1277.9862060546875\n",
      "    val_loss       : -1070.1636166779892\n",
      "    val_ess        : 1.9628232717514038\n",
      "    val_log_marginal: 1070.1941501783288\n",
      "    val_log_joint  : 1278.1772248641305\n",
      "Train Epoch: 416 [0/101520 (0%)] Loss: -1071.886963\n",
      "Train Epoch: 416 [11264/101520 (11%)] Loss: -1070.468506\n",
      "Train Epoch: 416 [22528/101520 (22%)] Loss: -1071.156616\n",
      "Train Epoch: 416 [33792/101520 (33%)] Loss: -1071.375122\n",
      "Train Epoch: 416 [45056/101520 (44%)] Loss: -1072.826904\n",
      "Train Epoch: 416 [56320/101520 (55%)] Loss: -1063.232788\n",
      "Train Epoch: 416 [67584/101520 (67%)] Loss: -1072.387207\n",
      "Train Epoch: 416 [78848/101520 (78%)] Loss: -1070.983643\n",
      "Train Epoch: 416 [90112/101520 (89%)] Loss: -1071.704102\n",
      "Train Epoch: 416 [101376/101520 (100%)] Loss: -1071.681396\n",
      "    epoch          : 416\n",
      "    loss           : -1070.5509376717573\n",
      "    ess            : 1.9632828493214132\n",
      "    log_marginal   : 1070.5823079018137\n",
      "    log_joint      : 1278.6131303490106\n",
      "    val_loss       : -1070.648777173913\n",
      "    val_ess        : 1.9665080101593682\n",
      "    val_log_marginal: 1070.675484035326\n",
      "    val_log_joint  : 1278.6658139436142\n",
      "Train Epoch: 417 [0/101520 (0%)] Loss: -1069.086426\n",
      "Train Epoch: 417 [11264/101520 (11%)] Loss: -1060.881226\n",
      "Train Epoch: 417 [22528/101520 (22%)] Loss: -1079.041992\n",
      "Train Epoch: 417 [33792/101520 (33%)] Loss: -1072.594360\n",
      "Train Epoch: 417 [45056/101520 (44%)] Loss: -1065.553711\n",
      "Train Epoch: 417 [56320/101520 (55%)] Loss: -1067.516113\n",
      "Train Epoch: 417 [67584/101520 (67%)] Loss: -1071.249878\n",
      "Train Epoch: 417 [78848/101520 (78%)] Loss: -1071.723877\n",
      "Train Epoch: 417 [90112/101520 (89%)] Loss: -1066.272217\n",
      "Train Epoch: 417 [101376/101520 (100%)] Loss: -1055.046265\n",
      "    epoch          : 417\n",
      "    loss           : -1070.1776270267353\n",
      "    ess            : 1.96289156190115\n",
      "    log_marginal   : 1070.2098443879554\n",
      "    log_joint      : 1278.2207344093515\n",
      "    val_loss       : -1068.1072891898777\n",
      "    val_ess        : 1.9625501840010933\n",
      "    val_log_marginal: 1068.136665675951\n",
      "    val_log_joint  : 1276.3173615828805\n",
      "Train Epoch: 418 [0/101520 (0%)] Loss: -1074.205322\n",
      "Train Epoch: 418 [11264/101520 (11%)] Loss: -1066.443970\n",
      "Train Epoch: 418 [22528/101520 (22%)] Loss: -1072.336426\n",
      "Train Epoch: 418 [33792/101520 (33%)] Loss: -1071.289185\n",
      "Train Epoch: 418 [45056/101520 (44%)] Loss: -1068.895630\n",
      "Train Epoch: 418 [56320/101520 (55%)] Loss: -1071.206299\n",
      "Train Epoch: 418 [67584/101520 (67%)] Loss: -1070.966553\n",
      "Train Epoch: 418 [78848/101520 (78%)] Loss: -1067.259766\n",
      "Train Epoch: 418 [90112/101520 (89%)] Loss: -1066.140747\n",
      "Train Epoch: 418 [101376/101520 (100%)] Loss: -1070.014282\n",
      "    epoch          : 418\n",
      "    loss           : -1070.7400932887092\n",
      "    ess            : 1.962650072035478\n",
      "    log_marginal   : 1070.772076324003\n",
      "    log_joint      : 1278.7256080205716\n",
      "    val_loss       : -1068.9075768512228\n",
      "    val_ess        : 1.9634128705314968\n",
      "    val_log_marginal: 1068.9382961107337\n",
      "    val_log_joint  : 1277.2021272078805\n",
      "Train Epoch: 419 [0/101520 (0%)] Loss: -1070.010986\n",
      "Train Epoch: 419 [11264/101520 (11%)] Loss: -1068.006226\n",
      "Train Epoch: 419 [22528/101520 (22%)] Loss: -1073.187744\n",
      "Train Epoch: 419 [33792/101520 (33%)] Loss: -1073.152832\n",
      "Train Epoch: 419 [45056/101520 (44%)] Loss: -1069.278442\n",
      "Train Epoch: 419 [56320/101520 (55%)] Loss: -1068.625000\n",
      "Train Epoch: 419 [67584/101520 (67%)] Loss: -1071.376831\n",
      "Train Epoch: 419 [78848/101520 (78%)] Loss: -1064.379028\n",
      "Train Epoch: 419 [90112/101520 (89%)] Loss: -1064.881592\n",
      "Train Epoch: 419 [101376/101520 (100%)] Loss: -1067.750244\n",
      "    epoch          : 419\n",
      "    loss           : -1071.1579080706265\n",
      "    ess            : 1.963140827926559\n",
      "    log_marginal   : 1071.1892727799152\n",
      "    log_joint      : 1279.155747610121\n",
      "    val_loss       : -1069.4271824048913\n",
      "    val_ess        : 1.9643892982731694\n",
      "    val_log_marginal: 1069.4578804347825\n",
      "    val_log_joint  : 1277.4096573539402\n",
      "Train Epoch: 420 [0/101520 (0%)] Loss: -1073.383179\n",
      "Train Epoch: 420 [11264/101520 (11%)] Loss: -1067.995117\n",
      "Train Epoch: 420 [22528/101520 (22%)] Loss: -1070.929810\n",
      "Train Epoch: 420 [33792/101520 (33%)] Loss: -1074.008789\n",
      "Train Epoch: 420 [45056/101520 (44%)] Loss: -1074.854248\n",
      "Train Epoch: 420 [56320/101520 (55%)] Loss: -1067.777100\n",
      "Train Epoch: 420 [67584/101520 (67%)] Loss: -1070.589600\n",
      "Train Epoch: 420 [78848/101520 (78%)] Loss: -1074.641846\n",
      "Train Epoch: 420 [90112/101520 (89%)] Loss: -1077.218140\n",
      "Train Epoch: 420 [101376/101520 (100%)] Loss: -1075.579956\n",
      "    epoch          : 420\n",
      "    loss           : -1070.901158625157\n",
      "    ess            : 1.9629883634385152\n",
      "    log_marginal   : 1070.9326748488536\n",
      "    log_joint      : 1278.934673980253\n",
      "    val_loss       : -1068.5593474014945\n",
      "    val_ess        : 1.9611452973407248\n",
      "    val_log_marginal: 1068.5906451681385\n",
      "    val_log_joint  : 1276.2437160326087\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch420.pth ...\n",
      "Train Epoch: 421 [0/101520 (0%)] Loss: -1064.830078\n",
      "Train Epoch: 421 [11264/101520 (11%)] Loss: -1069.515503\n",
      "Train Epoch: 421 [22528/101520 (22%)] Loss: -1072.412109\n",
      "Train Epoch: 421 [33792/101520 (33%)] Loss: -1073.714722\n",
      "Train Epoch: 421 [45056/101520 (44%)] Loss: -1070.917725\n",
      "Train Epoch: 421 [56320/101520 (55%)] Loss: -1072.746582\n",
      "Train Epoch: 421 [67584/101520 (67%)] Loss: -1076.917236\n",
      "Train Epoch: 421 [78848/101520 (78%)] Loss: -1072.349854\n",
      "Train Epoch: 421 [90112/101520 (89%)] Loss: -1073.052490\n",
      "Train Epoch: 421 [101376/101520 (100%)] Loss: -1062.694580\n",
      "    epoch          : 421\n",
      "    loss           : -1071.3969064070352\n",
      "    ess            : 1.9641697562519629\n",
      "    log_marginal   : 1071.4267161000314\n",
      "    log_joint      : 1279.403851533056\n",
      "    val_loss       : -1070.4327976392663\n",
      "    val_ess        : 1.9636851600978686\n",
      "    val_log_marginal: 1070.4654594089675\n",
      "    val_log_joint  : 1278.5794306216033\n",
      "Train Epoch: 422 [0/101520 (0%)] Loss: -1072.339844\n",
      "Train Epoch: 422 [11264/101520 (11%)] Loss: -1072.305786\n",
      "Train Epoch: 422 [22528/101520 (22%)] Loss: -1067.380859\n",
      "Train Epoch: 422 [33792/101520 (33%)] Loss: -1069.943604\n",
      "Train Epoch: 422 [45056/101520 (44%)] Loss: -1076.374878\n",
      "Train Epoch: 422 [56320/101520 (55%)] Loss: -1074.288452\n",
      "Train Epoch: 422 [67584/101520 (67%)] Loss: -1071.786987\n",
      "Train Epoch: 422 [78848/101520 (78%)] Loss: -1066.176880\n",
      "Train Epoch: 422 [90112/101520 (89%)] Loss: -1074.257324\n",
      "Train Epoch: 422 [101376/101520 (100%)] Loss: -1079.133057\n",
      "    epoch          : 422\n",
      "    loss           : -1071.645417026539\n",
      "    ess            : 1.9629743344819726\n",
      "    log_marginal   : 1071.6770718828518\n",
      "    log_joint      : 1279.6846709131596\n",
      "    val_loss       : -1070.43042522928\n",
      "    val_ess        : 1.9611566429552825\n",
      "    val_log_marginal: 1070.4626040251358\n",
      "    val_log_joint  : 1278.5481275475543\n",
      "Train Epoch: 423 [0/101520 (0%)] Loss: -1068.460938\n",
      "Train Epoch: 423 [11264/101520 (11%)] Loss: -1068.523926\n",
      "Train Epoch: 423 [22528/101520 (22%)] Loss: -1074.228271\n",
      "Train Epoch: 423 [33792/101520 (33%)] Loss: -1071.338623\n",
      "Train Epoch: 423 [45056/101520 (44%)] Loss: -1073.418457\n",
      "Train Epoch: 423 [56320/101520 (55%)] Loss: -1071.381958\n",
      "Train Epoch: 423 [67584/101520 (67%)] Loss: -1071.194580\n",
      "Train Epoch: 423 [78848/101520 (78%)] Loss: -1068.990967\n",
      "Train Epoch: 423 [90112/101520 (89%)] Loss: -1071.234619\n",
      "Train Epoch: 423 [101376/101520 (100%)] Loss: -1069.684448\n",
      "    epoch          : 423\n",
      "    loss           : -1071.5390183338568\n",
      "    ess            : 1.9633596440655503\n",
      "    log_marginal   : 1071.5700953497958\n",
      "    log_joint      : 1279.5005103643216\n",
      "    val_loss       : -1071.222995923913\n",
      "    val_ess        : 1.9610182254210762\n",
      "    val_log_marginal: 1071.2565652598505\n",
      "    val_log_joint  : 1279.2502069887908\n",
      "Train Epoch: 424 [0/101520 (0%)] Loss: -1074.970337\n",
      "Train Epoch: 424 [11264/101520 (11%)] Loss: -1074.628662\n",
      "Train Epoch: 424 [22528/101520 (22%)] Loss: -1076.265625\n",
      "Train Epoch: 424 [33792/101520 (33%)] Loss: -1072.376343\n",
      "Train Epoch: 424 [45056/101520 (44%)] Loss: -1072.408691\n",
      "Train Epoch: 424 [56320/101520 (55%)] Loss: -1080.178223\n",
      "Train Epoch: 424 [67584/101520 (67%)] Loss: -1071.717773\n",
      "Train Epoch: 424 [78848/101520 (78%)] Loss: -1076.382324\n",
      "Train Epoch: 424 [90112/101520 (89%)] Loss: -1069.192139\n",
      "Train Epoch: 424 [101376/101520 (100%)] Loss: -1087.676758\n",
      "    epoch          : 424\n",
      "    loss           : -1072.331558917635\n",
      "    ess            : 1.9631602416685479\n",
      "    log_marginal   : 1072.3635425663474\n",
      "    log_joint      : 1280.2820026646907\n",
      "    val_loss       : -1074.9981105638587\n",
      "    val_ess        : 1.9642018960869831\n",
      "    val_log_marginal: 1075.030225670856\n",
      "    val_log_joint  : 1282.9823528787365\n",
      "Train Epoch: 425 [0/101520 (0%)] Loss: -1076.813477\n",
      "Train Epoch: 425 [11264/101520 (11%)] Loss: -1067.843262\n",
      "Train Epoch: 425 [22528/101520 (22%)] Loss: -1075.503906\n",
      "Train Epoch: 425 [33792/101520 (33%)] Loss: -1072.991699\n",
      "Train Epoch: 425 [45056/101520 (44%)] Loss: -1078.041992\n",
      "Train Epoch: 425 [56320/101520 (55%)] Loss: -1067.417725\n",
      "Train Epoch: 425 [67584/101520 (67%)] Loss: -1074.510254\n",
      "Train Epoch: 425 [78848/101520 (78%)] Loss: -1065.420166\n",
      "Train Epoch: 425 [90112/101520 (89%)] Loss: -1075.153320\n",
      "Train Epoch: 425 [101376/101520 (100%)] Loss: -1059.011597\n",
      "    epoch          : 425\n",
      "    loss           : -1071.8166835152324\n",
      "    ess            : 1.9626218769418535\n",
      "    log_marginal   : 1071.84885180296\n",
      "    log_joint      : 1279.8391604016174\n",
      "    val_loss       : -1069.4252292798913\n",
      "    val_ess        : 1.9633638392324033\n",
      "    val_log_marginal: 1069.4550038213315\n",
      "    val_log_joint  : 1277.4220554517663\n",
      "Train Epoch: 426 [0/101520 (0%)] Loss: -1078.087524\n",
      "Train Epoch: 426 [11264/101520 (11%)] Loss: -1068.392822\n",
      "Train Epoch: 426 [22528/101520 (22%)] Loss: -1074.588379\n",
      "Train Epoch: 426 [33792/101520 (33%)] Loss: -1074.300659\n",
      "Train Epoch: 426 [45056/101520 (44%)] Loss: -1066.927002\n",
      "Train Epoch: 426 [56320/101520 (55%)] Loss: -1074.964478\n",
      "Train Epoch: 426 [67584/101520 (67%)] Loss: -1075.592529\n",
      "Train Epoch: 426 [78848/101520 (78%)] Loss: -1069.364258\n",
      "Train Epoch: 426 [90112/101520 (89%)] Loss: -1073.929443\n",
      "Train Epoch: 426 [101376/101520 (100%)] Loss: -1063.128906\n",
      "    epoch          : 426\n",
      "    loss           : -1072.359046821019\n",
      "    ess            : 1.9641877556566019\n",
      "    log_marginal   : 1072.3887602072864\n",
      "    log_joint      : 1280.3212381487515\n",
      "    val_loss       : -1071.3192775560462\n",
      "    val_ess        : 1.9645517546197642\n",
      "    val_log_marginal: 1071.34888226053\n",
      "    val_log_joint  : 1279.2839620838995\n",
      "Train Epoch: 427 [0/101520 (0%)] Loss: -1075.407593\n",
      "Train Epoch: 427 [11264/101520 (11%)] Loss: -1071.585938\n",
      "Train Epoch: 427 [22528/101520 (22%)] Loss: -1070.129883\n",
      "Train Epoch: 427 [33792/101520 (33%)] Loss: -1074.113525\n",
      "Train Epoch: 427 [45056/101520 (44%)] Loss: -1065.447510\n",
      "Train Epoch: 427 [56320/101520 (55%)] Loss: -1065.428955\n",
      "Train Epoch: 427 [67584/101520 (67%)] Loss: -1073.735474\n",
      "Train Epoch: 427 [78848/101520 (78%)] Loss: -1071.495361\n",
      "Train Epoch: 427 [90112/101520 (89%)] Loss: -1073.818237\n",
      "Train Epoch: 427 [101376/101520 (100%)] Loss: -1069.397827\n",
      "    epoch          : 427\n",
      "    loss           : -1072.723107112712\n",
      "    ess            : 1.9634127862489403\n",
      "    log_marginal   : 1072.7545736494974\n",
      "    log_joint      : 1280.7807568114008\n",
      "    val_loss       : -1069.2992474099865\n",
      "    val_ess        : 1.9666947022728298\n",
      "    val_log_marginal: 1069.325458029042\n",
      "    val_log_joint  : 1277.1258544921875\n",
      "Train Epoch: 428 [0/101520 (0%)] Loss: -1069.740234\n",
      "Train Epoch: 428 [11264/101520 (11%)] Loss: -1071.702881\n",
      "Train Epoch: 428 [22528/101520 (22%)] Loss: -1069.718140\n",
      "Train Epoch: 428 [33792/101520 (33%)] Loss: -1072.798096\n",
      "Train Epoch: 428 [45056/101520 (44%)] Loss: -1070.690796\n",
      "Train Epoch: 428 [56320/101520 (55%)] Loss: -1077.805542\n",
      "Train Epoch: 428 [67584/101520 (67%)] Loss: -1077.419067\n",
      "Train Epoch: 428 [78848/101520 (78%)] Loss: -1077.366455\n",
      "Train Epoch: 428 [90112/101520 (89%)] Loss: -1068.735107\n",
      "Train Epoch: 428 [101376/101520 (100%)] Loss: -1089.702759\n",
      "    epoch          : 428\n",
      "    loss           : -1072.585833832247\n",
      "    ess            : 1.9627688380342032\n",
      "    log_marginal   : 1072.618338886817\n",
      "    log_joint      : 1280.6819050084407\n",
      "    val_loss       : -1071.9614629330842\n",
      "    val_ess        : 1.9613215353177942\n",
      "    val_log_marginal: 1071.994485606318\n",
      "    val_log_joint  : 1279.9873153023098\n",
      "Train Epoch: 429 [0/101520 (0%)] Loss: -1071.167969\n",
      "Train Epoch: 429 [11264/101520 (11%)] Loss: -1075.269531\n",
      "Train Epoch: 429 [22528/101520 (22%)] Loss: -1075.842773\n",
      "Train Epoch: 429 [33792/101520 (33%)] Loss: -1073.583740\n",
      "Train Epoch: 429 [45056/101520 (44%)] Loss: -1077.735474\n",
      "Train Epoch: 429 [56320/101520 (55%)] Loss: -1072.602783\n",
      "Train Epoch: 429 [67584/101520 (67%)] Loss: -1067.725708\n",
      "Train Epoch: 429 [78848/101520 (78%)] Loss: -1072.766235\n",
      "Train Epoch: 429 [90112/101520 (89%)] Loss: -1078.848145\n",
      "Train Epoch: 429 [101376/101520 (100%)] Loss: -1076.649292\n",
      "    epoch          : 429\n",
      "    loss           : -1073.0432300663474\n",
      "    ess            : 1.9635043497660651\n",
      "    log_marginal   : 1073.074224270768\n",
      "    log_joint      : 1281.0738580598304\n",
      "    val_loss       : -1072.880365786345\n",
      "    val_ess        : 1.9656519786171291\n",
      "    val_log_marginal: 1072.9089780061142\n",
      "    val_log_joint  : 1280.8296641474185\n",
      "Train Epoch: 430 [0/101520 (0%)] Loss: -1081.703369\n",
      "Train Epoch: 430 [11264/101520 (11%)] Loss: -1069.238525\n",
      "Train Epoch: 430 [22528/101520 (22%)] Loss: -1076.778442\n",
      "Train Epoch: 430 [33792/101520 (33%)] Loss: -1068.035522\n",
      "Train Epoch: 430 [45056/101520 (44%)] Loss: -1075.302368\n",
      "Train Epoch: 430 [56320/101520 (55%)] Loss: -1069.742676\n",
      "Train Epoch: 430 [67584/101520 (67%)] Loss: -1071.586182\n",
      "Train Epoch: 430 [78848/101520 (78%)] Loss: -1076.804443\n",
      "Train Epoch: 430 [90112/101520 (89%)] Loss: -1070.586426\n",
      "Train Epoch: 430 [101376/101520 (100%)] Loss: -1069.720337\n",
      "    epoch          : 430\n",
      "    loss           : -1073.234087920069\n",
      "    ess            : 1.9631572967797668\n",
      "    log_marginal   : 1073.2659041054883\n",
      "    log_joint      : 1281.314962875903\n",
      "    val_loss       : -1071.8610999065897\n",
      "    val_ess        : 1.963212339774422\n",
      "    val_log_marginal: 1071.893506920856\n",
      "    val_log_joint  : 1279.8294093919837\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch430.pth ...\n",
      "Train Epoch: 431 [0/101520 (0%)] Loss: -1078.099609\n",
      "Train Epoch: 431 [11264/101520 (11%)] Loss: -1079.461426\n",
      "Train Epoch: 431 [22528/101520 (22%)] Loss: -1071.048340\n",
      "Train Epoch: 431 [33792/101520 (33%)] Loss: -1071.635010\n",
      "Train Epoch: 431 [45056/101520 (44%)] Loss: -1075.423828\n",
      "Train Epoch: 431 [56320/101520 (55%)] Loss: -1074.469482\n",
      "Train Epoch: 431 [67584/101520 (67%)] Loss: -1076.583984\n",
      "Train Epoch: 431 [78848/101520 (78%)] Loss: -1067.286499\n",
      "Train Epoch: 431 [90112/101520 (89%)] Loss: -1072.893555\n",
      "Train Epoch: 431 [101376/101520 (100%)] Loss: -1081.446899\n",
      "    epoch          : 431\n",
      "    loss           : -1073.0694776372095\n",
      "    ess            : 1.962574054248369\n",
      "    log_marginal   : 1073.100863816151\n",
      "    log_joint      : 1281.1142430904522\n",
      "    val_loss       : -1073.769870923913\n",
      "    val_ess        : 1.9633871213249539\n",
      "    val_log_marginal: 1073.800834324049\n",
      "    val_log_joint  : 1281.6943571671195\n",
      "Train Epoch: 432 [0/101520 (0%)] Loss: -1073.505493\n",
      "Train Epoch: 432 [11264/101520 (11%)] Loss: -1079.588989\n",
      "Train Epoch: 432 [22528/101520 (22%)] Loss: -1069.121826\n",
      "Train Epoch: 432 [33792/101520 (33%)] Loss: -1066.900879\n",
      "Train Epoch: 432 [45056/101520 (44%)] Loss: -1073.437134\n",
      "Train Epoch: 432 [56320/101520 (55%)] Loss: -1067.366333\n",
      "Train Epoch: 432 [67584/101520 (67%)] Loss: -1064.888550\n",
      "Train Epoch: 432 [78848/101520 (78%)] Loss: -1074.683838\n",
      "Train Epoch: 432 [90112/101520 (89%)] Loss: -1073.669189\n",
      "Train Epoch: 432 [101376/101520 (100%)] Loss: -1070.053589\n",
      "    epoch          : 432\n",
      "    loss           : -1073.806148663238\n",
      "    ess            : 1.9635278723347727\n",
      "    log_marginal   : 1073.8378611809046\n",
      "    log_joint      : 1281.8348137170226\n",
      "    val_loss       : -1074.0491094174592\n",
      "    val_ess        : 1.9645609855651855\n",
      "    val_log_marginal: 1074.0775040336277\n",
      "    val_log_joint  : 1282.002340565557\n",
      "Train Epoch: 433 [0/101520 (0%)] Loss: -1068.148315\n",
      "Train Epoch: 433 [11264/101520 (11%)] Loss: -1076.204346\n",
      "Train Epoch: 433 [22528/101520 (22%)] Loss: -1075.965820\n",
      "Train Epoch: 433 [33792/101520 (33%)] Loss: -1079.313965\n",
      "Train Epoch: 433 [45056/101520 (44%)] Loss: -1066.650879\n",
      "Train Epoch: 433 [56320/101520 (55%)] Loss: -1073.441650\n",
      "Train Epoch: 433 [67584/101520 (67%)] Loss: -1076.200928\n",
      "Train Epoch: 433 [78848/101520 (78%)] Loss: -1072.160034\n",
      "Train Epoch: 433 [90112/101520 (89%)] Loss: -1071.101318\n",
      "Train Epoch: 433 [101376/101520 (100%)] Loss: -1062.856323\n",
      "    epoch          : 433\n",
      "    loss           : -1073.526759162021\n",
      "    ess            : 1.963177181368497\n",
      "    log_marginal   : 1073.5585667595792\n",
      "    log_joint      : 1281.6960952222048\n",
      "    val_loss       : -1072.2045473845108\n",
      "    val_ess        : 1.9628278120704319\n",
      "    val_log_marginal: 1072.2379150390625\n",
      "    val_log_joint  : 1280.1114130434783\n",
      "Train Epoch: 434 [0/101520 (0%)] Loss: -1068.704468\n",
      "Train Epoch: 434 [11264/101520 (11%)] Loss: -1075.544189\n",
      "Train Epoch: 434 [22528/101520 (22%)] Loss: -1074.324951\n",
      "Train Epoch: 434 [33792/101520 (33%)] Loss: -1081.277100\n",
      "Train Epoch: 434 [45056/101520 (44%)] Loss: -1069.456787\n",
      "Train Epoch: 434 [56320/101520 (55%)] Loss: -1072.528564\n",
      "Train Epoch: 434 [67584/101520 (67%)] Loss: -1075.279297\n",
      "Train Epoch: 434 [78848/101520 (78%)] Loss: -1077.801270\n",
      "Train Epoch: 434 [90112/101520 (89%)] Loss: -1063.551270\n",
      "Train Epoch: 434 [101376/101520 (100%)] Loss: -1077.583984\n",
      "    epoch          : 434\n",
      "    loss           : -1073.9319387464668\n",
      "    ess            : 1.9630089273404836\n",
      "    log_marginal   : 1073.9634899350267\n",
      "    log_joint      : 1282.079135300526\n",
      "    val_loss       : -1073.4308445142663\n",
      "    val_ess        : 1.9642246339632117\n",
      "    val_log_marginal: 1073.4602846891983\n",
      "    val_log_joint  : 1281.7406377377717\n",
      "Train Epoch: 435 [0/101520 (0%)] Loss: -1070.235596\n",
      "Train Epoch: 435 [11264/101520 (11%)] Loss: -1074.279907\n",
      "Train Epoch: 435 [22528/101520 (22%)] Loss: -1078.004883\n",
      "Train Epoch: 435 [33792/101520 (33%)] Loss: -1070.562012\n",
      "Train Epoch: 435 [45056/101520 (44%)] Loss: -1080.959229\n",
      "Train Epoch: 435 [56320/101520 (55%)] Loss: -1066.838623\n",
      "Train Epoch: 435 [67584/101520 (67%)] Loss: -1071.507568\n",
      "Train Epoch: 435 [78848/101520 (78%)] Loss: -1072.525879\n",
      "Train Epoch: 435 [90112/101520 (89%)] Loss: -1084.733643\n",
      "Train Epoch: 435 [101376/101520 (100%)] Loss: -1075.433960\n",
      "    epoch          : 435\n",
      "    loss           : -1074.5627416869504\n",
      "    ess            : 1.9635942275799698\n",
      "    log_marginal   : 1074.5933249008715\n",
      "    log_joint      : 1282.615592611495\n",
      "    val_loss       : -1076.1381411345108\n",
      "    val_ess        : 1.9655130220496135\n",
      "    val_log_marginal: 1076.166010317595\n",
      "    val_log_joint  : 1283.9690312924592\n",
      "Train Epoch: 436 [0/101520 (0%)] Loss: -1069.808105\n",
      "Train Epoch: 436 [11264/101520 (11%)] Loss: -1071.021973\n",
      "Train Epoch: 436 [22528/101520 (22%)] Loss: -1082.448975\n",
      "Train Epoch: 436 [33792/101520 (33%)] Loss: -1066.200195\n",
      "Train Epoch: 436 [45056/101520 (44%)] Loss: -1070.514038\n",
      "Train Epoch: 436 [56320/101520 (55%)] Loss: -1074.453613\n",
      "Train Epoch: 436 [67584/101520 (67%)] Loss: -1067.732056\n",
      "Train Epoch: 436 [78848/101520 (78%)] Loss: -1075.369995\n",
      "Train Epoch: 436 [90112/101520 (89%)] Loss: -1072.500122\n",
      "Train Epoch: 436 [101376/101520 (100%)] Loss: -1080.525024\n",
      "    epoch          : 436\n",
      "    loss           : -1074.3465576171875\n",
      "    ess            : 1.9631096669776955\n",
      "    log_marginal   : 1074.3775039749528\n",
      "    log_joint      : 1282.380971630614\n",
      "    val_loss       : -1071.3586319633152\n",
      "    val_ess        : 1.9634056557779727\n",
      "    val_log_marginal: 1071.39185101053\n",
      "    val_log_joint  : 1279.5746911090353\n",
      "Train Epoch: 437 [0/101520 (0%)] Loss: -1079.725342\n",
      "Train Epoch: 437 [11264/101520 (11%)] Loss: -1080.080322\n",
      "Train Epoch: 437 [22528/101520 (22%)] Loss: -1072.313965\n",
      "Train Epoch: 437 [33792/101520 (33%)] Loss: -1072.064453\n",
      "Train Epoch: 437 [45056/101520 (44%)] Loss: -1070.984497\n",
      "Train Epoch: 437 [56320/101520 (55%)] Loss: -1067.746582\n",
      "Train Epoch: 437 [67584/101520 (67%)] Loss: -1072.819336\n",
      "Train Epoch: 437 [78848/101520 (78%)] Loss: -1074.678467\n",
      "Train Epoch: 437 [90112/101520 (89%)] Loss: -1075.540283\n",
      "Train Epoch: 437 [101376/101520 (100%)] Loss: -1069.966553\n",
      "    epoch          : 437\n",
      "    loss           : -1074.7299547051664\n",
      "    ess            : 1.9631122392625664\n",
      "    log_marginal   : 1074.761220040633\n",
      "    log_joint      : 1282.815702658802\n",
      "    val_loss       : -1075.3139754585598\n",
      "    val_ess        : 1.9635484788728796\n",
      "    val_log_marginal: 1075.3463930876358\n",
      "    val_log_joint  : 1283.4735054347825\n",
      "Train Epoch: 438 [0/101520 (0%)] Loss: -1075.679077\n",
      "Train Epoch: 438 [11264/101520 (11%)] Loss: -1069.864990\n",
      "Train Epoch: 438 [22528/101520 (22%)] Loss: -1070.591675\n",
      "Train Epoch: 438 [33792/101520 (33%)] Loss: -1078.580566\n",
      "Train Epoch: 438 [45056/101520 (44%)] Loss: -1078.700684\n",
      "Train Epoch: 438 [56320/101520 (55%)] Loss: -1079.062988\n",
      "Train Epoch: 438 [67584/101520 (67%)] Loss: -1072.477295\n",
      "Train Epoch: 438 [78848/101520 (78%)] Loss: -1076.837036\n",
      "Train Epoch: 438 [90112/101520 (89%)] Loss: -1073.306030\n",
      "Train Epoch: 438 [101376/101520 (100%)] Loss: -1068.948364\n",
      "    epoch          : 438\n",
      "    loss           : -1075.032285450691\n",
      "    ess            : 1.9628121870846005\n",
      "    log_marginal   : 1075.064583169755\n",
      "    log_joint      : 1283.154507277599\n",
      "    val_loss       : -1075.0721329398777\n",
      "    val_ess        : 1.9646031752876614\n",
      "    val_log_marginal: 1075.101949940557\n",
      "    val_log_joint  : 1283.0827530570652\n",
      "Train Epoch: 439 [0/101520 (0%)] Loss: -1081.717285\n",
      "Train Epoch: 439 [11264/101520 (11%)] Loss: -1075.681030\n",
      "Train Epoch: 439 [22528/101520 (22%)] Loss: -1076.645264\n",
      "Train Epoch: 439 [33792/101520 (33%)] Loss: -1070.807983\n",
      "Train Epoch: 439 [45056/101520 (44%)] Loss: -1073.233887\n",
      "Train Epoch: 439 [56320/101520 (55%)] Loss: -1074.584473\n",
      "Train Epoch: 439 [67584/101520 (67%)] Loss: -1080.631226\n",
      "Train Epoch: 439 [78848/101520 (78%)] Loss: -1083.306885\n",
      "Train Epoch: 439 [90112/101520 (89%)] Loss: -1070.647583\n",
      "Train Epoch: 439 [101376/101520 (100%)] Loss: -1083.146851\n",
      "    epoch          : 439\n",
      "    loss           : -1074.9131883784155\n",
      "    ess            : 1.9636798623818248\n",
      "    log_marginal   : 1074.9437722057553\n",
      "    log_joint      : 1282.9675771435302\n",
      "    val_loss       : -1072.7270773182745\n",
      "    val_ess        : 1.9614063967829165\n",
      "    val_log_marginal: 1072.7600787618885\n",
      "    val_log_joint  : 1280.731201171875\n",
      "Train Epoch: 440 [0/101520 (0%)] Loss: -1071.951416\n",
      "Train Epoch: 440 [11264/101520 (11%)] Loss: -1073.310791\n",
      "Train Epoch: 440 [22528/101520 (22%)] Loss: -1068.725708\n",
      "Train Epoch: 440 [33792/101520 (33%)] Loss: -1072.468018\n",
      "Train Epoch: 440 [45056/101520 (44%)] Loss: -1071.713013\n",
      "Train Epoch: 440 [56320/101520 (55%)] Loss: -1072.344482\n",
      "Train Epoch: 440 [67584/101520 (67%)] Loss: -1074.285645\n",
      "Train Epoch: 440 [78848/101520 (78%)] Loss: -1075.724854\n",
      "Train Epoch: 440 [90112/101520 (89%)] Loss: -1079.867920\n",
      "Train Epoch: 440 [101376/101520 (100%)] Loss: -1066.976196\n",
      "    epoch          : 440\n",
      "    loss           : -1075.5422197658213\n",
      "    ess            : 1.962339262267453\n",
      "    log_marginal   : 1075.574289906564\n",
      "    log_joint      : 1283.5798419588175\n",
      "    val_loss       : -1074.8581383746603\n",
      "    val_ess        : 1.9636552696642668\n",
      "    val_log_marginal: 1074.887838612432\n",
      "    val_log_joint  : 1282.912401282269\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch440.pth ...\n",
      "Train Epoch: 441 [0/101520 (0%)] Loss: -1079.368652\n",
      "Train Epoch: 441 [11264/101520 (11%)] Loss: -1075.693604\n",
      "Train Epoch: 441 [22528/101520 (22%)] Loss: -1076.724365\n",
      "Train Epoch: 441 [33792/101520 (33%)] Loss: -1076.727173\n",
      "Train Epoch: 441 [45056/101520 (44%)] Loss: -1076.963135\n",
      "Train Epoch: 441 [56320/101520 (55%)] Loss: -1074.959961\n",
      "Train Epoch: 441 [67584/101520 (67%)] Loss: -1078.373047\n",
      "Train Epoch: 441 [78848/101520 (78%)] Loss: -1068.936768\n",
      "Train Epoch: 441 [90112/101520 (89%)] Loss: -1073.172852\n",
      "Train Epoch: 441 [101376/101520 (100%)] Loss: -1076.866821\n",
      "    epoch          : 441\n",
      "    loss           : -1075.3558619513583\n",
      "    ess            : 1.9637896361662515\n",
      "    log_marginal   : 1075.3863825965766\n",
      "    log_joint      : 1283.4203862329225\n",
      "    val_loss       : -1075.582418690557\n",
      "    val_ess        : 1.9644290778947913\n",
      "    val_log_marginal: 1075.612400220788\n",
      "    val_log_joint  : 1283.2659381368885\n",
      "Train Epoch: 442 [0/101520 (0%)] Loss: -1073.254883\n",
      "Train Epoch: 442 [11264/101520 (11%)] Loss: -1075.154297\n",
      "Train Epoch: 442 [22528/101520 (22%)] Loss: -1072.827148\n",
      "Train Epoch: 442 [33792/101520 (33%)] Loss: -1072.668701\n",
      "Train Epoch: 442 [45056/101520 (44%)] Loss: -1081.198853\n",
      "Train Epoch: 442 [56320/101520 (55%)] Loss: -1078.555420\n",
      "Train Epoch: 442 [67584/101520 (67%)] Loss: -1073.840088\n",
      "Train Epoch: 442 [78848/101520 (78%)] Loss: -1076.886719\n",
      "Train Epoch: 442 [90112/101520 (89%)] Loss: -1075.288452\n",
      "Train Epoch: 442 [101376/101520 (100%)] Loss: -1072.652588\n",
      "    epoch          : 442\n",
      "    loss           : -1075.7325654149654\n",
      "    ess            : 1.9633219769252606\n",
      "    log_marginal   : 1075.7635976513427\n",
      "    log_joint      : 1283.7846422051664\n",
      "    val_loss       : -1074.2776621942935\n",
      "    val_ess        : 1.964729288349981\n",
      "    val_log_marginal: 1074.3071607506793\n",
      "    val_log_joint  : 1282.1706914487092\n",
      "Train Epoch: 443 [0/101520 (0%)] Loss: -1081.681152\n",
      "Train Epoch: 443 [11264/101520 (11%)] Loss: -1080.949585\n",
      "Train Epoch: 443 [22528/101520 (22%)] Loss: -1076.926514\n",
      "Train Epoch: 443 [33792/101520 (33%)] Loss: -1073.595093\n",
      "Train Epoch: 443 [45056/101520 (44%)] Loss: -1080.892090\n",
      "Train Epoch: 443 [56320/101520 (55%)] Loss: -1084.557739\n",
      "Train Epoch: 443 [67584/101520 (67%)] Loss: -1085.281250\n",
      "Train Epoch: 443 [78848/101520 (78%)] Loss: -1076.763916\n",
      "Train Epoch: 443 [90112/101520 (89%)] Loss: -1065.622803\n",
      "Train Epoch: 443 [101376/101520 (100%)] Loss: -1085.437988\n",
      "    epoch          : 443\n",
      "    loss           : -1076.3058198708386\n",
      "    ess            : 1.9635213547615549\n",
      "    log_marginal   : 1076.3362804010285\n",
      "    log_joint      : 1284.2645140988143\n",
      "    val_loss       : -1074.5720851732337\n",
      "    val_ess        : 1.9649604558944702\n",
      "    val_log_marginal: 1074.6005434782608\n",
      "    val_log_joint  : 1282.4581192680027\n",
      "Train Epoch: 444 [0/101520 (0%)] Loss: -1080.477539\n",
      "Train Epoch: 444 [11264/101520 (11%)] Loss: -1074.333984\n",
      "Train Epoch: 444 [22528/101520 (22%)] Loss: -1075.742310\n",
      "Train Epoch: 444 [33792/101520 (33%)] Loss: -1082.159668\n",
      "Train Epoch: 444 [45056/101520 (44%)] Loss: -1078.217041\n",
      "Train Epoch: 444 [56320/101520 (55%)] Loss: -1083.891602\n",
      "Train Epoch: 444 [67584/101520 (67%)] Loss: -1086.119507\n",
      "Train Epoch: 444 [78848/101520 (78%)] Loss: -1074.952393\n",
      "Train Epoch: 444 [90112/101520 (89%)] Loss: -1077.188354\n",
      "Train Epoch: 444 [101376/101520 (100%)] Loss: -1057.326416\n",
      "    epoch          : 444\n",
      "    loss           : -1075.843110204342\n",
      "    ess            : 1.9630971517994176\n",
      "    log_marginal   : 1075.8750478466552\n",
      "    log_joint      : 1283.9112426144393\n",
      "    val_loss       : -1076.4993578040082\n",
      "    val_ess        : 1.9628676020580789\n",
      "    val_log_marginal: 1076.5290208899457\n",
      "    val_log_joint  : 1284.6164656929348\n",
      "Train Epoch: 445 [0/101520 (0%)] Loss: -1072.552490\n",
      "Train Epoch: 445 [11264/101520 (11%)] Loss: -1086.865234\n",
      "Train Epoch: 445 [22528/101520 (22%)] Loss: -1072.980957\n",
      "Train Epoch: 445 [33792/101520 (33%)] Loss: -1075.159180\n",
      "Train Epoch: 445 [45056/101520 (44%)] Loss: -1076.115967\n",
      "Train Epoch: 445 [56320/101520 (55%)] Loss: -1075.713623\n",
      "Train Epoch: 445 [67584/101520 (67%)] Loss: -1074.943115\n",
      "Train Epoch: 445 [78848/101520 (78%)] Loss: -1074.739258\n",
      "Train Epoch: 445 [90112/101520 (89%)] Loss: -1073.245728\n",
      "Train Epoch: 445 [101376/101520 (100%)] Loss: -1063.381104\n",
      "    epoch          : 445\n",
      "    loss           : -1076.4785395483275\n",
      "    ess            : 1.9644451548705748\n",
      "    log_marginal   : 1076.5088412030857\n",
      "    log_joint      : 1284.553242285647\n",
      "    val_loss       : -1077.2836330247962\n",
      "    val_ess        : 1.9635127420010774\n",
      "    val_log_marginal: 1077.3133014181385\n",
      "    val_log_joint  : 1285.213235606318\n",
      "Train Epoch: 446 [0/101520 (0%)] Loss: -1080.627930\n",
      "Train Epoch: 446 [11264/101520 (11%)] Loss: -1080.383789\n",
      "Train Epoch: 446 [22528/101520 (22%)] Loss: -1078.147339\n",
      "Train Epoch: 446 [33792/101520 (33%)] Loss: -1082.439697\n",
      "Train Epoch: 446 [45056/101520 (44%)] Loss: -1070.521240\n",
      "Train Epoch: 446 [56320/101520 (55%)] Loss: -1072.553711\n",
      "Train Epoch: 446 [67584/101520 (67%)] Loss: -1082.232910\n",
      "Train Epoch: 446 [78848/101520 (78%)] Loss: -1078.408813\n",
      "Train Epoch: 446 [90112/101520 (89%)] Loss: -1085.064941\n",
      "Train Epoch: 446 [101376/101520 (100%)] Loss: -1069.415771\n",
      "    epoch          : 446\n",
      "    loss           : -1076.4795682514134\n",
      "    ess            : 1.964084589900683\n",
      "    log_marginal   : 1076.5091761296717\n",
      "    log_joint      : 1284.5777495877826\n",
      "    val_loss       : -1076.8633927055027\n",
      "    val_ess        : 1.9659331415010535\n",
      "    val_log_marginal: 1076.8927161175272\n",
      "    val_log_joint  : 1285.266410495924\n",
      "Train Epoch: 447 [0/101520 (0%)] Loss: -1076.184814\n",
      "Train Epoch: 447 [11264/101520 (11%)] Loss: -1081.187988\n",
      "Train Epoch: 447 [22528/101520 (22%)] Loss: -1077.904175\n",
      "Train Epoch: 447 [33792/101520 (33%)] Loss: -1074.601318\n",
      "Train Epoch: 447 [45056/101520 (44%)] Loss: -1080.051025\n",
      "Train Epoch: 447 [56320/101520 (55%)] Loss: -1076.983398\n",
      "Train Epoch: 447 [67584/101520 (67%)] Loss: -1076.638428\n",
      "Train Epoch: 447 [78848/101520 (78%)] Loss: -1070.232056\n",
      "Train Epoch: 447 [90112/101520 (89%)] Loss: -1079.800659\n",
      "Train Epoch: 447 [101376/101520 (100%)] Loss: -1076.388672\n",
      "    epoch          : 447\n",
      "    loss           : -1076.763063977112\n",
      "    ess            : 1.963460371721929\n",
      "    log_marginal   : 1076.794253862084\n",
      "    log_joint      : 1284.8363380623823\n",
      "    val_loss       : -1076.242091966712\n",
      "    val_ess        : 1.9622297908948816\n",
      "    val_log_marginal: 1076.27318805197\n",
      "    val_log_joint  : 1284.252733313519\n",
      "Train Epoch: 448 [0/101520 (0%)] Loss: -1076.150635\n",
      "Train Epoch: 448 [11264/101520 (11%)] Loss: -1078.062134\n",
      "Train Epoch: 448 [22528/101520 (22%)] Loss: -1081.292969\n",
      "Train Epoch: 448 [33792/101520 (33%)] Loss: -1070.251465\n",
      "Train Epoch: 448 [45056/101520 (44%)] Loss: -1075.079590\n",
      "Train Epoch: 448 [56320/101520 (55%)] Loss: -1076.089600\n",
      "Train Epoch: 448 [67584/101520 (67%)] Loss: -1076.603516\n",
      "Train Epoch: 448 [78848/101520 (78%)] Loss: -1072.889404\n",
      "Train Epoch: 448 [90112/101520 (89%)] Loss: -1083.350098\n",
      "Train Epoch: 448 [101376/101520 (100%)] Loss: -1074.197144\n",
      "    epoch          : 448\n",
      "    loss           : -1077.2430757302136\n",
      "    ess            : 1.9622903858596956\n",
      "    log_marginal   : 1077.2752029188914\n",
      "    log_joint      : 1285.258400768491\n",
      "    val_loss       : -1077.65307086447\n",
      "    val_ess        : 1.9643370068591575\n",
      "    val_log_marginal: 1077.6812797214675\n",
      "    val_log_joint  : 1285.539067807405\n",
      "Train Epoch: 449 [0/101520 (0%)] Loss: -1077.940674\n",
      "Train Epoch: 449 [11264/101520 (11%)] Loss: -1077.974365\n",
      "Train Epoch: 449 [22528/101520 (22%)] Loss: -1075.129883\n",
      "Train Epoch: 449 [33792/101520 (33%)] Loss: -1079.156250\n",
      "Train Epoch: 449 [45056/101520 (44%)] Loss: -1072.779053\n",
      "Train Epoch: 449 [56320/101520 (55%)] Loss: -1079.177368\n",
      "Train Epoch: 449 [67584/101520 (67%)] Loss: -1078.207764\n",
      "Train Epoch: 449 [78848/101520 (78%)] Loss: -1078.405518\n",
      "Train Epoch: 449 [90112/101520 (89%)] Loss: -1080.937744\n",
      "Train Epoch: 449 [101376/101520 (100%)] Loss: -1079.004761\n",
      "    epoch          : 449\n",
      "    loss           : -1076.999028958268\n",
      "    ess            : 1.9640301136515248\n",
      "    log_marginal   : 1077.0291833925487\n",
      "    log_joint      : 1285.0689347616992\n",
      "    val_loss       : -1075.7094461192255\n",
      "    val_ess        : 1.9660291464432427\n",
      "    val_log_marginal: 1075.737941576087\n",
      "    val_log_joint  : 1283.4574983016305\n",
      "Train Epoch: 450 [0/101520 (0%)] Loss: -1079.663208\n",
      "Train Epoch: 450 [11264/101520 (11%)] Loss: -1083.292969\n",
      "Train Epoch: 450 [22528/101520 (22%)] Loss: -1074.706055\n",
      "Train Epoch: 450 [33792/101520 (33%)] Loss: -1075.059692\n",
      "Train Epoch: 450 [45056/101520 (44%)] Loss: -1078.575928\n",
      "Train Epoch: 450 [56320/101520 (55%)] Loss: -1080.055908\n",
      "Train Epoch: 450 [67584/101520 (67%)] Loss: -1079.743652\n",
      "Train Epoch: 450 [78848/101520 (78%)] Loss: -1077.495605\n",
      "Train Epoch: 450 [90112/101520 (89%)] Loss: -1076.122803\n",
      "Train Epoch: 450 [101376/101520 (100%)] Loss: -1075.632812\n",
      "    epoch          : 450\n",
      "    loss           : -1077.5271026764683\n",
      "    ess            : 1.9636879752029726\n",
      "    log_marginal   : 1077.557444203439\n",
      "    log_joint      : 1285.567987029876\n",
      "    val_loss       : -1075.1272291100543\n",
      "    val_ess        : 1.9615079423655635\n",
      "    val_log_marginal: 1075.1608621348505\n",
      "    val_log_joint  : 1283.2204961362092\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [0/101520 (0%)] Loss: -1087.405273\n",
      "Train Epoch: 451 [11264/101520 (11%)] Loss: -1072.391602\n",
      "Train Epoch: 451 [22528/101520 (22%)] Loss: -1084.497070\n",
      "Train Epoch: 451 [33792/101520 (33%)] Loss: -1076.496094\n",
      "Train Epoch: 451 [45056/101520 (44%)] Loss: -1080.218506\n",
      "Train Epoch: 451 [56320/101520 (55%)] Loss: -1085.154053\n",
      "Train Epoch: 451 [67584/101520 (67%)] Loss: -1072.397217\n",
      "Train Epoch: 451 [78848/101520 (78%)] Loss: -1086.586792\n",
      "Train Epoch: 451 [90112/101520 (89%)] Loss: -1070.825073\n",
      "Train Epoch: 451 [101376/101520 (100%)] Loss: -1070.465332\n",
      "    epoch          : 451\n",
      "    loss           : -1077.619593327968\n",
      "    ess            : 1.9646552896978866\n",
      "    log_marginal   : 1077.6493171423524\n",
      "    log_joint      : 1285.7287744876728\n",
      "    val_loss       : -1078.5325131623642\n",
      "    val_ess        : 1.9640388229618901\n",
      "    val_log_marginal: 1078.560594641644\n",
      "    val_log_joint  : 1286.7230649201767\n",
      "Train Epoch: 452 [0/101520 (0%)] Loss: -1077.570801\n",
      "Train Epoch: 452 [11264/101520 (11%)] Loss: -1091.682129\n",
      "Train Epoch: 452 [22528/101520 (22%)] Loss: -1078.458008\n",
      "Train Epoch: 452 [33792/101520 (33%)] Loss: -1081.752686\n",
      "Train Epoch: 452 [45056/101520 (44%)] Loss: -1074.557495\n",
      "Train Epoch: 452 [56320/101520 (55%)] Loss: -1080.800415\n",
      "Train Epoch: 452 [67584/101520 (67%)] Loss: -1076.125244\n",
      "Train Epoch: 452 [78848/101520 (78%)] Loss: -1072.221558\n",
      "Train Epoch: 452 [90112/101520 (89%)] Loss: -1076.853027\n",
      "Train Epoch: 452 [101376/101520 (100%)] Loss: -1068.810791\n",
      "    epoch          : 452\n",
      "    loss           : -1077.5663504289023\n",
      "    ess            : 1.964200209133589\n",
      "    log_marginal   : 1077.5968220006282\n",
      "    log_joint      : 1285.6847543380968\n",
      "    val_loss       : -1076.7000785495925\n",
      "    val_ess        : 1.9637429403222126\n",
      "    val_log_marginal: 1076.7337009595788\n",
      "    val_log_joint  : 1284.885938561481\n",
      "Train Epoch: 453 [0/101520 (0%)] Loss: -1078.453369\n",
      "Train Epoch: 453 [11264/101520 (11%)] Loss: -1079.331055\n",
      "Train Epoch: 453 [22528/101520 (22%)] Loss: -1074.314209\n",
      "Train Epoch: 453 [33792/101520 (33%)] Loss: -1079.158813\n",
      "Train Epoch: 453 [45056/101520 (44%)] Loss: -1085.420654\n",
      "Train Epoch: 453 [56320/101520 (55%)] Loss: -1075.385254\n",
      "Train Epoch: 453 [67584/101520 (67%)] Loss: -1071.610352\n",
      "Train Epoch: 453 [78848/101520 (78%)] Loss: -1074.369629\n",
      "Train Epoch: 453 [90112/101520 (89%)] Loss: -1072.965820\n",
      "Train Epoch: 453 [101376/101520 (100%)] Loss: -1084.356079\n",
      "    epoch          : 453\n",
      "    loss           : -1078.288942883362\n",
      "    ess            : 1.9635269725742053\n",
      "    log_marginal   : 1078.319699694763\n",
      "    log_joint      : 1286.272967007891\n",
      "    val_loss       : -1077.089058254076\n",
      "    val_ess        : 1.96611371765966\n",
      "    val_log_marginal: 1077.1161472486413\n",
      "    val_log_joint  : 1285.2740000849185\n",
      "Train Epoch: 454 [0/101520 (0%)] Loss: -1085.570435\n",
      "Train Epoch: 454 [11264/101520 (11%)] Loss: -1072.869263\n",
      "Train Epoch: 454 [22528/101520 (22%)] Loss: -1080.502441\n",
      "Train Epoch: 454 [33792/101520 (33%)] Loss: -1070.554932\n",
      "Train Epoch: 454 [45056/101520 (44%)] Loss: -1077.235229\n",
      "Train Epoch: 454 [56320/101520 (55%)] Loss: -1074.942139\n",
      "Train Epoch: 454 [67584/101520 (67%)] Loss: -1078.803711\n",
      "Train Epoch: 454 [78848/101520 (78%)] Loss: -1078.273682\n",
      "Train Epoch: 454 [90112/101520 (89%)] Loss: -1079.260376\n",
      "Train Epoch: 454 [101376/101520 (100%)] Loss: -1064.409912\n",
      "    epoch          : 454\n",
      "    loss           : -1078.2265851964903\n",
      "    ess            : 1.9638873129034762\n",
      "    log_marginal   : 1078.2565727808967\n",
      "    log_joint      : 1286.2783338077104\n",
      "    val_loss       : -1076.345655358356\n",
      "    val_ess        : 1.9635857861974966\n",
      "    val_log_marginal: 1076.3768788213315\n",
      "    val_log_joint  : 1284.7690164317255\n",
      "Train Epoch: 455 [0/101520 (0%)] Loss: -1075.223755\n",
      "Train Epoch: 455 [11264/101520 (11%)] Loss: -1078.148560\n",
      "Train Epoch: 455 [22528/101520 (22%)] Loss: -1076.309570\n",
      "Train Epoch: 455 [33792/101520 (33%)] Loss: -1080.796997\n",
      "Train Epoch: 455 [45056/101520 (44%)] Loss: -1076.841553\n",
      "Train Epoch: 455 [56320/101520 (55%)] Loss: -1074.124756\n",
      "Train Epoch: 455 [67584/101520 (67%)] Loss: -1076.482910\n",
      "Train Epoch: 455 [78848/101520 (78%)] Loss: -1087.387207\n",
      "Train Epoch: 455 [90112/101520 (89%)] Loss: -1075.133545\n",
      "Train Epoch: 455 [101376/101520 (100%)] Loss: -1071.740845\n",
      "    epoch          : 455\n",
      "    loss           : -1078.2400607775205\n",
      "    ess            : 1.9629342550009339\n",
      "    log_marginal   : 1078.2715285411432\n",
      "    log_joint      : 1286.307504931886\n",
      "    val_loss       : -1077.6048424762228\n",
      "    val_ess        : 1.9633712094763052\n",
      "    val_log_marginal: 1077.635890794837\n",
      "    val_log_joint  : 1285.8117569633152\n",
      "Train Epoch: 456 [0/101520 (0%)] Loss: -1084.662354\n",
      "Train Epoch: 456 [11264/101520 (11%)] Loss: -1081.322266\n",
      "Train Epoch: 456 [22528/101520 (22%)] Loss: -1071.145874\n",
      "Train Epoch: 456 [33792/101520 (33%)] Loss: -1075.204834\n",
      "Train Epoch: 456 [45056/101520 (44%)] Loss: -1072.885986\n",
      "Train Epoch: 456 [56320/101520 (55%)] Loss: -1069.136475\n",
      "Train Epoch: 456 [67584/101520 (67%)] Loss: -1079.267944\n",
      "Train Epoch: 456 [78848/101520 (78%)] Loss: -1072.410767\n",
      "Train Epoch: 456 [90112/101520 (89%)] Loss: -1077.081299\n",
      "Train Epoch: 456 [101376/101520 (100%)] Loss: -1080.605713\n",
      "    epoch          : 456\n",
      "    loss           : -1078.8292401951162\n",
      "    ess            : 1.9640353774305563\n",
      "    log_marginal   : 1078.859565773202\n",
      "    log_joint      : 1286.9122719309437\n",
      "    val_loss       : -1080.240579356318\n",
      "    val_ess        : 1.962703481964443\n",
      "    val_log_marginal: 1080.270805027174\n",
      "    val_log_joint  : 1288.3406186311142\n",
      "Train Epoch: 457 [0/101520 (0%)] Loss: -1079.928345\n",
      "Train Epoch: 457 [11264/101520 (11%)] Loss: -1076.576904\n",
      "Train Epoch: 457 [22528/101520 (22%)] Loss: -1081.776611\n",
      "Train Epoch: 457 [33792/101520 (33%)] Loss: -1079.258057\n",
      "Train Epoch: 457 [45056/101520 (44%)] Loss: -1075.833496\n",
      "Train Epoch: 457 [56320/101520 (55%)] Loss: -1075.226562\n",
      "Train Epoch: 457 [67584/101520 (67%)] Loss: -1082.624878\n",
      "Train Epoch: 457 [78848/101520 (78%)] Loss: -1079.114746\n",
      "Train Epoch: 457 [90112/101520 (89%)] Loss: -1073.355713\n",
      "Train Epoch: 457 [101376/101520 (100%)] Loss: -1067.004395\n",
      "    epoch          : 457\n",
      "    loss           : -1078.7009522711212\n",
      "    ess            : 1.9640772552346464\n",
      "    log_marginal   : 1078.7313766096106\n",
      "    log_joint      : 1286.79981388878\n",
      "    val_loss       : -1077.0184432319973\n",
      "    val_ess        : 1.9607330560684204\n",
      "    val_log_marginal: 1077.0538754670517\n",
      "    val_log_joint  : 1284.9113981827445\n",
      "Train Epoch: 458 [0/101520 (0%)] Loss: -1086.843750\n",
      "Train Epoch: 458 [11264/101520 (11%)] Loss: -1084.247314\n",
      "Train Epoch: 458 [22528/101520 (22%)] Loss: -1075.951660\n",
      "Train Epoch: 458 [33792/101520 (33%)] Loss: -1076.529785\n",
      "Train Epoch: 458 [45056/101520 (44%)] Loss: -1079.451050\n",
      "Train Epoch: 458 [56320/101520 (55%)] Loss: -1081.843018\n",
      "Train Epoch: 458 [67584/101520 (67%)] Loss: -1083.148438\n",
      "Train Epoch: 458 [78848/101520 (78%)] Loss: -1080.747437\n",
      "Train Epoch: 458 [90112/101520 (89%)] Loss: -1083.599854\n",
      "Train Epoch: 458 [101376/101520 (100%)] Loss: -1074.950439\n",
      "    epoch          : 458\n",
      "    loss           : -1078.9444629151617\n",
      "    ess            : 1.963509222969937\n",
      "    log_marginal   : 1078.9753344358512\n",
      "    log_joint      : 1286.983300290515\n",
      "    val_loss       : -1077.3395253057065\n",
      "    val_ess        : 1.9658020164655603\n",
      "    val_log_marginal: 1077.3679889181385\n",
      "    val_log_joint  : 1285.3998811141305\n",
      "Train Epoch: 459 [0/101520 (0%)] Loss: -1083.389038\n",
      "Train Epoch: 459 [11264/101520 (11%)] Loss: -1075.469971\n",
      "Train Epoch: 459 [22528/101520 (22%)] Loss: -1088.317261\n",
      "Train Epoch: 459 [33792/101520 (33%)] Loss: -1083.260376\n",
      "Train Epoch: 459 [45056/101520 (44%)] Loss: -1081.264160\n",
      "Train Epoch: 459 [56320/101520 (55%)] Loss: -1077.350830\n",
      "Train Epoch: 459 [67584/101520 (67%)] Loss: -1086.348389\n",
      "Train Epoch: 459 [78848/101520 (78%)] Loss: -1081.782959\n",
      "Train Epoch: 459 [90112/101520 (89%)] Loss: -1066.394165\n",
      "Train Epoch: 459 [101376/101520 (100%)] Loss: -1075.997681\n",
      "    epoch          : 459\n",
      "    loss           : -1079.539930487398\n",
      "    ess            : 1.9645816626860269\n",
      "    log_marginal   : 1079.569132895925\n",
      "    log_joint      : 1287.6184965354114\n",
      "    val_loss       : -1077.8320577870245\n",
      "    val_ess        : 1.9624985715617305\n",
      "    val_log_marginal: 1077.8651972231658\n",
      "    val_log_joint  : 1286.118599269701\n",
      "Train Epoch: 460 [0/101520 (0%)] Loss: -1083.235352\n",
      "Train Epoch: 460 [11264/101520 (11%)] Loss: -1084.024414\n",
      "Train Epoch: 460 [22528/101520 (22%)] Loss: -1076.297363\n",
      "Train Epoch: 460 [33792/101520 (33%)] Loss: -1075.439209\n",
      "Train Epoch: 460 [45056/101520 (44%)] Loss: -1082.553223\n",
      "Train Epoch: 460 [56320/101520 (55%)] Loss: -1073.381226\n",
      "Train Epoch: 460 [67584/101520 (67%)] Loss: -1077.957520\n",
      "Train Epoch: 460 [78848/101520 (78%)] Loss: -1074.618408\n",
      "Train Epoch: 460 [90112/101520 (89%)] Loss: -1078.937378\n",
      "Train Epoch: 460 [101376/101520 (100%)] Loss: -1074.606323\n",
      "    epoch          : 460\n",
      "    loss           : -1079.079181920344\n",
      "    ess            : 1.9643018323572436\n",
      "    log_marginal   : 1079.108835191583\n",
      "    log_joint      : 1287.159794332993\n",
      "    val_loss       : -1078.4326861837635\n",
      "    val_ess        : 1.9610419221546338\n",
      "    val_log_marginal: 1078.46533203125\n",
      "    val_log_joint  : 1286.570169200068\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch460.pth ...\n",
      "Train Epoch: 461 [0/101520 (0%)] Loss: -1081.467773\n",
      "Train Epoch: 461 [11264/101520 (11%)] Loss: -1076.805420\n",
      "Train Epoch: 461 [22528/101520 (22%)] Loss: -1075.274170\n",
      "Train Epoch: 461 [33792/101520 (33%)] Loss: -1077.022583\n",
      "Train Epoch: 461 [45056/101520 (44%)] Loss: -1077.514893\n",
      "Train Epoch: 461 [56320/101520 (55%)] Loss: -1082.277100\n",
      "Train Epoch: 461 [67584/101520 (67%)] Loss: -1080.085205\n",
      "Train Epoch: 461 [78848/101520 (78%)] Loss: -1080.636719\n",
      "Train Epoch: 461 [90112/101520 (89%)] Loss: -1088.549438\n",
      "Train Epoch: 461 [101376/101520 (100%)] Loss: -1089.171265\n",
      "    epoch          : 461\n",
      "    loss           : -1079.7828632910646\n",
      "    ess            : 1.9642047211153424\n",
      "    log_marginal   : 1079.8131207796796\n",
      "    log_joint      : 1287.8238175741992\n",
      "    val_loss       : -1080.4570471722147\n",
      "    val_ess        : 1.9656511907992156\n",
      "    val_log_marginal: 1080.485834536345\n",
      "    val_log_joint  : 1288.3467964504075\n",
      "Train Epoch: 462 [0/101520 (0%)] Loss: -1074.562622\n",
      "Train Epoch: 462 [11264/101520 (11%)] Loss: -1079.959473\n",
      "Train Epoch: 462 [22528/101520 (22%)] Loss: -1083.037109\n",
      "Train Epoch: 462 [33792/101520 (33%)] Loss: -1077.567871\n",
      "Train Epoch: 462 [45056/101520 (44%)] Loss: -1079.204956\n",
      "Train Epoch: 462 [56320/101520 (55%)] Loss: -1084.091431\n",
      "Train Epoch: 462 [67584/101520 (67%)] Loss: -1079.118652\n",
      "Train Epoch: 462 [78848/101520 (78%)] Loss: -1080.993652\n",
      "Train Epoch: 462 [90112/101520 (89%)] Loss: -1081.715088\n",
      "Train Epoch: 462 [101376/101520 (100%)] Loss: -1086.808228\n",
      "    epoch          : 462\n",
      "    loss           : -1080.045167855881\n",
      "    ess            : 1.9640778584695942\n",
      "    log_marginal   : 1080.075152373194\n",
      "    log_joint      : 1288.1053239831972\n",
      "    val_loss       : -1078.6681226647418\n",
      "    val_ess        : 1.9605002558749656\n",
      "    val_log_marginal: 1078.7012621008832\n",
      "    val_log_joint  : 1286.7117389181385\n",
      "Train Epoch: 463 [0/101520 (0%)] Loss: -1080.532471\n",
      "Train Epoch: 463 [11264/101520 (11%)] Loss: -1073.090088\n",
      "Train Epoch: 463 [22528/101520 (22%)] Loss: -1080.929077\n",
      "Train Epoch: 463 [33792/101520 (33%)] Loss: -1085.561401\n",
      "Train Epoch: 463 [45056/101520 (44%)] Loss: -1077.800293\n",
      "Train Epoch: 463 [56320/101520 (55%)] Loss: -1078.472168\n",
      "Train Epoch: 463 [67584/101520 (67%)] Loss: -1080.646240\n",
      "Train Epoch: 463 [78848/101520 (78%)] Loss: -1076.975952\n",
      "Train Epoch: 463 [90112/101520 (89%)] Loss: -1086.348267\n",
      "Train Epoch: 463 [101376/101520 (100%)] Loss: -1081.085571\n",
      "    epoch          : 463\n",
      "    loss           : -1079.9556903168184\n",
      "    ess            : 1.9639794844478817\n",
      "    log_marginal   : 1079.9867538375472\n",
      "    log_joint      : 1287.973946269433\n",
      "    val_loss       : -1080.057622494905\n",
      "    val_ess        : 1.9660311574521272\n",
      "    val_log_marginal: 1080.0840958305027\n",
      "    val_log_joint  : 1288.3987081776495\n",
      "Train Epoch: 464 [0/101520 (0%)] Loss: -1078.771362\n",
      "Train Epoch: 464 [11264/101520 (11%)] Loss: -1083.520752\n",
      "Train Epoch: 464 [22528/101520 (22%)] Loss: -1090.546631\n",
      "Train Epoch: 464 [33792/101520 (33%)] Loss: -1079.987305\n",
      "Train Epoch: 464 [45056/101520 (44%)] Loss: -1074.918335\n",
      "Train Epoch: 464 [56320/101520 (55%)] Loss: -1084.231567\n",
      "Train Epoch: 464 [67584/101520 (67%)] Loss: -1078.945557\n",
      "Train Epoch: 464 [78848/101520 (78%)] Loss: -1078.865601\n",
      "Train Epoch: 464 [90112/101520 (89%)] Loss: -1086.581787\n",
      "Train Epoch: 464 [101376/101520 (100%)] Loss: -1082.858032\n",
      "    epoch          : 464\n",
      "    loss           : -1080.5569405867227\n",
      "    ess            : 1.9645626413163229\n",
      "    log_marginal   : 1080.586368733315\n",
      "    log_joint      : 1288.5957037384187\n",
      "    val_loss       : -1079.5330810546875\n",
      "    val_ess        : 1.9651942356773044\n",
      "    val_log_marginal: 1079.5627653702445\n",
      "    val_log_joint  : 1287.856832753057\n",
      "Train Epoch: 465 [0/101520 (0%)] Loss: -1079.385254\n",
      "Train Epoch: 465 [11264/101520 (11%)] Loss: -1087.252197\n",
      "Train Epoch: 465 [22528/101520 (22%)] Loss: -1084.355957\n",
      "Train Epoch: 465 [33792/101520 (33%)] Loss: -1082.918823\n",
      "Train Epoch: 465 [45056/101520 (44%)] Loss: -1077.463989\n",
      "Train Epoch: 465 [56320/101520 (55%)] Loss: -1088.353516\n",
      "Train Epoch: 465 [67584/101520 (67%)] Loss: -1084.684204\n",
      "Train Epoch: 465 [78848/101520 (78%)] Loss: -1074.248657\n",
      "Train Epoch: 465 [90112/101520 (89%)] Loss: -1079.374512\n",
      "Train Epoch: 465 [101376/101520 (100%)] Loss: -1079.285156\n",
      "    epoch          : 465\n",
      "    loss           : -1080.4895031799624\n",
      "    ess            : 1.9633918467478537\n",
      "    log_marginal   : 1080.5200121702262\n",
      "    log_joint      : 1288.5963245180983\n",
      "    val_loss       : -1078.7713569972825\n",
      "    val_ess        : 1.9642952628757642\n",
      "    val_log_marginal: 1078.8007387907608\n",
      "    val_log_joint  : 1286.7460725203805\n",
      "Train Epoch: 466 [0/101520 (0%)] Loss: -1078.634399\n",
      "Train Epoch: 466 [11264/101520 (11%)] Loss: -1081.230347\n",
      "Train Epoch: 466 [22528/101520 (22%)] Loss: -1085.721802\n",
      "Train Epoch: 466 [33792/101520 (33%)] Loss: -1079.330322\n",
      "Train Epoch: 466 [45056/101520 (44%)] Loss: -1080.241211\n",
      "Train Epoch: 466 [56320/101520 (55%)] Loss: -1075.531982\n",
      "Train Epoch: 466 [67584/101520 (67%)] Loss: -1084.221436\n",
      "Train Epoch: 466 [78848/101520 (78%)] Loss: -1077.792480\n",
      "Train Epoch: 466 [90112/101520 (89%)] Loss: -1086.218994\n",
      "Train Epoch: 466 [101376/101520 (100%)] Loss: -1081.860962\n",
      "    epoch          : 466\n",
      "    loss           : -1080.6733527255417\n",
      "    ess            : 1.9640209105745632\n",
      "    log_marginal   : 1080.7034629936793\n",
      "    log_joint      : 1288.738047537492\n",
      "    val_loss       : -1080.2962009595788\n",
      "    val_ess        : 1.958975076675415\n",
      "    val_log_marginal: 1080.3353802224865\n",
      "    val_log_joint  : 1288.1598537279212\n",
      "Train Epoch: 467 [0/101520 (0%)] Loss: -1085.438843\n",
      "Train Epoch: 467 [11264/101520 (11%)] Loss: -1076.418701\n",
      "Train Epoch: 467 [22528/101520 (22%)] Loss: -1085.181641\n",
      "Train Epoch: 467 [33792/101520 (33%)] Loss: -1082.687744\n",
      "Train Epoch: 467 [45056/101520 (44%)] Loss: -1069.024170\n",
      "Train Epoch: 467 [56320/101520 (55%)] Loss: -1088.970581\n",
      "Train Epoch: 467 [67584/101520 (67%)] Loss: -1077.226807\n",
      "Train Epoch: 467 [78848/101520 (78%)] Loss: -1080.075073\n",
      "Train Epoch: 467 [90112/101520 (89%)] Loss: -1079.723511\n",
      "Train Epoch: 467 [101376/101520 (100%)] Loss: -1084.816162\n",
      "    epoch          : 467\n",
      "    loss           : -1081.2979313069252\n",
      "    ess            : 1.9642328239565519\n",
      "    log_marginal   : 1081.3276735238694\n",
      "    log_joint      : 1289.315522313717\n",
      "    val_loss       : -1080.2491455078125\n",
      "    val_ess        : 1.9603175339491472\n",
      "    val_log_marginal: 1080.2901292883832\n",
      "    val_log_joint  : 1288.315822435462\n",
      "Train Epoch: 468 [0/101520 (0%)] Loss: -1078.893433\n",
      "Train Epoch: 468 [11264/101520 (11%)] Loss: -1082.601074\n",
      "Train Epoch: 468 [22528/101520 (22%)] Loss: -1076.300171\n",
      "Train Epoch: 468 [33792/101520 (33%)] Loss: -1082.500122\n",
      "Train Epoch: 468 [45056/101520 (44%)] Loss: -1071.098999\n",
      "Train Epoch: 468 [56320/101520 (55%)] Loss: -1078.309082\n",
      "Train Epoch: 468 [67584/101520 (67%)] Loss: -1079.409058\n",
      "Train Epoch: 468 [78848/101520 (78%)] Loss: -1080.651489\n",
      "Train Epoch: 468 [90112/101520 (89%)] Loss: -1078.107910\n",
      "Train Epoch: 468 [101376/101520 (100%)] Loss: -1090.735962\n",
      "    epoch          : 468\n",
      "    loss           : -1081.0134154660018\n",
      "    ess            : 1.9642573654951163\n",
      "    log_marginal   : 1081.0433895551978\n",
      "    log_joint      : 1289.0459419902245\n",
      "    val_loss       : -1080.4242102581522\n",
      "    val_ess        : 1.9655943538831628\n",
      "    val_log_marginal: 1080.4527216372283\n",
      "    val_log_joint  : 1288.613472316576\n",
      "Train Epoch: 469 [0/101520 (0%)] Loss: -1084.218140\n",
      "Train Epoch: 469 [11264/101520 (11%)] Loss: -1077.044189\n",
      "Train Epoch: 469 [22528/101520 (22%)] Loss: -1076.614258\n",
      "Train Epoch: 469 [33792/101520 (33%)] Loss: -1073.371826\n",
      "Train Epoch: 469 [45056/101520 (44%)] Loss: -1078.557373\n",
      "Train Epoch: 469 [56320/101520 (55%)] Loss: -1082.265137\n",
      "Train Epoch: 469 [67584/101520 (67%)] Loss: -1086.418579\n",
      "Train Epoch: 469 [78848/101520 (78%)] Loss: -1083.777222\n",
      "Train Epoch: 469 [90112/101520 (89%)] Loss: -1090.253418\n",
      "Train Epoch: 469 [101376/101520 (100%)] Loss: -1108.608765\n",
      "    epoch          : 469\n",
      "    loss           : -1081.5852431100816\n",
      "    ess            : 1.9633718950664578\n",
      "    log_marginal   : 1081.6159643432004\n",
      "    log_joint      : 1289.679926119857\n",
      "    val_loss       : -1081.3813901154892\n",
      "    val_ess        : 1.9605863042499707\n",
      "    val_log_marginal: 1081.4165304432745\n",
      "    val_log_joint  : 1289.1212476647418\n",
      "Train Epoch: 470 [0/101520 (0%)] Loss: -1086.047852\n",
      "Train Epoch: 470 [11264/101520 (11%)] Loss: -1079.064941\n",
      "Train Epoch: 470 [22528/101520 (22%)] Loss: -1081.405518\n",
      "Train Epoch: 470 [33792/101520 (33%)] Loss: -1085.591187\n",
      "Train Epoch: 470 [45056/101520 (44%)] Loss: -1084.721680\n",
      "Train Epoch: 470 [56320/101520 (55%)] Loss: -1081.433105\n",
      "Train Epoch: 470 [67584/101520 (67%)] Loss: -1084.194580\n",
      "Train Epoch: 470 [78848/101520 (78%)] Loss: -1089.339233\n",
      "Train Epoch: 470 [90112/101520 (89%)] Loss: -1086.570068\n",
      "Train Epoch: 470 [101376/101520 (100%)] Loss: -1096.510010\n",
      "    epoch          : 470\n",
      "    loss           : -1081.742386861063\n",
      "    ess            : 1.9637147799209134\n",
      "    log_marginal   : 1081.7725235062028\n",
      "    log_joint      : 1289.8247340216708\n",
      "    val_loss       : -1079.1636591372283\n",
      "    val_ess        : 1.9670161216155342\n",
      "    val_log_marginal: 1079.1913637907608\n",
      "    val_log_joint  : 1287.3917077105978\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch470.pth ...\n",
      "Train Epoch: 471 [0/101520 (0%)] Loss: -1079.789062\n",
      "Train Epoch: 471 [11264/101520 (11%)] Loss: -1076.418945\n",
      "Train Epoch: 471 [22528/101520 (22%)] Loss: -1086.940674\n",
      "Train Epoch: 471 [33792/101520 (33%)] Loss: -1075.716675\n",
      "Train Epoch: 471 [45056/101520 (44%)] Loss: -1082.572510\n",
      "Train Epoch: 471 [56320/101520 (55%)] Loss: -1084.729980\n",
      "Train Epoch: 471 [67584/101520 (67%)] Loss: -1081.975586\n",
      "Train Epoch: 471 [78848/101520 (78%)] Loss: -1073.759155\n",
      "Train Epoch: 471 [90112/101520 (89%)] Loss: -1088.490601\n",
      "Train Epoch: 471 [101376/101520 (100%)] Loss: -1091.480347\n",
      "    epoch          : 471\n",
      "    loss           : -1081.7561795795384\n",
      "    ess            : 1.9646593799543142\n",
      "    log_marginal   : 1081.7861217709642\n",
      "    log_joint      : 1289.7917885325062\n",
      "    val_loss       : -1080.463861880095\n",
      "    val_ess        : 1.9636131006738413\n",
      "    val_log_marginal: 1080.4962052055027\n",
      "    val_log_joint  : 1288.304639733356\n",
      "Train Epoch: 472 [0/101520 (0%)] Loss: -1087.079590\n",
      "Train Epoch: 472 [11264/101520 (11%)] Loss: -1078.911377\n",
      "Train Epoch: 472 [22528/101520 (22%)] Loss: -1079.541992\n",
      "Train Epoch: 472 [33792/101520 (33%)] Loss: -1088.223877\n",
      "Train Epoch: 472 [45056/101520 (44%)] Loss: -1077.899414\n",
      "Train Epoch: 472 [56320/101520 (55%)] Loss: -1085.362549\n",
      "Train Epoch: 472 [67584/101520 (67%)] Loss: -1079.624023\n",
      "Train Epoch: 472 [78848/101520 (78%)] Loss: -1080.942139\n",
      "Train Epoch: 472 [90112/101520 (89%)] Loss: -1080.729004\n",
      "Train Epoch: 472 [101376/101520 (100%)] Loss: -1081.921753\n",
      "    epoch          : 472\n",
      "    loss           : -1082.1880483962782\n",
      "    ess            : 1.9634926355064815\n",
      "    log_marginal   : 1082.2185457315877\n",
      "    log_joint      : 1290.2895084553627\n",
      "    val_loss       : -1081.912841796875\n",
      "    val_ess        : 1.9645451929258264\n",
      "    val_log_marginal: 1081.9434655230978\n",
      "    val_log_joint  : 1290.1441544242527\n",
      "Train Epoch: 473 [0/101520 (0%)] Loss: -1089.569824\n",
      "Train Epoch: 473 [11264/101520 (11%)] Loss: -1079.322632\n",
      "Train Epoch: 473 [22528/101520 (22%)] Loss: -1085.438965\n",
      "Train Epoch: 473 [33792/101520 (33%)] Loss: -1076.685791\n",
      "Train Epoch: 473 [45056/101520 (44%)] Loss: -1080.223022\n",
      "Train Epoch: 473 [56320/101520 (55%)] Loss: -1072.921265\n",
      "Train Epoch: 473 [67584/101520 (67%)] Loss: -1082.727783\n",
      "Train Epoch: 473 [78848/101520 (78%)] Loss: -1086.888672\n",
      "Train Epoch: 473 [90112/101520 (89%)] Loss: -1075.532471\n",
      "Train Epoch: 473 [101376/101520 (100%)] Loss: -1082.654053\n",
      "    epoch          : 473\n",
      "    loss           : -1082.2671959651775\n",
      "    ess            : 1.9635614330445104\n",
      "    log_marginal   : 1082.2974019266253\n",
      "    log_joint      : 1290.3249977916928\n",
      "    val_loss       : -1081.0382610818615\n",
      "    val_ess        : 1.961806753407354\n",
      "    val_log_marginal: 1081.0717508067255\n",
      "    val_log_joint  : 1288.8880615234375\n",
      "Train Epoch: 474 [0/101520 (0%)] Loss: -1086.335571\n",
      "Train Epoch: 474 [11264/101520 (11%)] Loss: -1082.679443\n",
      "Train Epoch: 474 [22528/101520 (22%)] Loss: -1080.062988\n",
      "Train Epoch: 474 [33792/101520 (33%)] Loss: -1081.426392\n",
      "Train Epoch: 474 [45056/101520 (44%)] Loss: -1085.338379\n",
      "Train Epoch: 474 [56320/101520 (55%)] Loss: -1084.992676\n",
      "Train Epoch: 474 [67584/101520 (67%)] Loss: -1085.221558\n",
      "Train Epoch: 474 [78848/101520 (78%)] Loss: -1080.219727\n",
      "Train Epoch: 474 [90112/101520 (89%)] Loss: -1084.853760\n",
      "Train Epoch: 474 [101376/101520 (100%)] Loss: -1078.054932\n",
      "    epoch          : 474\n",
      "    loss           : -1082.3604558436714\n",
      "    ess            : 1.9646180131327566\n",
      "    log_marginal   : 1082.3906973834014\n",
      "    log_joint      : 1290.490050962822\n",
      "    val_loss       : -1081.2418372112772\n",
      "    val_ess        : 1.9638706134713215\n",
      "    val_log_marginal: 1081.2711128566575\n",
      "    val_log_joint  : 1289.6451522163723\n",
      "Train Epoch: 475 [0/101520 (0%)] Loss: -1085.302490\n",
      "Train Epoch: 475 [11264/101520 (11%)] Loss: -1089.003906\n",
      "Train Epoch: 475 [22528/101520 (22%)] Loss: -1080.693359\n",
      "Train Epoch: 475 [33792/101520 (33%)] Loss: -1076.068848\n",
      "Train Epoch: 475 [45056/101520 (44%)] Loss: -1080.423584\n",
      "Train Epoch: 475 [56320/101520 (55%)] Loss: -1085.943115\n",
      "Train Epoch: 475 [67584/101520 (67%)] Loss: -1081.310303\n",
      "Train Epoch: 475 [78848/101520 (78%)] Loss: -1084.194092\n",
      "Train Epoch: 475 [90112/101520 (89%)] Loss: -1078.648682\n",
      "Train Epoch: 475 [101376/101520 (100%)] Loss: -1084.979370\n",
      "    epoch          : 475\n",
      "    loss           : -1082.9817575425957\n",
      "    ess            : 1.964114746855731\n",
      "    log_marginal   : 1083.0118058554492\n",
      "    log_joint      : 1290.9741947039886\n",
      "    val_loss       : -1081.9383014181385\n",
      "    val_ess        : 1.9638742571291716\n",
      "    val_log_marginal: 1081.9678477411685\n",
      "    val_log_joint  : 1289.8474917204483\n",
      "Train Epoch: 476 [0/101520 (0%)] Loss: -1083.735107\n",
      "Train Epoch: 476 [11264/101520 (11%)] Loss: -1082.394043\n",
      "Train Epoch: 476 [22528/101520 (22%)] Loss: -1080.422852\n",
      "Train Epoch: 476 [33792/101520 (33%)] Loss: -1085.915527\n",
      "Train Epoch: 476 [45056/101520 (44%)] Loss: -1082.818237\n",
      "Train Epoch: 476 [56320/101520 (55%)] Loss: -1077.459717\n",
      "Train Epoch: 476 [67584/101520 (67%)] Loss: -1086.074219\n",
      "Train Epoch: 476 [78848/101520 (78%)] Loss: -1082.043701\n",
      "Train Epoch: 476 [90112/101520 (89%)] Loss: -1081.074341\n",
      "Train Epoch: 476 [101376/101520 (100%)] Loss: -1080.215698\n",
      "    epoch          : 476\n",
      "    loss           : -1082.6720025910804\n",
      "    ess            : 1.9631991955503147\n",
      "    log_marginal   : 1082.703139108629\n",
      "    log_joint      : 1290.7825915466\n",
      "    val_loss       : -1080.083931300951\n",
      "    val_ess        : 1.9618014460024626\n",
      "    val_log_marginal: 1080.1153033712635\n",
      "    val_log_joint  : 1288.327684485394\n",
      "Train Epoch: 477 [0/101520 (0%)] Loss: -1076.792480\n",
      "Train Epoch: 477 [11264/101520 (11%)] Loss: -1075.494873\n",
      "Train Epoch: 477 [22528/101520 (22%)] Loss: -1084.029053\n",
      "Train Epoch: 477 [33792/101520 (33%)] Loss: -1085.220459\n",
      "Train Epoch: 477 [45056/101520 (44%)] Loss: -1086.990967\n",
      "Train Epoch: 477 [56320/101520 (55%)] Loss: -1086.696167\n",
      "Train Epoch: 477 [67584/101520 (67%)] Loss: -1085.081055\n",
      "Train Epoch: 477 [78848/101520 (78%)] Loss: -1080.621338\n",
      "Train Epoch: 477 [90112/101520 (89%)] Loss: -1079.780762\n",
      "Train Epoch: 477 [101376/101520 (100%)] Loss: -1072.732300\n",
      "    epoch          : 477\n",
      "    loss           : -1083.0642672591473\n",
      "    ess            : 1.9644024072580002\n",
      "    log_marginal   : 1083.093566587822\n",
      "    log_joint      : 1291.173660661707\n",
      "    val_loss       : -1081.4592975118885\n",
      "    val_ess        : 1.9654424242351367\n",
      "    val_log_marginal: 1081.4875116762908\n",
      "    val_log_joint  : 1289.4626146399457\n",
      "Train Epoch: 478 [0/101520 (0%)] Loss: -1085.150757\n",
      "Train Epoch: 478 [11264/101520 (11%)] Loss: -1087.856201\n",
      "Train Epoch: 478 [22528/101520 (22%)] Loss: -1085.634399\n",
      "Train Epoch: 478 [33792/101520 (33%)] Loss: -1087.353394\n",
      "Train Epoch: 478 [45056/101520 (44%)] Loss: -1082.329712\n",
      "Train Epoch: 478 [56320/101520 (55%)] Loss: -1082.513184\n",
      "Train Epoch: 478 [67584/101520 (67%)] Loss: -1080.022461\n",
      "Train Epoch: 478 [78848/101520 (78%)] Loss: -1083.355225\n",
      "Train Epoch: 478 [90112/101520 (89%)] Loss: -1083.180298\n",
      "Train Epoch: 478 [101376/101520 (100%)] Loss: -1084.281372\n",
      "    epoch          : 478\n",
      "    loss           : -1083.4547671217415\n",
      "    ess            : 1.9652257175301786\n",
      "    log_marginal   : 1083.483737044598\n",
      "    log_joint      : 1291.500930556101\n",
      "    val_loss       : -1082.1755211871603\n",
      "    val_ess        : 1.95977216699849\n",
      "    val_log_marginal: 1082.2156610903533\n",
      "    val_log_joint  : 1290.3764489215353\n",
      "Train Epoch: 479 [0/101520 (0%)] Loss: -1084.008057\n",
      "Train Epoch: 479 [11264/101520 (11%)] Loss: -1078.951660\n",
      "Train Epoch: 479 [22528/101520 (22%)] Loss: -1079.661621\n",
      "Train Epoch: 479 [33792/101520 (33%)] Loss: -1083.866943\n",
      "Train Epoch: 479 [45056/101520 (44%)] Loss: -1085.854858\n",
      "Train Epoch: 479 [56320/101520 (55%)] Loss: -1082.508789\n",
      "Train Epoch: 479 [67584/101520 (67%)] Loss: -1085.204712\n",
      "Train Epoch: 479 [78848/101520 (78%)] Loss: -1086.354736\n",
      "Train Epoch: 479 [90112/101520 (89%)] Loss: -1087.953735\n",
      "Train Epoch: 479 [101376/101520 (100%)] Loss: -1090.885376\n",
      "    epoch          : 479\n",
      "    loss           : -1083.469999533802\n",
      "    ess            : 1.9646643196518099\n",
      "    log_marginal   : 1083.4997264152796\n",
      "    log_joint      : 1291.5172278629475\n",
      "    val_loss       : -1082.3041779891305\n",
      "    val_ess        : 1.9598325750102168\n",
      "    val_log_marginal: 1082.3377526324728\n",
      "    val_log_joint  : 1290.555616295856\n",
      "Train Epoch: 480 [0/101520 (0%)] Loss: -1076.068359\n",
      "Train Epoch: 480 [11264/101520 (11%)] Loss: -1084.132812\n",
      "Train Epoch: 480 [22528/101520 (22%)] Loss: -1080.116333\n",
      "Train Epoch: 480 [33792/101520 (33%)] Loss: -1089.817871\n",
      "Train Epoch: 480 [45056/101520 (44%)] Loss: -1083.455078\n",
      "Train Epoch: 480 [56320/101520 (55%)] Loss: -1088.245117\n",
      "Train Epoch: 480 [67584/101520 (67%)] Loss: -1089.524536\n",
      "Train Epoch: 480 [78848/101520 (78%)] Loss: -1074.257568\n",
      "Train Epoch: 480 [90112/101520 (89%)] Loss: -1087.448730\n",
      "Train Epoch: 480 [101376/101520 (100%)] Loss: -1083.368530\n",
      "    epoch          : 480\n",
      "    loss           : -1083.8852569733433\n",
      "    ess            : 1.9649539802541685\n",
      "    log_marginal   : 1083.9145691838096\n",
      "    log_joint      : 1291.9924678323257\n",
      "    val_loss       : -1080.2463299295177\n",
      "    val_ess        : 1.9648945694384368\n",
      "    val_log_marginal: 1080.27404519786\n",
      "    val_log_joint  : 1288.5559931216033\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch480.pth ...\n",
      "Train Epoch: 481 [0/101520 (0%)] Loss: -1082.179688\n",
      "Train Epoch: 481 [11264/101520 (11%)] Loss: -1086.606934\n",
      "Train Epoch: 481 [22528/101520 (22%)] Loss: -1082.134888\n",
      "Train Epoch: 481 [33792/101520 (33%)] Loss: -1089.074341\n",
      "Train Epoch: 481 [45056/101520 (44%)] Loss: -1088.865845\n",
      "Train Epoch: 481 [56320/101520 (55%)] Loss: -1084.375977\n",
      "Train Epoch: 481 [67584/101520 (67%)] Loss: -1090.224854\n",
      "Train Epoch: 481 [78848/101520 (78%)] Loss: -1078.485107\n",
      "Train Epoch: 481 [90112/101520 (89%)] Loss: -1078.635010\n",
      "Train Epoch: 481 [101376/101520 (100%)] Loss: -1082.786255\n",
      "    epoch          : 481\n",
      "    loss           : -1083.8707993090452\n",
      "    ess            : 1.965743533330946\n",
      "    log_marginal   : 1083.8989036981784\n",
      "    log_joint      : 1291.9040128621623\n",
      "    val_loss       : -1082.2290675951087\n",
      "    val_ess        : 1.9632805585861206\n",
      "    val_log_marginal: 1082.2589535920517\n",
      "    val_log_joint  : 1290.5823815387228\n",
      "Train Epoch: 482 [0/101520 (0%)] Loss: -1090.078857\n",
      "Train Epoch: 482 [11264/101520 (11%)] Loss: -1086.268799\n",
      "Train Epoch: 482 [22528/101520 (22%)] Loss: -1080.221436\n",
      "Train Epoch: 482 [33792/101520 (33%)] Loss: -1092.943726\n",
      "Train Epoch: 482 [45056/101520 (44%)] Loss: -1081.093018\n",
      "Train Epoch: 482 [56320/101520 (55%)] Loss: -1082.817017\n",
      "Train Epoch: 482 [67584/101520 (67%)] Loss: -1080.441162\n",
      "Train Epoch: 482 [78848/101520 (78%)] Loss: -1085.218750\n",
      "Train Epoch: 482 [90112/101520 (89%)] Loss: -1086.031494\n",
      "Train Epoch: 482 [101376/101520 (100%)] Loss: -1080.306030\n",
      "    epoch          : 482\n",
      "    loss           : -1084.0340293999293\n",
      "    ess            : 1.964428572798494\n",
      "    log_marginal   : 1084.063458773359\n",
      "    log_joint      : 1292.12890625\n",
      "    val_loss       : -1081.7763512652853\n",
      "    val_ess        : 1.9604630003804746\n",
      "    val_log_marginal: 1081.8110988451087\n",
      "    val_log_joint  : 1289.6944739300272\n",
      "Train Epoch: 483 [0/101520 (0%)] Loss: -1093.083984\n",
      "Train Epoch: 483 [11264/101520 (11%)] Loss: -1086.959351\n",
      "Train Epoch: 483 [22528/101520 (22%)] Loss: -1082.230347\n",
      "Train Epoch: 483 [33792/101520 (33%)] Loss: -1087.483154\n",
      "Train Epoch: 483 [45056/101520 (44%)] Loss: -1086.298096\n",
      "Train Epoch: 483 [56320/101520 (55%)] Loss: -1085.538330\n",
      "Train Epoch: 483 [67584/101520 (67%)] Loss: -1093.254883\n",
      "Train Epoch: 483 [78848/101520 (78%)] Loss: -1077.014404\n",
      "Train Epoch: 483 [90112/101520 (89%)] Loss: -1089.454224\n",
      "Train Epoch: 483 [101376/101520 (100%)] Loss: -1091.572388\n",
      "    epoch          : 483\n",
      "    loss           : -1084.5820637611887\n",
      "    ess            : 1.964132709119787\n",
      "    log_marginal   : 1084.6122629750314\n",
      "    log_joint      : 1292.627790441465\n",
      "    val_loss       : -1082.9516336192255\n",
      "    val_ess        : 1.9670829461968464\n",
      "    val_log_marginal: 1082.9777991253397\n",
      "    val_log_joint  : 1291.3359534222147\n",
      "Train Epoch: 484 [0/101520 (0%)] Loss: -1083.613525\n",
      "Train Epoch: 484 [11264/101520 (11%)] Loss: -1086.165283\n",
      "Train Epoch: 484 [22528/101520 (22%)] Loss: -1084.136230\n",
      "Train Epoch: 484 [33792/101520 (33%)] Loss: -1084.572266\n",
      "Train Epoch: 484 [45056/101520 (44%)] Loss: -1081.648682\n",
      "Train Epoch: 484 [56320/101520 (55%)] Loss: -1080.753418\n",
      "Train Epoch: 484 [67584/101520 (67%)] Loss: -1094.557251\n",
      "Train Epoch: 484 [78848/101520 (78%)] Loss: -1087.757568\n",
      "Train Epoch: 484 [90112/101520 (89%)] Loss: -1086.041016\n",
      "Train Epoch: 484 [101376/101520 (100%)] Loss: -1083.625854\n",
      "    epoch          : 484\n",
      "    loss           : -1084.3385083375863\n",
      "    ess            : 1.9648454842255942\n",
      "    log_marginal   : 1084.367266017588\n",
      "    log_joint      : 1292.4343378268295\n",
      "    val_loss       : -1082.1985765540082\n",
      "    val_ess        : 1.969036076379859\n",
      "    val_log_marginal: 1082.2227252462635\n",
      "    val_log_joint  : 1290.3541206691575\n",
      "Train Epoch: 485 [0/101520 (0%)] Loss: -1081.172119\n",
      "Train Epoch: 485 [11264/101520 (11%)] Loss: -1086.798340\n",
      "Train Epoch: 485 [22528/101520 (22%)] Loss: -1077.902222\n",
      "Train Epoch: 485 [33792/101520 (33%)] Loss: -1081.597168\n",
      "Train Epoch: 485 [45056/101520 (44%)] Loss: -1084.166260\n",
      "Train Epoch: 485 [56320/101520 (55%)] Loss: -1089.398682\n",
      "Train Epoch: 485 [67584/101520 (67%)] Loss: -1080.885620\n",
      "Train Epoch: 485 [78848/101520 (78%)] Loss: -1079.143066\n",
      "Train Epoch: 485 [90112/101520 (89%)] Loss: -1082.201660\n",
      "Train Epoch: 485 [101376/101520 (100%)] Loss: -1106.377319\n",
      "    epoch          : 485\n",
      "    loss           : -1084.824411363458\n",
      "    ess            : 1.965149415198283\n",
      "    log_marginal   : 1084.853610091473\n",
      "    log_joint      : 1292.8797110552764\n",
      "    val_loss       : -1085.158208432405\n",
      "    val_ess        : 1.966013763261878\n",
      "    val_log_marginal: 1085.1852178158967\n",
      "    val_log_joint  : 1293.150539232337\n",
      "Train Epoch: 486 [0/101520 (0%)] Loss: -1085.252441\n",
      "Train Epoch: 486 [11264/101520 (11%)] Loss: -1082.807129\n",
      "Train Epoch: 486 [22528/101520 (22%)] Loss: -1078.048584\n",
      "Train Epoch: 486 [33792/101520 (33%)] Loss: -1086.826416\n",
      "Train Epoch: 486 [45056/101520 (44%)] Loss: -1086.515015\n",
      "Train Epoch: 486 [56320/101520 (55%)] Loss: -1083.386719\n",
      "Train Epoch: 486 [67584/101520 (67%)] Loss: -1076.659424\n",
      "Train Epoch: 486 [78848/101520 (78%)] Loss: -1084.546143\n",
      "Train Epoch: 486 [90112/101520 (89%)] Loss: -1078.188477\n",
      "Train Epoch: 486 [101376/101520 (100%)] Loss: -1094.608032\n",
      "    epoch          : 486\n",
      "    loss           : -1085.0804240931218\n",
      "    ess            : 1.9635832974659138\n",
      "    log_marginal   : 1085.111231818271\n",
      "    log_joint      : 1293.2152259098225\n",
      "    val_loss       : -1083.5694261633832\n",
      "    val_ess        : 1.964428777280061\n",
      "    val_log_marginal: 1083.5980596127717\n",
      "    val_log_joint  : 1291.4665845788043\n",
      "Train Epoch: 487 [0/101520 (0%)] Loss: -1085.906738\n",
      "Train Epoch: 487 [11264/101520 (11%)] Loss: -1082.496582\n",
      "Train Epoch: 487 [22528/101520 (22%)] Loss: -1088.737549\n",
      "Train Epoch: 487 [33792/101520 (33%)] Loss: -1087.085205\n",
      "Train Epoch: 487 [45056/101520 (44%)] Loss: -1088.731567\n",
      "Train Epoch: 487 [56320/101520 (55%)] Loss: -1084.579834\n",
      "Train Epoch: 487 [67584/101520 (67%)] Loss: -1092.622070\n",
      "Train Epoch: 487 [78848/101520 (78%)] Loss: -1075.268066\n",
      "Train Epoch: 487 [90112/101520 (89%)] Loss: -1085.798828\n",
      "Train Epoch: 487 [101376/101520 (100%)] Loss: -1079.314087\n",
      "    epoch          : 487\n",
      "    loss           : -1085.018682891999\n",
      "    ess            : 1.9650793728516929\n",
      "    log_marginal   : 1085.0468774536746\n",
      "    log_joint      : 1293.0930089902638\n",
      "    val_loss       : -1084.0656950577445\n",
      "    val_ess        : 1.9661828953286875\n",
      "    val_log_marginal: 1084.0947849439538\n",
      "    val_log_joint  : 1291.6674698539402\n",
      "Train Epoch: 488 [0/101520 (0%)] Loss: -1084.933350\n",
      "Train Epoch: 488 [11264/101520 (11%)] Loss: -1081.711182\n",
      "Train Epoch: 488 [22528/101520 (22%)] Loss: -1079.087402\n",
      "Train Epoch: 488 [33792/101520 (33%)] Loss: -1084.149658\n",
      "Train Epoch: 488 [45056/101520 (44%)] Loss: -1082.190308\n",
      "Train Epoch: 488 [56320/101520 (55%)] Loss: -1078.104248\n",
      "Train Epoch: 488 [67584/101520 (67%)] Loss: -1083.158936\n",
      "Train Epoch: 488 [78848/101520 (78%)] Loss: -1084.455566\n",
      "Train Epoch: 488 [90112/101520 (89%)] Loss: -1084.364014\n",
      "Train Epoch: 488 [101376/101520 (100%)] Loss: -1084.149414\n",
      "    epoch          : 488\n",
      "    loss           : -1085.433426286707\n",
      "    ess            : 1.9648739620668805\n",
      "    log_marginal   : 1085.4627556728958\n",
      "    log_joint      : 1293.543765580834\n",
      "    val_loss       : -1083.7345448369565\n",
      "    val_ess        : 1.9664565583933955\n",
      "    val_log_marginal: 1083.7630721382473\n",
      "    val_log_joint  : 1291.898830247962\n",
      "Train Epoch: 489 [0/101520 (0%)] Loss: -1083.373047\n",
      "Train Epoch: 489 [11264/101520 (11%)] Loss: -1075.795288\n",
      "Train Epoch: 489 [22528/101520 (22%)] Loss: -1087.101440\n",
      "Train Epoch: 489 [33792/101520 (33%)] Loss: -1077.645020\n",
      "Train Epoch: 489 [45056/101520 (44%)] Loss: -1089.850098\n",
      "Train Epoch: 489 [56320/101520 (55%)] Loss: -1084.238037\n",
      "Train Epoch: 489 [67584/101520 (67%)] Loss: -1079.797241\n",
      "Train Epoch: 489 [78848/101520 (78%)] Loss: -1083.130615\n",
      "Train Epoch: 489 [90112/101520 (89%)] Loss: -1084.390259\n",
      "Train Epoch: 489 [101376/101520 (100%)] Loss: -1088.450806\n",
      "    epoch          : 489\n",
      "    loss           : -1085.6065348716238\n",
      "    ess            : 1.9647689003441202\n",
      "    log_marginal   : 1085.6359390948885\n",
      "    log_joint      : 1293.718675162924\n",
      "    val_loss       : -1083.2234099014945\n",
      "    val_ess        : 1.963566085566645\n",
      "    val_log_marginal: 1083.2522238026495\n",
      "    val_log_joint  : 1291.3179931640625\n",
      "Train Epoch: 490 [0/101520 (0%)] Loss: -1083.256104\n",
      "Train Epoch: 490 [11264/101520 (11%)] Loss: -1086.406128\n",
      "Train Epoch: 490 [22528/101520 (22%)] Loss: -1089.516235\n",
      "Train Epoch: 490 [33792/101520 (33%)] Loss: -1082.547852\n",
      "Train Epoch: 490 [45056/101520 (44%)] Loss: -1080.017944\n",
      "Train Epoch: 490 [56320/101520 (55%)] Loss: -1081.348877\n",
      "Train Epoch: 490 [67584/101520 (67%)] Loss: -1086.295654\n",
      "Train Epoch: 490 [78848/101520 (78%)] Loss: -1088.095703\n",
      "Train Epoch: 490 [90112/101520 (89%)] Loss: -1087.261230\n",
      "Train Epoch: 490 [101376/101520 (100%)] Loss: -1091.898804\n",
      "    epoch          : 490\n",
      "    loss           : -1085.594413105567\n",
      "    ess            : 1.9635740063298288\n",
      "    log_marginal   : 1085.6245988241992\n",
      "    log_joint      : 1293.7072428794363\n",
      "    val_loss       : -1084.091117527174\n",
      "    val_ess        : 1.9637283242267112\n",
      "    val_log_marginal: 1084.1212211277175\n",
      "    val_log_joint  : 1292.0149615743885\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch490.pth ...\n",
      "Train Epoch: 491 [0/101520 (0%)] Loss: -1092.591675\n",
      "Train Epoch: 491 [11264/101520 (11%)] Loss: -1076.175049\n",
      "Train Epoch: 491 [22528/101520 (22%)] Loss: -1087.539551\n",
      "Train Epoch: 491 [33792/101520 (33%)] Loss: -1085.663452\n",
      "Train Epoch: 491 [45056/101520 (44%)] Loss: -1088.587158\n",
      "Train Epoch: 491 [56320/101520 (55%)] Loss: -1088.554932\n",
      "Train Epoch: 491 [67584/101520 (67%)] Loss: -1095.704102\n",
      "Train Epoch: 491 [78848/101520 (78%)] Loss: -1083.245361\n",
      "Train Epoch: 491 [90112/101520 (89%)] Loss: -1084.049316\n",
      "Train Epoch: 491 [101376/101520 (100%)] Loss: -1084.979004\n",
      "    epoch          : 491\n",
      "    loss           : -1086.148270036707\n",
      "    ess            : 1.9651308119596549\n",
      "    log_marginal   : 1086.177679167321\n",
      "    log_joint      : 1294.2209533998116\n",
      "    val_loss       : -1084.9411249575408\n",
      "    val_ess        : 1.9682164192199707\n",
      "    val_log_marginal: 1084.96703570822\n",
      "    val_log_joint  : 1293.0946151069973\n",
      "Train Epoch: 492 [0/101520 (0%)] Loss: -1085.312500\n",
      "Train Epoch: 492 [11264/101520 (11%)] Loss: -1086.404175\n",
      "Train Epoch: 492 [22528/101520 (22%)] Loss: -1082.272339\n",
      "Train Epoch: 492 [33792/101520 (33%)] Loss: -1094.402954\n",
      "Train Epoch: 492 [45056/101520 (44%)] Loss: -1084.391602\n",
      "Train Epoch: 492 [56320/101520 (55%)] Loss: -1080.394287\n",
      "Train Epoch: 492 [67584/101520 (67%)] Loss: -1084.718018\n",
      "Train Epoch: 492 [78848/101520 (78%)] Loss: -1083.438599\n",
      "Train Epoch: 492 [90112/101520 (89%)] Loss: -1088.139648\n",
      "Train Epoch: 492 [101376/101520 (100%)] Loss: -1105.503906\n",
      "    epoch          : 492\n",
      "    loss           : -1086.2824854251728\n",
      "    ess            : 1.9646416531136288\n",
      "    log_marginal   : 1086.3120301213096\n",
      "    log_joint      : 1294.3440751118876\n",
      "    val_loss       : -1086.3942658797555\n",
      "    val_ess        : 1.9609641769657964\n",
      "    val_log_marginal: 1086.4264712126358\n",
      "    val_log_joint  : 1294.2934782608695\n",
      "Train Epoch: 493 [0/101520 (0%)] Loss: -1089.846680\n",
      "Train Epoch: 493 [11264/101520 (11%)] Loss: -1082.734741\n",
      "Train Epoch: 493 [22528/101520 (22%)] Loss: -1085.082275\n",
      "Train Epoch: 493 [33792/101520 (33%)] Loss: -1089.992920\n",
      "Train Epoch: 493 [45056/101520 (44%)] Loss: -1085.479980\n",
      "Train Epoch: 493 [56320/101520 (55%)] Loss: -1083.816895\n",
      "Train Epoch: 493 [67584/101520 (67%)] Loss: -1089.558350\n",
      "Train Epoch: 493 [78848/101520 (78%)] Loss: -1086.611816\n",
      "Train Epoch: 493 [90112/101520 (89%)] Loss: -1082.146362\n",
      "Train Epoch: 493 [101376/101520 (100%)] Loss: -1085.444946\n",
      "    epoch          : 493\n",
      "    loss           : -1086.195722263662\n",
      "    ess            : 1.964714325253089\n",
      "    log_marginal   : 1086.2248062823885\n",
      "    log_joint      : 1294.2442430659155\n",
      "    val_loss       : -1084.9776770550272\n",
      "    val_ess        : 1.9651328947233118\n",
      "    val_log_marginal: 1085.007371985394\n",
      "    val_log_joint  : 1293.1011909816575\n",
      "Train Epoch: 494 [0/101520 (0%)] Loss: -1087.850708\n",
      "Train Epoch: 494 [11264/101520 (11%)] Loss: -1084.164062\n",
      "Train Epoch: 494 [22528/101520 (22%)] Loss: -1083.407959\n",
      "Train Epoch: 494 [33792/101520 (33%)] Loss: -1083.664551\n",
      "Train Epoch: 494 [45056/101520 (44%)] Loss: -1089.516113\n",
      "Train Epoch: 494 [56320/101520 (55%)] Loss: -1089.815186\n",
      "Train Epoch: 494 [67584/101520 (67%)] Loss: -1087.825928\n",
      "Train Epoch: 494 [78848/101520 (78%)] Loss: -1085.115967\n",
      "Train Epoch: 494 [90112/101520 (89%)] Loss: -1086.678345\n",
      "Train Epoch: 494 [101376/101520 (100%)] Loss: -1083.405273\n",
      "    epoch          : 494\n",
      "    loss           : -1086.6498943693075\n",
      "    ess            : 1.9643853040196788\n",
      "    log_marginal   : 1086.679378950416\n",
      "    log_joint      : 1294.829885511542\n",
      "    val_loss       : -1085.4035432235055\n",
      "    val_ess        : 1.9616345633631167\n",
      "    val_log_marginal: 1085.4346870754075\n",
      "    val_log_joint  : 1293.675245202106\n",
      "Train Epoch: 495 [0/101520 (0%)] Loss: -1093.824707\n",
      "Train Epoch: 495 [11264/101520 (11%)] Loss: -1085.738892\n",
      "Train Epoch: 495 [22528/101520 (22%)] Loss: -1082.454346\n",
      "Train Epoch: 495 [33792/101520 (33%)] Loss: -1082.761108\n",
      "Train Epoch: 495 [45056/101520 (44%)] Loss: -1090.537842\n",
      "Train Epoch: 495 [56320/101520 (55%)] Loss: -1085.394897\n",
      "Train Epoch: 495 [67584/101520 (67%)] Loss: -1086.796265\n",
      "Train Epoch: 495 [78848/101520 (78%)] Loss: -1085.872559\n",
      "Train Epoch: 495 [90112/101520 (89%)] Loss: -1081.390381\n",
      "Train Epoch: 495 [101376/101520 (100%)] Loss: -1071.873047\n",
      "    epoch          : 495\n",
      "    loss           : -1086.5162359649812\n",
      "    ess            : 1.9643571388781371\n",
      "    log_marginal   : 1086.546121108472\n",
      "    log_joint      : 1294.604556596459\n",
      "    val_loss       : -1085.3626178243885\n",
      "    val_ess        : 1.9641661851302437\n",
      "    val_log_marginal: 1085.3928488026495\n",
      "    val_log_joint  : 1293.3386018172555\n",
      "Train Epoch: 496 [0/101520 (0%)] Loss: -1087.212891\n",
      "Train Epoch: 496 [11264/101520 (11%)] Loss: -1086.904785\n",
      "Train Epoch: 496 [22528/101520 (22%)] Loss: -1093.494141\n",
      "Train Epoch: 496 [33792/101520 (33%)] Loss: -1091.387817\n",
      "Train Epoch: 496 [45056/101520 (44%)] Loss: -1087.829956\n",
      "Train Epoch: 496 [56320/101520 (55%)] Loss: -1083.799316\n",
      "Train Epoch: 496 [67584/101520 (67%)] Loss: -1082.346924\n",
      "Train Epoch: 496 [78848/101520 (78%)] Loss: -1086.128540\n",
      "Train Epoch: 496 [90112/101520 (89%)] Loss: -1088.061768\n",
      "Train Epoch: 496 [101376/101520 (100%)] Loss: -1082.497925\n",
      "    epoch          : 496\n",
      "    loss           : -1086.8737118208228\n",
      "    ess            : 1.9649479125612346\n",
      "    log_marginal   : 1086.9031319929727\n",
      "    log_joint      : 1294.9768857716315\n",
      "    val_loss       : -1084.9149806810462\n",
      "    val_ess        : 1.9644659964934639\n",
      "    val_log_marginal: 1084.9455831776495\n",
      "    val_log_joint  : 1292.8985064962635\n",
      "Train Epoch: 497 [0/101520 (0%)] Loss: -1088.304443\n",
      "Train Epoch: 497 [11264/101520 (11%)] Loss: -1088.945312\n",
      "Train Epoch: 497 [22528/101520 (22%)] Loss: -1089.229736\n",
      "Train Epoch: 497 [33792/101520 (33%)] Loss: -1094.229736\n",
      "Train Epoch: 497 [45056/101520 (44%)] Loss: -1082.179199\n",
      "Train Epoch: 497 [56320/101520 (55%)] Loss: -1086.285156\n",
      "Train Epoch: 497 [67584/101520 (67%)] Loss: -1086.750366\n",
      "Train Epoch: 497 [78848/101520 (78%)] Loss: -1090.301270\n",
      "Train Epoch: 497 [90112/101520 (89%)] Loss: -1081.725342\n",
      "Train Epoch: 497 [101376/101520 (100%)] Loss: -1081.181030\n",
      "    epoch          : 497\n",
      "    loss           : -1087.4309143373116\n",
      "    ess            : 1.9661614936799858\n",
      "    log_marginal   : 1087.457817039298\n",
      "    log_joint      : 1295.467199891057\n",
      "    val_loss       : -1086.2348314368207\n",
      "    val_ess        : 1.9601740577946538\n",
      "    val_log_marginal: 1086.270852793818\n",
      "    val_log_joint  : 1294.3161833389945\n",
      "Train Epoch: 498 [0/101520 (0%)] Loss: -1086.666016\n",
      "Train Epoch: 498 [11264/101520 (11%)] Loss: -1084.376099\n",
      "Train Epoch: 498 [22528/101520 (22%)] Loss: -1083.653442\n",
      "Train Epoch: 498 [33792/101520 (33%)] Loss: -1092.795532\n",
      "Train Epoch: 498 [45056/101520 (44%)] Loss: -1090.504028\n",
      "Train Epoch: 498 [56320/101520 (55%)] Loss: -1085.487671\n",
      "Train Epoch: 498 [67584/101520 (67%)] Loss: -1084.536133\n",
      "Train Epoch: 498 [78848/101520 (78%)] Loss: -1090.270996\n",
      "Train Epoch: 498 [90112/101520 (89%)] Loss: -1086.565796\n",
      "Train Epoch: 498 [101376/101520 (100%)] Loss: -1089.493652\n",
      "    epoch          : 498\n",
      "    loss           : -1087.1968104683574\n",
      "    ess            : 1.964958040558513\n",
      "    log_marginal   : 1087.2260423209798\n",
      "    log_joint      : 1295.263454111377\n",
      "    val_loss       : -1085.4152248216712\n",
      "    val_ess        : 1.964574907137\n",
      "    val_log_marginal: 1085.4469524881115\n",
      "    val_log_joint  : 1293.2419592815897\n",
      "Train Epoch: 499 [0/101520 (0%)] Loss: -1089.742432\n",
      "Train Epoch: 499 [11264/101520 (11%)] Loss: -1088.272949\n",
      "Train Epoch: 499 [22528/101520 (22%)] Loss: -1095.837646\n",
      "Train Epoch: 499 [33792/101520 (33%)] Loss: -1086.248047\n",
      "Train Epoch: 499 [45056/101520 (44%)] Loss: -1096.447021\n",
      "Train Epoch: 499 [56320/101520 (55%)] Loss: -1086.621582\n",
      "Train Epoch: 499 [67584/101520 (67%)] Loss: -1091.953125\n",
      "Train Epoch: 499 [78848/101520 (78%)] Loss: -1085.606079\n",
      "Train Epoch: 499 [90112/101520 (89%)] Loss: -1089.044922\n",
      "Train Epoch: 499 [101376/101520 (100%)] Loss: -1083.705566\n",
      "    epoch          : 499\n",
      "    loss           : -1087.709866471027\n",
      "    ess            : 1.9647356223820442\n",
      "    log_marginal   : 1087.7396645090687\n",
      "    log_joint      : 1295.7420605223383\n",
      "    val_loss       : -1085.9444527004075\n",
      "    val_ess        : 1.9654563043428503\n",
      "    val_log_marginal: 1085.9715257727582\n",
      "    val_log_joint  : 1294.259128736413\n",
      "Train Epoch: 500 [0/101520 (0%)] Loss: -1082.706177\n",
      "Train Epoch: 500 [11264/101520 (11%)] Loss: -1086.471924\n",
      "Train Epoch: 500 [22528/101520 (22%)] Loss: -1089.543579\n",
      "Train Epoch: 500 [33792/101520 (33%)] Loss: -1089.872070\n",
      "Train Epoch: 500 [45056/101520 (44%)] Loss: -1082.180664\n",
      "Train Epoch: 500 [56320/101520 (55%)] Loss: -1089.727783\n",
      "Train Epoch: 500 [67584/101520 (67%)] Loss: -1091.233154\n",
      "Train Epoch: 500 [78848/101520 (78%)] Loss: -1081.403320\n",
      "Train Epoch: 500 [90112/101520 (89%)] Loss: -1092.001465\n",
      "Train Epoch: 500 [101376/101520 (100%)] Loss: -1088.929932\n",
      "    epoch          : 500\n",
      "    loss           : -1087.7710218573336\n",
      "    ess            : 1.9650046304242694\n",
      "    log_marginal   : 1087.800164764251\n",
      "    log_joint      : 1295.8880584563442\n",
      "    val_loss       : -1087.4761007557745\n",
      "    val_ess        : 1.965057398961938\n",
      "    val_log_marginal: 1087.505567467731\n",
      "    val_log_joint  : 1295.5089058254075\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch500.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 501 [0/101520 (0%)] Loss: -1085.222412\n",
      "Train Epoch: 501 [11264/101520 (11%)] Loss: -1089.288574\n",
      "Train Epoch: 501 [22528/101520 (22%)] Loss: -1088.298950\n",
      "Train Epoch: 501 [33792/101520 (33%)] Loss: -1089.525635\n",
      "Train Epoch: 501 [45056/101520 (44%)] Loss: -1083.554810\n",
      "Train Epoch: 501 [56320/101520 (55%)] Loss: -1087.515381\n",
      "Train Epoch: 501 [67584/101520 (67%)] Loss: -1087.937622\n",
      "Train Epoch: 501 [78848/101520 (78%)] Loss: -1087.649536\n",
      "Train Epoch: 501 [90112/101520 (89%)] Loss: -1096.828369\n",
      "Train Epoch: 501 [101376/101520 (100%)] Loss: -1085.090088\n",
      "    epoch          : 501\n",
      "    loss           : -1087.8361270463647\n",
      "    ess            : 1.9652842403057234\n",
      "    log_marginal   : 1087.8650963558025\n",
      "    log_joint      : 1295.8927596969222\n",
      "    val_loss       : -1085.9793064283288\n",
      "    val_ess        : 1.9678834054781043\n",
      "    val_log_marginal: 1086.0053870159647\n",
      "    val_log_joint  : 1294.1077084748642\n",
      "Train Epoch: 502 [0/101520 (0%)] Loss: -1084.954712\n",
      "Train Epoch: 502 [11264/101520 (11%)] Loss: -1093.746826\n",
      "Train Epoch: 502 [22528/101520 (22%)] Loss: -1091.296875\n",
      "Train Epoch: 502 [33792/101520 (33%)] Loss: -1088.261963\n",
      "Train Epoch: 502 [45056/101520 (44%)] Loss: -1091.209473\n",
      "Train Epoch: 502 [56320/101520 (55%)] Loss: -1084.794922\n",
      "Train Epoch: 502 [67584/101520 (67%)] Loss: -1089.837402\n",
      "Train Epoch: 502 [78848/101520 (78%)] Loss: -1089.340210\n",
      "Train Epoch: 502 [90112/101520 (89%)] Loss: -1089.696289\n",
      "Train Epoch: 502 [101376/101520 (100%)] Loss: -1091.333374\n",
      "    epoch          : 502\n",
      "    loss           : -1088.3340064580716\n",
      "    ess            : 1.9642141817802161\n",
      "    log_marginal   : 1088.3643946048603\n",
      "    log_joint      : 1296.4343427341787\n",
      "    val_loss       : -1087.489401112432\n",
      "    val_ess        : 1.9654106990150784\n",
      "    val_log_marginal: 1087.5158001443615\n",
      "    val_log_joint  : 1295.7784848420517\n",
      "Train Epoch: 503 [0/101520 (0%)] Loss: -1093.661255\n",
      "Train Epoch: 503 [11264/101520 (11%)] Loss: -1090.773804\n",
      "Train Epoch: 503 [22528/101520 (22%)] Loss: -1088.083984\n",
      "Train Epoch: 503 [33792/101520 (33%)] Loss: -1086.228882\n",
      "Train Epoch: 503 [45056/101520 (44%)] Loss: -1085.476562\n",
      "Train Epoch: 503 [56320/101520 (55%)] Loss: -1082.706299\n",
      "Train Epoch: 503 [67584/101520 (67%)] Loss: -1085.365234\n",
      "Train Epoch: 503 [78848/101520 (78%)] Loss: -1090.560791\n",
      "Train Epoch: 503 [90112/101520 (89%)] Loss: -1094.520752\n",
      "Train Epoch: 503 [101376/101520 (100%)] Loss: -1092.996704\n",
      "    epoch          : 503\n",
      "    loss           : -1088.3393008744897\n",
      "    ess            : 1.9653577954325843\n",
      "    log_marginal   : 1088.3677285352544\n",
      "    log_joint      : 1296.3960328988694\n",
      "    val_loss       : -1086.90478515625\n",
      "    val_ess        : 1.9676793969195823\n",
      "    val_log_marginal: 1086.930128014606\n",
      "    val_log_joint  : 1295.1791620669158\n",
      "Train Epoch: 504 [0/101520 (0%)] Loss: -1088.047852\n",
      "Train Epoch: 504 [11264/101520 (11%)] Loss: -1088.253418\n",
      "Train Epoch: 504 [22528/101520 (22%)] Loss: -1088.063232\n",
      "Train Epoch: 504 [33792/101520 (33%)] Loss: -1091.907104\n",
      "Train Epoch: 504 [45056/101520 (44%)] Loss: -1091.529541\n",
      "Train Epoch: 504 [56320/101520 (55%)] Loss: -1083.104980\n",
      "Train Epoch: 504 [67584/101520 (67%)] Loss: -1088.480103\n",
      "Train Epoch: 504 [78848/101520 (78%)] Loss: -1091.906616\n",
      "Train Epoch: 504 [90112/101520 (89%)] Loss: -1087.862793\n",
      "Train Epoch: 504 [101376/101520 (100%)] Loss: -1084.611816\n",
      "    epoch          : 504\n",
      "    loss           : -1088.621182695705\n",
      "    ess            : 1.9653767988310387\n",
      "    log_marginal   : 1088.6492453723697\n",
      "    log_joint      : 1296.7151596606077\n",
      "    val_loss       : -1087.2037247367527\n",
      "    val_ess        : 1.9653183014496514\n",
      "    val_log_marginal: 1087.2308986497962\n",
      "    val_log_joint  : 1295.2760115913723\n",
      "Train Epoch: 505 [0/101520 (0%)] Loss: -1092.069336\n",
      "Train Epoch: 505 [11264/101520 (11%)] Loss: -1087.540771\n",
      "Train Epoch: 505 [22528/101520 (22%)] Loss: -1092.985107\n",
      "Train Epoch: 505 [33792/101520 (33%)] Loss: -1091.344727\n",
      "Train Epoch: 505 [45056/101520 (44%)] Loss: -1092.087524\n",
      "Train Epoch: 505 [56320/101520 (55%)] Loss: -1087.544067\n",
      "Train Epoch: 505 [67584/101520 (67%)] Loss: -1083.844238\n",
      "Train Epoch: 505 [78848/101520 (78%)] Loss: -1087.683105\n",
      "Train Epoch: 505 [90112/101520 (89%)] Loss: -1092.832031\n",
      "Train Epoch: 505 [101376/101520 (100%)] Loss: -1084.973755\n",
      "    epoch          : 505\n",
      "    loss           : -1088.896231033095\n",
      "    ess            : 1.9648381519557243\n",
      "    log_marginal   : 1088.9259634353407\n",
      "    log_joint      : 1297.0205151735238\n",
      "    val_loss       : -1089.2546068274457\n",
      "    val_ess        : 1.9657302887543389\n",
      "    val_log_marginal: 1089.2831182065217\n",
      "    val_log_joint  : 1297.3669115149457\n",
      "Train Epoch: 506 [0/101520 (0%)] Loss: -1089.837158\n",
      "Train Epoch: 506 [11264/101520 (11%)] Loss: -1095.719482\n",
      "Train Epoch: 506 [22528/101520 (22%)] Loss: -1090.068848\n",
      "Train Epoch: 506 [33792/101520 (33%)] Loss: -1089.038696\n",
      "Train Epoch: 506 [45056/101520 (44%)] Loss: -1087.046631\n",
      "Train Epoch: 506 [56320/101520 (55%)] Loss: -1088.253662\n",
      "Train Epoch: 506 [67584/101520 (67%)] Loss: -1092.753662\n",
      "Train Epoch: 506 [78848/101520 (78%)] Loss: -1091.611572\n",
      "Train Epoch: 506 [90112/101520 (89%)] Loss: -1091.590576\n",
      "Train Epoch: 506 [101376/101520 (100%)] Loss: -1078.057495\n",
      "    epoch          : 506\n",
      "    loss           : -1088.937464421718\n",
      "    ess            : 1.9647167477775458\n",
      "    log_marginal   : 1088.9667723382538\n",
      "    log_joint      : 1297.052673646553\n",
      "    val_loss       : -1088.6253184442935\n",
      "    val_ess        : 1.9648003681846287\n",
      "    val_log_marginal: 1088.6555388077445\n",
      "    val_log_joint  : 1296.7176142153533\n",
      "Train Epoch: 507 [0/101520 (0%)] Loss: -1089.051392\n",
      "Train Epoch: 507 [11264/101520 (11%)] Loss: -1087.449463\n",
      "Train Epoch: 507 [22528/101520 (22%)] Loss: -1092.104980\n",
      "Train Epoch: 507 [33792/101520 (33%)] Loss: -1087.828979\n",
      "Train Epoch: 507 [45056/101520 (44%)] Loss: -1094.656494\n",
      "Train Epoch: 507 [56320/101520 (55%)] Loss: -1087.440186\n",
      "Train Epoch: 507 [67584/101520 (67%)] Loss: -1091.953369\n",
      "Train Epoch: 507 [78848/101520 (78%)] Loss: -1088.363525\n",
      "Train Epoch: 507 [90112/101520 (89%)] Loss: -1091.550659\n",
      "Train Epoch: 507 [101376/101520 (100%)] Loss: -1076.743530\n",
      "    epoch          : 507\n",
      "    loss           : -1089.1996579577576\n",
      "    ess            : 1.9647860646846906\n",
      "    log_marginal   : 1089.22930509481\n",
      "    log_joint      : 1297.2440866441582\n",
      "    val_loss       : -1090.5637047809103\n",
      "    val_ess        : 1.9639622284018474\n",
      "    val_log_marginal: 1090.593314792799\n",
      "    val_log_joint  : 1298.4792321246603\n",
      "Train Epoch: 508 [0/101520 (0%)] Loss: -1090.060791\n",
      "Train Epoch: 508 [11264/101520 (11%)] Loss: -1086.045898\n",
      "Train Epoch: 508 [22528/101520 (22%)] Loss: -1092.103271\n",
      "Train Epoch: 508 [33792/101520 (33%)] Loss: -1090.282959\n",
      "Train Epoch: 508 [45056/101520 (44%)] Loss: -1089.086182\n",
      "Train Epoch: 508 [56320/101520 (55%)] Loss: -1089.027588\n",
      "Train Epoch: 508 [67584/101520 (67%)] Loss: -1089.800781\n",
      "Train Epoch: 508 [78848/101520 (78%)] Loss: -1095.138916\n",
      "Train Epoch: 508 [90112/101520 (89%)] Loss: -1094.502930\n",
      "Train Epoch: 508 [101376/101520 (100%)] Loss: -1093.767944\n",
      "    epoch          : 508\n",
      "    loss           : -1089.6399649615264\n",
      "    ess            : 1.9657420848482219\n",
      "    log_marginal   : 1089.6685944370288\n",
      "    log_joint      : 1297.655383852858\n",
      "    val_loss       : -1088.8002505095108\n",
      "    val_ess        : 1.9672519849694294\n",
      "    val_log_marginal: 1088.8268830672555\n",
      "    val_log_joint  : 1296.7891421110733\n",
      "Train Epoch: 509 [0/101520 (0%)] Loss: -1092.679199\n",
      "Train Epoch: 509 [11264/101520 (11%)] Loss: -1089.850098\n",
      "Train Epoch: 509 [22528/101520 (22%)] Loss: -1085.547974\n",
      "Train Epoch: 509 [33792/101520 (33%)] Loss: -1090.473755\n",
      "Train Epoch: 509 [45056/101520 (44%)] Loss: -1098.895996\n",
      "Train Epoch: 509 [56320/101520 (55%)] Loss: -1096.536377\n",
      "Train Epoch: 509 [67584/101520 (67%)] Loss: -1091.825562\n",
      "Train Epoch: 509 [78848/101520 (78%)] Loss: -1089.459717\n",
      "Train Epoch: 509 [90112/101520 (89%)] Loss: -1088.861084\n",
      "Train Epoch: 509 [101376/101520 (100%)] Loss: -1102.191528\n",
      "    epoch          : 509\n",
      "    loss           : -1089.533823291261\n",
      "    ess            : 1.9650295164117861\n",
      "    log_marginal   : 1089.5628018019786\n",
      "    log_joint      : 1297.6249312971106\n",
      "    val_loss       : -1088.3683232846467\n",
      "    val_ess        : 1.968561685603598\n",
      "    val_log_marginal: 1088.3939262058425\n",
      "    val_log_joint  : 1296.6254139775815\n",
      "Train Epoch: 510 [0/101520 (0%)] Loss: -1095.239502\n",
      "Train Epoch: 510 [11264/101520 (11%)] Loss: -1091.583740\n",
      "Train Epoch: 510 [22528/101520 (22%)] Loss: -1080.680420\n",
      "Train Epoch: 510 [33792/101520 (33%)] Loss: -1088.463379\n",
      "Train Epoch: 510 [45056/101520 (44%)] Loss: -1086.363281\n",
      "Train Epoch: 510 [56320/101520 (55%)] Loss: -1090.571411\n",
      "Train Epoch: 510 [67584/101520 (67%)] Loss: -1087.130859\n",
      "Train Epoch: 510 [78848/101520 (78%)] Loss: -1091.504883\n",
      "Train Epoch: 510 [90112/101520 (89%)] Loss: -1090.331177\n",
      "Train Epoch: 510 [101376/101520 (100%)] Loss: -1077.961792\n",
      "    epoch          : 510\n",
      "    loss           : -1089.8971750844064\n",
      "    ess            : 1.966103065553023\n",
      "    log_marginal   : 1089.9250089559123\n",
      "    log_joint      : 1297.89960115519\n",
      "    val_loss       : -1090.7740584663723\n",
      "    val_ess        : 1.961175244787465\n",
      "    val_log_marginal: 1090.8059984290082\n",
      "    val_log_joint  : 1299.1277970023777\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch510.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 511 [0/101520 (0%)] Loss: -1091.356323\n",
      "Train Epoch: 511 [11264/101520 (11%)] Loss: -1087.270996\n",
      "Train Epoch: 511 [22528/101520 (22%)] Loss: -1085.485718\n",
      "Train Epoch: 511 [33792/101520 (33%)] Loss: -1097.950439\n",
      "Train Epoch: 511 [45056/101520 (44%)] Loss: -1093.632690\n",
      "Train Epoch: 511 [56320/101520 (55%)] Loss: -1094.771729\n",
      "Train Epoch: 511 [67584/101520 (67%)] Loss: -1095.023926\n",
      "Train Epoch: 511 [78848/101520 (78%)] Loss: -1087.785889\n",
      "Train Epoch: 511 [90112/101520 (89%)] Loss: -1087.215210\n",
      "Train Epoch: 511 [101376/101520 (100%)] Loss: -1100.264282\n",
      "    epoch          : 511\n",
      "    loss           : -1090.048877811911\n",
      "    ess            : 1.965915825498763\n",
      "    log_marginal   : 1090.0768944821766\n",
      "    log_joint      : 1298.183347769119\n",
      "    val_loss       : -1087.6972072435462\n",
      "    val_ess        : 1.9666074825369793\n",
      "    val_log_marginal: 1087.7251613451087\n",
      "    val_log_joint  : 1296.0922108525815\n",
      "Train Epoch: 512 [0/101520 (0%)] Loss: -1087.400024\n",
      "Train Epoch: 512 [11264/101520 (11%)] Loss: -1100.150146\n",
      "Train Epoch: 512 [22528/101520 (22%)] Loss: -1091.342651\n",
      "Train Epoch: 512 [33792/101520 (33%)] Loss: -1089.172852\n",
      "Train Epoch: 512 [45056/101520 (44%)] Loss: -1091.534302\n",
      "Train Epoch: 512 [56320/101520 (55%)] Loss: -1085.717529\n",
      "Train Epoch: 512 [67584/101520 (67%)] Loss: -1099.806641\n",
      "Train Epoch: 512 [78848/101520 (78%)] Loss: -1086.858398\n",
      "Train Epoch: 512 [90112/101520 (89%)] Loss: -1096.287598\n",
      "Train Epoch: 512 [101376/101520 (100%)] Loss: -1105.664673\n",
      "    epoch          : 512\n",
      "    loss           : -1090.2238959691033\n",
      "    ess            : 1.9654594749661547\n",
      "    log_marginal   : 1090.2532584798994\n",
      "    log_joint      : 1298.2578646405857\n",
      "    val_loss       : -1089.3636527683425\n",
      "    val_ess        : 1.9653394533240276\n",
      "    val_log_marginal: 1089.392142917799\n",
      "    val_log_joint  : 1297.4118758491848\n",
      "Train Epoch: 513 [0/101520 (0%)] Loss: -1090.728394\n",
      "Train Epoch: 513 [11264/101520 (11%)] Loss: -1090.656982\n",
      "Train Epoch: 513 [22528/101520 (22%)] Loss: -1091.371826\n",
      "Train Epoch: 513 [33792/101520 (33%)] Loss: -1093.792969\n",
      "Train Epoch: 513 [45056/101520 (44%)] Loss: -1086.781006\n",
      "Train Epoch: 513 [56320/101520 (55%)] Loss: -1090.514038\n",
      "Train Epoch: 513 [67584/101520 (67%)] Loss: -1087.917969\n",
      "Train Epoch: 513 [78848/101520 (78%)] Loss: -1089.519165\n",
      "Train Epoch: 513 [90112/101520 (89%)] Loss: -1084.952637\n",
      "Train Epoch: 513 [101376/101520 (100%)] Loss: -1095.705688\n",
      "    epoch          : 513\n",
      "    loss           : -1090.411672620917\n",
      "    ess            : 1.964124252448729\n",
      "    log_marginal   : 1090.4417792085428\n",
      "    log_joint      : 1298.4530937156485\n",
      "    val_loss       : -1090.206686268682\n",
      "    val_ess        : 1.9644325453302134\n",
      "    val_log_marginal: 1090.2333772078805\n",
      "    val_log_joint  : 1298.096042798913\n",
      "Train Epoch: 514 [0/101520 (0%)] Loss: -1085.665039\n",
      "Train Epoch: 514 [11264/101520 (11%)] Loss: -1094.372314\n",
      "Train Epoch: 514 [22528/101520 (22%)] Loss: -1098.665283\n",
      "Train Epoch: 514 [33792/101520 (33%)] Loss: -1088.387207\n",
      "Train Epoch: 514 [45056/101520 (44%)] Loss: -1095.517700\n",
      "Train Epoch: 514 [56320/101520 (55%)] Loss: -1088.171875\n",
      "Train Epoch: 514 [67584/101520 (67%)] Loss: -1091.182739\n",
      "Train Epoch: 514 [78848/101520 (78%)] Loss: -1093.887695\n",
      "Train Epoch: 514 [90112/101520 (89%)] Loss: -1090.510254\n",
      "Train Epoch: 514 [101376/101520 (100%)] Loss: -1086.613647\n",
      "    epoch          : 514\n",
      "    loss           : -1090.545845070077\n",
      "    ess            : 1.9656438551955486\n",
      "    log_marginal   : 1090.5743892803864\n",
      "    log_joint      : 1298.7214797130182\n",
      "    val_loss       : -1089.8052023182745\n",
      "    val_ess        : 1.9657076960024626\n",
      "    val_log_marginal: 1089.8339100713315\n",
      "    val_log_joint  : 1297.6996911090353\n",
      "Train Epoch: 515 [0/101520 (0%)] Loss: -1084.857178\n",
      "Train Epoch: 515 [11264/101520 (11%)] Loss: -1087.169678\n",
      "Train Epoch: 515 [22528/101520 (22%)] Loss: -1091.406738\n",
      "Train Epoch: 515 [33792/101520 (33%)] Loss: -1090.079834\n",
      "Train Epoch: 515 [45056/101520 (44%)] Loss: -1093.980347\n",
      "Train Epoch: 515 [56320/101520 (55%)] Loss: -1082.599121\n",
      "Train Epoch: 515 [67584/101520 (67%)] Loss: -1083.926636\n",
      "Train Epoch: 515 [78848/101520 (78%)] Loss: -1093.433960\n",
      "Train Epoch: 515 [90112/101520 (89%)] Loss: -1087.517090\n",
      "Train Epoch: 515 [101376/101520 (100%)] Loss: -1096.063354\n",
      "    epoch          : 515\n",
      "    loss           : -1090.560316843004\n",
      "    ess            : 1.9653322151557884\n",
      "    log_marginal   : 1090.5889236220164\n",
      "    log_joint      : 1298.7432953340924\n",
      "    val_loss       : -1089.1336776069973\n",
      "    val_ess        : 1.9687105261761209\n",
      "    val_log_marginal: 1089.1579749065897\n",
      "    val_log_joint  : 1297.130763841712\n",
      "Train Epoch: 516 [0/101520 (0%)] Loss: -1089.009033\n",
      "Train Epoch: 516 [11264/101520 (11%)] Loss: -1086.279053\n",
      "Train Epoch: 516 [22528/101520 (22%)] Loss: -1086.401489\n",
      "Train Epoch: 516 [33792/101520 (33%)] Loss: -1091.388428\n",
      "Train Epoch: 516 [45056/101520 (44%)] Loss: -1093.655518\n",
      "Train Epoch: 516 [56320/101520 (55%)] Loss: -1095.249146\n",
      "Train Epoch: 516 [67584/101520 (67%)] Loss: -1095.624756\n",
      "Train Epoch: 516 [78848/101520 (78%)] Loss: -1088.019287\n",
      "Train Epoch: 516 [90112/101520 (89%)] Loss: -1094.275879\n",
      "Train Epoch: 516 [101376/101520 (100%)] Loss: -1099.146851\n",
      "    epoch          : 516\n",
      "    loss           : -1091.028005015311\n",
      "    ess            : 1.9652580052763973\n",
      "    log_marginal   : 1091.057212944606\n",
      "    log_joint      : 1299.132074557357\n",
      "    val_loss       : -1088.6544720193615\n",
      "    val_ess        : 1.9677126770434172\n",
      "    val_log_marginal: 1088.680610988451\n",
      "    val_log_joint  : 1296.4864077360733\n",
      "Train Epoch: 517 [0/101520 (0%)] Loss: -1091.077881\n",
      "Train Epoch: 517 [11264/101520 (11%)] Loss: -1090.182007\n",
      "Train Epoch: 517 [22528/101520 (22%)] Loss: -1093.601685\n",
      "Train Epoch: 517 [33792/101520 (33%)] Loss: -1096.934448\n",
      "Train Epoch: 517 [45056/101520 (44%)] Loss: -1098.150879\n",
      "Train Epoch: 517 [56320/101520 (55%)] Loss: -1099.868774\n",
      "Train Epoch: 517 [67584/101520 (67%)] Loss: -1086.709839\n",
      "Train Epoch: 517 [78848/101520 (78%)] Loss: -1090.279053\n",
      "Train Epoch: 517 [90112/101520 (89%)] Loss: -1102.832275\n",
      "Train Epoch: 517 [101376/101520 (100%)] Loss: -1107.400513\n",
      "    epoch          : 517\n",
      "    loss           : -1091.1723589873195\n",
      "    ess            : 1.965659771133308\n",
      "    log_marginal   : 1091.200827747134\n",
      "    log_joint      : 1299.374976690091\n",
      "    val_loss       : -1088.04468304178\n",
      "    val_ess        : 1.9641606859538867\n",
      "    val_log_marginal: 1088.0785336701767\n",
      "    val_log_joint  : 1295.9996815557065\n",
      "Train Epoch: 518 [0/101520 (0%)] Loss: -1092.379150\n",
      "Train Epoch: 518 [11264/101520 (11%)] Loss: -1089.195557\n",
      "Train Epoch: 518 [22528/101520 (22%)] Loss: -1082.248047\n",
      "Train Epoch: 518 [33792/101520 (33%)] Loss: -1090.584595\n",
      "Train Epoch: 518 [45056/101520 (44%)] Loss: -1081.493896\n",
      "Train Epoch: 518 [56320/101520 (55%)] Loss: -1091.960938\n",
      "Train Epoch: 518 [67584/101520 (67%)] Loss: -1086.838867\n",
      "Train Epoch: 518 [78848/101520 (78%)] Loss: -1088.557129\n",
      "Train Epoch: 518 [90112/101520 (89%)] Loss: -1089.863525\n",
      "Train Epoch: 518 [101376/101520 (100%)] Loss: -1093.156738\n",
      "    epoch          : 518\n",
      "    loss           : -1091.3058149634894\n",
      "    ess            : 1.9661161438304575\n",
      "    log_marginal   : 1091.333770291889\n",
      "    log_joint      : 1299.3138746712077\n",
      "    val_loss       : -1089.9031451681385\n",
      "    val_ess        : 1.9657444124636443\n",
      "    val_log_marginal: 1089.932813561481\n",
      "    val_log_joint  : 1298.2671323029892\n",
      "Train Epoch: 519 [0/101520 (0%)] Loss: -1097.802490\n",
      "Train Epoch: 519 [11264/101520 (11%)] Loss: -1102.170166\n",
      "Train Epoch: 519 [22528/101520 (22%)] Loss: -1092.457764\n",
      "Train Epoch: 519 [33792/101520 (33%)] Loss: -1089.314819\n",
      "Train Epoch: 519 [45056/101520 (44%)] Loss: -1089.995850\n",
      "Train Epoch: 519 [56320/101520 (55%)] Loss: -1086.391602\n",
      "Train Epoch: 519 [67584/101520 (67%)] Loss: -1091.886230\n",
      "Train Epoch: 519 [78848/101520 (78%)] Loss: -1096.614136\n",
      "Train Epoch: 519 [90112/101520 (89%)] Loss: -1086.563965\n",
      "Train Epoch: 519 [101376/101520 (100%)] Loss: -1096.786255\n",
      "    epoch          : 519\n",
      "    loss           : -1091.605919612712\n",
      "    ess            : 1.9658454872255948\n",
      "    log_marginal   : 1091.6335620975974\n",
      "    log_joint      : 1299.734859600738\n",
      "    val_loss       : -1090.8223399286685\n",
      "    val_ess        : 1.9631870622220247\n",
      "    val_log_marginal: 1090.8566682235055\n",
      "    val_log_joint  : 1299.0791227921195\n",
      "Train Epoch: 520 [0/101520 (0%)] Loss: -1092.586182\n",
      "Train Epoch: 520 [11264/101520 (11%)] Loss: -1090.185791\n",
      "Train Epoch: 520 [22528/101520 (22%)] Loss: -1090.047607\n",
      "Train Epoch: 520 [33792/101520 (33%)] Loss: -1094.257812\n",
      "Train Epoch: 520 [45056/101520 (44%)] Loss: -1088.518066\n",
      "Train Epoch: 520 [56320/101520 (55%)] Loss: -1097.383057\n",
      "Train Epoch: 520 [67584/101520 (67%)] Loss: -1089.419922\n",
      "Train Epoch: 520 [78848/101520 (78%)] Loss: -1086.688232\n",
      "Train Epoch: 520 [90112/101520 (89%)] Loss: -1092.301147\n",
      "Train Epoch: 520 [101376/101520 (100%)] Loss: -1086.351807\n",
      "    epoch          : 520\n",
      "    loss           : -1091.8036459151224\n",
      "    ess            : 1.9654399402177514\n",
      "    log_marginal   : 1091.832423224521\n",
      "    log_joint      : 1299.954064143962\n",
      "    val_loss       : -1091.3582922894022\n",
      "    val_ess        : 1.9672602726065593\n",
      "    val_log_marginal: 1091.3859067170517\n",
      "    val_log_joint  : 1299.4361519191575\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch520.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 521 [0/101520 (0%)] Loss: -1098.649780\n",
      "Train Epoch: 521 [11264/101520 (11%)] Loss: -1093.332275\n",
      "Train Epoch: 521 [22528/101520 (22%)] Loss: -1090.797852\n",
      "Train Epoch: 521 [33792/101520 (33%)] Loss: -1089.988892\n",
      "Train Epoch: 521 [45056/101520 (44%)] Loss: -1094.598633\n",
      "Train Epoch: 521 [56320/101520 (55%)] Loss: -1089.656860\n",
      "Train Epoch: 521 [67584/101520 (67%)] Loss: -1085.542480\n",
      "Train Epoch: 521 [78848/101520 (78%)] Loss: -1090.611816\n",
      "Train Epoch: 521 [90112/101520 (89%)] Loss: -1086.915283\n",
      "Train Epoch: 521 [101376/101520 (100%)] Loss: -1094.028198\n",
      "    epoch          : 521\n",
      "    loss           : -1091.804370362555\n",
      "    ess            : 1.9661319812937597\n",
      "    log_marginal   : 1091.832490700573\n",
      "    log_joint      : 1299.88318668538\n",
      "    val_loss       : -1091.4865510360055\n",
      "    val_ess        : 1.9629445490629778\n",
      "    val_log_marginal: 1091.5184166949728\n",
      "    val_log_joint  : 1299.6291079313858\n",
      "Train Epoch: 522 [0/101520 (0%)] Loss: -1090.904053\n",
      "Train Epoch: 522 [11264/101520 (11%)] Loss: -1097.654297\n",
      "Train Epoch: 522 [22528/101520 (22%)] Loss: -1085.841309\n",
      "Train Epoch: 522 [33792/101520 (33%)] Loss: -1088.813599\n",
      "Train Epoch: 522 [45056/101520 (44%)] Loss: -1091.248291\n",
      "Train Epoch: 522 [56320/101520 (55%)] Loss: -1090.831299\n",
      "Train Epoch: 522 [67584/101520 (67%)] Loss: -1098.085449\n",
      "Train Epoch: 522 [78848/101520 (78%)] Loss: -1088.807129\n",
      "Train Epoch: 522 [90112/101520 (89%)] Loss: -1097.624268\n",
      "Train Epoch: 522 [101376/101520 (100%)] Loss: -1094.797119\n",
      "    epoch          : 522\n",
      "    loss           : -1092.2717156338333\n",
      "    ess            : 1.9661021202652897\n",
      "    log_marginal   : 1092.2995771091787\n",
      "    log_joint      : 1300.370361328125\n",
      "    val_loss       : -1092.10424273947\n",
      "    val_ess        : 1.9668279575264973\n",
      "    val_log_marginal: 1092.1301694123642\n",
      "    val_log_joint  : 1300.1456723420517\n",
      "Train Epoch: 523 [0/101520 (0%)] Loss: -1093.130371\n",
      "Train Epoch: 523 [11264/101520 (11%)] Loss: -1094.708496\n",
      "Train Epoch: 523 [22528/101520 (22%)] Loss: -1095.767822\n",
      "Train Epoch: 523 [33792/101520 (33%)] Loss: -1097.430298\n",
      "Train Epoch: 523 [45056/101520 (44%)] Loss: -1093.162231\n",
      "Train Epoch: 523 [56320/101520 (55%)] Loss: -1085.660156\n",
      "Train Epoch: 523 [67584/101520 (67%)] Loss: -1095.245850\n",
      "Train Epoch: 523 [78848/101520 (78%)] Loss: -1096.620972\n",
      "Train Epoch: 523 [90112/101520 (89%)] Loss: -1089.417236\n",
      "Train Epoch: 523 [101376/101520 (100%)] Loss: -1088.498657\n",
      "    epoch          : 523\n",
      "    loss           : -1092.4377146965296\n",
      "    ess            : 1.966435505517164\n",
      "    log_marginal   : 1092.4651835839354\n",
      "    log_joint      : 1300.524132503337\n",
      "    val_loss       : -1091.4316512398098\n",
      "    val_ess        : 1.9662238100300664\n",
      "    val_log_marginal: 1091.4595522673233\n",
      "    val_log_joint  : 1299.2661769701087\n",
      "Train Epoch: 524 [0/101520 (0%)] Loss: -1083.453491\n",
      "Train Epoch: 524 [11264/101520 (11%)] Loss: -1093.377686\n",
      "Train Epoch: 524 [22528/101520 (22%)] Loss: -1097.543823\n",
      "Train Epoch: 524 [33792/101520 (33%)] Loss: -1095.176758\n",
      "Train Epoch: 524 [45056/101520 (44%)] Loss: -1086.690918\n",
      "Train Epoch: 524 [56320/101520 (55%)] Loss: -1091.265747\n",
      "Train Epoch: 524 [67584/101520 (67%)] Loss: -1090.328125\n",
      "Train Epoch: 524 [78848/101520 (78%)] Loss: -1099.560547\n",
      "Train Epoch: 524 [90112/101520 (89%)] Loss: -1101.169189\n",
      "Train Epoch: 524 [101376/101520 (100%)] Loss: -1100.907104\n",
      "    epoch          : 524\n",
      "    loss           : -1092.4297451613536\n",
      "    ess            : 1.9654560406603405\n",
      "    log_marginal   : 1092.4583844515546\n",
      "    log_joint      : 1300.5948989331423\n",
      "    val_loss       : -1092.5280443274457\n",
      "    val_ess        : 1.9672366950822913\n",
      "    val_log_marginal: 1092.5542841372283\n",
      "    val_log_joint  : 1300.5318921959918\n",
      "Train Epoch: 525 [0/101520 (0%)] Loss: -1087.723389\n",
      "Train Epoch: 525 [11264/101520 (11%)] Loss: -1095.285889\n",
      "Train Epoch: 525 [22528/101520 (22%)] Loss: -1092.978760\n",
      "Train Epoch: 525 [33792/101520 (33%)] Loss: -1097.069946\n",
      "Train Epoch: 525 [45056/101520 (44%)] Loss: -1091.669800\n",
      "Train Epoch: 525 [56320/101520 (55%)] Loss: -1090.823364\n",
      "Train Epoch: 525 [67584/101520 (67%)] Loss: -1097.151733\n",
      "Train Epoch: 525 [78848/101520 (78%)] Loss: -1090.129883\n",
      "Train Epoch: 525 [90112/101520 (89%)] Loss: -1093.389404\n",
      "Train Epoch: 525 [101376/101520 (100%)] Loss: -1097.066895\n",
      "    epoch          : 525\n",
      "    loss           : -1092.9360768687186\n",
      "    ess            : 1.9653373723054053\n",
      "    log_marginal   : 1092.965146778816\n",
      "    log_joint      : 1301.072693177803\n",
      "    val_loss       : -1091.9336680536685\n",
      "    val_ess        : 1.9665728392808333\n",
      "    val_log_marginal: 1091.9600299337635\n",
      "    val_log_joint  : 1300.3377579398777\n",
      "Train Epoch: 526 [0/101520 (0%)] Loss: -1097.899414\n",
      "Train Epoch: 526 [11264/101520 (11%)] Loss: -1100.284058\n",
      "Train Epoch: 526 [22528/101520 (22%)] Loss: -1087.450684\n",
      "Train Epoch: 526 [33792/101520 (33%)] Loss: -1091.639893\n",
      "Train Epoch: 526 [45056/101520 (44%)] Loss: -1091.856079\n",
      "Train Epoch: 526 [56320/101520 (55%)] Loss: -1102.176758\n",
      "Train Epoch: 526 [67584/101520 (67%)] Loss: -1090.522461\n",
      "Train Epoch: 526 [78848/101520 (78%)] Loss: -1091.319580\n",
      "Train Epoch: 526 [90112/101520 (89%)] Loss: -1087.660889\n",
      "Train Epoch: 526 [101376/101520 (100%)] Loss: -1102.940430\n",
      "    epoch          : 526\n",
      "    loss           : -1092.8781002178864\n",
      "    ess            : 1.9659368524599314\n",
      "    log_marginal   : 1092.9065346262562\n",
      "    log_joint      : 1301.0011385050252\n",
      "    val_loss       : -1093.250732421875\n",
      "    val_ess        : 1.9683881013289741\n",
      "    val_log_marginal: 1093.276314113451\n",
      "    val_log_joint  : 1301.0638799252717\n",
      "Train Epoch: 527 [0/101520 (0%)] Loss: -1092.919189\n",
      "Train Epoch: 527 [11264/101520 (11%)] Loss: -1094.061646\n",
      "Train Epoch: 527 [22528/101520 (22%)] Loss: -1094.883301\n",
      "Train Epoch: 527 [33792/101520 (33%)] Loss: -1089.561523\n",
      "Train Epoch: 527 [45056/101520 (44%)] Loss: -1092.075684\n",
      "Train Epoch: 527 [56320/101520 (55%)] Loss: -1094.810059\n",
      "Train Epoch: 527 [67584/101520 (67%)] Loss: -1094.247803\n",
      "Train Epoch: 527 [78848/101520 (78%)] Loss: -1096.041626\n",
      "Train Epoch: 527 [90112/101520 (89%)] Loss: -1092.048340\n",
      "Train Epoch: 527 [101376/101520 (100%)] Loss: -1087.664795\n",
      "    epoch          : 527\n",
      "    loss           : -1093.062903016057\n",
      "    ess            : 1.9656885185433393\n",
      "    log_marginal   : 1093.0910613860317\n",
      "    log_joint      : 1301.1995095104428\n",
      "    val_loss       : -1091.9278564453125\n",
      "    val_ess        : 1.9651456397512685\n",
      "    val_log_marginal: 1091.9556831691575\n",
      "    val_log_joint  : 1299.915962550951\n",
      "Train Epoch: 528 [0/101520 (0%)] Loss: -1091.757568\n",
      "Train Epoch: 528 [11264/101520 (11%)] Loss: -1089.512451\n",
      "Train Epoch: 528 [22528/101520 (22%)] Loss: -1094.666016\n",
      "Train Epoch: 528 [33792/101520 (33%)] Loss: -1094.301636\n",
      "Train Epoch: 528 [45056/101520 (44%)] Loss: -1093.345703\n",
      "Train Epoch: 528 [56320/101520 (55%)] Loss: -1090.033691\n",
      "Train Epoch: 528 [67584/101520 (67%)] Loss: -1091.913574\n",
      "Train Epoch: 528 [78848/101520 (78%)] Loss: -1101.688965\n",
      "Train Epoch: 528 [90112/101520 (89%)] Loss: -1094.662109\n",
      "Train Epoch: 528 [101376/101520 (100%)] Loss: -1095.421875\n",
      "    epoch          : 528\n",
      "    loss           : -1093.6103325465217\n",
      "    ess            : 1.9662938465425117\n",
      "    log_marginal   : 1093.6376634147298\n",
      "    log_joint      : 1301.6697930570822\n",
      "    val_loss       : -1093.347364342731\n",
      "    val_ess        : 1.9622044666953709\n",
      "    val_log_marginal: 1093.3788744055707\n",
      "    val_log_joint  : 1301.3451564622962\n",
      "Train Epoch: 529 [0/101520 (0%)] Loss: -1095.740723\n",
      "Train Epoch: 529 [11264/101520 (11%)] Loss: -1102.318604\n",
      "Train Epoch: 529 [22528/101520 (22%)] Loss: -1085.813965\n",
      "Train Epoch: 529 [33792/101520 (33%)] Loss: -1094.521240\n",
      "Train Epoch: 529 [45056/101520 (44%)] Loss: -1097.842041\n",
      "Train Epoch: 529 [56320/101520 (55%)] Loss: -1100.937256\n",
      "Train Epoch: 529 [67584/101520 (67%)] Loss: -1096.901855\n",
      "Train Epoch: 529 [78848/101520 (78%)] Loss: -1089.320435\n",
      "Train Epoch: 529 [90112/101520 (89%)] Loss: -1093.019897\n",
      "Train Epoch: 529 [101376/101520 (100%)] Loss: -1083.121704\n",
      "    epoch          : 529\n",
      "    loss           : -1093.2889238673838\n",
      "    ess            : 1.9664313218102383\n",
      "    log_marginal   : 1093.3168154002435\n",
      "    log_joint      : 1301.448525586919\n",
      "    val_loss       : -1092.1011909816575\n",
      "    val_ess        : 1.9684721231460571\n",
      "    val_log_marginal: 1092.127786387568\n",
      "    val_log_joint  : 1300.5930600373642\n",
      "Train Epoch: 530 [0/101520 (0%)] Loss: -1094.614990\n",
      "Train Epoch: 530 [11264/101520 (11%)] Loss: -1096.580933\n",
      "Train Epoch: 530 [22528/101520 (22%)] Loss: -1095.979370\n",
      "Train Epoch: 530 [33792/101520 (33%)] Loss: -1089.472168\n",
      "Train Epoch: 530 [45056/101520 (44%)] Loss: -1096.185303\n",
      "Train Epoch: 530 [56320/101520 (55%)] Loss: -1091.813477\n",
      "Train Epoch: 530 [67584/101520 (67%)] Loss: -1098.913330\n",
      "Train Epoch: 530 [78848/101520 (78%)] Loss: -1100.703247\n",
      "Train Epoch: 530 [90112/101520 (89%)] Loss: -1092.857422\n",
      "Train Epoch: 530 [101376/101520 (100%)] Loss: -1098.624878\n",
      "    epoch          : 530\n",
      "    loss           : -1093.6900364861417\n",
      "    ess            : 1.9658976577634188\n",
      "    log_marginal   : 1093.7183629328283\n",
      "    log_joint      : 1301.8135109139448\n",
      "    val_loss       : -1093.4935886548913\n",
      "    val_ess        : 1.9648172751716946\n",
      "    val_log_marginal: 1093.522805918818\n",
      "    val_log_joint  : 1301.9225171959918\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch530.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 531 [0/101520 (0%)] Loss: -1097.961670\n",
      "Train Epoch: 531 [11264/101520 (11%)] Loss: -1089.538086\n",
      "Train Epoch: 531 [22528/101520 (22%)] Loss: -1092.154907\n",
      "Train Epoch: 531 [33792/101520 (33%)] Loss: -1104.682861\n",
      "Train Epoch: 531 [45056/101520 (44%)] Loss: -1089.674561\n",
      "Train Epoch: 531 [56320/101520 (55%)] Loss: -1101.628662\n",
      "Train Epoch: 531 [67584/101520 (67%)] Loss: -1097.468628\n",
      "Train Epoch: 531 [78848/101520 (78%)] Loss: -1093.042114\n",
      "Train Epoch: 531 [90112/101520 (89%)] Loss: -1087.608398\n",
      "Train Epoch: 531 [101376/101520 (100%)] Loss: -1099.762695\n",
      "    epoch          : 531\n",
      "    loss           : -1094.1259360768688\n",
      "    ess            : 1.9660579289623241\n",
      "    log_marginal   : 1094.1535693604742\n",
      "    log_joint      : 1302.227958027442\n",
      "    val_loss       : -1094.127006199049\n",
      "    val_ess        : 1.966121538825657\n",
      "    val_log_marginal: 1094.153665293818\n",
      "    val_log_joint  : 1301.9688296110733\n",
      "Train Epoch: 532 [0/101520 (0%)] Loss: -1091.202148\n",
      "Train Epoch: 532 [11264/101520 (11%)] Loss: -1092.155396\n",
      "Train Epoch: 532 [22528/101520 (22%)] Loss: -1103.540894\n",
      "Train Epoch: 532 [33792/101520 (33%)] Loss: -1093.427734\n",
      "Train Epoch: 532 [45056/101520 (44%)] Loss: -1091.619141\n",
      "Train Epoch: 532 [56320/101520 (55%)] Loss: -1096.147217\n",
      "Train Epoch: 532 [67584/101520 (67%)] Loss: -1091.696289\n",
      "Train Epoch: 532 [78848/101520 (78%)] Loss: -1097.101440\n",
      "Train Epoch: 532 [90112/101520 (89%)] Loss: -1093.719360\n",
      "Train Epoch: 532 [101376/101520 (100%)] Loss: -1092.904541\n",
      "    epoch          : 532\n",
      "    loss           : -1093.9291433976523\n",
      "    ess            : 1.966485410479445\n",
      "    log_marginal   : 1093.956988310694\n",
      "    log_joint      : 1302.058802312343\n",
      "    val_loss       : -1094.0325556216033\n",
      "    val_ess        : 1.9674559157827627\n",
      "    val_log_marginal: 1094.0601275900135\n",
      "    val_log_joint  : 1301.9711489470108\n",
      "Train Epoch: 533 [0/101520 (0%)] Loss: -1094.931152\n",
      "Train Epoch: 533 [11264/101520 (11%)] Loss: -1096.185669\n",
      "Train Epoch: 533 [22528/101520 (22%)] Loss: -1090.359619\n",
      "Train Epoch: 533 [33792/101520 (33%)] Loss: -1091.261230\n",
      "Train Epoch: 533 [45056/101520 (44%)] Loss: -1090.008423\n",
      "Train Epoch: 533 [56320/101520 (55%)] Loss: -1096.004639\n",
      "Train Epoch: 533 [67584/101520 (67%)] Loss: -1098.181885\n",
      "Train Epoch: 533 [78848/101520 (78%)] Loss: -1095.046265\n",
      "Train Epoch: 533 [90112/101520 (89%)] Loss: -1096.734619\n",
      "Train Epoch: 533 [101376/101520 (100%)] Loss: -1101.980225\n",
      "    epoch          : 533\n",
      "    loss           : -1094.3086342356312\n",
      "    ess            : 1.9667043296536009\n",
      "    log_marginal   : 1094.336281627866\n",
      "    log_joint      : 1302.4193661176978\n",
      "    val_loss       : -1093.3425876783288\n",
      "    val_ess        : 1.9611057405886443\n",
      "    val_log_marginal: 1093.3778129245925\n",
      "    val_log_joint  : 1301.6330407184103\n",
      "Train Epoch: 534 [0/101520 (0%)] Loss: -1087.069214\n",
      "Train Epoch: 534 [11264/101520 (11%)] Loss: -1104.510742\n",
      "Train Epoch: 534 [22528/101520 (22%)] Loss: -1088.936035\n",
      "Train Epoch: 534 [33792/101520 (33%)] Loss: -1094.161255\n",
      "Train Epoch: 534 [45056/101520 (44%)] Loss: -1100.575684\n",
      "Train Epoch: 534 [56320/101520 (55%)] Loss: -1096.166504\n",
      "Train Epoch: 534 [67584/101520 (67%)] Loss: -1090.856445\n",
      "Train Epoch: 534 [78848/101520 (78%)] Loss: -1092.966797\n",
      "Train Epoch: 534 [90112/101520 (89%)] Loss: -1095.371338\n",
      "Train Epoch: 534 [101376/101520 (100%)] Loss: -1104.415405\n",
      "    epoch          : 534\n",
      "    loss           : -1094.7098946882852\n",
      "    ess            : 1.9663727031880287\n",
      "    log_marginal   : 1094.7377886748195\n",
      "    log_joint      : 1302.812177341787\n",
      "    val_loss       : -1094.9901123046875\n",
      "    val_ess        : 1.9662752047829006\n",
      "    val_log_marginal: 1095.0170473845108\n",
      "    val_log_joint  : 1303.2864353345788\n",
      "Train Epoch: 535 [0/101520 (0%)] Loss: -1093.161621\n",
      "Train Epoch: 535 [11264/101520 (11%)] Loss: -1093.907959\n",
      "Train Epoch: 535 [22528/101520 (22%)] Loss: -1089.041992\n",
      "Train Epoch: 535 [33792/101520 (33%)] Loss: -1094.963867\n",
      "Train Epoch: 535 [45056/101520 (44%)] Loss: -1105.831787\n",
      "Train Epoch: 535 [56320/101520 (55%)] Loss: -1091.808472\n",
      "Train Epoch: 535 [67584/101520 (67%)] Loss: -1094.706787\n",
      "Train Epoch: 535 [78848/101520 (78%)] Loss: -1099.846436\n",
      "Train Epoch: 535 [90112/101520 (89%)] Loss: -1089.699707\n",
      "Train Epoch: 535 [101376/101520 (100%)] Loss: -1088.771240\n",
      "    epoch          : 535\n",
      "    loss           : -1094.3776653040593\n",
      "    ess            : 1.9663313459511378\n",
      "    log_marginal   : 1094.405472798563\n",
      "    log_joint      : 1302.4693370416535\n",
      "    val_loss       : -1093.1915336277175\n",
      "    val_ess        : 1.9692267283149387\n",
      "    val_log_marginal: 1093.2159583050272\n",
      "    val_log_joint  : 1301.3286451256793\n",
      "Train Epoch: 536 [0/101520 (0%)] Loss: -1103.691895\n",
      "Train Epoch: 536 [11264/101520 (11%)] Loss: -1096.411377\n",
      "Train Epoch: 536 [22528/101520 (22%)] Loss: -1102.816406\n",
      "Train Epoch: 536 [33792/101520 (33%)] Loss: -1090.984741\n",
      "Train Epoch: 536 [45056/101520 (44%)] Loss: -1096.268066\n",
      "Train Epoch: 536 [56320/101520 (55%)] Loss: -1095.157471\n",
      "Train Epoch: 536 [67584/101520 (67%)] Loss: -1093.011841\n",
      "Train Epoch: 536 [78848/101520 (78%)] Loss: -1100.634644\n",
      "Train Epoch: 536 [90112/101520 (89%)] Loss: -1102.477295\n",
      "Train Epoch: 536 [101376/101520 (100%)] Loss: -1099.376831\n",
      "    epoch          : 536\n",
      "    loss           : -1094.9318878327183\n",
      "    ess            : 1.965811073480539\n",
      "    log_marginal   : 1094.9600744199513\n",
      "    log_joint      : 1303.0506395502905\n",
      "    val_loss       : -1094.864602793818\n",
      "    val_ess        : 1.967569164607836\n",
      "    val_log_marginal: 1094.8916280995245\n",
      "    val_log_joint  : 1302.7914455247962\n",
      "Train Epoch: 537 [0/101520 (0%)] Loss: -1085.411377\n",
      "Train Epoch: 537 [11264/101520 (11%)] Loss: -1089.203613\n",
      "Train Epoch: 537 [22528/101520 (22%)] Loss: -1092.860107\n",
      "Train Epoch: 537 [33792/101520 (33%)] Loss: -1093.798096\n",
      "Train Epoch: 537 [45056/101520 (44%)] Loss: -1098.983032\n",
      "Train Epoch: 537 [56320/101520 (55%)] Loss: -1087.156250\n",
      "Train Epoch: 537 [67584/101520 (67%)] Loss: -1096.749390\n",
      "Train Epoch: 537 [78848/101520 (78%)] Loss: -1095.225342\n",
      "Train Epoch: 537 [90112/101520 (89%)] Loss: -1096.769531\n",
      "Train Epoch: 537 [101376/101520 (100%)] Loss: -1083.714600\n",
      "    epoch          : 537\n",
      "    loss           : -1095.0444875745918\n",
      "    ess            : 1.96617703162246\n",
      "    log_marginal   : 1095.0726569861024\n",
      "    log_joint      : 1303.2235199434674\n",
      "    val_loss       : -1094.266357421875\n",
      "    val_ess        : 1.9649257867232612\n",
      "    val_log_marginal: 1094.294624660326\n",
      "    val_log_joint  : 1302.5745000424592\n",
      "Train Epoch: 538 [0/101520 (0%)] Loss: -1098.555908\n",
      "Train Epoch: 538 [11264/101520 (11%)] Loss: -1095.219849\n",
      "Train Epoch: 538 [22528/101520 (22%)] Loss: -1101.443970\n",
      "Train Epoch: 538 [33792/101520 (33%)] Loss: -1086.918335\n",
      "Train Epoch: 538 [45056/101520 (44%)] Loss: -1099.503662\n",
      "Train Epoch: 538 [56320/101520 (55%)] Loss: -1094.504150\n",
      "Train Epoch: 538 [67584/101520 (67%)] Loss: -1090.782959\n",
      "Train Epoch: 538 [78848/101520 (78%)] Loss: -1092.668335\n",
      "Train Epoch: 538 [90112/101520 (89%)] Loss: -1092.943848\n",
      "Train Epoch: 538 [101376/101520 (100%)] Loss: -1093.083984\n",
      "    epoch          : 538\n",
      "    loss           : -1095.2157479290986\n",
      "    ess            : 1.9663533199971646\n",
      "    log_marginal   : 1095.2438958709563\n",
      "    log_joint      : 1303.282957144119\n",
      "    val_loss       : -1093.9991985818615\n",
      "    val_ess        : 1.9647103962690935\n",
      "    val_log_marginal: 1094.029052734375\n",
      "    val_log_joint  : 1302.2043722401495\n",
      "Train Epoch: 539 [0/101520 (0%)] Loss: -1098.596191\n",
      "Train Epoch: 539 [11264/101520 (11%)] Loss: -1098.975952\n",
      "Train Epoch: 539 [22528/101520 (22%)] Loss: -1101.470947\n",
      "Train Epoch: 539 [33792/101520 (33%)] Loss: -1091.592285\n",
      "Train Epoch: 539 [45056/101520 (44%)] Loss: -1089.278809\n",
      "Train Epoch: 539 [56320/101520 (55%)] Loss: -1103.614014\n",
      "Train Epoch: 539 [67584/101520 (67%)] Loss: -1094.820801\n",
      "Train Epoch: 539 [78848/101520 (78%)] Loss: -1096.850830\n",
      "Train Epoch: 539 [90112/101520 (89%)] Loss: -1098.747314\n",
      "Train Epoch: 539 [101376/101520 (100%)] Loss: -1101.161499\n",
      "    epoch          : 539\n",
      "    loss           : -1095.7172919038553\n",
      "    ess            : 1.9666797312060793\n",
      "    log_marginal   : 1095.7443823119504\n",
      "    log_joint      : 1303.7921160980684\n",
      "    val_loss       : -1095.7923106317935\n",
      "    val_ess        : 1.9665428970171057\n",
      "    val_log_marginal: 1095.8199515964675\n",
      "    val_log_joint  : 1304.0764425526495\n",
      "Train Epoch: 540 [0/101520 (0%)] Loss: -1094.174316\n",
      "Train Epoch: 540 [11264/101520 (11%)] Loss: -1105.247803\n",
      "Train Epoch: 540 [22528/101520 (22%)] Loss: -1100.314941\n",
      "Train Epoch: 540 [33792/101520 (33%)] Loss: -1089.517822\n",
      "Train Epoch: 540 [45056/101520 (44%)] Loss: -1096.776123\n",
      "Train Epoch: 540 [56320/101520 (55%)] Loss: -1097.155273\n",
      "Train Epoch: 540 [67584/101520 (67%)] Loss: -1095.311035\n",
      "Train Epoch: 540 [78848/101520 (78%)] Loss: -1099.894531\n",
      "Train Epoch: 540 [90112/101520 (89%)] Loss: -1096.822998\n",
      "Train Epoch: 540 [101376/101520 (100%)] Loss: -1098.485229\n",
      "    epoch          : 540\n",
      "    loss           : -1095.7032630191975\n",
      "    ess            : 1.9669770224010525\n",
      "    log_marginal   : 1095.730411702065\n",
      "    log_joint      : 1303.8006193074748\n",
      "    val_loss       : -1095.2516187584918\n",
      "    val_ess        : 1.967119019964467\n",
      "    val_log_marginal: 1095.2786546790082\n",
      "    val_log_joint  : 1303.3527460512908\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch540.pth ...\n",
      "Train Epoch: 541 [0/101520 (0%)] Loss: -1097.284180\n",
      "Train Epoch: 541 [11264/101520 (11%)] Loss: -1090.337524\n",
      "Train Epoch: 541 [22528/101520 (22%)] Loss: -1097.383545\n",
      "Train Epoch: 541 [33792/101520 (33%)] Loss: -1093.751587\n",
      "Train Epoch: 541 [45056/101520 (44%)] Loss: -1097.021729\n",
      "Train Epoch: 541 [56320/101520 (55%)] Loss: -1091.189697\n",
      "Train Epoch: 541 [67584/101520 (67%)] Loss: -1101.865967\n",
      "Train Epoch: 541 [78848/101520 (78%)] Loss: -1091.615356\n",
      "Train Epoch: 541 [90112/101520 (89%)] Loss: -1093.578247\n",
      "Train Epoch: 541 [101376/101520 (100%)] Loss: -1092.353149\n",
      "    epoch          : 541\n",
      "    loss           : -1095.850890806572\n",
      "    ess            : 1.9661590400053628\n",
      "    log_marginal   : 1095.8796497134108\n",
      "    log_joint      : 1303.9847148339354\n",
      "    val_loss       : -1094.8259542713995\n",
      "    val_ess        : 1.966933830924656\n",
      "    val_log_marginal: 1094.8518650220788\n",
      "    val_log_joint  : 1303.2078538977582\n",
      "Train Epoch: 542 [0/101520 (0%)] Loss: -1095.599609\n",
      "Train Epoch: 542 [11264/101520 (11%)] Loss: -1099.462769\n",
      "Train Epoch: 542 [22528/101520 (22%)] Loss: -1095.944580\n",
      "Train Epoch: 542 [33792/101520 (33%)] Loss: -1093.677246\n",
      "Train Epoch: 542 [45056/101520 (44%)] Loss: -1092.631836\n",
      "Train Epoch: 542 [56320/101520 (55%)] Loss: -1091.486572\n",
      "Train Epoch: 542 [67584/101520 (67%)] Loss: -1090.967773\n",
      "Train Epoch: 542 [78848/101520 (78%)] Loss: -1100.525879\n",
      "Train Epoch: 542 [90112/101520 (89%)] Loss: -1095.531982\n",
      "Train Epoch: 542 [101376/101520 (100%)] Loss: -1093.744141\n",
      "    epoch          : 542\n",
      "    loss           : -1096.2824572079146\n",
      "    ess            : 1.9662066638170175\n",
      "    log_marginal   : 1096.3106566769393\n",
      "    log_joint      : 1304.4256285087547\n",
      "    val_loss       : -1094.4732984459918\n",
      "    val_ess        : 1.9666954382606174\n",
      "    val_log_marginal: 1094.5016665251358\n",
      "    val_log_joint  : 1302.6708294412365\n",
      "Train Epoch: 543 [0/101520 (0%)] Loss: -1096.635010\n",
      "Train Epoch: 543 [11264/101520 (11%)] Loss: -1087.928101\n",
      "Train Epoch: 543 [22528/101520 (22%)] Loss: -1095.233398\n",
      "Train Epoch: 543 [33792/101520 (33%)] Loss: -1095.492920\n",
      "Train Epoch: 543 [45056/101520 (44%)] Loss: -1093.809326\n",
      "Train Epoch: 543 [56320/101520 (55%)] Loss: -1102.622314\n",
      "Train Epoch: 543 [67584/101520 (67%)] Loss: -1094.775879\n",
      "Train Epoch: 543 [78848/101520 (78%)] Loss: -1096.372925\n",
      "Train Epoch: 543 [90112/101520 (89%)] Loss: -1097.338989\n",
      "Train Epoch: 543 [101376/101520 (100%)] Loss: -1095.400757\n",
      "    epoch          : 543\n",
      "    loss           : -1096.2073833523084\n",
      "    ess            : 1.9667351419602208\n",
      "    log_marginal   : 1096.2349215560223\n",
      "    log_joint      : 1304.3430838273398\n",
      "    val_loss       : -1095.5129606827445\n",
      "    val_ess        : 1.9636537717736287\n",
      "    val_log_marginal: 1095.542337168818\n",
      "    val_log_joint  : 1303.6632557744565\n",
      "Train Epoch: 544 [0/101520 (0%)] Loss: -1092.583984\n",
      "Train Epoch: 544 [11264/101520 (11%)] Loss: -1102.905396\n",
      "Train Epoch: 544 [22528/101520 (22%)] Loss: -1095.485718\n",
      "Train Epoch: 544 [33792/101520 (33%)] Loss: -1094.253662\n",
      "Train Epoch: 544 [45056/101520 (44%)] Loss: -1093.462891\n",
      "Train Epoch: 544 [56320/101520 (55%)] Loss: -1093.874023\n",
      "Train Epoch: 544 [67584/101520 (67%)] Loss: -1094.198730\n",
      "Train Epoch: 544 [78848/101520 (78%)] Loss: -1106.694580\n",
      "Train Epoch: 544 [90112/101520 (89%)] Loss: -1093.462769\n",
      "Train Epoch: 544 [101376/101520 (100%)] Loss: -1103.550049\n",
      "    epoch          : 544\n",
      "    loss           : -1096.5536244454695\n",
      "    ess            : 1.9671740705643468\n",
      "    log_marginal   : 1096.5809117609533\n",
      "    log_joint      : 1304.6739060291693\n",
      "    val_loss       : -1095.1427957286005\n",
      "    val_ess        : 1.9672396442164546\n",
      "    val_log_marginal: 1095.17016070822\n",
      "    val_log_joint  : 1303.4309188179348\n",
      "Train Epoch: 545 [0/101520 (0%)] Loss: -1098.250244\n",
      "Train Epoch: 545 [11264/101520 (11%)] Loss: -1087.860596\n",
      "Train Epoch: 545 [22528/101520 (22%)] Loss: -1102.544189\n",
      "Train Epoch: 545 [33792/101520 (33%)] Loss: -1086.438232\n",
      "Train Epoch: 545 [45056/101520 (44%)] Loss: -1097.399902\n",
      "Train Epoch: 545 [56320/101520 (55%)] Loss: -1100.389893\n",
      "Train Epoch: 545 [67584/101520 (67%)] Loss: -1094.981689\n",
      "Train Epoch: 545 [78848/101520 (78%)] Loss: -1097.333008\n",
      "Train Epoch: 545 [90112/101520 (89%)] Loss: -1097.218506\n",
      "Train Epoch: 545 [101376/101520 (100%)] Loss: -1099.890137\n",
      "    epoch          : 545\n",
      "    loss           : -1096.7494798209798\n",
      "    ess            : 1.967509346990729\n",
      "    log_marginal   : 1096.7764898712312\n",
      "    log_joint      : 1304.8691651617462\n",
      "    val_loss       : -1096.0051375679348\n",
      "    val_ess        : 1.9682768479637478\n",
      "    val_log_marginal: 1096.031685207201\n",
      "    val_log_joint  : 1304.1445896314538\n",
      "Train Epoch: 546 [0/101520 (0%)] Loss: -1095.801147\n",
      "Train Epoch: 546 [11264/101520 (11%)] Loss: -1100.078369\n",
      "Train Epoch: 546 [22528/101520 (22%)] Loss: -1093.504150\n",
      "Train Epoch: 546 [33792/101520 (33%)] Loss: -1097.100830\n",
      "Train Epoch: 546 [45056/101520 (44%)] Loss: -1092.761230\n",
      "Train Epoch: 546 [56320/101520 (55%)] Loss: -1094.866699\n",
      "Train Epoch: 546 [67584/101520 (67%)] Loss: -1101.553467\n",
      "Train Epoch: 546 [78848/101520 (78%)] Loss: -1100.281128\n",
      "Train Epoch: 546 [90112/101520 (89%)] Loss: -1097.220215\n",
      "Train Epoch: 546 [101376/101520 (100%)] Loss: -1098.222900\n",
      "    epoch          : 546\n",
      "    loss           : -1096.874538095752\n",
      "    ess            : 1.9658664620701392\n",
      "    log_marginal   : 1096.9034847086998\n",
      "    log_joint      : 1304.965798842847\n",
      "    val_loss       : -1095.7470119310462\n",
      "    val_ess        : 1.970035449318264\n",
      "    val_log_marginal: 1095.771091627038\n",
      "    val_log_joint  : 1303.8212519106658\n",
      "Train Epoch: 547 [0/101520 (0%)] Loss: -1098.110229\n",
      "Train Epoch: 547 [11264/101520 (11%)] Loss: -1092.511719\n",
      "Train Epoch: 547 [22528/101520 (22%)] Loss: -1097.499756\n",
      "Train Epoch: 547 [33792/101520 (33%)] Loss: -1101.771729\n",
      "Train Epoch: 547 [45056/101520 (44%)] Loss: -1098.144287\n",
      "Train Epoch: 547 [56320/101520 (55%)] Loss: -1094.246826\n",
      "Train Epoch: 547 [67584/101520 (67%)] Loss: -1097.752197\n",
      "Train Epoch: 547 [78848/101520 (78%)] Loss: -1096.379517\n",
      "Train Epoch: 547 [90112/101520 (89%)] Loss: -1102.145752\n",
      "Train Epoch: 547 [101376/101520 (100%)] Loss: -1104.900879\n",
      "    epoch          : 547\n",
      "    loss           : -1097.1217544018923\n",
      "    ess            : 1.96584725799273\n",
      "    log_marginal   : 1097.1501047719064\n",
      "    log_joint      : 1305.2963204695352\n",
      "    val_loss       : -1096.3219365658967\n",
      "    val_ess        : 1.9649053863857104\n",
      "    val_log_marginal: 1096.3518756368885\n",
      "    val_log_joint  : 1304.6052405315897\n",
      "Train Epoch: 548 [0/101520 (0%)] Loss: -1099.990479\n",
      "Train Epoch: 548 [11264/101520 (11%)] Loss: -1100.548950\n",
      "Train Epoch: 548 [22528/101520 (22%)] Loss: -1101.989624\n",
      "Train Epoch: 548 [33792/101520 (33%)] Loss: -1099.195435\n",
      "Train Epoch: 548 [45056/101520 (44%)] Loss: -1101.426392\n",
      "Train Epoch: 548 [56320/101520 (55%)] Loss: -1101.115723\n",
      "Train Epoch: 548 [67584/101520 (67%)] Loss: -1104.878662\n",
      "Train Epoch: 548 [78848/101520 (78%)] Loss: -1096.212158\n",
      "Train Epoch: 548 [90112/101520 (89%)] Loss: -1094.941162\n",
      "Train Epoch: 548 [101376/101520 (100%)] Loss: -1086.095703\n",
      "    epoch          : 548\n",
      "    loss           : -1097.2995139270572\n",
      "    ess            : 1.9673707670901888\n",
      "    log_marginal   : 1097.326316028384\n",
      "    log_joint      : 1305.4769219633322\n",
      "    val_loss       : -1097.0167607846467\n",
      "    val_ess        : 1.963688181794208\n",
      "    val_log_marginal: 1097.0514340608017\n",
      "    val_log_joint  : 1305.3092518682065\n",
      "Train Epoch: 549 [0/101520 (0%)] Loss: -1099.438965\n",
      "Train Epoch: 549 [11264/101520 (11%)] Loss: -1106.698242\n",
      "Train Epoch: 549 [22528/101520 (22%)] Loss: -1096.025879\n",
      "Train Epoch: 549 [33792/101520 (33%)] Loss: -1091.295410\n",
      "Train Epoch: 549 [45056/101520 (44%)] Loss: -1094.325195\n",
      "Train Epoch: 549 [56320/101520 (55%)] Loss: -1099.774414\n",
      "Train Epoch: 549 [67584/101520 (67%)] Loss: -1096.718628\n",
      "Train Epoch: 549 [78848/101520 (78%)] Loss: -1104.582642\n",
      "Train Epoch: 549 [90112/101520 (89%)] Loss: -1097.671753\n",
      "Train Epoch: 549 [101376/101520 (100%)] Loss: -1083.102661\n",
      "    epoch          : 549\n",
      "    loss           : -1097.3624034479035\n",
      "    ess            : 1.9673846229236929\n",
      "    log_marginal   : 1097.3893012425408\n",
      "    log_joint      : 1305.4707577192603\n",
      "    val_loss       : -1097.464307702106\n",
      "    val_ess        : 1.9656104626862898\n",
      "    val_log_marginal: 1097.4964175016983\n",
      "    val_log_joint  : 1305.7123439622962\n",
      "Train Epoch: 550 [0/101520 (0%)] Loss: -1094.746948\n",
      "Train Epoch: 550 [11264/101520 (11%)] Loss: -1104.398682\n",
      "Train Epoch: 550 [22528/101520 (22%)] Loss: -1096.840942\n",
      "Train Epoch: 550 [33792/101520 (33%)] Loss: -1099.291016\n",
      "Train Epoch: 550 [45056/101520 (44%)] Loss: -1091.516724\n",
      "Train Epoch: 550 [56320/101520 (55%)] Loss: -1099.708496\n",
      "Train Epoch: 550 [67584/101520 (67%)] Loss: -1105.614380\n",
      "Train Epoch: 550 [78848/101520 (78%)] Loss: -1099.913086\n",
      "Train Epoch: 550 [90112/101520 (89%)] Loss: -1093.759521\n",
      "Train Epoch: 550 [101376/101520 (100%)] Loss: -1095.305420\n",
      "    epoch          : 550\n",
      "    loss           : -1097.7154252708856\n",
      "    ess            : 1.967153587533002\n",
      "    log_marginal   : 1097.7426432700613\n",
      "    log_joint      : 1305.8342187009266\n",
      "    val_loss       : -1096.651223887568\n",
      "    val_ess        : 1.9662198605744734\n",
      "    val_log_marginal: 1096.6787321671195\n",
      "    val_log_joint  : 1304.6852868121603\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch550.pth ...\n",
      "Train Epoch: 551 [0/101520 (0%)] Loss: -1103.673218\n",
      "Train Epoch: 551 [11264/101520 (11%)] Loss: -1096.886719\n",
      "Train Epoch: 551 [22528/101520 (22%)] Loss: -1099.581055\n",
      "Train Epoch: 551 [33792/101520 (33%)] Loss: -1099.104370\n",
      "Train Epoch: 551 [45056/101520 (44%)] Loss: -1094.245361\n",
      "Train Epoch: 551 [56320/101520 (55%)] Loss: -1097.658325\n",
      "Train Epoch: 551 [67584/101520 (67%)] Loss: -1098.606812\n",
      "Train Epoch: 551 [78848/101520 (78%)] Loss: -1091.912109\n",
      "Train Epoch: 551 [90112/101520 (89%)] Loss: -1092.476318\n",
      "Train Epoch: 551 [101376/101520 (100%)] Loss: -1099.419556\n",
      "    epoch          : 551\n",
      "    loss           : -1097.8696178647142\n",
      "    ess            : 1.9663250877629572\n",
      "    log_marginal   : 1097.8981688226288\n",
      "    log_joint      : 1306.0292404404836\n",
      "    val_loss       : -1096.1688232421875\n",
      "    val_ess        : 1.964872526085895\n",
      "    val_log_marginal: 1096.1979768172555\n",
      "    val_log_joint  : 1304.9128099524457\n",
      "Train Epoch: 552 [0/101520 (0%)] Loss: -1100.264404\n",
      "Train Epoch: 552 [11264/101520 (11%)] Loss: -1092.217651\n",
      "Train Epoch: 552 [22528/101520 (22%)] Loss: -1092.899414\n",
      "Train Epoch: 552 [33792/101520 (33%)] Loss: -1101.412109\n",
      "Train Epoch: 552 [45056/101520 (44%)] Loss: -1098.614014\n",
      "Train Epoch: 552 [56320/101520 (55%)] Loss: -1102.153076\n",
      "Train Epoch: 552 [67584/101520 (67%)] Loss: -1099.526123\n",
      "Train Epoch: 552 [78848/101520 (78%)] Loss: -1103.419189\n",
      "Train Epoch: 552 [90112/101520 (89%)] Loss: -1102.109131\n",
      "Train Epoch: 552 [101376/101520 (100%)] Loss: -1105.383057\n",
      "    epoch          : 552\n",
      "    loss           : -1098.020769742266\n",
      "    ess            : 1.9670193836317589\n",
      "    log_marginal   : 1098.048542885325\n",
      "    log_joint      : 1306.1435430325455\n",
      "    val_loss       : -1096.755318019701\n",
      "    val_ess        : 1.9682263239570286\n",
      "    val_log_marginal: 1096.7826776919158\n",
      "    val_log_joint  : 1304.9355681046195\n",
      "Train Epoch: 553 [0/101520 (0%)] Loss: -1098.553101\n",
      "Train Epoch: 553 [11264/101520 (11%)] Loss: -1094.516846\n",
      "Train Epoch: 553 [22528/101520 (22%)] Loss: -1101.538330\n",
      "Train Epoch: 553 [33792/101520 (33%)] Loss: -1096.109863\n",
      "Train Epoch: 553 [45056/101520 (44%)] Loss: -1096.884888\n",
      "Train Epoch: 553 [56320/101520 (55%)] Loss: -1091.176514\n",
      "Train Epoch: 553 [67584/101520 (67%)] Loss: -1095.573486\n",
      "Train Epoch: 553 [78848/101520 (78%)] Loss: -1098.570435\n",
      "Train Epoch: 553 [90112/101520 (89%)] Loss: -1100.122070\n",
      "Train Epoch: 553 [101376/101520 (100%)] Loss: -1096.875977\n",
      "    epoch          : 553\n",
      "    loss           : -1098.2843520581423\n",
      "    ess            : 1.9671854481625197\n",
      "    log_marginal   : 1098.3119994503768\n",
      "    log_joint      : 1306.359505044755\n",
      "    val_loss       : -1099.7963177224865\n",
      "    val_ess        : 1.9699474935946257\n",
      "    val_log_marginal: 1099.8223717730978\n",
      "    val_log_joint  : 1307.7159529976223\n",
      "Train Epoch: 554 [0/101520 (0%)] Loss: -1101.885986\n",
      "Train Epoch: 554 [11264/101520 (11%)] Loss: -1106.806641\n",
      "Train Epoch: 554 [22528/101520 (22%)] Loss: -1100.320312\n",
      "Train Epoch: 554 [33792/101520 (33%)] Loss: -1100.861084\n",
      "Train Epoch: 554 [45056/101520 (44%)] Loss: -1104.038452\n",
      "Train Epoch: 554 [56320/101520 (55%)] Loss: -1099.480957\n",
      "Train Epoch: 554 [67584/101520 (67%)] Loss: -1095.746338\n",
      "Train Epoch: 554 [78848/101520 (78%)] Loss: -1094.368042\n",
      "Train Epoch: 554 [90112/101520 (89%)] Loss: -1096.383545\n",
      "Train Epoch: 554 [101376/101520 (100%)] Loss: -1098.558716\n",
      "    epoch          : 554\n",
      "    loss           : -1098.4865158311086\n",
      "    ess            : 1.9670596110760865\n",
      "    log_marginal   : 1098.513716654562\n",
      "    log_joint      : 1306.6017127875707\n",
      "    val_loss       : -1097.446294369905\n",
      "    val_ess        : 1.9663572415061619\n",
      "    val_log_marginal: 1097.4749755859375\n",
      "    val_log_joint  : 1305.624315344769\n",
      "Train Epoch: 555 [0/101520 (0%)] Loss: -1101.504395\n",
      "Train Epoch: 555 [11264/101520 (11%)] Loss: -1097.812500\n",
      "Train Epoch: 555 [22528/101520 (22%)] Loss: -1100.297241\n",
      "Train Epoch: 555 [33792/101520 (33%)] Loss: -1098.881592\n",
      "Train Epoch: 555 [45056/101520 (44%)] Loss: -1097.409912\n",
      "Train Epoch: 555 [56320/101520 (55%)] Loss: -1097.226562\n",
      "Train Epoch: 555 [67584/101520 (67%)] Loss: -1100.961914\n",
      "Train Epoch: 555 [78848/101520 (78%)] Loss: -1100.380859\n",
      "Train Epoch: 555 [90112/101520 (89%)] Loss: -1097.760620\n",
      "Train Epoch: 555 [101376/101520 (100%)] Loss: -1105.002197\n",
      "    epoch          : 555\n",
      "    loss           : -1098.442653330127\n",
      "    ess            : 1.9672556432647321\n",
      "    log_marginal   : 1098.4694995975974\n",
      "    log_joint      : 1306.6369359002042\n",
      "    val_loss       : -1096.9845607591712\n",
      "    val_ess        : 1.966743635094684\n",
      "    val_log_marginal: 1097.012647545856\n",
      "    val_log_joint  : 1305.0184909986413\n",
      "Train Epoch: 556 [0/101520 (0%)] Loss: -1103.352295\n",
      "Train Epoch: 556 [11264/101520 (11%)] Loss: -1093.073730\n",
      "Train Epoch: 556 [22528/101520 (22%)] Loss: -1090.195557\n",
      "Train Epoch: 556 [33792/101520 (33%)] Loss: -1102.771484\n",
      "Train Epoch: 556 [45056/101520 (44%)] Loss: -1102.278809\n",
      "Train Epoch: 556 [56320/101520 (55%)] Loss: -1098.646484\n",
      "Train Epoch: 556 [67584/101520 (67%)] Loss: -1095.544922\n",
      "Train Epoch: 556 [78848/101520 (78%)] Loss: -1107.082886\n",
      "Train Epoch: 556 [90112/101520 (89%)] Loss: -1105.503784\n",
      "Train Epoch: 556 [101376/101520 (100%)] Loss: -1111.711182\n",
      "    epoch          : 556\n",
      "    loss           : -1098.9487194272142\n",
      "    ess            : 1.967489879934033\n",
      "    log_marginal   : 1098.9758184231705\n",
      "    log_joint      : 1307.0090982255026\n",
      "    val_loss       : -1097.2489438264267\n",
      "    val_ess        : 1.96802368371383\n",
      "    val_log_marginal: 1097.277046535326\n",
      "    val_log_joint  : 1305.5582434612772\n",
      "Train Epoch: 557 [0/101520 (0%)] Loss: -1098.893066\n",
      "Train Epoch: 557 [11264/101520 (11%)] Loss: -1093.092773\n",
      "Train Epoch: 557 [22528/101520 (22%)] Loss: -1095.480103\n",
      "Train Epoch: 557 [33792/101520 (33%)] Loss: -1099.938110\n",
      "Train Epoch: 557 [45056/101520 (44%)] Loss: -1094.306641\n",
      "Train Epoch: 557 [56320/101520 (55%)] Loss: -1095.907227\n",
      "Train Epoch: 557 [67584/101520 (67%)] Loss: -1106.242188\n",
      "Train Epoch: 557 [78848/101520 (78%)] Loss: -1097.557129\n",
      "Train Epoch: 557 [90112/101520 (89%)] Loss: -1098.619141\n",
      "Train Epoch: 557 [101376/101520 (100%)] Loss: -1106.684937\n",
      "    epoch          : 557\n",
      "    loss           : -1098.984922169441\n",
      "    ess            : 1.96750338592721\n",
      "    log_marginal   : 1099.0121205392195\n",
      "    log_joint      : 1307.096512224207\n",
      "    val_loss       : -1096.492628014606\n",
      "    val_ess        : 1.9692771071973054\n",
      "    val_log_marginal: 1096.5198125424592\n",
      "    val_log_joint  : 1304.7371666949728\n",
      "Train Epoch: 558 [0/101520 (0%)] Loss: -1100.134766\n",
      "Train Epoch: 558 [11264/101520 (11%)] Loss: -1107.123291\n",
      "Train Epoch: 558 [22528/101520 (22%)] Loss: -1097.713989\n",
      "Train Epoch: 558 [33792/101520 (33%)] Loss: -1094.773193\n",
      "Train Epoch: 558 [45056/101520 (44%)] Loss: -1098.702271\n",
      "Train Epoch: 558 [56320/101520 (55%)] Loss: -1100.025513\n",
      "Train Epoch: 558 [67584/101520 (67%)] Loss: -1099.670410\n",
      "Train Epoch: 558 [78848/101520 (78%)] Loss: -1103.835449\n",
      "Train Epoch: 558 [90112/101520 (89%)] Loss: -1091.609375\n",
      "Train Epoch: 558 [101376/101520 (100%)] Loss: -1107.060059\n",
      "    epoch          : 558\n",
      "    loss           : -1099.0135338558025\n",
      "    ess            : 1.9674188749275017\n",
      "    log_marginal   : 1099.0404733629082\n",
      "    log_joint      : 1307.153774855724\n",
      "    val_loss       : -1098.3462975543478\n",
      "    val_ess        : 1.9645430938057278\n",
      "    val_log_marginal: 1098.374076511549\n",
      "    val_log_joint  : 1306.2522078804348\n",
      "Train Epoch: 559 [0/101520 (0%)] Loss: -1099.156982\n",
      "Train Epoch: 559 [11264/101520 (11%)] Loss: -1096.838867\n",
      "Train Epoch: 559 [22528/101520 (22%)] Loss: -1102.189941\n",
      "Train Epoch: 559 [33792/101520 (33%)] Loss: -1103.124268\n",
      "Train Epoch: 559 [45056/101520 (44%)] Loss: -1105.149902\n",
      "Train Epoch: 559 [56320/101520 (55%)] Loss: -1101.080322\n",
      "Train Epoch: 559 [67584/101520 (67%)] Loss: -1106.300903\n",
      "Train Epoch: 559 [78848/101520 (78%)] Loss: -1096.477905\n",
      "Train Epoch: 559 [90112/101520 (89%)] Loss: -1094.298096\n",
      "Train Epoch: 559 [101376/101520 (100%)] Loss: -1106.571655\n",
      "    epoch          : 559\n",
      "    loss           : -1099.428236764879\n",
      "    ess            : 1.9677633478413874\n",
      "    log_marginal   : 1099.4550400930434\n",
      "    log_joint      : 1307.584224835113\n",
      "    val_loss       : -1098.0392854110055\n",
      "    val_ess        : 1.9681273284165755\n",
      "    val_log_marginal: 1098.0656844429348\n",
      "    val_log_joint  : 1306.4159253991168\n",
      "Train Epoch: 560 [0/101520 (0%)] Loss: -1101.734375\n",
      "Train Epoch: 560 [11264/101520 (11%)] Loss: -1105.185913\n",
      "Train Epoch: 560 [22528/101520 (22%)] Loss: -1102.206177\n",
      "Train Epoch: 560 [33792/101520 (33%)] Loss: -1097.939575\n",
      "Train Epoch: 560 [45056/101520 (44%)] Loss: -1095.715820\n",
      "Train Epoch: 560 [56320/101520 (55%)] Loss: -1096.612305\n",
      "Train Epoch: 560 [67584/101520 (67%)] Loss: -1097.639526\n",
      "Train Epoch: 560 [78848/101520 (78%)] Loss: -1093.925293\n",
      "Train Epoch: 560 [90112/101520 (89%)] Loss: -1105.062256\n",
      "Train Epoch: 560 [101376/101520 (100%)] Loss: -1122.613892\n",
      "    epoch          : 560\n",
      "    loss           : -1099.5745254593278\n",
      "    ess            : 1.9676668649941833\n",
      "    log_marginal   : 1099.6018268834407\n",
      "    log_joint      : 1307.7183482107805\n",
      "    val_loss       : -1098.4747473675272\n",
      "    val_ess        : 1.9674152446829754\n",
      "    val_log_marginal: 1098.5009765625\n",
      "    val_log_joint  : 1306.7634595788043\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch560.pth ...\n",
      "Train Epoch: 561 [0/101520 (0%)] Loss: -1104.054688\n",
      "Train Epoch: 561 [11264/101520 (11%)] Loss: -1107.216187\n",
      "Train Epoch: 561 [22528/101520 (22%)] Loss: -1098.339111\n",
      "Train Epoch: 561 [33792/101520 (33%)] Loss: -1096.265869\n",
      "Train Epoch: 561 [45056/101520 (44%)] Loss: -1096.864014\n",
      "Train Epoch: 561 [56320/101520 (55%)] Loss: -1099.657227\n",
      "Train Epoch: 561 [67584/101520 (67%)] Loss: -1100.960449\n",
      "Train Epoch: 561 [78848/101520 (78%)] Loss: -1100.457520\n",
      "Train Epoch: 561 [90112/101520 (89%)] Loss: -1094.471191\n",
      "Train Epoch: 561 [101376/101520 (100%)] Loss: -1110.084229\n",
      "    epoch          : 561\n",
      "    loss           : -1099.6460838126177\n",
      "    ess            : 1.9674305232925031\n",
      "    log_marginal   : 1099.6732760482098\n",
      "    log_joint      : 1307.7447669254475\n",
      "    val_loss       : -1098.5208899456522\n",
      "    val_ess        : 1.9672354978063833\n",
      "    val_log_marginal: 1098.5464928668478\n",
      "    val_log_joint  : 1306.53003460428\n",
      "Train Epoch: 562 [0/101520 (0%)] Loss: -1097.085938\n",
      "Train Epoch: 562 [11264/101520 (11%)] Loss: -1101.891968\n",
      "Train Epoch: 562 [22528/101520 (22%)] Loss: -1099.839355\n",
      "Train Epoch: 562 [33792/101520 (33%)] Loss: -1096.807983\n",
      "Train Epoch: 562 [45056/101520 (44%)] Loss: -1098.190796\n",
      "Train Epoch: 562 [56320/101520 (55%)] Loss: -1096.387817\n",
      "Train Epoch: 562 [67584/101520 (67%)] Loss: -1099.243774\n",
      "Train Epoch: 562 [78848/101520 (78%)] Loss: -1094.538330\n",
      "Train Epoch: 562 [90112/101520 (89%)] Loss: -1106.050903\n",
      "Train Epoch: 562 [101376/101520 (100%)] Loss: -1089.326050\n",
      "    epoch          : 562\n",
      "    loss           : -1099.9314148869346\n",
      "    ess            : 1.9678301200195774\n",
      "    log_marginal   : 1099.9581280425566\n",
      "    log_joint      : 1308.1237891115734\n",
      "    val_loss       : -1098.9404721467392\n",
      "    val_ess        : 1.9673663740572722\n",
      "    val_log_marginal: 1098.9684474779212\n",
      "    val_log_joint  : 1307.2357655400815\n",
      "Train Epoch: 563 [0/101520 (0%)] Loss: -1108.175171\n",
      "Train Epoch: 563 [11264/101520 (11%)] Loss: -1098.961914\n",
      "Train Epoch: 563 [22528/101520 (22%)] Loss: -1101.301514\n",
      "Train Epoch: 563 [33792/101520 (33%)] Loss: -1104.179077\n",
      "Train Epoch: 563 [45056/101520 (44%)] Loss: -1103.161865\n",
      "Train Epoch: 563 [56320/101520 (55%)] Loss: -1102.593140\n",
      "Train Epoch: 563 [67584/101520 (67%)] Loss: -1091.697998\n",
      "Train Epoch: 563 [78848/101520 (78%)] Loss: -1099.974976\n",
      "Train Epoch: 563 [90112/101520 (89%)] Loss: -1097.124268\n",
      "Train Epoch: 563 [101376/101520 (100%)] Loss: -1111.348145\n",
      "    epoch          : 563\n",
      "    loss           : -1099.9086141154994\n",
      "    ess            : 1.9672987023789679\n",
      "    log_marginal   : 1099.935840702536\n",
      "    log_joint      : 1308.1197908487752\n",
      "    val_loss       : -1097.8335332455842\n",
      "    val_ess        : 1.9678025401156882\n",
      "    val_log_marginal: 1097.8593431555707\n",
      "    val_log_joint  : 1306.24829632303\n",
      "Train Epoch: 564 [0/101520 (0%)] Loss: -1096.378906\n",
      "Train Epoch: 564 [11264/101520 (11%)] Loss: -1097.648560\n",
      "Train Epoch: 564 [22528/101520 (22%)] Loss: -1104.255737\n",
      "Train Epoch: 564 [33792/101520 (33%)] Loss: -1094.275879\n",
      "Train Epoch: 564 [45056/101520 (44%)] Loss: -1092.968262\n",
      "Train Epoch: 564 [56320/101520 (55%)] Loss: -1100.452393\n",
      "Train Epoch: 564 [67584/101520 (67%)] Loss: -1101.138428\n",
      "Train Epoch: 564 [78848/101520 (78%)] Loss: -1096.952148\n",
      "Train Epoch: 564 [90112/101520 (89%)] Loss: -1103.759277\n",
      "Train Epoch: 564 [101376/101520 (100%)] Loss: -1103.718750\n",
      "    epoch          : 564\n",
      "    loss           : -1100.2714481832993\n",
      "    ess            : 1.9671005903176926\n",
      "    log_marginal   : 1100.298945901382\n",
      "    log_joint      : 1308.3206345447943\n",
      "    val_loss       : -1098.559273097826\n",
      "    val_ess        : 1.9686819470447043\n",
      "    val_log_marginal: 1098.5849025560462\n",
      "    val_log_joint  : 1306.564256751019\n",
      "Train Epoch: 565 [0/101520 (0%)] Loss: -1104.688965\n",
      "Train Epoch: 565 [11264/101520 (11%)] Loss: -1107.449707\n",
      "Train Epoch: 565 [22528/101520 (22%)] Loss: -1099.702881\n",
      "Train Epoch: 565 [33792/101520 (33%)] Loss: -1104.858154\n",
      "Train Epoch: 565 [45056/101520 (44%)] Loss: -1092.351074\n",
      "Train Epoch: 565 [56320/101520 (55%)] Loss: -1103.565918\n",
      "Train Epoch: 565 [67584/101520 (67%)] Loss: -1100.884277\n",
      "Train Epoch: 565 [78848/101520 (78%)] Loss: -1103.047852\n",
      "Train Epoch: 565 [90112/101520 (89%)] Loss: -1103.058716\n",
      "Train Epoch: 565 [101376/101520 (100%)] Loss: -1097.421631\n",
      "    epoch          : 565\n",
      "    loss           : -1100.4651240823257\n",
      "    ess            : 1.9679612981614156\n",
      "    log_marginal   : 1100.492240867423\n",
      "    log_joint      : 1308.5981396239008\n",
      "    val_loss       : -1099.4925590183425\n",
      "    val_ess        : 1.9678280923677527\n",
      "    val_log_marginal: 1099.5196639351223\n",
      "    val_log_joint  : 1307.4900220788043\n",
      "Train Epoch: 566 [0/101520 (0%)] Loss: -1106.152466\n",
      "Train Epoch: 566 [11264/101520 (11%)] Loss: -1098.724609\n",
      "Train Epoch: 566 [22528/101520 (22%)] Loss: -1100.400635\n",
      "Train Epoch: 566 [33792/101520 (33%)] Loss: -1106.160278\n",
      "Train Epoch: 566 [45056/101520 (44%)] Loss: -1089.786865\n",
      "Train Epoch: 566 [56320/101520 (55%)] Loss: -1106.195679\n",
      "Train Epoch: 566 [67584/101520 (67%)] Loss: -1097.502686\n",
      "Train Epoch: 566 [78848/101520 (78%)] Loss: -1106.100098\n",
      "Train Epoch: 566 [90112/101520 (89%)] Loss: -1100.944092\n",
      "Train Epoch: 566 [101376/101520 (100%)] Loss: -1093.058350\n",
      "    epoch          : 566\n",
      "    loss           : -1100.3472747035962\n",
      "    ess            : 1.9669977869819757\n",
      "    log_marginal   : 1100.374946632577\n",
      "    log_joint      : 1308.43614925212\n",
      "    val_loss       : -1101.160989512568\n",
      "    val_ess        : 1.9680753635323567\n",
      "    val_log_marginal: 1101.1900050951087\n",
      "    val_log_joint  : 1309.379490064538\n",
      "Train Epoch: 567 [0/101520 (0%)] Loss: -1098.599609\n",
      "Train Epoch: 567 [11264/101520 (11%)] Loss: -1096.479736\n",
      "Train Epoch: 567 [22528/101520 (22%)] Loss: -1104.161133\n",
      "Train Epoch: 567 [33792/101520 (33%)] Loss: -1092.666748\n",
      "Train Epoch: 567 [45056/101520 (44%)] Loss: -1102.808594\n",
      "Train Epoch: 567 [56320/101520 (55%)] Loss: -1097.288818\n",
      "Train Epoch: 567 [67584/101520 (67%)] Loss: -1104.552002\n",
      "Train Epoch: 567 [78848/101520 (78%)] Loss: -1104.341187\n",
      "Train Epoch: 567 [90112/101520 (89%)] Loss: -1098.615356\n",
      "Train Epoch: 567 [101376/101520 (100%)] Loss: -1096.181763\n",
      "    epoch          : 567\n",
      "    loss           : -1100.8251118875628\n",
      "    ess            : 1.9673064282192059\n",
      "    log_marginal   : 1100.852663586487\n",
      "    log_joint      : 1308.853275164887\n",
      "    val_loss       : -1100.183933423913\n",
      "    val_ess        : 1.9626495682674905\n",
      "    val_log_marginal: 1100.215385105299\n",
      "    val_log_joint  : 1308.673727284307\n",
      "Train Epoch: 568 [0/101520 (0%)] Loss: -1101.131104\n",
      "Train Epoch: 568 [11264/101520 (11%)] Loss: -1098.609619\n",
      "Train Epoch: 568 [22528/101520 (22%)] Loss: -1097.048950\n",
      "Train Epoch: 568 [33792/101520 (33%)] Loss: -1096.523193\n",
      "Train Epoch: 568 [45056/101520 (44%)] Loss: -1101.950195\n",
      "Train Epoch: 568 [56320/101520 (55%)] Loss: -1103.203247\n",
      "Train Epoch: 568 [67584/101520 (67%)] Loss: -1102.994629\n",
      "Train Epoch: 568 [78848/101520 (78%)] Loss: -1097.553223\n",
      "Train Epoch: 568 [90112/101520 (89%)] Loss: -1107.180420\n",
      "Train Epoch: 568 [101376/101520 (100%)] Loss: -1109.289917\n",
      "    epoch          : 568\n",
      "    loss           : -1100.9479256634736\n",
      "    ess            : 1.967830186513201\n",
      "    log_marginal   : 1100.9747388063363\n",
      "    log_joint      : 1309.0472823099874\n",
      "    val_loss       : -1098.778713060462\n",
      "    val_ess        : 1.9674683290979136\n",
      "    val_log_marginal: 1098.8047777258832\n",
      "    val_log_joint  : 1306.7017238451087\n",
      "Train Epoch: 569 [0/101520 (0%)] Loss: -1101.454346\n",
      "Train Epoch: 569 [11264/101520 (11%)] Loss: -1102.977051\n",
      "Train Epoch: 569 [22528/101520 (22%)] Loss: -1101.635986\n",
      "Train Epoch: 569 [33792/101520 (33%)] Loss: -1094.255005\n",
      "Train Epoch: 569 [45056/101520 (44%)] Loss: -1101.583252\n",
      "Train Epoch: 569 [56320/101520 (55%)] Loss: -1100.885376\n",
      "Train Epoch: 569 [67584/101520 (67%)] Loss: -1106.000488\n",
      "Train Epoch: 569 [78848/101520 (78%)] Loss: -1104.292236\n",
      "Train Epoch: 569 [90112/101520 (89%)] Loss: -1100.999878\n",
      "Train Epoch: 569 [101376/101520 (100%)] Loss: -1112.902954\n",
      "    epoch          : 569\n",
      "    loss           : -1100.8572145394944\n",
      "    ess            : 1.9675463042666566\n",
      "    log_marginal   : 1100.884373650479\n",
      "    log_joint      : 1309.0541249950927\n",
      "    val_loss       : -1098.2166854194973\n",
      "    val_ess        : 1.9680875643439915\n",
      "    val_log_marginal: 1098.243264903193\n",
      "    val_log_joint  : 1306.4578273607337\n",
      "Train Epoch: 570 [0/101520 (0%)] Loss: -1104.384521\n",
      "Train Epoch: 570 [11264/101520 (11%)] Loss: -1099.736084\n",
      "Train Epoch: 570 [22528/101520 (22%)] Loss: -1105.586548\n",
      "Train Epoch: 570 [33792/101520 (33%)] Loss: -1100.742432\n",
      "Train Epoch: 570 [45056/101520 (44%)] Loss: -1099.836914\n",
      "Train Epoch: 570 [56320/101520 (55%)] Loss: -1101.213501\n",
      "Train Epoch: 570 [67584/101520 (67%)] Loss: -1096.434326\n",
      "Train Epoch: 570 [78848/101520 (78%)] Loss: -1102.736328\n",
      "Train Epoch: 570 [90112/101520 (89%)] Loss: -1093.054199\n",
      "Train Epoch: 570 [101376/101520 (100%)] Loss: -1100.787354\n",
      "    epoch          : 570\n",
      "    loss           : -1101.31569897829\n",
      "    ess            : 1.9680089141855288\n",
      "    log_marginal   : 1101.3427415397298\n",
      "    log_joint      : 1309.4811073188207\n",
      "    val_loss       : -1100.9068178923233\n",
      "    val_ess        : 1.9691546585248865\n",
      "    val_log_marginal: 1100.932123598845\n",
      "    val_log_joint  : 1309.0862506368885\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch570.pth ...\n",
      "Train Epoch: 571 [0/101520 (0%)] Loss: -1104.603882\n",
      "Train Epoch: 571 [11264/101520 (11%)] Loss: -1105.022949\n",
      "Train Epoch: 571 [22528/101520 (22%)] Loss: -1106.102295\n",
      "Train Epoch: 571 [33792/101520 (33%)] Loss: -1097.519531\n",
      "Train Epoch: 571 [45056/101520 (44%)] Loss: -1103.123169\n",
      "Train Epoch: 571 [56320/101520 (55%)] Loss: -1104.077148\n",
      "Train Epoch: 571 [67584/101520 (67%)] Loss: -1110.180420\n",
      "Train Epoch: 571 [78848/101520 (78%)] Loss: -1102.691895\n",
      "Train Epoch: 571 [90112/101520 (89%)] Loss: -1102.616211\n",
      "Train Epoch: 571 [101376/101520 (100%)] Loss: -1106.090332\n",
      "    epoch          : 571\n",
      "    loss           : -1101.5891456795698\n",
      "    ess            : 1.967149977108941\n",
      "    log_marginal   : 1101.616301723461\n",
      "    log_joint      : 1309.6778950906878\n",
      "    val_loss       : -1100.9594567340353\n",
      "    val_ess        : 1.9668688203977502\n",
      "    val_log_marginal: 1100.9871295431385\n",
      "    val_log_joint  : 1309.0843877377717\n",
      "Train Epoch: 572 [0/101520 (0%)] Loss: -1104.952515\n",
      "Train Epoch: 572 [11264/101520 (11%)] Loss: -1105.138428\n",
      "Train Epoch: 572 [22528/101520 (22%)] Loss: -1098.762695\n",
      "Train Epoch: 572 [33792/101520 (33%)] Loss: -1104.653320\n",
      "Train Epoch: 572 [45056/101520 (44%)] Loss: -1103.930542\n",
      "Train Epoch: 572 [56320/101520 (55%)] Loss: -1099.035645\n",
      "Train Epoch: 572 [67584/101520 (67%)] Loss: -1102.818604\n",
      "Train Epoch: 572 [78848/101520 (78%)] Loss: -1097.627319\n",
      "Train Epoch: 572 [90112/101520 (89%)] Loss: -1106.292969\n",
      "Train Epoch: 572 [101376/101520 (100%)] Loss: -1109.975952\n",
      "    epoch          : 572\n",
      "    loss           : -1101.4202770444017\n",
      "    ess            : 1.967322683813584\n",
      "    log_marginal   : 1101.4481857529836\n",
      "    log_joint      : 1309.588097347087\n",
      "    val_loss       : -1099.6966871178668\n",
      "    val_ess        : 1.969331347424051\n",
      "    val_log_marginal: 1099.72192913553\n",
      "    val_log_joint  : 1307.809963060462\n",
      "Train Epoch: 573 [0/101520 (0%)] Loss: -1103.609863\n",
      "Train Epoch: 573 [11264/101520 (11%)] Loss: -1098.788330\n",
      "Train Epoch: 573 [22528/101520 (22%)] Loss: -1101.551270\n",
      "Train Epoch: 573 [33792/101520 (33%)] Loss: -1097.854858\n",
      "Train Epoch: 573 [45056/101520 (44%)] Loss: -1099.906738\n",
      "Train Epoch: 573 [56320/101520 (55%)] Loss: -1111.008301\n",
      "Train Epoch: 573 [67584/101520 (67%)] Loss: -1103.583618\n",
      "Train Epoch: 573 [78848/101520 (78%)] Loss: -1101.217529\n",
      "Train Epoch: 573 [90112/101520 (89%)] Loss: -1105.136230\n",
      "Train Epoch: 573 [101376/101520 (100%)] Loss: -1110.946411\n",
      "    epoch          : 573\n",
      "    loss           : -1101.878537585388\n",
      "    ess            : 1.967629096016812\n",
      "    log_marginal   : 1101.9064358658527\n",
      "    log_joint      : 1309.982945734532\n",
      "    val_loss       : -1100.666116465693\n",
      "    val_ess        : 1.9683838564416636\n",
      "    val_log_marginal: 1100.690376613451\n",
      "    val_log_joint  : 1308.6932426120925\n",
      "Train Epoch: 574 [0/101520 (0%)] Loss: -1100.924072\n",
      "Train Epoch: 574 [11264/101520 (11%)] Loss: -1102.003784\n",
      "Train Epoch: 574 [22528/101520 (22%)] Loss: -1100.763306\n",
      "Train Epoch: 574 [33792/101520 (33%)] Loss: -1102.324951\n",
      "Train Epoch: 574 [45056/101520 (44%)] Loss: -1102.844849\n",
      "Train Epoch: 574 [56320/101520 (55%)] Loss: -1096.817261\n",
      "Train Epoch: 574 [67584/101520 (67%)] Loss: -1100.982056\n",
      "Train Epoch: 574 [78848/101520 (78%)] Loss: -1104.289551\n",
      "Train Epoch: 574 [90112/101520 (89%)] Loss: -1108.003540\n",
      "Train Epoch: 574 [101376/101520 (100%)] Loss: -1104.330078\n",
      "    epoch          : 574\n",
      "    loss           : -1101.8927603103407\n",
      "    ess            : 1.966486361158553\n",
      "    log_marginal   : 1101.9214640095006\n",
      "    log_joint      : 1310.094283060812\n",
      "    val_loss       : -1100.4615106997283\n",
      "    val_ess        : 1.9693379350330518\n",
      "    val_log_marginal: 1100.48853069803\n",
      "    val_log_joint  : 1308.9973781419837\n",
      "Train Epoch: 575 [0/101520 (0%)] Loss: -1108.388184\n",
      "Train Epoch: 575 [11264/101520 (11%)] Loss: -1103.682617\n",
      "Train Epoch: 575 [22528/101520 (22%)] Loss: -1105.289307\n",
      "Train Epoch: 575 [33792/101520 (33%)] Loss: -1103.398438\n",
      "Train Epoch: 575 [45056/101520 (44%)] Loss: -1109.823853\n",
      "Train Epoch: 575 [56320/101520 (55%)] Loss: -1109.979370\n",
      "Train Epoch: 575 [67584/101520 (67%)] Loss: -1101.475708\n",
      "Train Epoch: 575 [78848/101520 (78%)] Loss: -1099.166260\n",
      "Train Epoch: 575 [90112/101520 (89%)] Loss: -1099.610596\n",
      "Train Epoch: 575 [101376/101520 (100%)] Loss: -1098.180786\n",
      "    epoch          : 575\n",
      "    loss           : -1101.9649020247723\n",
      "    ess            : 1.9676362935023093\n",
      "    log_marginal   : 1101.9925077045384\n",
      "    log_joint      : 1310.1042560213175\n",
      "    val_loss       : -1102.6134139351223\n",
      "    val_ess        : 1.9626256642134294\n",
      "    val_log_marginal: 1102.6426152768342\n",
      "    val_log_joint  : 1310.9649339758832\n",
      "Train Epoch: 576 [0/101520 (0%)] Loss: -1100.353271\n",
      "Train Epoch: 576 [11264/101520 (11%)] Loss: -1100.789307\n",
      "Train Epoch: 576 [22528/101520 (22%)] Loss: -1102.770264\n",
      "Train Epoch: 576 [33792/101520 (33%)] Loss: -1107.705322\n",
      "Train Epoch: 576 [45056/101520 (44%)] Loss: -1100.432251\n",
      "Train Epoch: 576 [56320/101520 (55%)] Loss: -1092.197021\n",
      "Train Epoch: 576 [67584/101520 (67%)] Loss: -1100.119141\n",
      "Train Epoch: 576 [78848/101520 (78%)] Loss: -1107.197021\n",
      "Train Epoch: 576 [90112/101520 (89%)] Loss: -1104.194458\n",
      "Train Epoch: 576 [101376/101520 (100%)] Loss: -1109.516479\n",
      "    epoch          : 576\n",
      "    loss           : -1102.4704829077025\n",
      "    ess            : 1.9671645913291815\n",
      "    log_marginal   : 1102.4982278335035\n",
      "    log_joint      : 1310.6284645885678\n",
      "    val_loss       : -1099.978807532269\n",
      "    val_ess        : 1.9671032895212588\n",
      "    val_log_marginal: 1100.0073029891305\n",
      "    val_log_joint  : 1308.197069251019\n",
      "Train Epoch: 577 [0/101520 (0%)] Loss: -1104.855835\n",
      "Train Epoch: 577 [11264/101520 (11%)] Loss: -1106.128174\n",
      "Train Epoch: 577 [22528/101520 (22%)] Loss: -1094.483643\n",
      "Train Epoch: 577 [33792/101520 (33%)] Loss: -1101.672852\n",
      "Train Epoch: 577 [45056/101520 (44%)] Loss: -1103.871460\n",
      "Train Epoch: 577 [56320/101520 (55%)] Loss: -1100.164551\n",
      "Train Epoch: 577 [67584/101520 (67%)] Loss: -1099.666626\n",
      "Train Epoch: 577 [78848/101520 (78%)] Loss: -1101.206543\n",
      "Train Epoch: 577 [90112/101520 (89%)] Loss: -1098.255981\n",
      "Train Epoch: 577 [101376/101520 (100%)] Loss: -1095.998779\n",
      "    epoch          : 577\n",
      "    loss           : -1102.4040318781408\n",
      "    ess            : 1.9674481261315657\n",
      "    log_marginal   : 1102.4315314364792\n",
      "    log_joint      : 1310.5784771023084\n",
      "    val_loss       : -1099.9863865064538\n",
      "    val_ess        : 1.968154648075933\n",
      "    val_log_marginal: 1100.0115117612092\n",
      "    val_log_joint  : 1308.4380519701087\n",
      "Train Epoch: 578 [0/101520 (0%)] Loss: -1102.510254\n",
      "Train Epoch: 578 [11264/101520 (11%)] Loss: -1097.612305\n",
      "Train Epoch: 578 [22528/101520 (22%)] Loss: -1107.313721\n",
      "Train Epoch: 578 [33792/101520 (33%)] Loss: -1106.334473\n",
      "Train Epoch: 578 [45056/101520 (44%)] Loss: -1100.875000\n",
      "Train Epoch: 578 [56320/101520 (55%)] Loss: -1107.882568\n",
      "Train Epoch: 578 [67584/101520 (67%)] Loss: -1102.958496\n",
      "Train Epoch: 578 [78848/101520 (78%)] Loss: -1099.861572\n",
      "Train Epoch: 578 [90112/101520 (89%)] Loss: -1100.065674\n",
      "Train Epoch: 578 [101376/101520 (100%)] Loss: -1110.554443\n",
      "    epoch          : 578\n",
      "    loss           : -1102.6737023741755\n",
      "    ess            : 1.9675069023017309\n",
      "    log_marginal   : 1102.701305600267\n",
      "    log_joint      : 1310.859684776421\n",
      "    val_loss       : -1101.8327795940897\n",
      "    val_ess        : 1.9706420794777249\n",
      "    val_log_marginal: 1101.8551078464675\n",
      "    val_log_joint  : 1309.9514372452445\n",
      "Train Epoch: 579 [0/101520 (0%)] Loss: -1102.003540\n",
      "Train Epoch: 579 [11264/101520 (11%)] Loss: -1101.744751\n",
      "Train Epoch: 579 [22528/101520 (22%)] Loss: -1104.008057\n",
      "Train Epoch: 579 [33792/101520 (33%)] Loss: -1097.099365\n",
      "Train Epoch: 579 [45056/101520 (44%)] Loss: -1110.442139\n",
      "Train Epoch: 579 [56320/101520 (55%)] Loss: -1102.610596\n",
      "Train Epoch: 579 [67584/101520 (67%)] Loss: -1097.756836\n",
      "Train Epoch: 579 [78848/101520 (78%)] Loss: -1103.486572\n",
      "Train Epoch: 579 [90112/101520 (89%)] Loss: -1106.988037\n",
      "Train Epoch: 579 [101376/101520 (100%)] Loss: -1097.397461\n",
      "    epoch          : 579\n",
      "    loss           : -1103.0231853849325\n",
      "    ess            : 1.9678378830003977\n",
      "    log_marginal   : 1103.0504671796482\n",
      "    log_joint      : 1311.1763186047424\n",
      "    val_loss       : -1101.6822987432065\n",
      "    val_ess        : 1.9703669548034668\n",
      "    val_log_marginal: 1101.7078698199728\n",
      "    val_log_joint  : 1309.8442117442255\n",
      "Train Epoch: 580 [0/101520 (0%)] Loss: -1102.896973\n",
      "Train Epoch: 580 [11264/101520 (11%)] Loss: -1097.431885\n",
      "Train Epoch: 580 [22528/101520 (22%)] Loss: -1098.506592\n",
      "Train Epoch: 580 [33792/101520 (33%)] Loss: -1109.739990\n",
      "Train Epoch: 580 [45056/101520 (44%)] Loss: -1104.182129\n",
      "Train Epoch: 580 [56320/101520 (55%)] Loss: -1103.613525\n",
      "Train Epoch: 580 [67584/101520 (67%)] Loss: -1105.146240\n",
      "Train Epoch: 580 [78848/101520 (78%)] Loss: -1103.791016\n",
      "Train Epoch: 580 [90112/101520 (89%)] Loss: -1099.709717\n",
      "Train Epoch: 580 [101376/101520 (100%)] Loss: -1100.321533\n",
      "    epoch          : 580\n",
      "    loss           : -1102.9855128916065\n",
      "    ess            : 1.9677852535966653\n",
      "    log_marginal   : 1103.0125658811637\n",
      "    log_joint      : 1311.0855804903424\n",
      "    val_loss       : -1100.3598951256793\n",
      "    val_ess        : 1.9723384069359822\n",
      "    val_log_marginal: 1100.3821649966033\n",
      "    val_log_joint  : 1308.51343304178\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch580.pth ...\n",
      "Train Epoch: 581 [0/101520 (0%)] Loss: -1109.547974\n",
      "Train Epoch: 581 [11264/101520 (11%)] Loss: -1103.400024\n",
      "Train Epoch: 581 [22528/101520 (22%)] Loss: -1104.571655\n",
      "Train Epoch: 581 [33792/101520 (33%)] Loss: -1101.185791\n",
      "Train Epoch: 581 [45056/101520 (44%)] Loss: -1107.240112\n",
      "Train Epoch: 581 [56320/101520 (55%)] Loss: -1102.321167\n",
      "Train Epoch: 581 [67584/101520 (67%)] Loss: -1101.121582\n",
      "Train Epoch: 581 [78848/101520 (78%)] Loss: -1104.402466\n",
      "Train Epoch: 581 [90112/101520 (89%)] Loss: -1104.436768\n",
      "Train Epoch: 581 [101376/101520 (100%)] Loss: -1097.732178\n",
      "    epoch          : 581\n",
      "    loss           : -1103.2051376266097\n",
      "    ess            : 1.9677665916519549\n",
      "    log_marginal   : 1103.2325666418028\n",
      "    log_joint      : 1311.3643939914416\n",
      "    val_loss       : -1102.8577031674592\n",
      "    val_ess        : 1.9697699961454973\n",
      "    val_log_marginal: 1102.8824781334918\n",
      "    val_log_joint  : 1310.88109820822\n",
      "Train Epoch: 582 [0/101520 (0%)] Loss: -1096.698730\n",
      "Train Epoch: 582 [11264/101520 (11%)] Loss: -1105.397339\n",
      "Train Epoch: 582 [22528/101520 (22%)] Loss: -1102.791260\n",
      "Train Epoch: 582 [33792/101520 (33%)] Loss: -1103.803955\n",
      "Train Epoch: 582 [45056/101520 (44%)] Loss: -1107.053345\n",
      "Train Epoch: 582 [56320/101520 (55%)] Loss: -1104.000244\n",
      "Train Epoch: 582 [67584/101520 (67%)] Loss: -1104.008545\n",
      "Train Epoch: 582 [78848/101520 (78%)] Loss: -1110.433350\n",
      "Train Epoch: 582 [90112/101520 (89%)] Loss: -1103.546753\n",
      "Train Epoch: 582 [101376/101520 (100%)] Loss: -1103.107666\n",
      "    epoch          : 582\n",
      "    loss           : -1103.6297337517667\n",
      "    ess            : 1.967597019732298\n",
      "    log_marginal   : 1103.6571879171247\n",
      "    log_joint      : 1311.7162386640232\n",
      "    val_loss       : -1101.5955332880435\n",
      "    val_ess        : 1.961114183716152\n",
      "    val_log_marginal: 1101.6282905910325\n",
      "    val_log_joint  : 1309.717332922894\n",
      "Train Epoch: 583 [0/101520 (0%)] Loss: -1105.641357\n",
      "Train Epoch: 583 [11264/101520 (11%)] Loss: -1104.269531\n",
      "Train Epoch: 583 [22528/101520 (22%)] Loss: -1098.657959\n",
      "Train Epoch: 583 [33792/101520 (33%)] Loss: -1099.639282\n",
      "Train Epoch: 583 [45056/101520 (44%)] Loss: -1104.572510\n",
      "Train Epoch: 583 [56320/101520 (55%)] Loss: -1102.404297\n",
      "Train Epoch: 583 [67584/101520 (67%)] Loss: -1101.642334\n",
      "Train Epoch: 583 [78848/101520 (78%)] Loss: -1103.681152\n",
      "Train Epoch: 583 [90112/101520 (89%)] Loss: -1099.611816\n",
      "Train Epoch: 583 [101376/101520 (100%)] Loss: -1104.327393\n",
      "    epoch          : 583\n",
      "    loss           : -1103.409020812068\n",
      "    ess            : 1.9683116087362396\n",
      "    log_marginal   : 1103.436029635482\n",
      "    log_joint      : 1311.5910123125393\n",
      "    val_loss       : -1101.8010466202445\n",
      "    val_ess        : 1.966654000074967\n",
      "    val_log_marginal: 1101.8338251528533\n",
      "    val_log_joint  : 1310.0999862007473\n",
      "Train Epoch: 584 [0/101520 (0%)] Loss: -1105.415039\n",
      "Train Epoch: 584 [11264/101520 (11%)] Loss: -1103.363281\n",
      "Train Epoch: 584 [22528/101520 (22%)] Loss: -1109.595459\n",
      "Train Epoch: 584 [33792/101520 (33%)] Loss: -1101.951660\n",
      "Train Epoch: 584 [45056/101520 (44%)] Loss: -1097.227539\n",
      "Train Epoch: 584 [56320/101520 (55%)] Loss: -1101.525879\n",
      "Train Epoch: 584 [67584/101520 (67%)] Loss: -1105.326294\n",
      "Train Epoch: 584 [78848/101520 (78%)] Loss: -1101.470703\n",
      "Train Epoch: 584 [90112/101520 (89%)] Loss: -1099.804443\n",
      "Train Epoch: 584 [101376/101520 (100%)] Loss: -1097.959595\n",
      "    epoch          : 584\n",
      "    loss           : -1103.7746692446608\n",
      "    ess            : 1.9685050237118897\n",
      "    log_marginal   : 1103.8008548602386\n",
      "    log_joint      : 1311.9509449100974\n",
      "    val_loss       : -1103.4855585512908\n",
      "    val_ess        : 1.9707928636799688\n",
      "    val_log_marginal: 1103.5114003057065\n",
      "    val_log_joint  : 1311.6165877632473\n",
      "Train Epoch: 585 [0/101520 (0%)] Loss: -1098.158203\n",
      "Train Epoch: 585 [11264/101520 (11%)] Loss: -1112.954712\n",
      "Train Epoch: 585 [22528/101520 (22%)] Loss: -1101.021240\n",
      "Train Epoch: 585 [33792/101520 (33%)] Loss: -1104.501221\n",
      "Train Epoch: 585 [45056/101520 (44%)] Loss: -1101.884644\n",
      "Train Epoch: 585 [56320/101520 (55%)] Loss: -1106.864990\n",
      "Train Epoch: 585 [67584/101520 (67%)] Loss: -1107.284912\n",
      "Train Epoch: 585 [78848/101520 (78%)] Loss: -1105.791016\n",
      "Train Epoch: 585 [90112/101520 (89%)] Loss: -1107.039551\n",
      "Train Epoch: 585 [101376/101520 (100%)] Loss: -1104.796021\n",
      "    epoch          : 585\n",
      "    loss           : -1104.0290416928392\n",
      "    ess            : 1.9683897159806447\n",
      "    log_marginal   : 1104.056132714353\n",
      "    log_joint      : 1312.2259828193703\n",
      "    val_loss       : -1103.0474375849185\n",
      "    val_ess        : 1.9679209201232246\n",
      "    val_log_marginal: 1103.0728229025135\n",
      "    val_log_joint  : 1311.1299677309783\n",
      "Train Epoch: 586 [0/101520 (0%)] Loss: -1108.970703\n",
      "Train Epoch: 586 [11264/101520 (11%)] Loss: -1108.067139\n",
      "Train Epoch: 586 [22528/101520 (22%)] Loss: -1105.038574\n",
      "Train Epoch: 586 [33792/101520 (33%)] Loss: -1101.066162\n",
      "Train Epoch: 586 [45056/101520 (44%)] Loss: -1105.763916\n",
      "Train Epoch: 586 [56320/101520 (55%)] Loss: -1099.906616\n",
      "Train Epoch: 586 [67584/101520 (67%)] Loss: -1102.213257\n",
      "Train Epoch: 586 [78848/101520 (78%)] Loss: -1103.030518\n",
      "Train Epoch: 586 [90112/101520 (89%)] Loss: -1103.270996\n",
      "Train Epoch: 586 [101376/101520 (100%)] Loss: -1109.376587\n",
      "    epoch          : 586\n",
      "    loss           : -1104.0328025626177\n",
      "    ess            : 1.9678688935898057\n",
      "    log_marginal   : 1104.0596991304178\n",
      "    log_joint      : 1312.278489616049\n",
      "    val_loss       : -1103.1469408118207\n",
      "    val_ess        : 1.9669627314028533\n",
      "    val_log_marginal: 1103.1740351137908\n",
      "    val_log_joint  : 1311.6613451086957\n",
      "Train Epoch: 587 [0/101520 (0%)] Loss: -1107.650635\n",
      "Train Epoch: 587 [11264/101520 (11%)] Loss: -1103.526001\n",
      "Train Epoch: 587 [22528/101520 (22%)] Loss: -1107.346924\n",
      "Train Epoch: 587 [33792/101520 (33%)] Loss: -1109.331787\n",
      "Train Epoch: 587 [45056/101520 (44%)] Loss: -1103.096436\n",
      "Train Epoch: 587 [56320/101520 (55%)] Loss: -1102.228760\n",
      "Train Epoch: 587 [67584/101520 (67%)] Loss: -1097.914673\n",
      "Train Epoch: 587 [78848/101520 (78%)] Loss: -1104.255493\n",
      "Train Epoch: 587 [90112/101520 (89%)] Loss: -1097.017944\n",
      "Train Epoch: 587 [101376/101520 (100%)] Loss: -1107.902832\n",
      "    epoch          : 587\n",
      "    loss           : -1104.435028536236\n",
      "    ess            : 1.968309446794903\n",
      "    log_marginal   : 1104.4618698963568\n",
      "    log_joint      : 1312.5867901519316\n",
      "    val_loss       : -1104.3254659901495\n",
      "    val_ess        : 1.966799523519433\n",
      "    val_log_marginal: 1104.3531600288723\n",
      "    val_log_joint  : 1312.493657651155\n",
      "Train Epoch: 588 [0/101520 (0%)] Loss: -1112.105225\n",
      "Train Epoch: 588 [11264/101520 (11%)] Loss: -1103.543457\n",
      "Train Epoch: 588 [22528/101520 (22%)] Loss: -1106.539795\n",
      "Train Epoch: 588 [33792/101520 (33%)] Loss: -1104.275879\n",
      "Train Epoch: 588 [45056/101520 (44%)] Loss: -1100.719238\n",
      "Train Epoch: 588 [56320/101520 (55%)] Loss: -1099.709351\n",
      "Train Epoch: 588 [67584/101520 (67%)] Loss: -1102.851562\n",
      "Train Epoch: 588 [78848/101520 (78%)] Loss: -1110.181152\n",
      "Train Epoch: 588 [90112/101520 (89%)] Loss: -1104.589233\n",
      "Train Epoch: 588 [101376/101520 (100%)] Loss: -1099.619507\n",
      "    epoch          : 588\n",
      "    loss           : -1104.5893456540516\n",
      "    ess            : 1.968144007663631\n",
      "    log_marginal   : 1104.616520713921\n",
      "    log_joint      : 1312.7382413777873\n",
      "    val_loss       : -1103.3216181216033\n",
      "    val_ess        : 1.9704237191573433\n",
      "    val_log_marginal: 1103.3454271399457\n",
      "    val_log_joint  : 1311.4824855638587\n",
      "Train Epoch: 589 [0/101520 (0%)] Loss: -1114.023682\n",
      "Train Epoch: 589 [11264/101520 (11%)] Loss: -1103.290283\n",
      "Train Epoch: 589 [22528/101520 (22%)] Loss: -1102.427246\n",
      "Train Epoch: 589 [33792/101520 (33%)] Loss: -1099.844971\n",
      "Train Epoch: 589 [45056/101520 (44%)] Loss: -1104.911987\n",
      "Train Epoch: 589 [56320/101520 (55%)] Loss: -1108.901123\n",
      "Train Epoch: 589 [67584/101520 (67%)] Loss: -1106.497559\n",
      "Train Epoch: 589 [78848/101520 (78%)] Loss: -1108.275635\n",
      "Train Epoch: 589 [90112/101520 (89%)] Loss: -1099.316040\n",
      "Train Epoch: 589 [101376/101520 (100%)] Loss: -1115.496704\n",
      "    epoch          : 589\n",
      "    loss           : -1104.5509640487594\n",
      "    ess            : 1.9688199722587163\n",
      "    log_marginal   : 1104.5769711595085\n",
      "    log_joint      : 1312.717582664298\n",
      "    val_loss       : -1104.6697148862092\n",
      "    val_ess        : 1.9718303680419922\n",
      "    val_log_marginal: 1104.6929029381793\n",
      "    val_log_joint  : 1312.6053785241168\n",
      "Train Epoch: 590 [0/101520 (0%)] Loss: -1116.066406\n",
      "Train Epoch: 590 [11264/101520 (11%)] Loss: -1101.924072\n",
      "Train Epoch: 590 [22528/101520 (22%)] Loss: -1106.130005\n",
      "Train Epoch: 590 [33792/101520 (33%)] Loss: -1112.081787\n",
      "Train Epoch: 590 [45056/101520 (44%)] Loss: -1101.620850\n",
      "Train Epoch: 590 [56320/101520 (55%)] Loss: -1109.449219\n",
      "Train Epoch: 590 [67584/101520 (67%)] Loss: -1103.701172\n",
      "Train Epoch: 590 [78848/101520 (78%)] Loss: -1104.511230\n",
      "Train Epoch: 590 [90112/101520 (89%)] Loss: -1102.507324\n",
      "Train Epoch: 590 [101376/101520 (100%)] Loss: -1114.121704\n",
      "    epoch          : 590\n",
      "    loss           : -1105.0854694615657\n",
      "    ess            : 1.9680195645471315\n",
      "    log_marginal   : 1105.1121163679727\n",
      "    log_joint      : 1313.2061700102072\n",
      "    val_loss       : -1103.4155061141305\n",
      "    val_ess        : 1.9677570850952812\n",
      "    val_log_marginal: 1103.4417087720788\n",
      "    val_log_joint  : 1311.2514913807745\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch590.pth ...\n",
      "Train Epoch: 591 [0/101520 (0%)] Loss: -1111.536255\n",
      "Train Epoch: 591 [11264/101520 (11%)] Loss: -1106.964355\n",
      "Train Epoch: 591 [22528/101520 (22%)] Loss: -1103.771973\n",
      "Train Epoch: 591 [33792/101520 (33%)] Loss: -1104.293213\n",
      "Train Epoch: 591 [45056/101520 (44%)] Loss: -1106.504150\n",
      "Train Epoch: 591 [56320/101520 (55%)] Loss: -1109.203003\n",
      "Train Epoch: 591 [67584/101520 (67%)] Loss: -1110.214111\n",
      "Train Epoch: 591 [78848/101520 (78%)] Loss: -1103.272461\n",
      "Train Epoch: 591 [90112/101520 (89%)] Loss: -1107.432861\n",
      "Train Epoch: 591 [101376/101520 (100%)] Loss: -1109.390869\n",
      "    epoch          : 591\n",
      "    loss           : -1104.9603461644158\n",
      "    ess            : 1.9672090575922674\n",
      "    log_marginal   : 1104.98793896239\n",
      "    log_joint      : 1313.1464119915986\n",
      "    val_loss       : -1103.990091075068\n",
      "    val_ess        : 1.9686460754145747\n",
      "    val_log_marginal: 1104.0165325662365\n",
      "    val_log_joint  : 1311.9517185377038\n",
      "Train Epoch: 592 [0/101520 (0%)] Loss: -1104.101074\n",
      "Train Epoch: 592 [11264/101520 (11%)] Loss: -1105.755127\n",
      "Train Epoch: 592 [22528/101520 (22%)] Loss: -1105.930908\n",
      "Train Epoch: 592 [33792/101520 (33%)] Loss: -1104.567383\n",
      "Train Epoch: 592 [45056/101520 (44%)] Loss: -1106.243408\n",
      "Train Epoch: 592 [56320/101520 (55%)] Loss: -1104.370239\n",
      "Train Epoch: 592 [67584/101520 (67%)] Loss: -1105.391724\n",
      "Train Epoch: 592 [78848/101520 (78%)] Loss: -1107.529785\n",
      "Train Epoch: 592 [90112/101520 (89%)] Loss: -1103.660400\n",
      "Train Epoch: 592 [101376/101520 (100%)] Loss: -1094.582397\n",
      "    epoch          : 592\n",
      "    loss           : -1105.1547287217336\n",
      "    ess            : 1.9688440998594965\n",
      "    log_marginal   : 1105.1805628484217\n",
      "    log_joint      : 1313.2992158055904\n",
      "    val_loss       : -1104.0823550016983\n",
      "    val_ess        : 1.9715752446133157\n",
      "    val_log_marginal: 1104.1063285495925\n",
      "    val_log_joint  : 1312.3000700577445\n",
      "Train Epoch: 593 [0/101520 (0%)] Loss: -1103.944336\n",
      "Train Epoch: 593 [11264/101520 (11%)] Loss: -1100.489990\n",
      "Train Epoch: 593 [22528/101520 (22%)] Loss: -1120.781128\n",
      "Train Epoch: 593 [33792/101520 (33%)] Loss: -1106.642090\n",
      "Train Epoch: 593 [45056/101520 (44%)] Loss: -1107.834961\n",
      "Train Epoch: 593 [56320/101520 (55%)] Loss: -1103.371704\n",
      "Train Epoch: 593 [67584/101520 (67%)] Loss: -1102.147705\n",
      "Train Epoch: 593 [78848/101520 (78%)] Loss: -1109.684814\n",
      "Train Epoch: 593 [90112/101520 (89%)] Loss: -1109.178955\n",
      "Train Epoch: 593 [101376/101520 (100%)] Loss: -1105.224609\n",
      "    epoch          : 593\n",
      "    loss           : -1105.5793843485003\n",
      "    ess            : 1.968098599707062\n",
      "    log_marginal   : 1105.6055803921954\n",
      "    log_joint      : 1313.7177292713568\n",
      "    val_loss       : -1105.780565344769\n",
      "    val_ess        : 1.9674054902532827\n",
      "    val_log_marginal: 1105.8091032608695\n",
      "    val_log_joint  : 1313.8583719004755\n",
      "Train Epoch: 594 [0/101520 (0%)] Loss: -1111.241333\n",
      "Train Epoch: 594 [11264/101520 (11%)] Loss: -1104.097290\n",
      "Train Epoch: 594 [22528/101520 (22%)] Loss: -1110.390137\n",
      "Train Epoch: 594 [33792/101520 (33%)] Loss: -1110.974121\n",
      "Train Epoch: 594 [45056/101520 (44%)] Loss: -1103.135498\n",
      "Train Epoch: 594 [56320/101520 (55%)] Loss: -1104.927979\n",
      "Train Epoch: 594 [67584/101520 (67%)] Loss: -1105.908325\n",
      "Train Epoch: 594 [78848/101520 (78%)] Loss: -1108.991577\n",
      "Train Epoch: 594 [90112/101520 (89%)] Loss: -1102.099854\n",
      "Train Epoch: 594 [101376/101520 (100%)] Loss: -1106.055542\n",
      "    epoch          : 594\n",
      "    loss           : -1105.4569858570194\n",
      "    ess            : 1.9685194168857594\n",
      "    log_marginal   : 1105.4835652873744\n",
      "    log_joint      : 1313.611739728918\n",
      "    val_loss       : -1106.5245785920517\n",
      "    val_ess        : 1.9671839631122092\n",
      "    val_log_marginal: 1106.553806470788\n",
      "    val_log_joint  : 1314.8709822944973\n",
      "Train Epoch: 595 [0/101520 (0%)] Loss: -1100.203613\n",
      "Train Epoch: 595 [11264/101520 (11%)] Loss: -1105.869873\n",
      "Train Epoch: 595 [22528/101520 (22%)] Loss: -1105.539917\n",
      "Train Epoch: 595 [33792/101520 (33%)] Loss: -1110.636230\n",
      "Train Epoch: 595 [45056/101520 (44%)] Loss: -1112.849854\n",
      "Train Epoch: 595 [56320/101520 (55%)] Loss: -1110.592529\n",
      "Train Epoch: 595 [67584/101520 (67%)] Loss: -1112.463135\n",
      "Train Epoch: 595 [78848/101520 (78%)] Loss: -1098.937744\n",
      "Train Epoch: 595 [90112/101520 (89%)] Loss: -1106.816162\n",
      "Train Epoch: 595 [101376/101520 (100%)] Loss: -1098.974365\n",
      "    epoch          : 595\n",
      "    loss           : -1105.7248682376728\n",
      "    ess            : 1.9686620133606034\n",
      "    log_marginal   : 1105.751605316622\n",
      "    log_joint      : 1313.873146862241\n",
      "    val_loss       : -1105.9195609714675\n",
      "    val_ess        : 1.9677438891452292\n",
      "    val_log_marginal: 1105.9462731402853\n",
      "    val_log_joint  : 1314.1444622537365\n",
      "Train Epoch: 596 [0/101520 (0%)] Loss: -1107.889404\n",
      "Train Epoch: 596 [11264/101520 (11%)] Loss: -1096.504395\n",
      "Train Epoch: 596 [22528/101520 (22%)] Loss: -1103.448730\n",
      "Train Epoch: 596 [33792/101520 (33%)] Loss: -1107.268921\n",
      "Train Epoch: 596 [45056/101520 (44%)] Loss: -1111.334473\n",
      "Train Epoch: 596 [56320/101520 (55%)] Loss: -1105.402588\n",
      "Train Epoch: 596 [67584/101520 (67%)] Loss: -1100.577881\n",
      "Train Epoch: 596 [78848/101520 (78%)] Loss: -1106.778809\n",
      "Train Epoch: 596 [90112/101520 (89%)] Loss: -1108.592773\n",
      "Train Epoch: 596 [101376/101520 (100%)] Loss: -1114.488647\n",
      "    epoch          : 596\n",
      "    loss           : -1106.003966978447\n",
      "    ess            : 1.9684474809684944\n",
      "    log_marginal   : 1106.029909680237\n",
      "    log_joint      : 1314.224855355881\n",
      "    val_loss       : -1103.924125339674\n",
      "    val_ess        : 1.9694635971732761\n",
      "    val_log_marginal: 1103.9492559018342\n",
      "    val_log_joint  : 1311.804682192595\n",
      "Train Epoch: 597 [0/101520 (0%)] Loss: -1108.502686\n",
      "Train Epoch: 597 [11264/101520 (11%)] Loss: -1100.329224\n",
      "Train Epoch: 597 [22528/101520 (22%)] Loss: -1109.292725\n",
      "Train Epoch: 597 [33792/101520 (33%)] Loss: -1115.499023\n",
      "Train Epoch: 597 [45056/101520 (44%)] Loss: -1103.943848\n",
      "Train Epoch: 597 [56320/101520 (55%)] Loss: -1101.304199\n",
      "Train Epoch: 597 [67584/101520 (67%)] Loss: -1102.256104\n",
      "Train Epoch: 597 [78848/101520 (78%)] Loss: -1104.321655\n",
      "Train Epoch: 597 [90112/101520 (89%)] Loss: -1115.186768\n",
      "Train Epoch: 597 [101376/101520 (100%)] Loss: -1112.114868\n",
      "    epoch          : 597\n",
      "    loss           : -1105.9239391537767\n",
      "    ess            : 1.9674703451856297\n",
      "    log_marginal   : 1105.9517675045147\n",
      "    log_joint      : 1314.0563167399498\n",
      "    val_loss       : -1102.6458581012228\n",
      "    val_ess        : 1.965369260829428\n",
      "    val_log_marginal: 1102.673483143682\n",
      "    val_log_joint  : 1310.6557245669158\n",
      "Train Epoch: 598 [0/101520 (0%)] Loss: -1111.364990\n",
      "Train Epoch: 598 [11264/101520 (11%)] Loss: -1107.206055\n",
      "Train Epoch: 598 [22528/101520 (22%)] Loss: -1108.308350\n",
      "Train Epoch: 598 [33792/101520 (33%)] Loss: -1105.013672\n",
      "Train Epoch: 598 [45056/101520 (44%)] Loss: -1109.914307\n",
      "Train Epoch: 598 [56320/101520 (55%)] Loss: -1113.678589\n",
      "Train Epoch: 598 [67584/101520 (67%)] Loss: -1106.413940\n",
      "Train Epoch: 598 [78848/101520 (78%)] Loss: -1107.978516\n",
      "Train Epoch: 598 [90112/101520 (89%)] Loss: -1113.545410\n",
      "Train Epoch: 598 [101376/101520 (100%)] Loss: -1098.122559\n",
      "    epoch          : 598\n",
      "    loss           : -1106.1621927999372\n",
      "    ess            : 1.9686755439144883\n",
      "    log_marginal   : 1106.188681444331\n",
      "    log_joint      : 1314.341433117737\n",
      "    val_loss       : -1106.6312202785325\n",
      "    val_ess        : 1.9675393311873726\n",
      "    val_log_marginal: 1106.661615786345\n",
      "    val_log_joint  : 1314.6684251868207\n",
      "Train Epoch: 599 [0/101520 (0%)] Loss: -1102.862427\n",
      "Train Epoch: 599 [11264/101520 (11%)] Loss: -1099.212646\n",
      "Train Epoch: 599 [22528/101520 (22%)] Loss: -1100.656006\n",
      "Train Epoch: 599 [33792/101520 (33%)] Loss: -1108.577148\n",
      "Train Epoch: 599 [45056/101520 (44%)] Loss: -1106.273438\n",
      "Train Epoch: 599 [56320/101520 (55%)] Loss: -1107.830811\n",
      "Train Epoch: 599 [67584/101520 (67%)] Loss: -1105.648071\n",
      "Train Epoch: 599 [78848/101520 (78%)] Loss: -1115.012817\n",
      "Train Epoch: 599 [90112/101520 (89%)] Loss: -1103.727173\n",
      "Train Epoch: 599 [101376/101520 (100%)] Loss: -1112.708130\n",
      "    epoch          : 599\n",
      "    loss           : -1106.526233462233\n",
      "    ess            : 1.9682257301244304\n",
      "    log_marginal   : 1106.553008573139\n",
      "    log_joint      : 1314.6748678696215\n",
      "    val_loss       : -1105.4615796959918\n",
      "    val_ess        : 1.9697136671646782\n",
      "    val_log_marginal: 1105.486030910326\n",
      "    val_log_joint  : 1313.781393299932\n",
      "Train Epoch: 600 [0/101520 (0%)] Loss: -1102.297363\n",
      "Train Epoch: 600 [11264/101520 (11%)] Loss: -1106.372437\n",
      "Train Epoch: 600 [22528/101520 (22%)] Loss: -1112.132080\n",
      "Train Epoch: 600 [33792/101520 (33%)] Loss: -1113.587158\n",
      "Train Epoch: 600 [45056/101520 (44%)] Loss: -1105.177734\n",
      "Train Epoch: 600 [56320/101520 (55%)] Loss: -1107.934570\n",
      "Train Epoch: 600 [67584/101520 (67%)] Loss: -1101.311035\n",
      "Train Epoch: 600 [78848/101520 (78%)] Loss: -1103.775391\n",
      "Train Epoch: 600 [90112/101520 (89%)] Loss: -1103.497192\n",
      "Train Epoch: 600 [101376/101520 (100%)] Loss: -1100.116455\n",
      "    epoch          : 600\n",
      "    loss           : -1106.3430985493876\n",
      "    ess            : 1.9678302344365335\n",
      "    log_marginal   : 1106.3709195391018\n",
      "    log_joint      : 1314.5613234630182\n",
      "    val_loss       : -1105.0648564877717\n",
      "    val_ess        : 1.9715705799019856\n",
      "    val_log_marginal: 1105.086569081182\n",
      "    val_log_joint  : 1313.0049571161685\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [0/101520 (0%)] Loss: -1104.881226\n",
      "Train Epoch: 601 [11264/101520 (11%)] Loss: -1104.363281\n",
      "Train Epoch: 601 [22528/101520 (22%)] Loss: -1103.907471\n",
      "Train Epoch: 601 [33792/101520 (33%)] Loss: -1113.340820\n",
      "Train Epoch: 601 [45056/101520 (44%)] Loss: -1103.305786\n",
      "Train Epoch: 601 [56320/101520 (55%)] Loss: -1108.411743\n",
      "Train Epoch: 601 [67584/101520 (67%)] Loss: -1105.786743\n",
      "Train Epoch: 601 [78848/101520 (78%)] Loss: -1105.153320\n",
      "Train Epoch: 601 [90112/101520 (89%)] Loss: -1108.651978\n",
      "Train Epoch: 601 [101376/101520 (100%)] Loss: -1120.458496\n",
      "    epoch          : 601\n",
      "    loss           : -1106.885852602858\n",
      "    ess            : 1.9676671351619701\n",
      "    log_marginal   : 1106.9129847234217\n",
      "    log_joint      : 1315.0515204194803\n",
      "    val_loss       : -1106.3695121433425\n",
      "    val_ess        : 1.9654631770175437\n",
      "    val_log_marginal: 1106.3979863705842\n",
      "    val_log_joint  : 1314.6510062839675\n",
      "Train Epoch: 602 [0/101520 (0%)] Loss: -1109.212891\n",
      "Train Epoch: 602 [11264/101520 (11%)] Loss: -1106.812500\n",
      "Train Epoch: 602 [22528/101520 (22%)] Loss: -1116.850098\n",
      "Train Epoch: 602 [33792/101520 (33%)] Loss: -1103.229614\n",
      "Train Epoch: 602 [45056/101520 (44%)] Loss: -1106.418579\n",
      "Train Epoch: 602 [56320/101520 (55%)] Loss: -1106.659180\n",
      "Train Epoch: 602 [67584/101520 (67%)] Loss: -1105.195068\n",
      "Train Epoch: 602 [78848/101520 (78%)] Loss: -1109.375244\n",
      "Train Epoch: 602 [90112/101520 (89%)] Loss: -1106.789062\n",
      "Train Epoch: 602 [101376/101520 (100%)] Loss: -1108.058594\n",
      "    epoch          : 602\n",
      "    loss           : -1107.0514560105214\n",
      "    ess            : 1.9683075765868527\n",
      "    log_marginal   : 1107.0786108275754\n",
      "    log_joint      : 1315.190866441583\n",
      "    val_loss       : -1107.1287417204483\n",
      "    val_ess        : 1.9705687087515127\n",
      "    val_log_marginal: 1107.1530920940897\n",
      "    val_log_joint  : 1315.1739024286685\n",
      "Train Epoch: 603 [0/101520 (0%)] Loss: -1106.761230\n",
      "Train Epoch: 603 [11264/101520 (11%)] Loss: -1111.341187\n",
      "Train Epoch: 603 [22528/101520 (22%)] Loss: -1108.807129\n",
      "Train Epoch: 603 [33792/101520 (33%)] Loss: -1107.867432\n",
      "Train Epoch: 603 [45056/101520 (44%)] Loss: -1114.784424\n",
      "Train Epoch: 603 [56320/101520 (55%)] Loss: -1109.128174\n",
      "Train Epoch: 603 [67584/101520 (67%)] Loss: -1111.910400\n",
      "Train Epoch: 603 [78848/101520 (78%)] Loss: -1111.742065\n",
      "Train Epoch: 603 [90112/101520 (89%)] Loss: -1104.209473\n",
      "Train Epoch: 603 [101376/101520 (100%)] Loss: -1094.844116\n",
      "    epoch          : 603\n",
      "    loss           : -1106.9369485366285\n",
      "    ess            : 1.9689812342725208\n",
      "    log_marginal   : 1106.962951353447\n",
      "    log_joint      : 1315.1007895924938\n",
      "    val_loss       : -1105.923345151155\n",
      "    val_ess        : 1.9679030024487039\n",
      "    val_log_marginal: 1105.9496274201767\n",
      "    val_log_joint  : 1314.2327933933425\n",
      "Train Epoch: 604 [0/101520 (0%)] Loss: -1103.429199\n",
      "Train Epoch: 604 [11264/101520 (11%)] Loss: -1105.411743\n",
      "Train Epoch: 604 [22528/101520 (22%)] Loss: -1105.780273\n",
      "Train Epoch: 604 [33792/101520 (33%)] Loss: -1109.710449\n",
      "Train Epoch: 604 [45056/101520 (44%)] Loss: -1104.785767\n",
      "Train Epoch: 604 [56320/101520 (55%)] Loss: -1101.448486\n",
      "Train Epoch: 604 [67584/101520 (67%)] Loss: -1117.408203\n",
      "Train Epoch: 604 [78848/101520 (78%)] Loss: -1114.429077\n",
      "Train Epoch: 604 [90112/101520 (89%)] Loss: -1109.359619\n",
      "Train Epoch: 604 [101376/101520 (100%)] Loss: -1110.199463\n",
      "    epoch          : 604\n",
      "    loss           : -1107.359585402599\n",
      "    ess            : 1.9682504741390745\n",
      "    log_marginal   : 1107.3876155680748\n",
      "    log_joint      : 1315.551743703871\n",
      "    val_loss       : -1107.4589525305707\n",
      "    val_ess        : 1.9669631304948225\n",
      "    val_log_marginal: 1107.4858823029892\n",
      "    val_log_joint  : 1315.5723346212635\n",
      "Train Epoch: 605 [0/101520 (0%)] Loss: -1103.486572\n",
      "Train Epoch: 605 [11264/101520 (11%)] Loss: -1108.147095\n",
      "Train Epoch: 605 [22528/101520 (22%)] Loss: -1105.252319\n",
      "Train Epoch: 605 [33792/101520 (33%)] Loss: -1115.055176\n",
      "Train Epoch: 605 [45056/101520 (44%)] Loss: -1108.261353\n",
      "Train Epoch: 605 [56320/101520 (55%)] Loss: -1100.344238\n",
      "Train Epoch: 605 [67584/101520 (67%)] Loss: -1103.905396\n",
      "Train Epoch: 605 [78848/101520 (78%)] Loss: -1103.840698\n",
      "Train Epoch: 605 [90112/101520 (89%)] Loss: -1104.510620\n",
      "Train Epoch: 605 [101376/101520 (100%)] Loss: -1106.386475\n",
      "    epoch          : 605\n",
      "    loss           : -1107.464888529562\n",
      "    ess            : 1.9686747316140025\n",
      "    log_marginal   : 1107.4906846242934\n",
      "    log_joint      : 1315.6810732127435\n",
      "    val_loss       : -1106.4484120244565\n",
      "    val_ess        : 1.9705537816752559\n",
      "    val_log_marginal: 1106.473781419837\n",
      "    val_log_joint  : 1314.46362835428\n",
      "Train Epoch: 606 [0/101520 (0%)] Loss: -1110.916748\n",
      "Train Epoch: 606 [11264/101520 (11%)] Loss: -1103.522705\n",
      "Train Epoch: 606 [22528/101520 (22%)] Loss: -1107.795898\n",
      "Train Epoch: 606 [33792/101520 (33%)] Loss: -1100.687622\n",
      "Train Epoch: 606 [45056/101520 (44%)] Loss: -1104.916504\n",
      "Train Epoch: 606 [56320/101520 (55%)] Loss: -1101.285156\n",
      "Train Epoch: 606 [67584/101520 (67%)] Loss: -1103.743042\n",
      "Train Epoch: 606 [78848/101520 (78%)] Loss: -1110.633667\n",
      "Train Epoch: 606 [90112/101520 (89%)] Loss: -1097.773071\n",
      "Train Epoch: 606 [101376/101520 (100%)] Loss: -1107.223022\n",
      "    epoch          : 606\n",
      "    loss           : -1107.529316504397\n",
      "    ess            : 1.9682562177504725\n",
      "    log_marginal   : 1107.556733251217\n",
      "    log_joint      : 1315.701355287178\n",
      "    val_loss       : -1105.8258905825408\n",
      "    val_ess        : 1.9690186718235845\n",
      "    val_log_marginal: 1105.8519870923913\n",
      "    val_log_joint  : 1314.4563731317935\n",
      "Train Epoch: 607 [0/101520 (0%)] Loss: -1108.543701\n",
      "Train Epoch: 607 [11264/101520 (11%)] Loss: -1109.233154\n",
      "Train Epoch: 607 [22528/101520 (22%)] Loss: -1100.326660\n",
      "Train Epoch: 607 [33792/101520 (33%)] Loss: -1110.849365\n",
      "Train Epoch: 607 [45056/101520 (44%)] Loss: -1099.749268\n",
      "Train Epoch: 607 [56320/101520 (55%)] Loss: -1104.916748\n",
      "Train Epoch: 607 [67584/101520 (67%)] Loss: -1112.736938\n",
      "Train Epoch: 607 [78848/101520 (78%)] Loss: -1107.680542\n",
      "Train Epoch: 607 [90112/101520 (89%)] Loss: -1107.666504\n",
      "Train Epoch: 607 [101376/101520 (100%)] Loss: -1108.209595\n",
      "    epoch          : 607\n",
      "    loss           : -1107.831737422464\n",
      "    ess            : 1.9686681888810353\n",
      "    log_marginal   : 1107.858870769865\n",
      "    log_joint      : 1316.0177750323885\n",
      "    val_loss       : -1106.1178615404212\n",
      "    val_ess        : 1.9677274952764097\n",
      "    val_log_marginal: 1106.146288001019\n",
      "    val_log_joint  : 1314.1662173063858\n",
      "Train Epoch: 608 [0/101520 (0%)] Loss: -1107.667603\n",
      "Train Epoch: 608 [11264/101520 (11%)] Loss: -1107.927490\n",
      "Train Epoch: 608 [22528/101520 (22%)] Loss: -1113.554443\n",
      "Train Epoch: 608 [33792/101520 (33%)] Loss: -1113.020264\n",
      "Train Epoch: 608 [45056/101520 (44%)] Loss: -1107.129517\n",
      "Train Epoch: 608 [56320/101520 (55%)] Loss: -1109.587402\n",
      "Train Epoch: 608 [67584/101520 (67%)] Loss: -1099.601318\n",
      "Train Epoch: 608 [78848/101520 (78%)] Loss: -1112.771851\n",
      "Train Epoch: 608 [90112/101520 (89%)] Loss: -1099.775879\n",
      "Train Epoch: 608 [101376/101520 (100%)] Loss: -1115.490601\n",
      "    epoch          : 608\n",
      "    loss           : -1108.0598175202183\n",
      "    ess            : 1.968932982066169\n",
      "    log_marginal   : 1108.0856872251884\n",
      "    log_joint      : 1316.226901107098\n",
      "    val_loss       : -1106.5188253651495\n",
      "    val_ess        : 1.967343579167905\n",
      "    val_log_marginal: 1106.5440355383832\n",
      "    val_log_joint  : 1314.8211563773777\n",
      "Train Epoch: 609 [0/101520 (0%)] Loss: -1117.822998\n",
      "Train Epoch: 609 [11264/101520 (11%)] Loss: -1107.105957\n",
      "Train Epoch: 609 [22528/101520 (22%)] Loss: -1103.403564\n",
      "Train Epoch: 609 [33792/101520 (33%)] Loss: -1103.583496\n",
      "Train Epoch: 609 [45056/101520 (44%)] Loss: -1112.816162\n",
      "Train Epoch: 609 [56320/101520 (55%)] Loss: -1108.484619\n",
      "Train Epoch: 609 [67584/101520 (67%)] Loss: -1110.432861\n",
      "Train Epoch: 609 [78848/101520 (78%)] Loss: -1110.863525\n",
      "Train Epoch: 609 [90112/101520 (89%)] Loss: -1110.041504\n",
      "Train Epoch: 609 [101376/101520 (100%)] Loss: -1100.798462\n",
      "    epoch          : 609\n",
      "    loss           : -1108.1434559270965\n",
      "    ess            : 1.9681527770344336\n",
      "    log_marginal   : 1108.1707371083935\n",
      "    log_joint      : 1316.3442750863694\n",
      "    val_loss       : -1108.8449441661005\n",
      "    val_ess        : 1.9720578038174172\n",
      "    val_log_marginal: 1108.8654360563858\n",
      "    val_log_joint  : 1317.1310239045517\n",
      "Train Epoch: 610 [0/101520 (0%)] Loss: -1113.755859\n",
      "Train Epoch: 610 [11264/101520 (11%)] Loss: -1108.315552\n",
      "Train Epoch: 610 [22528/101520 (22%)] Loss: -1106.876831\n",
      "Train Epoch: 610 [33792/101520 (33%)] Loss: -1107.324097\n",
      "Train Epoch: 610 [45056/101520 (44%)] Loss: -1111.971924\n",
      "Train Epoch: 610 [56320/101520 (55%)] Loss: -1105.727539\n",
      "Train Epoch: 610 [67584/101520 (67%)] Loss: -1109.170166\n",
      "Train Epoch: 610 [78848/101520 (78%)] Loss: -1102.328735\n",
      "Train Epoch: 610 [90112/101520 (89%)] Loss: -1116.138672\n",
      "Train Epoch: 610 [101376/101520 (100%)] Loss: -1111.298340\n",
      "    epoch          : 610\n",
      "    loss           : -1108.459517435812\n",
      "    ess            : 1.968445720984109\n",
      "    log_marginal   : 1108.4865538630654\n",
      "    log_joint      : 1316.575116794912\n",
      "    val_loss       : -1108.6498439622962\n",
      "    val_ess        : 1.9661682170370351\n",
      "    val_log_marginal: 1108.6818582286005\n",
      "    val_log_joint  : 1316.8581118376358\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch610.pth ...\n",
      "Train Epoch: 611 [0/101520 (0%)] Loss: -1107.927979\n",
      "Train Epoch: 611 [11264/101520 (11%)] Loss: -1108.028564\n",
      "Train Epoch: 611 [22528/101520 (22%)] Loss: -1101.307617\n",
      "Train Epoch: 611 [33792/101520 (33%)] Loss: -1115.571167\n",
      "Train Epoch: 611 [45056/101520 (44%)] Loss: -1107.159302\n",
      "Train Epoch: 611 [56320/101520 (55%)] Loss: -1115.454956\n",
      "Train Epoch: 611 [67584/101520 (67%)] Loss: -1110.077393\n",
      "Train Epoch: 611 [78848/101520 (78%)] Loss: -1112.374146\n",
      "Train Epoch: 611 [90112/101520 (89%)] Loss: -1106.671875\n",
      "Train Epoch: 611 [101376/101520 (100%)] Loss: -1111.040283\n",
      "    epoch          : 611\n",
      "    loss           : -1108.479978628494\n",
      "    ess            : 1.9684524685893225\n",
      "    log_marginal   : 1108.5067598735866\n",
      "    log_joint      : 1316.653847852544\n",
      "    val_loss       : -1106.9158086362092\n",
      "    val_ess        : 1.9689001363256704\n",
      "    val_log_marginal: 1106.9426587975543\n",
      "    val_log_joint  : 1315.4242580247962\n",
      "Train Epoch: 612 [0/101520 (0%)] Loss: -1111.674072\n",
      "Train Epoch: 612 [11264/101520 (11%)] Loss: -1103.874268\n",
      "Train Epoch: 612 [22528/101520 (22%)] Loss: -1111.603271\n",
      "Train Epoch: 612 [33792/101520 (33%)] Loss: -1105.964478\n",
      "Train Epoch: 612 [45056/101520 (44%)] Loss: -1108.419312\n",
      "Train Epoch: 612 [56320/101520 (55%)] Loss: -1099.345215\n",
      "Train Epoch: 612 [67584/101520 (67%)] Loss: -1107.672363\n",
      "Train Epoch: 612 [78848/101520 (78%)] Loss: -1106.544678\n",
      "Train Epoch: 612 [90112/101520 (89%)] Loss: -1112.469482\n",
      "Train Epoch: 612 [101376/101520 (100%)] Loss: -1109.827759\n",
      "    epoch          : 612\n",
      "    loss           : -1108.782670064188\n",
      "    ess            : 1.9684417655120543\n",
      "    log_marginal   : 1108.8095893284783\n",
      "    log_joint      : 1316.940494096459\n",
      "    val_loss       : -1107.2517779806385\n",
      "    val_ess        : 1.971147454303244\n",
      "    val_log_marginal: 1107.273729407269\n",
      "    val_log_joint  : 1315.3349237856658\n",
      "Train Epoch: 613 [0/101520 (0%)] Loss: -1108.068848\n",
      "Train Epoch: 613 [11264/101520 (11%)] Loss: -1108.336304\n",
      "Train Epoch: 613 [22528/101520 (22%)] Loss: -1117.895020\n",
      "Train Epoch: 613 [33792/101520 (33%)] Loss: -1107.314331\n",
      "Train Epoch: 613 [45056/101520 (44%)] Loss: -1114.196655\n",
      "Train Epoch: 613 [56320/101520 (55%)] Loss: -1098.755859\n",
      "Train Epoch: 613 [67584/101520 (67%)] Loss: -1112.906006\n",
      "Train Epoch: 613 [78848/101520 (78%)] Loss: -1105.436890\n",
      "Train Epoch: 613 [90112/101520 (89%)] Loss: -1110.451904\n",
      "Train Epoch: 613 [101376/101520 (100%)] Loss: -1112.268677\n",
      "    epoch          : 613\n",
      "    loss           : -1108.7908788326397\n",
      "    ess            : 1.9687439041521082\n",
      "    log_marginal   : 1108.817307975424\n",
      "    log_joint      : 1316.9736923141097\n",
      "    val_loss       : -1108.9590746008832\n",
      "    val_ess        : 1.9674827534219492\n",
      "    val_log_marginal: 1108.9856540845788\n",
      "    val_log_joint  : 1317.1517015540082\n",
      "Train Epoch: 614 [0/101520 (0%)] Loss: -1112.632446\n",
      "Train Epoch: 614 [11264/101520 (11%)] Loss: -1111.284180\n",
      "Train Epoch: 614 [22528/101520 (22%)] Loss: -1105.490723\n",
      "Train Epoch: 614 [33792/101520 (33%)] Loss: -1103.229980\n",
      "Train Epoch: 614 [45056/101520 (44%)] Loss: -1107.263916\n",
      "Train Epoch: 614 [56320/101520 (55%)] Loss: -1103.883911\n",
      "Train Epoch: 614 [67584/101520 (67%)] Loss: -1114.091064\n",
      "Train Epoch: 614 [78848/101520 (78%)] Loss: -1103.815430\n",
      "Train Epoch: 614 [90112/101520 (89%)] Loss: -1107.318359\n",
      "Train Epoch: 614 [101376/101520 (100%)] Loss: -1102.804077\n",
      "    epoch          : 614\n",
      "    loss           : -1108.9477299829225\n",
      "    ess            : 1.9689540246024204\n",
      "    log_marginal   : 1108.9735984610552\n",
      "    log_joint      : 1317.1435092945194\n",
      "    val_loss       : -1108.150385317595\n",
      "    val_ess        : 1.9652551930883657\n",
      "    val_log_marginal: 1108.1808975883152\n",
      "    val_log_joint  : 1316.4213495669158\n",
      "Train Epoch: 615 [0/101520 (0%)] Loss: -1109.137207\n",
      "Train Epoch: 615 [11264/101520 (11%)] Loss: -1109.823608\n",
      "Train Epoch: 615 [22528/101520 (22%)] Loss: -1112.032959\n",
      "Train Epoch: 615 [33792/101520 (33%)] Loss: -1105.090820\n",
      "Train Epoch: 615 [45056/101520 (44%)] Loss: -1106.251465\n",
      "Train Epoch: 615 [56320/101520 (55%)] Loss: -1111.819092\n",
      "Train Epoch: 615 [67584/101520 (67%)] Loss: -1107.034668\n",
      "Train Epoch: 615 [78848/101520 (78%)] Loss: -1108.299683\n",
      "Train Epoch: 615 [90112/101520 (89%)] Loss: -1114.896484\n",
      "Train Epoch: 615 [101376/101520 (100%)] Loss: -1103.223511\n",
      "    epoch          : 615\n",
      "    loss           : -1109.1490049122565\n",
      "    ess            : 1.9684631501010914\n",
      "    log_marginal   : 1109.1753328409627\n",
      "    log_joint      : 1317.3893460221027\n",
      "    val_loss       : -1107.2651154891305\n",
      "    val_ess        : 1.9696433388668557\n",
      "    val_log_marginal: 1107.2899329144022\n",
      "    val_log_joint  : 1315.8774254840353\n",
      "Train Epoch: 616 [0/101520 (0%)] Loss: -1105.503174\n",
      "Train Epoch: 616 [11264/101520 (11%)] Loss: -1105.856445\n",
      "Train Epoch: 616 [22528/101520 (22%)] Loss: -1110.915039\n",
      "Train Epoch: 616 [33792/101520 (33%)] Loss: -1109.759766\n",
      "Train Epoch: 616 [45056/101520 (44%)] Loss: -1106.660034\n",
      "Train Epoch: 616 [56320/101520 (55%)] Loss: -1106.976807\n",
      "Train Epoch: 616 [67584/101520 (67%)] Loss: -1109.967773\n",
      "Train Epoch: 616 [78848/101520 (78%)] Loss: -1106.549561\n",
      "Train Epoch: 616 [90112/101520 (89%)] Loss: -1107.040283\n",
      "Train Epoch: 616 [101376/101520 (100%)] Loss: -1109.851196\n",
      "    epoch          : 616\n",
      "    loss           : -1109.252525444606\n",
      "    ess            : 1.968396995534849\n",
      "    log_marginal   : 1109.2796091050957\n",
      "    log_joint      : 1317.4216995622644\n",
      "    val_loss       : -1109.4933763586957\n",
      "    val_ess        : 1.966771846232207\n",
      "    val_log_marginal: 1109.520412279212\n",
      "    val_log_joint  : 1317.6842837126358\n",
      "Train Epoch: 617 [0/101520 (0%)] Loss: -1105.385986\n",
      "Train Epoch: 617 [11264/101520 (11%)] Loss: -1110.204834\n",
      "Train Epoch: 617 [22528/101520 (22%)] Loss: -1109.402344\n",
      "Train Epoch: 617 [33792/101520 (33%)] Loss: -1114.657349\n",
      "Train Epoch: 617 [45056/101520 (44%)] Loss: -1109.691406\n",
      "Train Epoch: 617 [56320/101520 (55%)] Loss: -1109.299316\n",
      "Train Epoch: 617 [67584/101520 (67%)] Loss: -1108.577881\n",
      "Train Epoch: 617 [78848/101520 (78%)] Loss: -1116.653809\n",
      "Train Epoch: 617 [90112/101520 (89%)] Loss: -1112.958496\n",
      "Train Epoch: 617 [101376/101520 (100%)] Loss: -1100.640869\n",
      "    epoch          : 617\n",
      "    loss           : -1109.3388040053785\n",
      "    ess            : 1.9684584506193\n",
      "    log_marginal   : 1109.3660244582286\n",
      "    log_joint      : 1317.5851259471185\n",
      "    val_loss       : -1109.1783871858017\n",
      "    val_ess        : 1.9672503264054009\n",
      "    val_log_marginal: 1109.204536769701\n",
      "    val_log_joint  : 1317.1387833305027\n",
      "Train Epoch: 618 [0/101520 (0%)] Loss: -1098.493652\n",
      "Train Epoch: 618 [11264/101520 (11%)] Loss: -1110.399658\n",
      "Train Epoch: 618 [22528/101520 (22%)] Loss: -1110.920166\n",
      "Train Epoch: 618 [33792/101520 (33%)] Loss: -1116.982056\n",
      "Train Epoch: 618 [45056/101520 (44%)] Loss: -1099.956055\n",
      "Train Epoch: 618 [56320/101520 (55%)] Loss: -1105.478027\n",
      "Train Epoch: 618 [67584/101520 (67%)] Loss: -1113.916016\n",
      "Train Epoch: 618 [78848/101520 (78%)] Loss: -1108.391113\n",
      "Train Epoch: 618 [90112/101520 (89%)] Loss: -1108.538574\n",
      "Train Epoch: 618 [101376/101520 (100%)] Loss: -1127.350220\n",
      "    epoch          : 618\n",
      "    loss           : -1109.8131072844692\n",
      "    ess            : 1.9686932114500497\n",
      "    log_marginal   : 1109.8397658458307\n",
      "    log_joint      : 1317.9880058250235\n",
      "    val_loss       : -1109.8745807150135\n",
      "    val_ess        : 1.9672423808471016\n",
      "    val_log_marginal: 1109.903813901155\n",
      "    val_log_joint  : 1317.9421599014945\n",
      "Train Epoch: 619 [0/101520 (0%)] Loss: -1105.827881\n",
      "Train Epoch: 619 [11264/101520 (11%)] Loss: -1102.436279\n",
      "Train Epoch: 619 [22528/101520 (22%)] Loss: -1109.352783\n",
      "Train Epoch: 619 [33792/101520 (33%)] Loss: -1111.944824\n",
      "Train Epoch: 619 [45056/101520 (44%)] Loss: -1110.736328\n",
      "Train Epoch: 619 [56320/101520 (55%)] Loss: -1111.609375\n",
      "Train Epoch: 619 [67584/101520 (67%)] Loss: -1111.764160\n",
      "Train Epoch: 619 [78848/101520 (78%)] Loss: -1102.209717\n",
      "Train Epoch: 619 [90112/101520 (89%)] Loss: -1116.104248\n",
      "Train Epoch: 619 [101376/101520 (100%)] Loss: -1125.589111\n",
      "    epoch          : 619\n",
      "    loss           : -1109.7115625736103\n",
      "    ess            : 1.9686394240987959\n",
      "    log_marginal   : 1109.739065199042\n",
      "    log_joint      : 1317.907775572197\n",
      "    val_loss       : -1107.8286928923233\n",
      "    val_ess        : 1.9667631180390068\n",
      "    val_log_marginal: 1107.8606753141983\n",
      "    val_log_joint  : 1316.2445917544158\n",
      "Train Epoch: 620 [0/101520 (0%)] Loss: -1109.219971\n",
      "Train Epoch: 620 [11264/101520 (11%)] Loss: -1112.740967\n",
      "Train Epoch: 620 [22528/101520 (22%)] Loss: -1112.851318\n",
      "Train Epoch: 620 [33792/101520 (33%)] Loss: -1111.182129\n",
      "Train Epoch: 620 [45056/101520 (44%)] Loss: -1107.261230\n",
      "Train Epoch: 620 [56320/101520 (55%)] Loss: -1103.231201\n",
      "Train Epoch: 620 [67584/101520 (67%)] Loss: -1105.772705\n",
      "Train Epoch: 620 [78848/101520 (78%)] Loss: -1114.399048\n",
      "Train Epoch: 620 [90112/101520 (89%)] Loss: -1116.071045\n",
      "Train Epoch: 620 [101376/101520 (100%)] Loss: -1111.176025\n",
      "    epoch          : 620\n",
      "    loss           : -1110.0221689502198\n",
      "    ess            : 1.9683274977171241\n",
      "    log_marginal   : 1110.0500555757303\n",
      "    log_joint      : 1318.187649674152\n",
      "    val_loss       : -1107.3523745329483\n",
      "    val_ess        : 1.9700468052988467\n",
      "    val_log_marginal: 1107.3780464504075\n",
      "    val_log_joint  : 1315.681200110394\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch620.pth ...\n",
      "Train Epoch: 621 [0/101520 (0%)] Loss: -1115.264160\n",
      "Train Epoch: 621 [11264/101520 (11%)] Loss: -1112.425659\n",
      "Train Epoch: 621 [22528/101520 (22%)] Loss: -1108.046021\n",
      "Train Epoch: 621 [33792/101520 (33%)] Loss: -1112.190552\n",
      "Train Epoch: 621 [45056/101520 (44%)] Loss: -1118.070801\n",
      "Train Epoch: 621 [56320/101520 (55%)] Loss: -1117.290405\n",
      "Train Epoch: 621 [67584/101520 (67%)] Loss: -1106.923706\n",
      "Train Epoch: 621 [78848/101520 (78%)] Loss: -1121.238159\n",
      "Train Epoch: 621 [90112/101520 (89%)] Loss: -1110.065552\n",
      "Train Epoch: 621 [101376/101520 (100%)] Loss: -1102.981201\n",
      "    epoch          : 621\n",
      "    loss           : -1110.1808965481705\n",
      "    ess            : 1.9686146878716935\n",
      "    log_marginal   : 1110.207334278816\n",
      "    log_joint      : 1318.3783529463724\n",
      "    val_loss       : -1109.3201426630435\n",
      "    val_ess        : 1.9688648089118626\n",
      "    val_log_marginal: 1109.3442595108695\n",
      "    val_log_joint  : 1317.424799380095\n",
      "Train Epoch: 622 [0/101520 (0%)] Loss: -1113.403320\n",
      "Train Epoch: 622 [11264/101520 (11%)] Loss: -1109.254639\n",
      "Train Epoch: 622 [22528/101520 (22%)] Loss: -1109.062500\n",
      "Train Epoch: 622 [33792/101520 (33%)] Loss: -1112.552979\n",
      "Train Epoch: 622 [45056/101520 (44%)] Loss: -1104.899780\n",
      "Train Epoch: 622 [56320/101520 (55%)] Loss: -1113.719727\n",
      "Train Epoch: 622 [67584/101520 (67%)] Loss: -1105.446167\n",
      "Train Epoch: 622 [78848/101520 (78%)] Loss: -1113.683594\n",
      "Train Epoch: 622 [90112/101520 (89%)] Loss: -1105.998291\n",
      "Train Epoch: 622 [101376/101520 (100%)] Loss: -1097.399048\n",
      "    epoch          : 622\n",
      "    loss           : -1110.102323752552\n",
      "    ess            : 1.9692036350767816\n",
      "    log_marginal   : 1110.1289031829067\n",
      "    log_joint      : 1318.3324152500786\n",
      "    val_loss       : -1107.440132472826\n",
      "    val_ess        : 1.9679466174996418\n",
      "    val_log_marginal: 1107.4653108016305\n",
      "    val_log_joint  : 1315.8898341966712\n",
      "Train Epoch: 623 [0/101520 (0%)] Loss: -1106.929688\n",
      "Train Epoch: 623 [11264/101520 (11%)] Loss: -1105.805420\n",
      "Train Epoch: 623 [22528/101520 (22%)] Loss: -1110.830078\n",
      "Train Epoch: 623 [33792/101520 (33%)] Loss: -1112.556396\n",
      "Train Epoch: 623 [45056/101520 (44%)] Loss: -1113.960449\n",
      "Train Epoch: 623 [56320/101520 (55%)] Loss: -1110.140625\n",
      "Train Epoch: 623 [67584/101520 (67%)] Loss: -1109.397827\n",
      "Train Epoch: 623 [78848/101520 (78%)] Loss: -1119.470459\n",
      "Train Epoch: 623 [90112/101520 (89%)] Loss: -1114.074463\n",
      "Train Epoch: 623 [101376/101520 (100%)] Loss: -1109.782715\n",
      "    epoch          : 623\n",
      "    loss           : -1110.4869311155387\n",
      "    ess            : 1.9692733359696277\n",
      "    log_marginal   : 1110.513206903659\n",
      "    log_joint      : 1318.6643790240264\n",
      "    val_loss       : -1110.603420091712\n",
      "    val_ess        : 1.9715261873991594\n",
      "    val_log_marginal: 1110.6278925356658\n",
      "    val_log_joint  : 1318.6380509086277\n",
      "Train Epoch: 624 [0/101520 (0%)] Loss: -1108.811157\n",
      "Train Epoch: 624 [11264/101520 (11%)] Loss: -1108.484009\n",
      "Train Epoch: 624 [22528/101520 (22%)] Loss: -1102.040771\n",
      "Train Epoch: 624 [33792/101520 (33%)] Loss: -1118.903809\n",
      "Train Epoch: 624 [45056/101520 (44%)] Loss: -1110.005859\n",
      "Train Epoch: 624 [56320/101520 (55%)] Loss: -1113.473877\n",
      "Train Epoch: 624 [67584/101520 (67%)] Loss: -1108.393066\n",
      "Train Epoch: 624 [78848/101520 (78%)] Loss: -1114.900391\n",
      "Train Epoch: 624 [90112/101520 (89%)] Loss: -1112.137207\n",
      "Train Epoch: 624 [101376/101520 (100%)] Loss: -1109.958496\n",
      "    epoch          : 624\n",
      "    loss           : -1110.781080696451\n",
      "    ess            : 1.969129629470595\n",
      "    log_marginal   : 1110.8073681395258\n",
      "    log_joint      : 1318.8886393638113\n",
      "    val_loss       : -1109.382228685462\n",
      "    val_ess        : 1.9708515405654907\n",
      "    val_log_marginal: 1109.405613111413\n",
      "    val_log_joint  : 1317.5251146399457\n",
      "Train Epoch: 625 [0/101520 (0%)] Loss: -1113.753906\n",
      "Train Epoch: 625 [11264/101520 (11%)] Loss: -1114.492920\n",
      "Train Epoch: 625 [22528/101520 (22%)] Loss: -1111.796021\n",
      "Train Epoch: 625 [33792/101520 (33%)] Loss: -1108.293945\n",
      "Train Epoch: 625 [45056/101520 (44%)] Loss: -1106.448242\n",
      "Train Epoch: 625 [56320/101520 (55%)] Loss: -1115.977295\n",
      "Train Epoch: 625 [67584/101520 (67%)] Loss: -1116.311768\n",
      "Train Epoch: 625 [78848/101520 (78%)] Loss: -1111.174805\n",
      "Train Epoch: 625 [90112/101520 (89%)] Loss: -1108.182129\n",
      "Train Epoch: 625 [101376/101520 (100%)] Loss: -1116.067017\n",
      "    epoch          : 625\n",
      "    loss           : -1110.6237142744974\n",
      "    ess            : 1.9687680431346797\n",
      "    log_marginal   : 1110.6505537943624\n",
      "    log_joint      : 1318.8676125991285\n",
      "    val_loss       : -1108.8121497112772\n",
      "    val_ess        : 1.9696130804393603\n",
      "    val_log_marginal: 1108.8390954059103\n",
      "    val_log_joint  : 1317.1362888502038\n",
      "Train Epoch: 626 [0/101520 (0%)] Loss: -1115.013062\n",
      "Train Epoch: 626 [11264/101520 (11%)] Loss: -1114.585205\n",
      "Train Epoch: 626 [22528/101520 (22%)] Loss: -1110.262939\n",
      "Train Epoch: 626 [33792/101520 (33%)] Loss: -1112.992432\n",
      "Train Epoch: 626 [45056/101520 (44%)] Loss: -1105.913086\n",
      "Train Epoch: 626 [56320/101520 (55%)] Loss: -1117.526367\n",
      "Train Epoch: 626 [67584/101520 (67%)] Loss: -1107.839478\n",
      "Train Epoch: 626 [78848/101520 (78%)] Loss: -1110.747437\n",
      "Train Epoch: 626 [90112/101520 (89%)] Loss: -1112.452148\n",
      "Train Epoch: 626 [101376/101520 (100%)] Loss: -1110.033081\n",
      "    epoch          : 626\n",
      "    loss           : -1111.0370155219457\n",
      "    ess            : 1.9687689920166629\n",
      "    log_marginal   : 1111.0640875274812\n",
      "    log_joint      : 1319.2046959651775\n",
      "    val_loss       : -1110.9502324643342\n",
      "    val_ess        : 1.9694974785265715\n",
      "    val_log_marginal: 1110.9750392747962\n",
      "    val_log_joint  : 1319.1508895210598\n",
      "Train Epoch: 627 [0/101520 (0%)] Loss: -1119.392090\n",
      "Train Epoch: 627 [11264/101520 (11%)] Loss: -1108.748657\n",
      "Train Epoch: 627 [22528/101520 (22%)] Loss: -1116.372925\n",
      "Train Epoch: 627 [33792/101520 (33%)] Loss: -1113.374023\n",
      "Train Epoch: 627 [45056/101520 (44%)] Loss: -1109.797119\n",
      "Train Epoch: 627 [56320/101520 (55%)] Loss: -1108.113892\n",
      "Train Epoch: 627 [67584/101520 (67%)] Loss: -1117.248413\n",
      "Train Epoch: 627 [78848/101520 (78%)] Loss: -1111.474731\n",
      "Train Epoch: 627 [90112/101520 (89%)] Loss: -1108.037231\n",
      "Train Epoch: 627 [101376/101520 (100%)] Loss: -1123.392212\n",
      "    epoch          : 627\n",
      "    loss           : -1111.2479481145965\n",
      "    ess            : 1.9694933759507223\n",
      "    log_marginal   : 1111.2744140625\n",
      "    log_joint      : 1319.382399055826\n",
      "    val_loss       : -1110.072557532269\n",
      "    val_ess        : 1.9689822352450828\n",
      "    val_log_marginal: 1110.0993864639945\n",
      "    val_log_joint  : 1318.141548488451\n",
      "Train Epoch: 628 [0/101520 (0%)] Loss: -1110.210938\n",
      "Train Epoch: 628 [11264/101520 (11%)] Loss: -1111.891357\n",
      "Train Epoch: 628 [22528/101520 (22%)] Loss: -1111.359497\n",
      "Train Epoch: 628 [33792/101520 (33%)] Loss: -1111.177490\n",
      "Train Epoch: 628 [45056/101520 (44%)] Loss: -1103.941162\n",
      "Train Epoch: 628 [56320/101520 (55%)] Loss: -1109.046997\n",
      "Train Epoch: 628 [67584/101520 (67%)] Loss: -1107.654907\n",
      "Train Epoch: 628 [78848/101520 (78%)] Loss: -1110.767334\n",
      "Train Epoch: 628 [90112/101520 (89%)] Loss: -1105.469116\n",
      "Train Epoch: 628 [101376/101520 (100%)] Loss: -1112.674927\n",
      "    epoch          : 628\n",
      "    loss           : -1111.1687698747644\n",
      "    ess            : 1.9683249613747524\n",
      "    log_marginal   : 1111.1966037462705\n",
      "    log_joint      : 1319.3594725335663\n",
      "    val_loss       : -1109.6145762567935\n",
      "    val_ess        : 1.9689282541689666\n",
      "    val_log_marginal: 1109.6421216881793\n",
      "    val_log_joint  : 1317.9372983186142\n",
      "Train Epoch: 629 [0/101520 (0%)] Loss: -1108.743896\n",
      "Train Epoch: 629 [11264/101520 (11%)] Loss: -1110.000244\n",
      "Train Epoch: 629 [22528/101520 (22%)] Loss: -1111.102783\n",
      "Train Epoch: 629 [33792/101520 (33%)] Loss: -1110.975708\n",
      "Train Epoch: 629 [45056/101520 (44%)] Loss: -1115.475220\n",
      "Train Epoch: 629 [56320/101520 (55%)] Loss: -1104.040283\n",
      "Train Epoch: 629 [67584/101520 (67%)] Loss: -1105.042725\n",
      "Train Epoch: 629 [78848/101520 (78%)] Loss: -1111.339844\n",
      "Train Epoch: 629 [90112/101520 (89%)] Loss: -1115.977295\n",
      "Train Epoch: 629 [101376/101520 (100%)] Loss: -1110.787598\n",
      "    epoch          : 629\n",
      "    loss           : -1111.4630679029915\n",
      "    ess            : 1.9679240293838272\n",
      "    log_marginal   : 1111.490758234532\n",
      "    log_joint      : 1319.731443472244\n",
      "    val_loss       : -1111.3216128141983\n",
      "    val_ess        : 1.9691195954447207\n",
      "    val_log_marginal: 1111.347359035326\n",
      "    val_log_joint  : 1319.4571002462635\n",
      "Train Epoch: 630 [0/101520 (0%)] Loss: -1109.191650\n",
      "Train Epoch: 630 [11264/101520 (11%)] Loss: -1115.128418\n",
      "Train Epoch: 630 [22528/101520 (22%)] Loss: -1111.001831\n",
      "Train Epoch: 630 [33792/101520 (33%)] Loss: -1114.936523\n",
      "Train Epoch: 630 [45056/101520 (44%)] Loss: -1108.891235\n",
      "Train Epoch: 630 [56320/101520 (55%)] Loss: -1111.120117\n",
      "Train Epoch: 630 [67584/101520 (67%)] Loss: -1106.890503\n",
      "Train Epoch: 630 [78848/101520 (78%)] Loss: -1111.232300\n",
      "Train Epoch: 630 [90112/101520 (89%)] Loss: -1106.197510\n",
      "Train Epoch: 630 [101376/101520 (100%)] Loss: -1118.236206\n",
      "    epoch          : 630\n",
      "    loss           : -1111.7477966001884\n",
      "    ess            : 1.968267610324687\n",
      "    log_marginal   : 1111.7757875068703\n",
      "    log_joint      : 1319.9400119493955\n",
      "    val_loss       : -1110.831489894701\n",
      "    val_ess        : 1.9695119909618213\n",
      "    val_log_marginal: 1110.8592157778533\n",
      "    val_log_joint  : 1319.0391845703125\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch630.pth ...\n",
      "Train Epoch: 631 [0/101520 (0%)] Loss: -1114.130005\n",
      "Train Epoch: 631 [11264/101520 (11%)] Loss: -1111.858765\n",
      "Train Epoch: 631 [22528/101520 (22%)] Loss: -1115.819580\n",
      "Train Epoch: 631 [33792/101520 (33%)] Loss: -1110.761719\n",
      "Train Epoch: 631 [45056/101520 (44%)] Loss: -1114.616699\n",
      "Train Epoch: 631 [56320/101520 (55%)] Loss: -1113.155884\n",
      "Train Epoch: 631 [67584/101520 (67%)] Loss: -1117.303345\n",
      "Train Epoch: 631 [78848/101520 (78%)] Loss: -1109.794922\n",
      "Train Epoch: 631 [90112/101520 (89%)] Loss: -1109.889160\n",
      "Train Epoch: 631 [101376/101520 (100%)] Loss: -1124.121826\n",
      "    epoch          : 631\n",
      "    loss           : -1111.6560641341473\n",
      "    ess            : 1.9689263842213693\n",
      "    log_marginal   : 1111.6829766508322\n",
      "    log_joint      : 1319.899514049741\n",
      "    val_loss       : -1111.6327222741168\n",
      "    val_ess        : 1.9653332959050718\n",
      "    val_log_marginal: 1111.6622739045517\n",
      "    val_log_joint  : 1319.7388279127038\n",
      "Train Epoch: 632 [0/101520 (0%)] Loss: -1114.064453\n",
      "Train Epoch: 632 [11264/101520 (11%)] Loss: -1110.359619\n",
      "Train Epoch: 632 [22528/101520 (22%)] Loss: -1115.953735\n",
      "Train Epoch: 632 [33792/101520 (33%)] Loss: -1114.597778\n",
      "Train Epoch: 632 [45056/101520 (44%)] Loss: -1109.964600\n",
      "Train Epoch: 632 [56320/101520 (55%)] Loss: -1113.663452\n",
      "Train Epoch: 632 [67584/101520 (67%)] Loss: -1111.937134\n",
      "Train Epoch: 632 [78848/101520 (78%)] Loss: -1113.202881\n",
      "Train Epoch: 632 [90112/101520 (89%)] Loss: -1106.784302\n",
      "Train Epoch: 632 [101376/101520 (100%)] Loss: -1109.461548\n",
      "    epoch          : 632\n",
      "    loss           : -1111.9927303755103\n",
      "    ess            : 1.9683253303844126\n",
      "    log_marginal   : 1112.020071058417\n",
      "    log_joint      : 1320.239019192643\n",
      "    val_loss       : -1110.2075991423233\n",
      "    val_ess        : 1.9688836439796116\n",
      "    val_log_marginal: 1110.2349694293478\n",
      "    val_log_joint  : 1318.422904636549\n",
      "Train Epoch: 633 [0/101520 (0%)] Loss: -1104.450317\n",
      "Train Epoch: 633 [11264/101520 (11%)] Loss: -1108.894531\n",
      "Train Epoch: 633 [22528/101520 (22%)] Loss: -1112.197876\n",
      "Train Epoch: 633 [33792/101520 (33%)] Loss: -1113.249756\n",
      "Train Epoch: 633 [45056/101520 (44%)] Loss: -1115.682861\n",
      "Train Epoch: 633 [56320/101520 (55%)] Loss: -1113.519287\n",
      "Train Epoch: 633 [67584/101520 (67%)] Loss: -1109.152344\n",
      "Train Epoch: 633 [78848/101520 (78%)] Loss: -1111.797852\n",
      "Train Epoch: 633 [90112/101520 (89%)] Loss: -1108.999023\n",
      "Train Epoch: 633 [101376/101520 (100%)] Loss: -1108.375000\n",
      "    epoch          : 633\n",
      "    loss           : -1112.1842807788944\n",
      "    ess            : 1.9691888129890864\n",
      "    log_marginal   : 1112.2100566308104\n",
      "    log_joint      : 1320.3830811773712\n",
      "    val_loss       : -1111.6828454059103\n",
      "    val_ess        : 1.9699839301731275\n",
      "    val_log_marginal: 1111.7084748641305\n",
      "    val_log_joint  : 1319.7998365319293\n",
      "Train Epoch: 634 [0/101520 (0%)] Loss: -1114.216064\n",
      "Train Epoch: 634 [11264/101520 (11%)] Loss: -1109.667969\n",
      "Train Epoch: 634 [22528/101520 (22%)] Loss: -1113.467041\n",
      "Train Epoch: 634 [33792/101520 (33%)] Loss: -1111.500366\n",
      "Train Epoch: 634 [45056/101520 (44%)] Loss: -1122.502197\n",
      "Train Epoch: 634 [56320/101520 (55%)] Loss: -1111.671631\n",
      "Train Epoch: 634 [67584/101520 (67%)] Loss: -1115.075562\n",
      "Train Epoch: 634 [78848/101520 (78%)] Loss: -1111.032959\n",
      "Train Epoch: 634 [90112/101520 (89%)] Loss: -1117.564331\n",
      "Train Epoch: 634 [101376/101520 (100%)] Loss: -1112.798584\n",
      "    epoch          : 634\n",
      "    loss           : -1112.2651085014918\n",
      "    ess            : 1.968583946251989\n",
      "    log_marginal   : 1112.29149470497\n",
      "    log_joint      : 1320.478640148987\n",
      "    val_loss       : -1112.65893023947\n",
      "    val_ess        : 1.9676402444424836\n",
      "    val_log_marginal: 1112.6906632133152\n",
      "    val_log_joint  : 1321.0555472995925\n",
      "Train Epoch: 635 [0/101520 (0%)] Loss: -1117.151123\n",
      "Train Epoch: 635 [11264/101520 (11%)] Loss: -1114.349365\n",
      "Train Epoch: 635 [22528/101520 (22%)] Loss: -1109.706543\n",
      "Train Epoch: 635 [33792/101520 (33%)] Loss: -1117.082275\n",
      "Train Epoch: 635 [45056/101520 (44%)] Loss: -1117.343994\n",
      "Train Epoch: 635 [56320/101520 (55%)] Loss: -1108.038330\n",
      "Train Epoch: 635 [67584/101520 (67%)] Loss: -1114.001953\n",
      "Train Epoch: 635 [78848/101520 (78%)] Loss: -1114.751953\n",
      "Train Epoch: 635 [90112/101520 (89%)] Loss: -1114.775146\n",
      "Train Epoch: 635 [101376/101520 (100%)] Loss: -1121.621582\n",
      "    epoch          : 635\n",
      "    loss           : -1112.5065801419205\n",
      "    ess            : 1.9690672931958682\n",
      "    log_marginal   : 1112.5326976680276\n",
      "    log_joint      : 1320.7359392175722\n",
      "    val_loss       : -1112.8583612856658\n",
      "    val_ess        : 1.9674901081168132\n",
      "    val_log_marginal: 1112.8874936311142\n",
      "    val_log_joint  : 1321.0706203294837\n",
      "Train Epoch: 636 [0/101520 (0%)] Loss: -1114.957275\n",
      "Train Epoch: 636 [11264/101520 (11%)] Loss: -1107.362793\n",
      "Train Epoch: 636 [22528/101520 (22%)] Loss: -1114.970703\n",
      "Train Epoch: 636 [33792/101520 (33%)] Loss: -1115.531128\n",
      "Train Epoch: 636 [45056/101520 (44%)] Loss: -1119.436646\n",
      "Train Epoch: 636 [56320/101520 (55%)] Loss: -1114.180664\n",
      "Train Epoch: 636 [67584/101520 (67%)] Loss: -1114.199585\n",
      "Train Epoch: 636 [78848/101520 (78%)] Loss: -1112.892822\n",
      "Train Epoch: 636 [90112/101520 (89%)] Loss: -1112.166260\n",
      "Train Epoch: 636 [101376/101520 (100%)] Loss: -1109.596558\n",
      "    epoch          : 636\n",
      "    loss           : -1112.6720504377356\n",
      "    ess            : 1.9692009327998712\n",
      "    log_marginal   : 1112.6976729349874\n",
      "    log_joint      : 1320.8546817338647\n",
      "    val_loss       : -1110.7578868036685\n",
      "    val_ess        : 1.9709146126456882\n",
      "    val_log_marginal: 1110.782709536345\n",
      "    val_log_joint  : 1319.3117357336957\n",
      "Train Epoch: 637 [0/101520 (0%)] Loss: -1114.131226\n",
      "Train Epoch: 637 [11264/101520 (11%)] Loss: -1117.569580\n",
      "Train Epoch: 637 [22528/101520 (22%)] Loss: -1108.170654\n",
      "Train Epoch: 637 [33792/101520 (33%)] Loss: -1112.115479\n",
      "Train Epoch: 637 [45056/101520 (44%)] Loss: -1119.647217\n",
      "Train Epoch: 637 [56320/101520 (55%)] Loss: -1110.436523\n",
      "Train Epoch: 637 [67584/101520 (67%)] Loss: -1108.620239\n",
      "Train Epoch: 637 [78848/101520 (78%)] Loss: -1114.506958\n",
      "Train Epoch: 637 [90112/101520 (89%)] Loss: -1113.882812\n",
      "Train Epoch: 637 [101376/101520 (100%)] Loss: -1114.493286\n",
      "    epoch          : 637\n",
      "    loss           : -1112.779767367109\n",
      "    ess            : 1.968851639996821\n",
      "    log_marginal   : 1112.80629833739\n",
      "    log_joint      : 1320.9799816955874\n",
      "    val_loss       : -1113.47582477072\n",
      "    val_ess        : 1.9700617686561916\n",
      "    val_log_marginal: 1113.501565684443\n",
      "    val_log_joint  : 1321.7831765879755\n",
      "Train Epoch: 638 [0/101520 (0%)] Loss: -1113.073975\n",
      "Train Epoch: 638 [11264/101520 (11%)] Loss: -1104.102173\n",
      "Train Epoch: 638 [22528/101520 (22%)] Loss: -1109.941284\n",
      "Train Epoch: 638 [33792/101520 (33%)] Loss: -1114.776611\n",
      "Train Epoch: 638 [45056/101520 (44%)] Loss: -1110.458862\n",
      "Train Epoch: 638 [56320/101520 (55%)] Loss: -1114.616211\n",
      "Train Epoch: 638 [67584/101520 (67%)] Loss: -1109.073242\n",
      "Train Epoch: 638 [78848/101520 (78%)] Loss: -1108.109253\n",
      "Train Epoch: 638 [90112/101520 (89%)] Loss: -1113.204346\n",
      "Train Epoch: 638 [101376/101520 (100%)] Loss: -1118.281738\n",
      "    epoch          : 638\n",
      "    loss           : -1112.8899306100816\n",
      "    ess            : 1.968004732275728\n",
      "    log_marginal   : 1112.9172676124765\n",
      "    log_joint      : 1321.1590570037688\n",
      "    val_loss       : -1112.640916907269\n",
      "    val_ess        : 1.96696544730145\n",
      "    val_log_marginal: 1112.6732124660325\n",
      "    val_log_joint  : 1320.6835088315217\n",
      "Train Epoch: 639 [0/101520 (0%)] Loss: -1119.091797\n",
      "Train Epoch: 639 [11264/101520 (11%)] Loss: -1113.843262\n",
      "Train Epoch: 639 [22528/101520 (22%)] Loss: -1113.296387\n",
      "Train Epoch: 639 [33792/101520 (33%)] Loss: -1108.317871\n",
      "Train Epoch: 639 [45056/101520 (44%)] Loss: -1115.399902\n",
      "Train Epoch: 639 [56320/101520 (55%)] Loss: -1107.411621\n",
      "Train Epoch: 639 [67584/101520 (67%)] Loss: -1122.375244\n",
      "Train Epoch: 639 [78848/101520 (78%)] Loss: -1118.338135\n",
      "Train Epoch: 639 [90112/101520 (89%)] Loss: -1111.984009\n",
      "Train Epoch: 639 [101376/101520 (100%)] Loss: -1111.159058\n",
      "    epoch          : 639\n",
      "    loss           : -1113.0958006585663\n",
      "    ess            : 1.9691900667832725\n",
      "    log_marginal   : 1113.1228493541928\n",
      "    log_joint      : 1321.2956469358512\n",
      "    val_loss       : -1111.5859109629755\n",
      "    val_ess        : 1.9693810110506804\n",
      "    val_log_marginal: 1111.6131963315217\n",
      "    val_log_joint  : 1319.9535973590353\n",
      "Train Epoch: 640 [0/101520 (0%)] Loss: -1120.270752\n",
      "Train Epoch: 640 [11264/101520 (11%)] Loss: -1106.852783\n",
      "Train Epoch: 640 [22528/101520 (22%)] Loss: -1120.681641\n",
      "Train Epoch: 640 [33792/101520 (33%)] Loss: -1111.877563\n",
      "Train Epoch: 640 [45056/101520 (44%)] Loss: -1115.749756\n",
      "Train Epoch: 640 [56320/101520 (55%)] Loss: -1106.681030\n",
      "Train Epoch: 640 [67584/101520 (67%)] Loss: -1112.444702\n",
      "Train Epoch: 640 [78848/101520 (78%)] Loss: -1123.298218\n",
      "Train Epoch: 640 [90112/101520 (89%)] Loss: -1113.555420\n",
      "Train Epoch: 640 [101376/101520 (100%)] Loss: -1106.874512\n",
      "    epoch          : 640\n",
      "    loss           : -1113.2849390997958\n",
      "    ess            : 1.9680226759694928\n",
      "    log_marginal   : 1113.3133753484217\n",
      "    log_joint      : 1321.4246832306062\n",
      "    val_loss       : -1111.2442626953125\n",
      "    val_ess        : 1.9691116084223208\n",
      "    val_log_marginal: 1111.2697860054348\n",
      "    val_log_joint  : 1319.3907895295517\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch640.pth ...\n",
      "Train Epoch: 641 [0/101520 (0%)] Loss: -1116.105957\n",
      "Train Epoch: 641 [11264/101520 (11%)] Loss: -1107.407227\n",
      "Train Epoch: 641 [22528/101520 (22%)] Loss: -1111.271606\n",
      "Train Epoch: 641 [33792/101520 (33%)] Loss: -1112.665283\n",
      "Train Epoch: 641 [45056/101520 (44%)] Loss: -1118.358643\n",
      "Train Epoch: 641 [56320/101520 (55%)] Loss: -1120.863037\n",
      "Train Epoch: 641 [67584/101520 (67%)] Loss: -1114.299927\n",
      "Train Epoch: 641 [78848/101520 (78%)] Loss: -1111.730103\n",
      "Train Epoch: 641 [90112/101520 (89%)] Loss: -1114.858154\n",
      "Train Epoch: 641 [101376/101520 (100%)] Loss: -1115.133423\n",
      "    epoch          : 641\n",
      "    loss           : -1113.4087392529052\n",
      "    ess            : 1.9683647251608383\n",
      "    log_marginal   : 1113.436031475738\n",
      "    log_joint      : 1321.6320487937735\n",
      "    val_loss       : -1112.6558678668478\n",
      "    val_ess        : 1.97043544313182\n",
      "    val_log_marginal: 1112.6797618036685\n",
      "    val_log_joint  : 1320.69359820822\n",
      "Train Epoch: 642 [0/101520 (0%)] Loss: -1113.575195\n",
      "Train Epoch: 642 [11264/101520 (11%)] Loss: -1114.488525\n",
      "Train Epoch: 642 [22528/101520 (22%)] Loss: -1117.392456\n",
      "Train Epoch: 642 [33792/101520 (33%)] Loss: -1113.741211\n",
      "Train Epoch: 642 [45056/101520 (44%)] Loss: -1117.699707\n",
      "Train Epoch: 642 [56320/101520 (55%)] Loss: -1119.805542\n",
      "Train Epoch: 642 [67584/101520 (67%)] Loss: -1113.926270\n",
      "Train Epoch: 642 [78848/101520 (78%)] Loss: -1105.592163\n",
      "Train Epoch: 642 [90112/101520 (89%)] Loss: -1111.627197\n",
      "Train Epoch: 642 [101376/101520 (100%)] Loss: -1112.818359\n",
      "    epoch          : 642\n",
      "    loss           : -1113.5487950003926\n",
      "    ess            : 1.9692576722284059\n",
      "    log_marginal   : 1113.5752511335977\n",
      "    log_joint      : 1321.767005805394\n",
      "    val_loss       : -1112.7246252972147\n",
      "    val_ess        : 1.9686315163322117\n",
      "    val_log_marginal: 1112.751417077106\n",
      "    val_log_joint  : 1320.7994437839675\n",
      "Train Epoch: 643 [0/101520 (0%)] Loss: -1111.140869\n",
      "Train Epoch: 643 [11264/101520 (11%)] Loss: -1117.119873\n",
      "Train Epoch: 643 [22528/101520 (22%)] Loss: -1117.765625\n",
      "Train Epoch: 643 [33792/101520 (33%)] Loss: -1117.403809\n",
      "Train Epoch: 643 [45056/101520 (44%)] Loss: -1114.387207\n",
      "Train Epoch: 643 [56320/101520 (55%)] Loss: -1112.839966\n",
      "Train Epoch: 643 [67584/101520 (67%)] Loss: -1116.502563\n",
      "Train Epoch: 643 [78848/101520 (78%)] Loss: -1106.291504\n",
      "Train Epoch: 643 [90112/101520 (89%)] Loss: -1111.570068\n",
      "Train Epoch: 643 [101376/101520 (100%)] Loss: -1110.243408\n",
      "    epoch          : 643\n",
      "    loss           : -1113.710187288984\n",
      "    ess            : 1.9684678352058833\n",
      "    log_marginal   : 1113.7373727769707\n",
      "    log_joint      : 1321.9500161942526\n",
      "    val_loss       : -1112.617134425951\n",
      "    val_ess        : 1.968118605406388\n",
      "    val_log_marginal: 1112.6464472231658\n",
      "    val_log_joint  : 1320.5317223590353\n",
      "Train Epoch: 644 [0/101520 (0%)] Loss: -1117.293213\n",
      "Train Epoch: 644 [11264/101520 (11%)] Loss: -1112.990723\n",
      "Train Epoch: 644 [22528/101520 (22%)] Loss: -1111.917480\n",
      "Train Epoch: 644 [33792/101520 (33%)] Loss: -1116.307617\n",
      "Train Epoch: 644 [45056/101520 (44%)] Loss: -1123.247314\n",
      "Train Epoch: 644 [56320/101520 (55%)] Loss: -1117.387329\n",
      "Train Epoch: 644 [67584/101520 (67%)] Loss: -1113.094238\n",
      "Train Epoch: 644 [78848/101520 (78%)] Loss: -1114.716064\n",
      "Train Epoch: 644 [90112/101520 (89%)] Loss: -1115.973267\n",
      "Train Epoch: 644 [101376/101520 (100%)] Loss: -1115.831909\n",
      "    epoch          : 644\n",
      "    loss           : -1114.003581751531\n",
      "    ess            : 1.968412150689705\n",
      "    log_marginal   : 1114.031422984061\n",
      "    log_joint      : 1322.1430369621546\n",
      "    val_loss       : -1113.0872961956522\n",
      "    val_ess        : 1.9704376977422964\n",
      "    val_log_marginal: 1113.1109300696332\n",
      "    val_log_joint  : 1321.0775783372962\n",
      "Train Epoch: 645 [0/101520 (0%)] Loss: -1112.048218\n",
      "Train Epoch: 645 [11264/101520 (11%)] Loss: -1120.153931\n",
      "Train Epoch: 645 [22528/101520 (22%)] Loss: -1111.449219\n",
      "Train Epoch: 645 [33792/101520 (33%)] Loss: -1119.150391\n",
      "Train Epoch: 645 [45056/101520 (44%)] Loss: -1114.248291\n",
      "Train Epoch: 645 [56320/101520 (55%)] Loss: -1111.537476\n",
      "Train Epoch: 645 [67584/101520 (67%)] Loss: -1110.672119\n",
      "Train Epoch: 645 [78848/101520 (78%)] Loss: -1110.211914\n",
      "Train Epoch: 645 [90112/101520 (89%)] Loss: -1125.570801\n",
      "Train Epoch: 645 [101376/101520 (100%)] Loss: -1107.520752\n",
      "    epoch          : 645\n",
      "    loss           : -1113.8827523849718\n",
      "    ess            : 1.969084282616275\n",
      "    log_marginal   : 1113.9089876874607\n",
      "    log_joint      : 1322.1127009559516\n",
      "    val_loss       : -1112.9273257048233\n",
      "    val_ess        : 1.9675229165865027\n",
      "    val_log_marginal: 1112.9546269955842\n",
      "    val_log_joint  : 1320.986763332201\n",
      "Train Epoch: 646 [0/101520 (0%)] Loss: -1115.822510\n",
      "Train Epoch: 646 [11264/101520 (11%)] Loss: -1114.642822\n",
      "Train Epoch: 646 [22528/101520 (22%)] Loss: -1106.198242\n",
      "Train Epoch: 646 [33792/101520 (33%)] Loss: -1119.289551\n",
      "Train Epoch: 646 [45056/101520 (44%)] Loss: -1108.985229\n",
      "Train Epoch: 646 [56320/101520 (55%)] Loss: -1107.958252\n",
      "Train Epoch: 646 [67584/101520 (67%)] Loss: -1115.337036\n",
      "Train Epoch: 646 [78848/101520 (78%)] Loss: -1113.807861\n",
      "Train Epoch: 646 [90112/101520 (89%)] Loss: -1107.619751\n",
      "Train Epoch: 646 [101376/101520 (100%)] Loss: -1110.346313\n",
      "    epoch          : 646\n",
      "    loss           : -1114.2700183044126\n",
      "    ess            : 1.9684246982162321\n",
      "    log_marginal   : 1114.297259613497\n",
      "    log_joint      : 1322.4544058181532\n",
      "    val_loss       : -1111.871974779212\n",
      "    val_ess        : 1.9715822209482607\n",
      "    val_log_marginal: 1111.8960226307745\n",
      "    val_log_joint  : 1319.9723218834918\n",
      "Train Epoch: 647 [0/101520 (0%)] Loss: -1112.918701\n",
      "Train Epoch: 647 [11264/101520 (11%)] Loss: -1117.928223\n",
      "Train Epoch: 647 [22528/101520 (22%)] Loss: -1119.558594\n",
      "Train Epoch: 647 [33792/101520 (33%)] Loss: -1116.830078\n",
      "Train Epoch: 647 [45056/101520 (44%)] Loss: -1120.518799\n",
      "Train Epoch: 647 [56320/101520 (55%)] Loss: -1117.250366\n",
      "Train Epoch: 647 [67584/101520 (67%)] Loss: -1111.847412\n",
      "Train Epoch: 647 [78848/101520 (78%)] Loss: -1122.268311\n",
      "Train Epoch: 647 [90112/101520 (89%)] Loss: -1114.441406\n",
      "Train Epoch: 647 [101376/101520 (100%)] Loss: -1110.468262\n",
      "    epoch          : 647\n",
      "    loss           : -1114.447326353447\n",
      "    ess            : 1.969317711178382\n",
      "    log_marginal   : 1114.4738511895414\n",
      "    log_joint      : 1322.612255000589\n",
      "    val_loss       : -1113.5608759341033\n",
      "    val_ess        : 1.9669095122295877\n",
      "    val_log_marginal: 1113.5921737007473\n",
      "    val_log_joint  : 1321.8632175611413\n",
      "Train Epoch: 648 [0/101520 (0%)] Loss: -1119.596680\n",
      "Train Epoch: 648 [11264/101520 (11%)] Loss: -1117.380615\n",
      "Train Epoch: 648 [22528/101520 (22%)] Loss: -1119.909058\n",
      "Train Epoch: 648 [33792/101520 (33%)] Loss: -1110.692871\n",
      "Train Epoch: 648 [45056/101520 (44%)] Loss: -1114.260254\n",
      "Train Epoch: 648 [56320/101520 (55%)] Loss: -1120.499756\n",
      "Train Epoch: 648 [67584/101520 (67%)] Loss: -1111.273438\n",
      "Train Epoch: 648 [78848/101520 (78%)] Loss: -1111.229248\n",
      "Train Epoch: 648 [90112/101520 (89%)] Loss: -1111.172363\n",
      "Train Epoch: 648 [101376/101520 (100%)] Loss: -1113.138550\n",
      "    epoch          : 648\n",
      "    loss           : -1114.4237453134815\n",
      "    ess            : 1.9682594010578327\n",
      "    log_marginal   : 1114.4514331913474\n",
      "    log_joint      : 1322.655820606941\n",
      "    val_loss       : -1111.682420813519\n",
      "    val_ess        : 1.9676494857539302\n",
      "    val_log_marginal: 1111.7108631963315\n",
      "    val_log_joint  : 1319.7949112601902\n",
      "Train Epoch: 649 [0/101520 (0%)] Loss: -1120.089600\n",
      "Train Epoch: 649 [11264/101520 (11%)] Loss: -1109.490723\n",
      "Train Epoch: 649 [22528/101520 (22%)] Loss: -1116.263916\n",
      "Train Epoch: 649 [33792/101520 (33%)] Loss: -1108.603760\n",
      "Train Epoch: 649 [45056/101520 (44%)] Loss: -1112.200928\n",
      "Train Epoch: 649 [56320/101520 (55%)] Loss: -1118.139526\n",
      "Train Epoch: 649 [67584/101520 (67%)] Loss: -1118.779785\n",
      "Train Epoch: 649 [78848/101520 (78%)] Loss: -1117.341064\n",
      "Train Epoch: 649 [90112/101520 (89%)] Loss: -1108.050537\n",
      "Train Epoch: 649 [101376/101520 (100%)] Loss: -1121.176270\n",
      "    epoch          : 649\n",
      "    loss           : -1114.707660004122\n",
      "    ess            : 1.9683166586574\n",
      "    log_marginal   : 1114.734547984061\n",
      "    log_joint      : 1323.0230627012013\n",
      "    val_loss       : -1115.5950343919837\n",
      "    val_ess        : 1.971550982931386\n",
      "    val_log_marginal: 1115.6187425696332\n",
      "    val_log_joint  : 1323.8771388841712\n",
      "Train Epoch: 650 [0/101520 (0%)] Loss: -1106.467041\n",
      "Train Epoch: 650 [11264/101520 (11%)] Loss: -1120.632568\n",
      "Train Epoch: 650 [22528/101520 (22%)] Loss: -1118.719238\n",
      "Train Epoch: 650 [33792/101520 (33%)] Loss: -1113.177490\n",
      "Train Epoch: 650 [45056/101520 (44%)] Loss: -1113.896362\n",
      "Train Epoch: 650 [56320/101520 (55%)] Loss: -1111.390869\n",
      "Train Epoch: 650 [67584/101520 (67%)] Loss: -1113.182129\n",
      "Train Epoch: 650 [78848/101520 (78%)] Loss: -1114.204102\n",
      "Train Epoch: 650 [90112/101520 (89%)] Loss: -1116.628418\n",
      "Train Epoch: 650 [101376/101520 (100%)] Loss: -1098.234497\n",
      "    epoch          : 650\n",
      "    loss           : -1114.7601852033606\n",
      "    ess            : 1.9680296440220357\n",
      "    log_marginal   : 1114.7878546786667\n",
      "    log_joint      : 1323.0183430580637\n",
      "    val_loss       : -1113.0694208559783\n",
      "    val_ess        : 1.970342807147814\n",
      "    val_log_marginal: 1113.095506751019\n",
      "    val_log_joint  : 1321.4817319123642\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch650.pth ...\n",
      "Train Epoch: 651 [0/101520 (0%)] Loss: -1114.992432\n",
      "Train Epoch: 651 [11264/101520 (11%)] Loss: -1117.512085\n",
      "Train Epoch: 651 [22528/101520 (22%)] Loss: -1108.969238\n",
      "Train Epoch: 651 [33792/101520 (33%)] Loss: -1112.750610\n",
      "Train Epoch: 651 [45056/101520 (44%)] Loss: -1109.382690\n",
      "Train Epoch: 651 [56320/101520 (55%)] Loss: -1116.834106\n",
      "Train Epoch: 651 [67584/101520 (67%)] Loss: -1116.243408\n",
      "Train Epoch: 651 [78848/101520 (78%)] Loss: -1118.017212\n",
      "Train Epoch: 651 [90112/101520 (89%)] Loss: -1115.434448\n",
      "Train Epoch: 651 [101376/101520 (100%)] Loss: -1111.654785\n",
      "    epoch          : 651\n",
      "    loss           : -1114.8091722037923\n",
      "    ess            : 1.9683758541567242\n",
      "    log_marginal   : 1114.8369649762485\n",
      "    log_joint      : 1323.0294784469222\n",
      "    val_loss       : -1113.5951989215353\n",
      "    val_ess        : 1.967359185218811\n",
      "    val_log_marginal: 1113.621677564538\n",
      "    val_log_joint  : 1321.8037215523098\n",
      "Train Epoch: 652 [0/101520 (0%)] Loss: -1120.227173\n",
      "Train Epoch: 652 [11264/101520 (11%)] Loss: -1124.348877\n",
      "Train Epoch: 652 [22528/101520 (22%)] Loss: -1109.878662\n",
      "Train Epoch: 652 [33792/101520 (33%)] Loss: -1114.892334\n",
      "Train Epoch: 652 [45056/101520 (44%)] Loss: -1107.296265\n",
      "Train Epoch: 652 [56320/101520 (55%)] Loss: -1117.430420\n",
      "Train Epoch: 652 [67584/101520 (67%)] Loss: -1117.489990\n",
      "Train Epoch: 652 [78848/101520 (78%)] Loss: -1113.104736\n",
      "Train Epoch: 652 [90112/101520 (89%)] Loss: -1111.048584\n",
      "Train Epoch: 652 [101376/101520 (100%)] Loss: -1113.788818\n",
      "    epoch          : 652\n",
      "    loss           : -1115.2233169018923\n",
      "    ess            : 1.9684333118362043\n",
      "    log_marginal   : 1115.2505796806297\n",
      "    log_joint      : 1323.4521852426194\n",
      "    val_loss       : -1113.1140508237092\n",
      "    val_ess        : 1.9685337439827297\n",
      "    val_log_marginal: 1113.1435812245245\n",
      "    val_log_joint  : 1321.2636824898098\n",
      "Train Epoch: 653 [0/101520 (0%)] Loss: -1112.144897\n",
      "Train Epoch: 653 [11264/101520 (11%)] Loss: -1114.006226\n",
      "Train Epoch: 653 [22528/101520 (22%)] Loss: -1120.671387\n",
      "Train Epoch: 653 [33792/101520 (33%)] Loss: -1117.449707\n",
      "Train Epoch: 653 [45056/101520 (44%)] Loss: -1115.317749\n",
      "Train Epoch: 653 [56320/101520 (55%)] Loss: -1115.587158\n",
      "Train Epoch: 653 [67584/101520 (67%)] Loss: -1115.834229\n",
      "Train Epoch: 653 [78848/101520 (78%)] Loss: -1111.645264\n",
      "Train Epoch: 653 [90112/101520 (89%)] Loss: -1115.405640\n",
      "Train Epoch: 653 [101376/101520 (100%)] Loss: -1118.060181\n",
      "    epoch          : 653\n",
      "    loss           : -1115.144117805826\n",
      "    ess            : 1.968948053355193\n",
      "    log_marginal   : 1115.170654296875\n",
      "    log_joint      : 1323.3975688991834\n",
      "    val_loss       : -1114.6418722401495\n",
      "    val_ess        : 1.9706160296564517\n",
      "    val_log_marginal: 1114.6673000169837\n",
      "    val_log_joint  : 1322.8768735139267\n",
      "Train Epoch: 654 [0/101520 (0%)] Loss: -1114.385498\n",
      "Train Epoch: 654 [11264/101520 (11%)] Loss: -1110.919434\n",
      "Train Epoch: 654 [22528/101520 (22%)] Loss: -1117.376465\n",
      "Train Epoch: 654 [33792/101520 (33%)] Loss: -1118.387329\n",
      "Train Epoch: 654 [45056/101520 (44%)] Loss: -1123.662354\n",
      "Train Epoch: 654 [56320/101520 (55%)] Loss: -1112.686523\n",
      "Train Epoch: 654 [67584/101520 (67%)] Loss: -1111.881714\n",
      "Train Epoch: 654 [78848/101520 (78%)] Loss: -1117.483398\n",
      "Train Epoch: 654 [90112/101520 (89%)] Loss: -1116.466797\n",
      "Train Epoch: 654 [101376/101520 (100%)] Loss: -1100.648193\n",
      "    epoch          : 654\n",
      "    loss           : -1115.2237340265783\n",
      "    ess            : 1.9692893363722606\n",
      "    log_marginal   : 1115.2492522426587\n",
      "    log_joint      : 1323.422288444174\n",
      "    val_loss       : -1113.833639393682\n",
      "    val_ess        : 1.969797792642013\n",
      "    val_log_marginal: 1113.8609778362772\n",
      "    val_log_joint  : 1321.9502059273098\n",
      "Train Epoch: 655 [0/101520 (0%)] Loss: -1118.047363\n",
      "Train Epoch: 655 [11264/101520 (11%)] Loss: -1114.656738\n",
      "Train Epoch: 655 [22528/101520 (22%)] Loss: -1116.118530\n",
      "Train Epoch: 655 [33792/101520 (33%)] Loss: -1118.516846\n",
      "Train Epoch: 655 [45056/101520 (44%)] Loss: -1120.384277\n",
      "Train Epoch: 655 [56320/101520 (55%)] Loss: -1115.557495\n",
      "Train Epoch: 655 [67584/101520 (67%)] Loss: -1114.058105\n",
      "Train Epoch: 655 [78848/101520 (78%)] Loss: -1118.706055\n",
      "Train Epoch: 655 [90112/101520 (89%)] Loss: -1117.385742\n",
      "Train Epoch: 655 [101376/101520 (100%)] Loss: -1119.963501\n",
      "    epoch          : 655\n",
      "    loss           : -1115.6387276960977\n",
      "    ess            : 1.9686543695890724\n",
      "    log_marginal   : 1115.6653267558495\n",
      "    log_joint      : 1323.8805072726916\n",
      "    val_loss       : -1115.7597231657608\n",
      "    val_ess        : 1.9700823555821958\n",
      "    val_log_marginal: 1115.7850182574728\n",
      "    val_log_joint  : 1323.9585120159647\n",
      "Train Epoch: 656 [0/101520 (0%)] Loss: -1112.887207\n",
      "Train Epoch: 656 [11264/101520 (11%)] Loss: -1117.133789\n",
      "Train Epoch: 656 [22528/101520 (22%)] Loss: -1117.231323\n",
      "Train Epoch: 656 [33792/101520 (33%)] Loss: -1114.810547\n",
      "Train Epoch: 656 [45056/101520 (44%)] Loss: -1111.319824\n",
      "Train Epoch: 656 [56320/101520 (55%)] Loss: -1114.047729\n",
      "Train Epoch: 656 [67584/101520 (67%)] Loss: -1114.984863\n",
      "Train Epoch: 656 [78848/101520 (78%)] Loss: -1118.067627\n",
      "Train Epoch: 656 [90112/101520 (89%)] Loss: -1114.320190\n",
      "Train Epoch: 656 [101376/101520 (100%)] Loss: -1122.162109\n",
      "    epoch          : 656\n",
      "    loss           : -1115.6882992845085\n",
      "    ess            : 1.9691695454132616\n",
      "    log_marginal   : 1115.714408836173\n",
      "    log_joint      : 1323.807433775322\n",
      "    val_loss       : -1112.2221467391305\n",
      "    val_ess        : 1.9667467656342879\n",
      "    val_log_marginal: 1112.2507961107337\n",
      "    val_log_joint  : 1320.27026897928\n",
      "Train Epoch: 657 [0/101520 (0%)] Loss: -1115.345215\n",
      "Train Epoch: 657 [11264/101520 (11%)] Loss: -1117.796875\n",
      "Train Epoch: 657 [22528/101520 (22%)] Loss: -1111.959473\n",
      "Train Epoch: 657 [33792/101520 (33%)] Loss: -1108.496826\n",
      "Train Epoch: 657 [45056/101520 (44%)] Loss: -1113.943848\n",
      "Train Epoch: 657 [56320/101520 (55%)] Loss: -1117.844482\n",
      "Train Epoch: 657 [67584/101520 (67%)] Loss: -1118.750854\n",
      "Train Epoch: 657 [78848/101520 (78%)] Loss: -1113.509033\n",
      "Train Epoch: 657 [90112/101520 (89%)] Loss: -1108.243408\n",
      "Train Epoch: 657 [101376/101520 (100%)] Loss: -1109.812500\n",
      "    epoch          : 657\n",
      "    loss           : -1115.709344451751\n",
      "    ess            : 1.9690813610901186\n",
      "    log_marginal   : 1115.7350583483826\n",
      "    log_joint      : 1323.8868978682474\n",
      "    val_loss       : -1115.2103643002717\n",
      "    val_ess        : 1.9675819407338682\n",
      "    val_log_marginal: 1115.237601902174\n",
      "    val_log_joint  : 1323.8254235309103\n",
      "Train Epoch: 658 [0/101520 (0%)] Loss: -1119.808350\n",
      "Train Epoch: 658 [11264/101520 (11%)] Loss: -1118.636597\n",
      "Train Epoch: 658 [22528/101520 (22%)] Loss: -1114.967407\n",
      "Train Epoch: 658 [33792/101520 (33%)] Loss: -1113.472900\n",
      "Train Epoch: 658 [45056/101520 (44%)] Loss: -1115.354248\n",
      "Train Epoch: 658 [56320/101520 (55%)] Loss: -1113.995361\n",
      "Train Epoch: 658 [67584/101520 (67%)] Loss: -1106.671631\n",
      "Train Epoch: 658 [78848/101520 (78%)] Loss: -1117.717529\n",
      "Train Epoch: 658 [90112/101520 (89%)] Loss: -1108.546387\n",
      "Train Epoch: 658 [101376/101520 (100%)] Loss: -1107.982910\n",
      "    epoch          : 658\n",
      "    loss           : -1116.063059437814\n",
      "    ess            : 1.9688754357285236\n",
      "    log_marginal   : 1116.0892959671403\n",
      "    log_joint      : 1324.2558918861887\n",
      "    val_loss       : -1114.7896887737772\n",
      "    val_ess        : 1.9689525935960852\n",
      "    val_log_marginal: 1114.8162204908288\n",
      "    val_log_joint  : 1323.142578125\n",
      "Train Epoch: 659 [0/101520 (0%)] Loss: -1116.777344\n",
      "Train Epoch: 659 [11264/101520 (11%)] Loss: -1114.032227\n",
      "Train Epoch: 659 [22528/101520 (22%)] Loss: -1112.819824\n",
      "Train Epoch: 659 [33792/101520 (33%)] Loss: -1123.855713\n",
      "Train Epoch: 659 [45056/101520 (44%)] Loss: -1123.387207\n",
      "Train Epoch: 659 [56320/101520 (55%)] Loss: -1116.458984\n",
      "Train Epoch: 659 [67584/101520 (67%)] Loss: -1110.842773\n",
      "Train Epoch: 659 [78848/101520 (78%)] Loss: -1122.547852\n",
      "Train Epoch: 659 [90112/101520 (89%)] Loss: -1120.172485\n",
      "Train Epoch: 659 [101376/101520 (100%)] Loss: -1114.558594\n",
      "    epoch          : 659\n",
      "    loss           : -1115.9494346733668\n",
      "    ess            : 1.9693107191641726\n",
      "    log_marginal   : 1115.9758668832444\n",
      "    log_joint      : 1324.2045475178627\n",
      "    val_loss       : -1113.7009065047555\n",
      "    val_ess        : 1.9664292180019876\n",
      "    val_log_marginal: 1113.7285050101902\n",
      "    val_log_joint  : 1322.3336765455163\n",
      "Train Epoch: 660 [0/101520 (0%)] Loss: -1119.598877\n",
      "Train Epoch: 660 [11264/101520 (11%)] Loss: -1119.003418\n",
      "Train Epoch: 660 [22528/101520 (22%)] Loss: -1116.225708\n",
      "Train Epoch: 660 [33792/101520 (33%)] Loss: -1123.090820\n",
      "Train Epoch: 660 [45056/101520 (44%)] Loss: -1121.408691\n",
      "Train Epoch: 660 [56320/101520 (55%)] Loss: -1114.752075\n",
      "Train Epoch: 660 [67584/101520 (67%)] Loss: -1112.760254\n",
      "Train Epoch: 660 [78848/101520 (78%)] Loss: -1121.541260\n",
      "Train Epoch: 660 [90112/101520 (89%)] Loss: -1116.275024\n",
      "Train Epoch: 660 [101376/101520 (100%)] Loss: -1122.595093\n",
      "    epoch          : 660\n",
      "    loss           : -1116.0907963891725\n",
      "    ess            : 1.9686337787302295\n",
      "    log_marginal   : 1116.1183125098146\n",
      "    log_joint      : 1324.3533328262406\n",
      "    val_loss       : -1114.0148607336957\n",
      "    val_ess        : 1.9701189528340879\n",
      "    val_log_marginal: 1114.0405167289402\n",
      "    val_log_joint  : 1322.295946204144\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch660.pth ...\n",
      "Train Epoch: 661 [0/101520 (0%)] Loss: -1117.577393\n",
      "Train Epoch: 661 [11264/101520 (11%)] Loss: -1113.440918\n",
      "Train Epoch: 661 [22528/101520 (22%)] Loss: -1118.636475\n",
      "Train Epoch: 661 [33792/101520 (33%)] Loss: -1109.944092\n",
      "Train Epoch: 661 [45056/101520 (44%)] Loss: -1115.756348\n",
      "Train Epoch: 661 [56320/101520 (55%)] Loss: -1118.463013\n",
      "Train Epoch: 661 [67584/101520 (67%)] Loss: -1116.610962\n",
      "Train Epoch: 661 [78848/101520 (78%)] Loss: -1116.162720\n",
      "Train Epoch: 661 [90112/101520 (89%)] Loss: -1114.928223\n",
      "Train Epoch: 661 [101376/101520 (100%)] Loss: -1116.272461\n",
      "    epoch          : 661\n",
      "    loss           : -1116.474391611377\n",
      "    ess            : 1.9682313383524142\n",
      "    log_marginal   : 1116.5022475659548\n",
      "    log_joint      : 1324.7085371928\n",
      "    val_loss       : -1116.311327063519\n",
      "    val_ess        : 1.9667071985161824\n",
      "    val_log_marginal: 1116.3381719174592\n",
      "    val_log_joint  : 1324.3457880434783\n",
      "Train Epoch: 662 [0/101520 (0%)] Loss: -1117.856445\n",
      "Train Epoch: 662 [11264/101520 (11%)] Loss: -1119.511719\n",
      "Train Epoch: 662 [22528/101520 (22%)] Loss: -1115.702759\n",
      "Train Epoch: 662 [33792/101520 (33%)] Loss: -1109.077271\n",
      "Train Epoch: 662 [45056/101520 (44%)] Loss: -1116.880615\n",
      "Train Epoch: 662 [56320/101520 (55%)] Loss: -1117.158447\n",
      "Train Epoch: 662 [67584/101520 (67%)] Loss: -1109.687988\n",
      "Train Epoch: 662 [78848/101520 (78%)] Loss: -1119.501587\n",
      "Train Epoch: 662 [90112/101520 (89%)] Loss: -1117.259277\n",
      "Train Epoch: 662 [101376/101520 (100%)] Loss: -1117.564087\n",
      "    epoch          : 662\n",
      "    loss           : -1116.3521679442133\n",
      "    ess            : 1.969256355534846\n",
      "    log_marginal   : 1116.3781259814698\n",
      "    log_joint      : 1324.6613837007303\n",
      "    val_loss       : -1115.7755180027175\n",
      "    val_ess        : 1.9725282088569973\n",
      "    val_log_marginal: 1115.797655188519\n",
      "    val_log_joint  : 1323.5682266898777\n",
      "Train Epoch: 663 [0/101520 (0%)] Loss: -1116.530762\n",
      "Train Epoch: 663 [11264/101520 (11%)] Loss: -1120.873291\n",
      "Train Epoch: 663 [22528/101520 (22%)] Loss: -1117.047363\n",
      "Train Epoch: 663 [33792/101520 (33%)] Loss: -1110.086914\n",
      "Train Epoch: 663 [45056/101520 (44%)] Loss: -1111.844482\n",
      "Train Epoch: 663 [56320/101520 (55%)] Loss: -1118.281250\n",
      "Train Epoch: 663 [67584/101520 (67%)] Loss: -1119.681885\n",
      "Train Epoch: 663 [78848/101520 (78%)] Loss: -1121.978882\n",
      "Train Epoch: 663 [90112/101520 (89%)] Loss: -1119.956543\n",
      "Train Epoch: 663 [101376/101520 (100%)] Loss: -1120.261719\n",
      "    epoch          : 663\n",
      "    loss           : -1116.6102957414023\n",
      "    ess            : 1.968521101390896\n",
      "    log_marginal   : 1116.6372788012327\n",
      "    log_joint      : 1324.855903663827\n",
      "    val_loss       : -1115.4020571501358\n",
      "    val_ess        : 1.967411792796591\n",
      "    val_log_marginal: 1115.4301492442255\n",
      "    val_log_joint  : 1323.63500445822\n",
      "Train Epoch: 664 [0/101520 (0%)] Loss: -1112.153809\n",
      "Train Epoch: 664 [11264/101520 (11%)] Loss: -1123.059692\n",
      "Train Epoch: 664 [22528/101520 (22%)] Loss: -1109.751831\n",
      "Train Epoch: 664 [33792/101520 (33%)] Loss: -1120.495728\n",
      "Train Epoch: 664 [45056/101520 (44%)] Loss: -1109.841064\n",
      "Train Epoch: 664 [56320/101520 (55%)] Loss: -1123.807007\n",
      "Train Epoch: 664 [67584/101520 (67%)] Loss: -1113.290649\n",
      "Train Epoch: 664 [78848/101520 (78%)] Loss: -1118.734131\n",
      "Train Epoch: 664 [90112/101520 (89%)] Loss: -1118.812378\n",
      "Train Epoch: 664 [101376/101520 (100%)] Loss: -1119.225220\n",
      "    epoch          : 664\n",
      "    loss           : -1116.9259480998744\n",
      "    ess            : 1.96872361521026\n",
      "    log_marginal   : 1116.95290600954\n",
      "    log_joint      : 1325.103009554609\n",
      "    val_loss       : -1114.8793733016305\n",
      "    val_ess        : 1.9702264536981997\n",
      "    val_log_marginal: 1114.9036281419837\n",
      "    val_log_joint  : 1323.4261368461277\n",
      "Train Epoch: 665 [0/101520 (0%)] Loss: -1122.851929\n",
      "Train Epoch: 665 [11264/101520 (11%)] Loss: -1116.917480\n",
      "Train Epoch: 665 [22528/101520 (22%)] Loss: -1123.443970\n",
      "Train Epoch: 665 [33792/101520 (33%)] Loss: -1113.284912\n",
      "Train Epoch: 665 [45056/101520 (44%)] Loss: -1118.121582\n",
      "Train Epoch: 665 [56320/101520 (55%)] Loss: -1113.846069\n",
      "Train Epoch: 665 [67584/101520 (67%)] Loss: -1105.723389\n",
      "Train Epoch: 665 [78848/101520 (78%)] Loss: -1112.032959\n",
      "Train Epoch: 665 [90112/101520 (89%)] Loss: -1120.720459\n",
      "Train Epoch: 665 [101376/101520 (100%)] Loss: -1128.493042\n",
      "    epoch          : 665\n",
      "    loss           : -1116.8429470349795\n",
      "    ess            : 1.9688642719882217\n",
      "    log_marginal   : 1116.870051551704\n",
      "    log_joint      : 1325.0824833395493\n",
      "    val_loss       : -1117.1838272758152\n",
      "    val_ess        : 1.9677410177562549\n",
      "    val_log_marginal: 1117.2116805366848\n",
      "    val_log_joint  : 1325.4952923318615\n",
      "Train Epoch: 666 [0/101520 (0%)] Loss: -1124.256470\n",
      "Train Epoch: 666 [11264/101520 (11%)] Loss: -1121.223511\n",
      "Train Epoch: 666 [22528/101520 (22%)] Loss: -1113.033813\n",
      "Train Epoch: 666 [33792/101520 (33%)] Loss: -1113.355713\n",
      "Train Epoch: 666 [45056/101520 (44%)] Loss: -1117.188965\n",
      "Train Epoch: 666 [56320/101520 (55%)] Loss: -1110.987183\n",
      "Train Epoch: 666 [67584/101520 (67%)] Loss: -1108.337280\n",
      "Train Epoch: 666 [78848/101520 (78%)] Loss: -1114.342651\n",
      "Train Epoch: 666 [90112/101520 (89%)] Loss: -1117.773438\n",
      "Train Epoch: 666 [101376/101520 (100%)] Loss: -1117.760864\n",
      "    epoch          : 666\n",
      "    loss           : -1117.0347513936872\n",
      "    ess            : 1.9691805653835661\n",
      "    log_marginal   : 1117.0610897505103\n",
      "    log_joint      : 1325.3246935360396\n",
      "    val_loss       : -1115.7027216372283\n",
      "    val_ess        : 1.9665576997010603\n",
      "    val_log_marginal: 1115.7302405315897\n",
      "    val_log_joint  : 1323.7770730723505\n",
      "Train Epoch: 667 [0/101520 (0%)] Loss: -1115.491577\n",
      "Train Epoch: 667 [11264/101520 (11%)] Loss: -1120.412231\n",
      "Train Epoch: 667 [22528/101520 (22%)] Loss: -1121.910767\n",
      "Train Epoch: 667 [33792/101520 (33%)] Loss: -1117.170410\n",
      "Train Epoch: 667 [45056/101520 (44%)] Loss: -1121.128906\n",
      "Train Epoch: 667 [56320/101520 (55%)] Loss: -1121.783691\n",
      "Train Epoch: 667 [67584/101520 (67%)] Loss: -1120.892090\n",
      "Train Epoch: 667 [78848/101520 (78%)] Loss: -1124.763672\n",
      "Train Epoch: 667 [90112/101520 (89%)] Loss: -1117.199829\n",
      "Train Epoch: 667 [101376/101520 (100%)] Loss: -1126.056641\n",
      "    epoch          : 667\n",
      "    loss           : -1117.3162259049152\n",
      "    ess            : 1.9687782633843733\n",
      "    log_marginal   : 1117.3426206962547\n",
      "    log_joint      : 1325.536412531407\n",
      "    val_loss       : -1116.2820408033288\n",
      "    val_ess        : 1.9724034641100012\n",
      "    val_log_marginal: 1116.3044168223505\n",
      "    val_log_joint  : 1324.438529636549\n",
      "Train Epoch: 668 [0/101520 (0%)] Loss: -1114.331421\n",
      "Train Epoch: 668 [11264/101520 (11%)] Loss: -1116.461548\n",
      "Train Epoch: 668 [22528/101520 (22%)] Loss: -1118.607544\n",
      "Train Epoch: 668 [33792/101520 (33%)] Loss: -1114.380127\n",
      "Train Epoch: 668 [45056/101520 (44%)] Loss: -1121.526978\n",
      "Train Epoch: 668 [56320/101520 (55%)] Loss: -1121.637451\n",
      "Train Epoch: 668 [67584/101520 (67%)] Loss: -1111.564453\n",
      "Train Epoch: 668 [78848/101520 (78%)] Loss: -1116.987549\n",
      "Train Epoch: 668 [90112/101520 (89%)] Loss: -1112.517700\n",
      "Train Epoch: 668 [101376/101520 (100%)] Loss: -1108.273682\n",
      "    epoch          : 668\n",
      "    loss           : -1117.1392558495604\n",
      "    ess            : 1.9688686587702688\n",
      "    log_marginal   : 1117.1661027304492\n",
      "    log_joint      : 1325.418175472087\n",
      "    val_loss       : -1117.1099057404892\n",
      "    val_ess        : 1.9698166225267493\n",
      "    val_log_marginal: 1117.1346594769022\n",
      "    val_log_joint  : 1325.2651473335598\n",
      "Train Epoch: 669 [0/101520 (0%)] Loss: -1122.774414\n",
      "Train Epoch: 669 [11264/101520 (11%)] Loss: -1112.953613\n",
      "Train Epoch: 669 [22528/101520 (22%)] Loss: -1120.296631\n",
      "Train Epoch: 669 [33792/101520 (33%)] Loss: -1115.445801\n",
      "Train Epoch: 669 [45056/101520 (44%)] Loss: -1115.500732\n",
      "Train Epoch: 669 [56320/101520 (55%)] Loss: -1118.534668\n",
      "Train Epoch: 669 [67584/101520 (67%)] Loss: -1116.118408\n",
      "Train Epoch: 669 [78848/101520 (78%)] Loss: -1120.250244\n",
      "Train Epoch: 669 [90112/101520 (89%)] Loss: -1120.607666\n",
      "Train Epoch: 669 [101376/101520 (100%)] Loss: -1118.572876\n",
      "    epoch          : 669\n",
      "    loss           : -1117.5763424053864\n",
      "    ess            : 1.9689552418550653\n",
      "    log_marginal   : 1117.6033825131517\n",
      "    log_joint      : 1325.8230796315563\n",
      "    val_loss       : -1116.5994873046875\n",
      "    val_ess        : 1.9709308976712434\n",
      "    val_log_marginal: 1116.6249840777853\n",
      "    val_log_joint  : 1324.8968824303668\n",
      "Train Epoch: 670 [0/101520 (0%)] Loss: -1117.043213\n",
      "Train Epoch: 670 [11264/101520 (11%)] Loss: -1124.351562\n",
      "Train Epoch: 670 [22528/101520 (22%)] Loss: -1116.958740\n",
      "Train Epoch: 670 [33792/101520 (33%)] Loss: -1119.569214\n",
      "Train Epoch: 670 [45056/101520 (44%)] Loss: -1105.763306\n",
      "Train Epoch: 670 [56320/101520 (55%)] Loss: -1124.033325\n",
      "Train Epoch: 670 [67584/101520 (67%)] Loss: -1114.343262\n",
      "Train Epoch: 670 [78848/101520 (78%)] Loss: -1112.370605\n",
      "Train Epoch: 670 [90112/101520 (89%)] Loss: -1117.923828\n",
      "Train Epoch: 670 [101376/101520 (100%)] Loss: -1113.932739\n",
      "    epoch          : 670\n",
      "    loss           : -1117.7113227269158\n",
      "    ess            : 1.968186818774621\n",
      "    log_marginal   : 1117.7389179785646\n",
      "    log_joint      : 1325.9246022593436\n",
      "    val_loss       : -1117.1858493970788\n",
      "    val_ess        : 1.968119351760201\n",
      "    val_log_marginal: 1117.2147269870925\n",
      "    val_log_joint  : 1325.5105405061142\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch670.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 671 [0/101520 (0%)] Loss: -1116.054443\n",
      "Train Epoch: 671 [11264/101520 (11%)] Loss: -1113.041016\n",
      "Train Epoch: 671 [22528/101520 (22%)] Loss: -1112.433838\n",
      "Train Epoch: 671 [33792/101520 (33%)] Loss: -1117.988892\n",
      "Train Epoch: 671 [45056/101520 (44%)] Loss: -1123.489502\n",
      "Train Epoch: 671 [56320/101520 (55%)] Loss: -1120.186523\n",
      "Train Epoch: 671 [67584/101520 (67%)] Loss: -1119.662231\n",
      "Train Epoch: 671 [78848/101520 (78%)] Loss: -1121.732178\n",
      "Train Epoch: 671 [90112/101520 (89%)] Loss: -1118.065308\n",
      "Train Epoch: 671 [101376/101520 (100%)] Loss: -1127.217163\n",
      "    epoch          : 671\n",
      "    loss           : -1117.776991034273\n",
      "    ess            : 1.968385806035756\n",
      "    log_marginal   : 1117.8040648800643\n",
      "    log_joint      : 1325.9783898741755\n",
      "    val_loss       : -1115.9049921450408\n",
      "    val_ess        : 1.9700443070867788\n",
      "    val_log_marginal: 1115.9303137737772\n",
      "    val_log_joint  : 1324.1995690387228\n",
      "Train Epoch: 672 [0/101520 (0%)] Loss: -1120.692017\n",
      "Train Epoch: 672 [11264/101520 (11%)] Loss: -1111.426147\n",
      "Train Epoch: 672 [22528/101520 (22%)] Loss: -1123.398560\n",
      "Train Epoch: 672 [33792/101520 (33%)] Loss: -1111.056885\n",
      "Train Epoch: 672 [45056/101520 (44%)] Loss: -1119.642578\n",
      "Train Epoch: 672 [56320/101520 (55%)] Loss: -1115.714600\n",
      "Train Epoch: 672 [67584/101520 (67%)] Loss: -1119.259155\n",
      "Train Epoch: 672 [78848/101520 (78%)] Loss: -1111.365479\n",
      "Train Epoch: 672 [90112/101520 (89%)] Loss: -1118.066895\n",
      "Train Epoch: 672 [101376/101520 (100%)] Loss: -1122.769165\n",
      "    epoch          : 672\n",
      "    loss           : -1118.0546457875314\n",
      "    ess            : 1.9683271035477146\n",
      "    log_marginal   : 1118.081673013505\n",
      "    log_joint      : 1326.2936968779445\n",
      "    val_loss       : -1116.3843834918478\n",
      "    val_ess        : 1.968993523846502\n",
      "    val_log_marginal: 1116.4111328125\n",
      "    val_log_joint  : 1324.6695185122283\n",
      "Train Epoch: 673 [0/101520 (0%)] Loss: -1119.237793\n",
      "Train Epoch: 673 [11264/101520 (11%)] Loss: -1105.474609\n",
      "Train Epoch: 673 [22528/101520 (22%)] Loss: -1120.410645\n",
      "Train Epoch: 673 [33792/101520 (33%)] Loss: -1120.071289\n",
      "Train Epoch: 673 [45056/101520 (44%)] Loss: -1113.551392\n",
      "Train Epoch: 673 [56320/101520 (55%)] Loss: -1113.172119\n",
      "Train Epoch: 673 [67584/101520 (67%)] Loss: -1110.561279\n",
      "Train Epoch: 673 [78848/101520 (78%)] Loss: -1123.409668\n",
      "Train Epoch: 673 [90112/101520 (89%)] Loss: -1115.784058\n",
      "Train Epoch: 673 [101376/101520 (100%)] Loss: -1119.007568\n",
      "    epoch          : 673\n",
      "    loss           : -1117.977848838921\n",
      "    ess            : 1.9684154172638553\n",
      "    log_marginal   : 1118.004567515311\n",
      "    log_joint      : 1326.194713803392\n",
      "    val_loss       : -1116.766208814538\n",
      "    val_ess        : 1.969688260036966\n",
      "    val_log_marginal: 1116.7936321756115\n",
      "    val_log_joint  : 1324.706399668818\n",
      "Train Epoch: 674 [0/101520 (0%)] Loss: -1119.406738\n",
      "Train Epoch: 674 [11264/101520 (11%)] Loss: -1118.979004\n",
      "Train Epoch: 674 [22528/101520 (22%)] Loss: -1118.167236\n",
      "Train Epoch: 674 [33792/101520 (33%)] Loss: -1114.373779\n",
      "Train Epoch: 674 [45056/101520 (44%)] Loss: -1113.447388\n",
      "Train Epoch: 674 [56320/101520 (55%)] Loss: -1125.155762\n",
      "Train Epoch: 674 [67584/101520 (67%)] Loss: -1116.583862\n",
      "Train Epoch: 674 [78848/101520 (78%)] Loss: -1116.633789\n",
      "Train Epoch: 674 [90112/101520 (89%)] Loss: -1118.174805\n",
      "Train Epoch: 674 [101376/101520 (100%)] Loss: -1133.532593\n",
      "    epoch          : 674\n",
      "    loss           : -1118.1873000255182\n",
      "    ess            : 1.9687602436123182\n",
      "    log_marginal   : 1118.2139886443938\n",
      "    log_joint      : 1326.425467793067\n",
      "    val_loss       : -1117.4681290336277\n",
      "    val_ess        : 1.9686141117759373\n",
      "    val_log_marginal: 1117.494135317595\n",
      "    val_log_joint  : 1325.6821607506793\n",
      "Train Epoch: 675 [0/101520 (0%)] Loss: -1120.587402\n",
      "Train Epoch: 675 [11264/101520 (11%)] Loss: -1113.838135\n",
      "Train Epoch: 675 [22528/101520 (22%)] Loss: -1121.565430\n",
      "Train Epoch: 675 [33792/101520 (33%)] Loss: -1116.033691\n",
      "Train Epoch: 675 [45056/101520 (44%)] Loss: -1115.170288\n",
      "Train Epoch: 675 [56320/101520 (55%)] Loss: -1121.939697\n",
      "Train Epoch: 675 [67584/101520 (67%)] Loss: -1119.477539\n",
      "Train Epoch: 675 [78848/101520 (78%)] Loss: -1114.656982\n",
      "Train Epoch: 675 [90112/101520 (89%)] Loss: -1107.490845\n",
      "Train Epoch: 675 [101376/101520 (100%)] Loss: -1111.294434\n",
      "    epoch          : 675\n",
      "    loss           : -1118.2849274448413\n",
      "    ess            : 1.9684642942706545\n",
      "    log_marginal   : 1118.312214760325\n",
      "    log_joint      : 1326.585397691583\n",
      "    val_loss       : -1116.89013671875\n",
      "    val_ess        : 1.9707393957220989\n",
      "    val_log_marginal: 1116.9146622367527\n",
      "    val_log_joint  : 1324.9452328889267\n",
      "Train Epoch: 676 [0/101520 (0%)] Loss: -1114.255859\n",
      "Train Epoch: 676 [11264/101520 (11%)] Loss: -1122.921875\n",
      "Train Epoch: 676 [22528/101520 (22%)] Loss: -1114.440186\n",
      "Train Epoch: 676 [33792/101520 (33%)] Loss: -1120.580200\n",
      "Train Epoch: 676 [45056/101520 (44%)] Loss: -1122.673584\n",
      "Train Epoch: 676 [56320/101520 (55%)] Loss: -1118.933716\n",
      "Train Epoch: 676 [67584/101520 (67%)] Loss: -1119.397583\n",
      "Train Epoch: 676 [78848/101520 (78%)] Loss: -1109.740723\n",
      "Train Epoch: 676 [90112/101520 (89%)] Loss: -1111.483154\n",
      "Train Epoch: 676 [101376/101520 (100%)] Loss: -1123.039673\n",
      "    epoch          : 676\n",
      "    loss           : -1118.4778022191033\n",
      "    ess            : 1.968798333676017\n",
      "    log_marginal   : 1118.5046767038316\n",
      "    log_joint      : 1326.7562893814777\n",
      "    val_loss       : -1116.7723972486413\n",
      "    val_ess        : 1.9695205429325933\n",
      "    val_log_marginal: 1116.8009457795517\n",
      "    val_log_joint  : 1324.992819081182\n",
      "Train Epoch: 677 [0/101520 (0%)] Loss: -1122.736450\n",
      "Train Epoch: 677 [11264/101520 (11%)] Loss: -1119.495117\n",
      "Train Epoch: 677 [22528/101520 (22%)] Loss: -1117.971313\n",
      "Train Epoch: 677 [33792/101520 (33%)] Loss: -1122.670898\n",
      "Train Epoch: 677 [45056/101520 (44%)] Loss: -1124.557495\n",
      "Train Epoch: 677 [56320/101520 (55%)] Loss: -1107.731689\n",
      "Train Epoch: 677 [67584/101520 (67%)] Loss: -1116.710083\n",
      "Train Epoch: 677 [78848/101520 (78%)] Loss: -1116.837402\n",
      "Train Epoch: 677 [90112/101520 (89%)] Loss: -1120.267090\n",
      "Train Epoch: 677 [101376/101520 (100%)] Loss: -1110.110718\n",
      "    epoch          : 677\n",
      "    loss           : -1118.626258121663\n",
      "    ess            : 1.9687707615857148\n",
      "    log_marginal   : 1118.6532552901224\n",
      "    log_joint      : 1326.8291481823178\n",
      "    val_loss       : -1117.9019669242527\n",
      "    val_ess        : 1.970253348350525\n",
      "    val_log_marginal: 1117.9266675866168\n",
      "    val_log_joint  : 1326.0656048318615\n",
      "Train Epoch: 678 [0/101520 (0%)] Loss: -1118.086304\n",
      "Train Epoch: 678 [11264/101520 (11%)] Loss: -1118.008545\n",
      "Train Epoch: 678 [22528/101520 (22%)] Loss: -1120.094971\n",
      "Train Epoch: 678 [33792/101520 (33%)] Loss: -1118.677979\n",
      "Train Epoch: 678 [45056/101520 (44%)] Loss: -1118.658691\n",
      "Train Epoch: 678 [56320/101520 (55%)] Loss: -1119.770752\n",
      "Train Epoch: 678 [67584/101520 (67%)] Loss: -1120.109131\n",
      "Train Epoch: 678 [78848/101520 (78%)] Loss: -1120.098877\n",
      "Train Epoch: 678 [90112/101520 (89%)] Loss: -1113.100708\n",
      "Train Epoch: 678 [101376/101520 (100%)] Loss: -1114.769653\n",
      "    epoch          : 678\n",
      "    loss           : -1118.880700499568\n",
      "    ess            : 1.9684565402754588\n",
      "    log_marginal   : 1118.9074952398712\n",
      "    log_joint      : 1327.119215462076\n",
      "    val_loss       : -1119.177346934443\n",
      "    val_ess        : 1.9685700665349546\n",
      "    val_log_marginal: 1119.2053116508152\n",
      "    val_log_joint  : 1327.7640274711277\n",
      "Train Epoch: 679 [0/101520 (0%)] Loss: -1118.429932\n",
      "Train Epoch: 679 [11264/101520 (11%)] Loss: -1124.050049\n",
      "Train Epoch: 679 [22528/101520 (22%)] Loss: -1117.042725\n",
      "Train Epoch: 679 [33792/101520 (33%)] Loss: -1120.751953\n",
      "Train Epoch: 679 [45056/101520 (44%)] Loss: -1116.845947\n",
      "Train Epoch: 679 [56320/101520 (55%)] Loss: -1120.895508\n",
      "Train Epoch: 679 [67584/101520 (67%)] Loss: -1122.226196\n",
      "Train Epoch: 679 [78848/101520 (78%)] Loss: -1126.589111\n",
      "Train Epoch: 679 [90112/101520 (89%)] Loss: -1116.207153\n",
      "Train Epoch: 679 [101376/101520 (100%)] Loss: -1117.113525\n",
      "    epoch          : 679\n",
      "    loss           : -1118.9326773025282\n",
      "    ess            : 1.9685234706006458\n",
      "    log_marginal   : 1118.9603756085114\n",
      "    log_joint      : 1327.2153559545775\n",
      "    val_loss       : -1118.0801630434783\n",
      "    val_ess        : 1.9689402528431104\n",
      "    val_log_marginal: 1118.1052511463995\n",
      "    val_log_joint  : 1326.4668181046195\n",
      "Train Epoch: 680 [0/101520 (0%)] Loss: -1114.298828\n",
      "Train Epoch: 680 [11264/101520 (11%)] Loss: -1123.754272\n",
      "Train Epoch: 680 [22528/101520 (22%)] Loss: -1115.619263\n",
      "Train Epoch: 680 [33792/101520 (33%)] Loss: -1111.286621\n",
      "Train Epoch: 680 [45056/101520 (44%)] Loss: -1112.741211\n",
      "Train Epoch: 680 [56320/101520 (55%)] Loss: -1113.218750\n",
      "Train Epoch: 680 [67584/101520 (67%)] Loss: -1123.784790\n",
      "Train Epoch: 680 [78848/101520 (78%)] Loss: -1120.349365\n",
      "Train Epoch: 680 [90112/101520 (89%)] Loss: -1121.241577\n",
      "Train Epoch: 680 [101376/101520 (100%)] Loss: -1112.284424\n",
      "    epoch          : 680\n",
      "    loss           : -1119.029699891057\n",
      "    ess            : 1.9691797147444146\n",
      "    log_marginal   : 1119.0559290593592\n",
      "    log_joint      : 1327.2844759687107\n",
      "    val_loss       : -1116.2899488366168\n",
      "    val_ess        : 1.9674036140027253\n",
      "    val_log_marginal: 1116.3186884341033\n",
      "    val_log_joint  : 1324.7223484205163\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch680.pth ...\n",
      "Train Epoch: 681 [0/101520 (0%)] Loss: -1121.230103\n",
      "Train Epoch: 681 [11264/101520 (11%)] Loss: -1111.348145\n",
      "Train Epoch: 681 [22528/101520 (22%)] Loss: -1121.554565\n",
      "Train Epoch: 681 [33792/101520 (33%)] Loss: -1121.511597\n",
      "Train Epoch: 681 [45056/101520 (44%)] Loss: -1123.263672\n",
      "Train Epoch: 681 [56320/101520 (55%)] Loss: -1118.744995\n",
      "Train Epoch: 681 [67584/101520 (67%)] Loss: -1120.669678\n",
      "Train Epoch: 681 [78848/101520 (78%)] Loss: -1122.074219\n",
      "Train Epoch: 681 [90112/101520 (89%)] Loss: -1124.176025\n",
      "Train Epoch: 681 [101376/101520 (100%)] Loss: -1119.165161\n",
      "    epoch          : 681\n",
      "    loss           : -1119.3017639466866\n",
      "    ess            : 1.9683001382866097\n",
      "    log_marginal   : 1119.329233447511\n",
      "    log_joint      : 1327.5123812421482\n",
      "    val_loss       : -1119.7233249830163\n",
      "    val_ess        : 1.9704540605130403\n",
      "    val_log_marginal: 1119.7494904891305\n",
      "    val_log_joint  : 1327.9502696161685\n",
      "Train Epoch: 682 [0/101520 (0%)] Loss: -1126.116821\n",
      "Train Epoch: 682 [11264/101520 (11%)] Loss: -1118.529541\n",
      "Train Epoch: 682 [22528/101520 (22%)] Loss: -1125.178345\n",
      "Train Epoch: 682 [33792/101520 (33%)] Loss: -1121.971191\n",
      "Train Epoch: 682 [45056/101520 (44%)] Loss: -1115.926880\n",
      "Train Epoch: 682 [56320/101520 (55%)] Loss: -1119.036255\n",
      "Train Epoch: 682 [67584/101520 (67%)] Loss: -1116.778320\n",
      "Train Epoch: 682 [78848/101520 (78%)] Loss: -1117.366943\n",
      "Train Epoch: 682 [90112/101520 (89%)] Loss: -1121.737427\n",
      "Train Epoch: 682 [101376/101520 (100%)] Loss: -1103.515869\n",
      "    epoch          : 682\n",
      "    loss           : -1119.3286991598618\n",
      "    ess            : 1.9686424935882414\n",
      "    log_marginal   : 1119.3554896062342\n",
      "    log_joint      : 1327.5490790132303\n",
      "    val_loss       : -1119.2204271399457\n",
      "    val_ess        : 1.971177603887475\n",
      "    val_log_marginal: 1119.2448040506115\n",
      "    val_log_joint  : 1327.2261113705842\n",
      "Train Epoch: 683 [0/101520 (0%)] Loss: -1122.739746\n",
      "Train Epoch: 683 [11264/101520 (11%)] Loss: -1118.979736\n",
      "Train Epoch: 683 [22528/101520 (22%)] Loss: -1120.361450\n",
      "Train Epoch: 683 [33792/101520 (33%)] Loss: -1124.537109\n",
      "Train Epoch: 683 [45056/101520 (44%)] Loss: -1124.950684\n",
      "Train Epoch: 683 [56320/101520 (55%)] Loss: -1115.183838\n",
      "Train Epoch: 683 [67584/101520 (67%)] Loss: -1117.280273\n",
      "Train Epoch: 683 [78848/101520 (78%)] Loss: -1118.853760\n",
      "Train Epoch: 683 [90112/101520 (89%)] Loss: -1115.540161\n",
      "Train Epoch: 683 [101376/101520 (100%)] Loss: -1116.340088\n",
      "    epoch          : 683\n",
      "    loss           : -1119.3946379848462\n",
      "    ess            : 1.968786947691261\n",
      "    log_marginal   : 1119.4213480733747\n",
      "    log_joint      : 1327.6943402314305\n",
      "    val_loss       : -1117.6701925526495\n",
      "    val_ess        : 1.9680161890776262\n",
      "    val_log_marginal: 1117.7014584748642\n",
      "    val_log_joint  : 1326.0938773777175\n",
      "Train Epoch: 684 [0/101520 (0%)] Loss: -1117.965210\n",
      "Train Epoch: 684 [11264/101520 (11%)] Loss: -1125.137573\n",
      "Train Epoch: 684 [22528/101520 (22%)] Loss: -1120.374390\n",
      "Train Epoch: 684 [33792/101520 (33%)] Loss: -1121.800781\n",
      "Train Epoch: 684 [45056/101520 (44%)] Loss: -1122.211304\n",
      "Train Epoch: 684 [56320/101520 (55%)] Loss: -1115.611328\n",
      "Train Epoch: 684 [67584/101520 (67%)] Loss: -1126.889404\n",
      "Train Epoch: 684 [78848/101520 (78%)] Loss: -1120.309204\n",
      "Train Epoch: 684 [90112/101520 (89%)] Loss: -1125.369385\n",
      "Train Epoch: 684 [101376/101520 (100%)] Loss: -1137.003052\n",
      "    epoch          : 684\n",
      "    loss           : -1119.7955285460505\n",
      "    ess            : 1.9681649537541759\n",
      "    log_marginal   : 1119.8238059192447\n",
      "    log_joint      : 1328.072463759226\n",
      "    val_loss       : -1118.237649668818\n",
      "    val_ess        : 1.966107321822125\n",
      "    val_log_marginal: 1118.2710810122283\n",
      "    val_log_joint  : 1326.5397312330163\n",
      "Train Epoch: 685 [0/101520 (0%)] Loss: -1117.607422\n",
      "Train Epoch: 685 [11264/101520 (11%)] Loss: -1113.452881\n",
      "Train Epoch: 685 [22528/101520 (22%)] Loss: -1121.309082\n",
      "Train Epoch: 685 [33792/101520 (33%)] Loss: -1121.280273\n",
      "Train Epoch: 685 [45056/101520 (44%)] Loss: -1119.291748\n",
      "Train Epoch: 685 [56320/101520 (55%)] Loss: -1113.942871\n",
      "Train Epoch: 685 [67584/101520 (67%)] Loss: -1116.121460\n",
      "Train Epoch: 685 [78848/101520 (78%)] Loss: -1112.952026\n",
      "Train Epoch: 685 [90112/101520 (89%)] Loss: -1122.093140\n",
      "Train Epoch: 685 [101376/101520 (100%)] Loss: -1124.527588\n",
      "    epoch          : 685\n",
      "    loss           : -1119.6917135727465\n",
      "    ess            : 1.9679795869031744\n",
      "    log_marginal   : 1119.719687303706\n",
      "    log_joint      : 1327.9608319919912\n",
      "    val_loss       : -1118.6190769361413\n",
      "    val_ess        : 1.9695260421089504\n",
      "    val_log_marginal: 1118.6450832201087\n",
      "    val_log_joint  : 1326.970846424932\n",
      "Train Epoch: 686 [0/101520 (0%)] Loss: -1117.623779\n",
      "Train Epoch: 686 [11264/101520 (11%)] Loss: -1120.119629\n",
      "Train Epoch: 686 [22528/101520 (22%)] Loss: -1118.337158\n",
      "Train Epoch: 686 [33792/101520 (33%)] Loss: -1114.331909\n",
      "Train Epoch: 686 [45056/101520 (44%)] Loss: -1112.183105\n",
      "Train Epoch: 686 [56320/101520 (55%)] Loss: -1114.355469\n",
      "Train Epoch: 686 [67584/101520 (67%)] Loss: -1117.487793\n",
      "Train Epoch: 686 [78848/101520 (78%)] Loss: -1127.157837\n",
      "Train Epoch: 686 [90112/101520 (89%)] Loss: -1121.992310\n",
      "Train Epoch: 686 [101376/101520 (100%)] Loss: -1132.646118\n",
      "    epoch          : 686\n",
      "    loss           : -1119.8744479232098\n",
      "    ess            : 1.9692509923148993\n",
      "    log_marginal   : 1119.901396018177\n",
      "    log_joint      : 1328.1224156672033\n",
      "    val_loss       : -1118.8019807235055\n",
      "    val_ess        : 1.9699659192043801\n",
      "    val_log_marginal: 1118.8275942595108\n",
      "    val_log_joint  : 1327.1955831776495\n",
      "Train Epoch: 687 [0/101520 (0%)] Loss: -1117.468018\n",
      "Train Epoch: 687 [11264/101520 (11%)] Loss: -1122.512207\n",
      "Train Epoch: 687 [22528/101520 (22%)] Loss: -1121.874756\n",
      "Train Epoch: 687 [33792/101520 (33%)] Loss: -1120.497559\n",
      "Train Epoch: 687 [45056/101520 (44%)] Loss: -1123.001343\n",
      "Train Epoch: 687 [56320/101520 (55%)] Loss: -1119.864502\n",
      "Train Epoch: 687 [67584/101520 (67%)] Loss: -1118.427979\n",
      "Train Epoch: 687 [78848/101520 (78%)] Loss: -1122.106689\n",
      "Train Epoch: 687 [90112/101520 (89%)] Loss: -1113.536133\n",
      "Train Epoch: 687 [101376/101520 (100%)] Loss: -1126.140259\n",
      "    epoch          : 687\n",
      "    loss           : -1120.1224960250472\n",
      "    ess            : 1.968314347554691\n",
      "    log_marginal   : 1120.1503440051822\n",
      "    log_joint      : 1328.3629628857177\n",
      "    val_loss       : -1118.2385572350543\n",
      "    val_ess        : 1.9652278319649075\n",
      "    val_log_marginal: 1118.268071713655\n",
      "    val_log_joint  : 1326.710454526155\n",
      "Train Epoch: 688 [0/101520 (0%)] Loss: -1127.344971\n",
      "Train Epoch: 688 [11264/101520 (11%)] Loss: -1126.454834\n",
      "Train Epoch: 688 [22528/101520 (22%)] Loss: -1122.910767\n",
      "Train Epoch: 688 [33792/101520 (33%)] Loss: -1116.840820\n",
      "Train Epoch: 688 [45056/101520 (44%)] Loss: -1123.914185\n",
      "Train Epoch: 688 [56320/101520 (55%)] Loss: -1113.336426\n",
      "Train Epoch: 688 [67584/101520 (67%)] Loss: -1122.515015\n",
      "Train Epoch: 688 [78848/101520 (78%)] Loss: -1121.016235\n",
      "Train Epoch: 688 [90112/101520 (89%)] Loss: -1110.552246\n",
      "Train Epoch: 688 [101376/101520 (100%)] Loss: -1116.755005\n",
      "    epoch          : 688\n",
      "    loss           : -1120.0980347293107\n",
      "    ess            : 1.9689450365814132\n",
      "    log_marginal   : 1120.1252502748116\n",
      "    log_joint      : 1328.309694836487\n",
      "    val_loss       : -1120.9934188179348\n",
      "    val_ess        : 1.9674772231475166\n",
      "    val_log_marginal: 1121.0238196331522\n",
      "    val_log_joint  : 1329.2680292544158\n",
      "Train Epoch: 689 [0/101520 (0%)] Loss: -1123.740601\n",
      "Train Epoch: 689 [11264/101520 (11%)] Loss: -1121.539185\n",
      "Train Epoch: 689 [22528/101520 (22%)] Loss: -1119.803467\n",
      "Train Epoch: 689 [33792/101520 (33%)] Loss: -1124.528320\n",
      "Train Epoch: 689 [45056/101520 (44%)] Loss: -1115.547363\n",
      "Train Epoch: 689 [56320/101520 (55%)] Loss: -1121.928711\n",
      "Train Epoch: 689 [67584/101520 (67%)] Loss: -1121.978027\n",
      "Train Epoch: 689 [78848/101520 (78%)] Loss: -1115.038818\n",
      "Train Epoch: 689 [90112/101520 (89%)] Loss: -1123.567627\n",
      "Train Epoch: 689 [101376/101520 (100%)] Loss: -1124.031494\n",
      "    epoch          : 689\n",
      "    loss           : -1120.3254443604742\n",
      "    ess            : 1.968220516664898\n",
      "    log_marginal   : 1120.3537094652952\n",
      "    log_joint      : 1328.5415223088096\n",
      "    val_loss       : -1118.650050951087\n",
      "    val_ess        : 1.9691641745360002\n",
      "    val_log_marginal: 1118.676513671875\n",
      "    val_log_joint  : 1327.018655528193\n",
      "Train Epoch: 690 [0/101520 (0%)] Loss: -1119.362183\n",
      "Train Epoch: 690 [11264/101520 (11%)] Loss: -1122.307251\n",
      "Train Epoch: 690 [22528/101520 (22%)] Loss: -1120.904785\n",
      "Train Epoch: 690 [33792/101520 (33%)] Loss: -1116.914307\n",
      "Train Epoch: 690 [45056/101520 (44%)] Loss: -1115.794678\n",
      "Train Epoch: 690 [56320/101520 (55%)] Loss: -1116.589722\n",
      "Train Epoch: 690 [67584/101520 (67%)] Loss: -1116.109375\n",
      "Train Epoch: 690 [78848/101520 (78%)] Loss: -1122.759766\n",
      "Train Epoch: 690 [90112/101520 (89%)] Loss: -1125.401123\n",
      "Train Epoch: 690 [101376/101520 (100%)] Loss: -1118.887573\n",
      "    epoch          : 690\n",
      "    loss           : -1120.5115856381517\n",
      "    ess            : 1.9684360973799049\n",
      "    log_marginal   : 1120.5389704872016\n",
      "    log_joint      : 1328.6973907624058\n",
      "    val_loss       : -1118.9609587296195\n",
      "    val_ess        : 1.970131957012674\n",
      "    val_log_marginal: 1118.9862378991168\n",
      "    val_log_joint  : 1327.3046025815217\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch690.pth ...\n",
      "Train Epoch: 691 [0/101520 (0%)] Loss: -1125.917480\n",
      "Train Epoch: 691 [11264/101520 (11%)] Loss: -1126.815308\n",
      "Train Epoch: 691 [22528/101520 (22%)] Loss: -1122.463257\n",
      "Train Epoch: 691 [33792/101520 (33%)] Loss: -1122.372803\n",
      "Train Epoch: 691 [45056/101520 (44%)] Loss: -1118.086304\n",
      "Train Epoch: 691 [56320/101520 (55%)] Loss: -1125.416382\n",
      "Train Epoch: 691 [67584/101520 (67%)] Loss: -1117.147949\n",
      "Train Epoch: 691 [78848/101520 (78%)] Loss: -1122.837402\n",
      "Train Epoch: 691 [90112/101520 (89%)] Loss: -1121.613525\n",
      "Train Epoch: 691 [101376/101520 (100%)] Loss: -1118.353271\n",
      "    epoch          : 691\n",
      "    loss           : -1120.4936474364008\n",
      "    ess            : 1.9687396245985176\n",
      "    log_marginal   : 1120.5205360297582\n",
      "    log_joint      : 1328.676543729389\n",
      "    val_loss       : -1120.6870860224185\n",
      "    val_ess        : 1.9667502123376597\n",
      "    val_log_marginal: 1120.7155390200408\n",
      "    val_log_joint  : 1328.9178997537365\n",
      "Train Epoch: 692 [0/101520 (0%)] Loss: -1124.569580\n",
      "Train Epoch: 692 [11264/101520 (11%)] Loss: -1118.337402\n",
      "Train Epoch: 692 [22528/101520 (22%)] Loss: -1124.964844\n",
      "Train Epoch: 692 [33792/101520 (33%)] Loss: -1114.865234\n",
      "Train Epoch: 692 [45056/101520 (44%)] Loss: -1121.770020\n",
      "Train Epoch: 692 [56320/101520 (55%)] Loss: -1126.972168\n",
      "Train Epoch: 692 [67584/101520 (67%)] Loss: -1123.172241\n",
      "Train Epoch: 692 [78848/101520 (78%)] Loss: -1122.095703\n",
      "Train Epoch: 692 [90112/101520 (89%)] Loss: -1116.115234\n",
      "Train Epoch: 692 [101376/101520 (100%)] Loss: -1120.548218\n",
      "    epoch          : 692\n",
      "    loss           : -1120.6849831432553\n",
      "    ess            : 1.9687697707708158\n",
      "    log_marginal   : 1120.712375966748\n",
      "    log_joint      : 1328.9978450602623\n",
      "    val_loss       : -1119.3196915336277\n",
      "    val_ess        : 1.9698261333548504\n",
      "    val_log_marginal: 1119.3466425356658\n",
      "    val_log_joint  : 1327.4876868206522\n",
      "Train Epoch: 693 [0/101520 (0%)] Loss: -1119.294312\n",
      "Train Epoch: 693 [11264/101520 (11%)] Loss: -1126.877930\n",
      "Train Epoch: 693 [22528/101520 (22%)] Loss: -1128.105591\n",
      "Train Epoch: 693 [33792/101520 (33%)] Loss: -1120.533447\n",
      "Train Epoch: 693 [45056/101520 (44%)] Loss: -1117.411133\n",
      "Train Epoch: 693 [56320/101520 (55%)] Loss: -1123.606689\n",
      "Train Epoch: 693 [67584/101520 (67%)] Loss: -1119.958740\n",
      "Train Epoch: 693 [78848/101520 (78%)] Loss: -1125.812866\n",
      "Train Epoch: 693 [90112/101520 (89%)] Loss: -1117.793457\n",
      "Train Epoch: 693 [101376/101520 (100%)] Loss: -1113.677979\n",
      "    epoch          : 693\n",
      "    loss           : -1120.831977882577\n",
      "    ess            : 1.9689861284428505\n",
      "    log_marginal   : 1120.8595357156878\n",
      "    log_joint      : 1329.06479541261\n",
      "    val_loss       : -1117.6408266813858\n",
      "    val_ess        : 1.9683522141498069\n",
      "    val_log_marginal: 1117.6707922894022\n",
      "    val_log_joint  : 1326.1921280570652\n",
      "Train Epoch: 694 [0/101520 (0%)] Loss: -1121.205322\n",
      "Train Epoch: 694 [11264/101520 (11%)] Loss: -1120.440674\n",
      "Train Epoch: 694 [22528/101520 (22%)] Loss: -1117.934448\n",
      "Train Epoch: 694 [33792/101520 (33%)] Loss: -1121.484375\n",
      "Train Epoch: 694 [45056/101520 (44%)] Loss: -1126.435547\n",
      "Train Epoch: 694 [56320/101520 (55%)] Loss: -1122.688721\n",
      "Train Epoch: 694 [67584/101520 (67%)] Loss: -1125.810791\n",
      "Train Epoch: 694 [78848/101520 (78%)] Loss: -1119.555786\n",
      "Train Epoch: 694 [90112/101520 (89%)] Loss: -1120.917480\n",
      "Train Epoch: 694 [101376/101520 (100%)] Loss: -1125.180176\n",
      "    epoch          : 694\n",
      "    loss           : -1120.8905293066896\n",
      "    ess            : 1.9692552550953237\n",
      "    log_marginal   : 1120.9170302194566\n",
      "    log_joint      : 1329.162098946883\n",
      "    val_loss       : -1118.5204228940217\n",
      "    val_ess        : 1.965210883513741\n",
      "    val_log_marginal: 1118.548775050951\n",
      "    val_log_joint  : 1326.5635137143342\n",
      "Train Epoch: 695 [0/101520 (0%)] Loss: -1125.882324\n",
      "Train Epoch: 695 [11264/101520 (11%)] Loss: -1118.481567\n",
      "Train Epoch: 695 [22528/101520 (22%)] Loss: -1126.173096\n",
      "Train Epoch: 695 [33792/101520 (33%)] Loss: -1116.313721\n",
      "Train Epoch: 695 [45056/101520 (44%)] Loss: -1122.478516\n",
      "Train Epoch: 695 [56320/101520 (55%)] Loss: -1109.664185\n",
      "Train Epoch: 695 [67584/101520 (67%)] Loss: -1119.117554\n",
      "Train Epoch: 695 [78848/101520 (78%)] Loss: -1132.261230\n",
      "Train Epoch: 695 [90112/101520 (89%)] Loss: -1130.333984\n",
      "Train Epoch: 695 [101376/101520 (100%)] Loss: -1107.725708\n",
      "    epoch          : 695\n",
      "    loss           : -1121.1184824267823\n",
      "    ess            : 1.9689198594596518\n",
      "    log_marginal   : 1121.1455617933418\n",
      "    log_joint      : 1329.3668728162295\n",
      "    val_loss       : -1119.5787937330163\n",
      "    val_ess        : 1.9686859586964482\n",
      "    val_log_marginal: 1119.6075280230978\n",
      "    val_log_joint  : 1327.786323879076\n",
      "Train Epoch: 696 [0/101520 (0%)] Loss: -1116.249756\n",
      "Train Epoch: 696 [11264/101520 (11%)] Loss: -1123.406250\n",
      "Train Epoch: 696 [22528/101520 (22%)] Loss: -1123.481689\n",
      "Train Epoch: 696 [33792/101520 (33%)] Loss: -1125.743896\n",
      "Train Epoch: 696 [45056/101520 (44%)] Loss: -1122.635132\n",
      "Train Epoch: 696 [56320/101520 (55%)] Loss: -1125.448242\n",
      "Train Epoch: 696 [67584/101520 (67%)] Loss: -1115.511597\n",
      "Train Epoch: 696 [78848/101520 (78%)] Loss: -1125.869141\n",
      "Train Epoch: 696 [90112/101520 (89%)] Loss: -1124.272705\n",
      "Train Epoch: 696 [101376/101520 (100%)] Loss: -1126.396484\n",
      "    epoch          : 696\n",
      "    loss           : -1121.2775198011543\n",
      "    ess            : 1.9688272907506281\n",
      "    log_marginal   : 1121.3047243051194\n",
      "    log_joint      : 1329.5004428882694\n",
      "    val_loss       : -1120.6474609375\n",
      "    val_ess        : 1.968462145846823\n",
      "    val_log_marginal: 1120.6745764690897\n",
      "    val_log_joint  : 1328.7710863196332\n",
      "Train Epoch: 697 [0/101520 (0%)] Loss: -1118.784180\n",
      "Train Epoch: 697 [11264/101520 (11%)] Loss: -1119.512939\n",
      "Train Epoch: 697 [22528/101520 (22%)] Loss: -1123.743408\n",
      "Train Epoch: 697 [33792/101520 (33%)] Loss: -1118.681030\n",
      "Train Epoch: 697 [45056/101520 (44%)] Loss: -1112.070190\n",
      "Train Epoch: 697 [56320/101520 (55%)] Loss: -1116.381592\n",
      "Train Epoch: 697 [67584/101520 (67%)] Loss: -1120.341064\n",
      "Train Epoch: 697 [78848/101520 (78%)] Loss: -1121.522949\n",
      "Train Epoch: 697 [90112/101520 (89%)] Loss: -1121.882812\n",
      "Train Epoch: 697 [101376/101520 (100%)] Loss: -1121.799927\n",
      "    epoch          : 697\n",
      "    loss           : -1121.2250252728486\n",
      "    ess            : 1.968568662303177\n",
      "    log_marginal   : 1121.252045751217\n",
      "    log_joint      : 1329.5548095703125\n",
      "    val_loss       : -1118.6502526324728\n",
      "    val_ess        : 1.9640353347944177\n",
      "    val_log_marginal: 1118.68529742697\n",
      "    val_log_joint  : 1326.9037289826767\n",
      "Train Epoch: 698 [0/101520 (0%)] Loss: -1117.617432\n",
      "Train Epoch: 698 [11264/101520 (11%)] Loss: -1127.349854\n",
      "Train Epoch: 698 [22528/101520 (22%)] Loss: -1132.308350\n",
      "Train Epoch: 698 [33792/101520 (33%)] Loss: -1119.305298\n",
      "Train Epoch: 698 [45056/101520 (44%)] Loss: -1119.057129\n",
      "Train Epoch: 698 [56320/101520 (55%)] Loss: -1124.315674\n",
      "Train Epoch: 698 [67584/101520 (67%)] Loss: -1123.953857\n",
      "Train Epoch: 698 [78848/101520 (78%)] Loss: -1123.774048\n",
      "Train Epoch: 698 [90112/101520 (89%)] Loss: -1120.002563\n",
      "Train Epoch: 698 [101376/101520 (100%)] Loss: -1138.715088\n",
      "    epoch          : 698\n",
      "    loss           : -1121.5107360533134\n",
      "    ess            : 1.9685491125787322\n",
      "    log_marginal   : 1121.538399394433\n",
      "    log_joint      : 1329.7798121466708\n",
      "    val_loss       : -1120.8727815047555\n",
      "    val_ess        : 1.9694119795509006\n",
      "    val_log_marginal: 1120.8988833220108\n",
      "    val_log_joint  : 1329.0467847741168\n",
      "Train Epoch: 699 [0/101520 (0%)] Loss: -1129.818359\n",
      "Train Epoch: 699 [11264/101520 (11%)] Loss: -1123.394043\n",
      "Train Epoch: 699 [22528/101520 (22%)] Loss: -1122.060791\n",
      "Train Epoch: 699 [33792/101520 (33%)] Loss: -1121.338135\n",
      "Train Epoch: 699 [45056/101520 (44%)] Loss: -1122.422119\n",
      "Train Epoch: 699 [56320/101520 (55%)] Loss: -1121.337402\n",
      "Train Epoch: 699 [67584/101520 (67%)] Loss: -1122.189331\n",
      "Train Epoch: 699 [78848/101520 (78%)] Loss: -1119.795410\n",
      "Train Epoch: 699 [90112/101520 (89%)] Loss: -1129.796631\n",
      "Train Epoch: 699 [101376/101520 (100%)] Loss: -1111.588623\n",
      "    epoch          : 699\n",
      "    loss           : -1121.5980310487987\n",
      "    ess            : 1.968870906973604\n",
      "    log_marginal   : 1121.6249079872016\n",
      "    log_joint      : 1329.8744663257694\n",
      "    val_loss       : -1118.2830704398777\n",
      "    val_ess        : 1.9612228403920713\n",
      "    val_log_marginal: 1118.3195163892663\n",
      "    val_log_joint  : 1326.6992559018342\n",
      "Train Epoch: 700 [0/101520 (0%)] Loss: -1117.142334\n",
      "Train Epoch: 700 [11264/101520 (11%)] Loss: -1122.312256\n",
      "Train Epoch: 700 [22528/101520 (22%)] Loss: -1122.046753\n",
      "Train Epoch: 700 [33792/101520 (33%)] Loss: -1124.926636\n",
      "Train Epoch: 700 [45056/101520 (44%)] Loss: -1118.992554\n",
      "Train Epoch: 700 [56320/101520 (55%)] Loss: -1118.903564\n",
      "Train Epoch: 700 [67584/101520 (67%)] Loss: -1123.458496\n",
      "Train Epoch: 700 [78848/101520 (78%)] Loss: -1119.111572\n",
      "Train Epoch: 700 [90112/101520 (89%)] Loss: -1118.189941\n",
      "Train Epoch: 700 [101376/101520 (100%)] Loss: -1110.337280\n",
      "    epoch          : 700\n",
      "    loss           : -1121.641699709485\n",
      "    ess            : 1.9683188733144021\n",
      "    log_marginal   : 1121.669904085859\n",
      "    log_joint      : 1329.888755913356\n",
      "    val_loss       : -1120.376661217731\n",
      "    val_ess        : 1.9689892634101536\n",
      "    val_log_marginal: 1120.4033627717392\n",
      "    val_log_joint  : 1328.6168849779212\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch700.pth ...\n",
      "Train Epoch: 701 [0/101520 (0%)] Loss: -1124.991211\n",
      "Train Epoch: 701 [11264/101520 (11%)] Loss: -1124.562012\n",
      "Train Epoch: 701 [22528/101520 (22%)] Loss: -1123.179565\n",
      "Train Epoch: 701 [33792/101520 (33%)] Loss: -1131.125122\n",
      "Train Epoch: 701 [45056/101520 (44%)] Loss: -1113.091309\n",
      "Train Epoch: 701 [56320/101520 (55%)] Loss: -1120.584473\n",
      "Train Epoch: 701 [67584/101520 (67%)] Loss: -1125.048828\n",
      "Train Epoch: 701 [78848/101520 (78%)] Loss: -1118.400391\n",
      "Train Epoch: 701 [90112/101520 (89%)] Loss: -1120.585938\n",
      "Train Epoch: 701 [101376/101520 (100%)] Loss: -1121.818481\n",
      "    epoch          : 701\n",
      "    loss           : -1121.8558938491285\n",
      "    ess            : 1.9690345292115332\n",
      "    log_marginal   : 1121.8830308770414\n",
      "    log_joint      : 1330.12406453655\n",
      "    val_loss       : -1119.1146399456522\n",
      "    val_ess        : 1.963276303332785\n",
      "    val_log_marginal: 1119.1504490064538\n",
      "    val_log_joint  : 1327.477056088655\n",
      "Train Epoch: 702 [0/101520 (0%)] Loss: -1119.323853\n",
      "Train Epoch: 702 [11264/101520 (11%)] Loss: -1126.943726\n",
      "Train Epoch: 702 [22528/101520 (22%)] Loss: -1124.431885\n",
      "Train Epoch: 702 [33792/101520 (33%)] Loss: -1122.722412\n",
      "Train Epoch: 702 [45056/101520 (44%)] Loss: -1119.494873\n",
      "Train Epoch: 702 [56320/101520 (55%)] Loss: -1130.377197\n",
      "Train Epoch: 702 [67584/101520 (67%)] Loss: -1120.779785\n",
      "Train Epoch: 702 [78848/101520 (78%)] Loss: -1124.799316\n",
      "Train Epoch: 702 [90112/101520 (89%)] Loss: -1113.939209\n",
      "Train Epoch: 702 [101376/101520 (100%)] Loss: -1128.173828\n",
      "    epoch          : 702\n",
      "    loss           : -1122.014769280975\n",
      "    ess            : 1.9682271199010724\n",
      "    log_marginal   : 1122.0425326093357\n",
      "    log_joint      : 1330.3718347597362\n",
      "    val_loss       : -1122.3940748131793\n",
      "    val_ess        : 1.9679544749467268\n",
      "    val_log_marginal: 1122.4283341117527\n",
      "    val_log_joint  : 1330.5210067085598\n",
      "Train Epoch: 703 [0/101520 (0%)] Loss: -1116.994141\n",
      "Train Epoch: 703 [11264/101520 (11%)] Loss: -1127.541992\n",
      "Train Epoch: 703 [22528/101520 (22%)] Loss: -1125.079346\n",
      "Train Epoch: 703 [33792/101520 (33%)] Loss: -1124.534546\n",
      "Train Epoch: 703 [45056/101520 (44%)] Loss: -1124.379028\n",
      "Train Epoch: 703 [56320/101520 (55%)] Loss: -1129.069336\n",
      "Train Epoch: 703 [67584/101520 (67%)] Loss: -1124.578369\n",
      "Train Epoch: 703 [78848/101520 (78%)] Loss: -1112.503418\n",
      "Train Epoch: 703 [90112/101520 (89%)] Loss: -1116.075073\n",
      "Train Epoch: 703 [101376/101520 (100%)] Loss: -1126.273682\n",
      "    epoch          : 703\n",
      "    loss           : -1122.0108090501335\n",
      "    ess            : 1.968855842274038\n",
      "    log_marginal   : 1122.0379884039337\n",
      "    log_joint      : 1330.2967670383166\n",
      "    val_loss       : -1123.4119342306385\n",
      "    val_ess        : 1.9704724913058074\n",
      "    val_log_marginal: 1123.4354725713315\n",
      "    val_log_joint  : 1331.2811226222825\n",
      "Train Epoch: 704 [0/101520 (0%)] Loss: -1114.318970\n",
      "Train Epoch: 704 [11264/101520 (11%)] Loss: -1123.426270\n",
      "Train Epoch: 704 [22528/101520 (22%)] Loss: -1117.639648\n",
      "Train Epoch: 704 [33792/101520 (33%)] Loss: -1118.947144\n",
      "Train Epoch: 704 [45056/101520 (44%)] Loss: -1129.386108\n",
      "Train Epoch: 704 [56320/101520 (55%)] Loss: -1119.200439\n",
      "Train Epoch: 704 [67584/101520 (67%)] Loss: -1128.557129\n",
      "Train Epoch: 704 [78848/101520 (78%)] Loss: -1122.963867\n",
      "Train Epoch: 704 [90112/101520 (89%)] Loss: -1130.066895\n",
      "Train Epoch: 704 [101376/101520 (100%)] Loss: -1120.594238\n",
      "    epoch          : 704\n",
      "    loss           : -1122.2391486239792\n",
      "    ess            : 1.9686826431571538\n",
      "    log_marginal   : 1122.2662641822394\n",
      "    log_joint      : 1330.4725666908762\n",
      "    val_loss       : -1122.3341435971467\n",
      "    val_ess        : 1.971032484717991\n",
      "    val_log_marginal: 1122.3581224524457\n",
      "    val_log_joint  : 1330.8311342985733\n",
      "Train Epoch: 705 [0/101520 (0%)] Loss: -1125.265381\n",
      "Train Epoch: 705 [11264/101520 (11%)] Loss: -1120.369019\n",
      "Train Epoch: 705 [22528/101520 (22%)] Loss: -1121.481812\n",
      "Train Epoch: 705 [33792/101520 (33%)] Loss: -1127.820801\n",
      "Train Epoch: 705 [45056/101520 (44%)] Loss: -1115.040771\n",
      "Train Epoch: 705 [56320/101520 (55%)] Loss: -1120.629028\n",
      "Train Epoch: 705 [67584/101520 (67%)] Loss: -1127.451660\n",
      "Train Epoch: 705 [78848/101520 (78%)] Loss: -1120.001709\n",
      "Train Epoch: 705 [90112/101520 (89%)] Loss: -1122.866455\n",
      "Train Epoch: 705 [101376/101520 (100%)] Loss: -1124.792480\n",
      "    epoch          : 705\n",
      "    loss           : -1122.3575206354035\n",
      "    ess            : 1.968894752425764\n",
      "    log_marginal   : 1122.3854275037295\n",
      "    log_joint      : 1330.606247178274\n",
      "    val_loss       : -1122.0373322860055\n",
      "    val_ess        : 1.9659487060878589\n",
      "    val_log_marginal: 1122.0652916949728\n",
      "    val_log_joint  : 1330.1753725798233\n",
      "Train Epoch: 706 [0/101520 (0%)] Loss: -1119.834839\n",
      "Train Epoch: 706 [11264/101520 (11%)] Loss: -1120.504150\n",
      "Train Epoch: 706 [22528/101520 (22%)] Loss: -1122.324707\n",
      "Train Epoch: 706 [33792/101520 (33%)] Loss: -1119.772949\n",
      "Train Epoch: 706 [45056/101520 (44%)] Loss: -1122.314575\n",
      "Train Epoch: 706 [56320/101520 (55%)] Loss: -1118.816895\n",
      "Train Epoch: 706 [67584/101520 (67%)] Loss: -1115.658691\n",
      "Train Epoch: 706 [78848/101520 (78%)] Loss: -1117.329468\n",
      "Train Epoch: 706 [90112/101520 (89%)] Loss: -1126.756104\n",
      "Train Epoch: 706 [101376/101520 (100%)] Loss: -1118.705200\n",
      "    epoch          : 706\n",
      "    loss           : -1122.411457310969\n",
      "    ess            : 1.9679926705719837\n",
      "    log_marginal   : 1122.4397573806532\n",
      "    log_joint      : 1330.6217979546168\n",
      "    val_loss       : -1121.5960321841033\n",
      "    val_ess        : 1.9684435958447664\n",
      "    val_log_marginal: 1121.6227921195652\n",
      "    val_log_joint  : 1329.9307224439538\n",
      "Train Epoch: 707 [0/101520 (0%)] Loss: -1126.273560\n",
      "Train Epoch: 707 [11264/101520 (11%)] Loss: -1120.865356\n",
      "Train Epoch: 707 [22528/101520 (22%)] Loss: -1124.360596\n",
      "Train Epoch: 707 [33792/101520 (33%)] Loss: -1121.627930\n",
      "Train Epoch: 707 [45056/101520 (44%)] Loss: -1124.970459\n",
      "Train Epoch: 707 [56320/101520 (55%)] Loss: -1111.974609\n",
      "Train Epoch: 707 [67584/101520 (67%)] Loss: -1121.916504\n",
      "Train Epoch: 707 [78848/101520 (78%)] Loss: -1123.523682\n",
      "Train Epoch: 707 [90112/101520 (89%)] Loss: -1127.246948\n",
      "Train Epoch: 707 [101376/101520 (100%)] Loss: -1128.646240\n",
      "    epoch          : 707\n",
      "    loss           : -1122.6042431395258\n",
      "    ess            : 1.9680091238501083\n",
      "    log_marginal   : 1122.6323033625158\n",
      "    log_joint      : 1330.8950827133715\n",
      "    val_loss       : -1121.5769945227582\n",
      "    val_ess        : 1.9642022588978643\n",
      "    val_log_marginal: 1121.6110627547555\n",
      "    val_log_joint  : 1329.9016272503397\n",
      "Train Epoch: 708 [0/101520 (0%)] Loss: -1125.345703\n",
      "Train Epoch: 708 [11264/101520 (11%)] Loss: -1125.894531\n",
      "Train Epoch: 708 [22528/101520 (22%)] Loss: -1129.737793\n",
      "Train Epoch: 708 [33792/101520 (33%)] Loss: -1120.211670\n",
      "Train Epoch: 708 [45056/101520 (44%)] Loss: -1124.892334\n",
      "Train Epoch: 708 [56320/101520 (55%)] Loss: -1130.831787\n",
      "Train Epoch: 708 [67584/101520 (67%)] Loss: -1119.150269\n",
      "Train Epoch: 708 [78848/101520 (78%)] Loss: -1123.482544\n",
      "Train Epoch: 708 [90112/101520 (89%)] Loss: -1127.588135\n",
      "Train Epoch: 708 [101376/101520 (100%)] Loss: -1114.487427\n",
      "    epoch          : 708\n",
      "    loss           : -1122.7000358236494\n",
      "    ess            : 1.9688831855304276\n",
      "    log_marginal   : 1122.727523113615\n",
      "    log_joint      : 1330.9451186597048\n",
      "    val_loss       : -1122.5784487516983\n",
      "    val_ess        : 1.9642113550849583\n",
      "    val_log_marginal: 1122.6133237092392\n",
      "    val_log_joint  : 1330.162693189538\n",
      "Train Epoch: 709 [0/101520 (0%)] Loss: -1124.572510\n",
      "Train Epoch: 709 [11264/101520 (11%)] Loss: -1128.416992\n",
      "Train Epoch: 709 [22528/101520 (22%)] Loss: -1121.694336\n",
      "Train Epoch: 709 [33792/101520 (33%)] Loss: -1131.622314\n",
      "Train Epoch: 709 [45056/101520 (44%)] Loss: -1120.840088\n",
      "Train Epoch: 709 [56320/101520 (55%)] Loss: -1125.255859\n",
      "Train Epoch: 709 [67584/101520 (67%)] Loss: -1122.440063\n",
      "Train Epoch: 709 [78848/101520 (78%)] Loss: -1119.176270\n",
      "Train Epoch: 709 [90112/101520 (89%)] Loss: -1122.170654\n",
      "Train Epoch: 709 [101376/101520 (100%)] Loss: -1121.818970\n",
      "    epoch          : 709\n",
      "    loss           : -1122.784541604507\n",
      "    ess            : 1.9687440970435215\n",
      "    log_marginal   : 1122.811466389565\n",
      "    log_joint      : 1330.9966182229507\n",
      "    val_loss       : -1121.194627844769\n",
      "    val_ess        : 1.9701187662456348\n",
      "    val_log_marginal: 1121.2199388586957\n",
      "    val_log_joint  : 1329.4154477326767\n",
      "Train Epoch: 710 [0/101520 (0%)] Loss: -1126.863159\n",
      "Train Epoch: 710 [11264/101520 (11%)] Loss: -1122.862061\n",
      "Train Epoch: 710 [22528/101520 (22%)] Loss: -1125.311035\n",
      "Train Epoch: 710 [33792/101520 (33%)] Loss: -1118.094482\n",
      "Train Epoch: 710 [45056/101520 (44%)] Loss: -1126.486206\n",
      "Train Epoch: 710 [56320/101520 (55%)] Loss: -1114.612427\n",
      "Train Epoch: 710 [67584/101520 (67%)] Loss: -1126.053711\n",
      "Train Epoch: 710 [78848/101520 (78%)] Loss: -1117.062012\n",
      "Train Epoch: 710 [90112/101520 (89%)] Loss: -1125.018433\n",
      "Train Epoch: 710 [101376/101520 (100%)] Loss: -1109.230469\n",
      "    epoch          : 710\n",
      "    loss           : -1122.855338705245\n",
      "    ess            : 1.9688044900271162\n",
      "    log_marginal   : 1122.8827278482256\n",
      "    log_joint      : 1331.159710908056\n",
      "    val_loss       : -1121.7692552649457\n",
      "    val_ess        : 1.97169333955516\n",
      "    val_log_marginal: 1121.7911430027175\n",
      "    val_log_joint  : 1329.9796726392663\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch710.pth ...\n",
      "Train Epoch: 711 [0/101520 (0%)] Loss: -1124.793579\n",
      "Train Epoch: 711 [11264/101520 (11%)] Loss: -1130.428955\n",
      "Train Epoch: 711 [22528/101520 (22%)] Loss: -1128.957031\n",
      "Train Epoch: 711 [33792/101520 (33%)] Loss: -1122.068359\n",
      "Train Epoch: 711 [45056/101520 (44%)] Loss: -1120.849487\n",
      "Train Epoch: 711 [56320/101520 (55%)] Loss: -1120.085815\n",
      "Train Epoch: 711 [67584/101520 (67%)] Loss: -1118.200439\n",
      "Train Epoch: 711 [78848/101520 (78%)] Loss: -1128.381226\n",
      "Train Epoch: 711 [90112/101520 (89%)] Loss: -1121.111450\n",
      "Train Epoch: 711 [101376/101520 (100%)] Loss: -1124.866089\n",
      "    epoch          : 711\n",
      "    loss           : -1123.1062766223697\n",
      "    ess            : 1.9683207519090355\n",
      "    log_marginal   : 1123.134369969967\n",
      "    log_joint      : 1331.3215540593592\n",
      "    val_loss       : -1122.3399498980978\n",
      "    val_ess        : 1.968412311180778\n",
      "    val_log_marginal: 1122.3687160326087\n",
      "    val_log_joint  : 1330.3963516898777\n",
      "Train Epoch: 712 [0/101520 (0%)] Loss: -1124.151733\n",
      "Train Epoch: 712 [11264/101520 (11%)] Loss: -1129.332642\n",
      "Train Epoch: 712 [22528/101520 (22%)] Loss: -1128.449219\n",
      "Train Epoch: 712 [33792/101520 (33%)] Loss: -1120.220703\n",
      "Train Epoch: 712 [45056/101520 (44%)] Loss: -1122.958618\n",
      "Train Epoch: 712 [56320/101520 (55%)] Loss: -1118.638672\n",
      "Train Epoch: 712 [67584/101520 (67%)] Loss: -1121.319702\n",
      "Train Epoch: 712 [78848/101520 (78%)] Loss: -1127.549072\n",
      "Train Epoch: 712 [90112/101520 (89%)] Loss: -1121.523682\n",
      "Train Epoch: 712 [101376/101520 (100%)] Loss: -1127.528198\n",
      "    epoch          : 712\n",
      "    loss           : -1122.998280587508\n",
      "    ess            : 1.9681657169332456\n",
      "    log_marginal   : 1123.026010791261\n",
      "    log_joint      : 1331.3035581962547\n",
      "    val_loss       : -1120.5829441236413\n",
      "    val_ess        : 1.9698867227720178\n",
      "    val_log_marginal: 1120.6116359544837\n",
      "    val_log_joint  : 1328.9923414147418\n",
      "Train Epoch: 713 [0/101520 (0%)] Loss: -1124.167480\n",
      "Train Epoch: 713 [11264/101520 (11%)] Loss: -1117.007568\n",
      "Train Epoch: 713 [22528/101520 (22%)] Loss: -1125.754028\n",
      "Train Epoch: 713 [33792/101520 (33%)] Loss: -1125.330688\n",
      "Train Epoch: 713 [45056/101520 (44%)] Loss: -1119.979004\n",
      "Train Epoch: 713 [56320/101520 (55%)] Loss: -1122.835205\n",
      "Train Epoch: 713 [67584/101520 (67%)] Loss: -1123.889771\n",
      "Train Epoch: 713 [78848/101520 (78%)] Loss: -1124.456787\n",
      "Train Epoch: 713 [90112/101520 (89%)] Loss: -1121.995483\n",
      "Train Epoch: 713 [101376/101520 (100%)] Loss: -1117.890503\n",
      "    epoch          : 713\n",
      "    loss           : -1123.2526867737124\n",
      "    ess            : 1.9682444064461406\n",
      "    log_marginal   : 1123.2804869071922\n",
      "    log_joint      : 1331.5493863359768\n",
      "    val_loss       : -1121.518119480299\n",
      "    val_ess        : 1.9692031093265698\n",
      "    val_log_marginal: 1121.5441735309103\n",
      "    val_log_joint  : 1329.8311661430027\n",
      "Train Epoch: 714 [0/101520 (0%)] Loss: -1113.236084\n",
      "Train Epoch: 714 [11264/101520 (11%)] Loss: -1123.075928\n",
      "Train Epoch: 714 [22528/101520 (22%)] Loss: -1124.131348\n",
      "Train Epoch: 714 [33792/101520 (33%)] Loss: -1127.158081\n",
      "Train Epoch: 714 [45056/101520 (44%)] Loss: -1123.468140\n",
      "Train Epoch: 714 [56320/101520 (55%)] Loss: -1125.718018\n",
      "Train Epoch: 714 [67584/101520 (67%)] Loss: -1128.508057\n",
      "Train Epoch: 714 [78848/101520 (78%)] Loss: -1118.073730\n",
      "Train Epoch: 714 [90112/101520 (89%)] Loss: -1127.567627\n",
      "Train Epoch: 714 [101376/101520 (100%)] Loss: -1115.589233\n",
      "    epoch          : 714\n",
      "    loss           : -1123.3495627551822\n",
      "    ess            : 1.9682294112354068\n",
      "    log_marginal   : 1123.3767868885443\n",
      "    log_joint      : 1331.6232670922975\n",
      "    val_loss       : -1122.3685833474865\n",
      "    val_ess        : 1.971434261487878\n",
      "    val_log_marginal: 1122.393167246943\n",
      "    val_log_joint  : 1330.6337625254755\n",
      "Train Epoch: 715 [0/101520 (0%)] Loss: -1128.459473\n",
      "Train Epoch: 715 [11264/101520 (11%)] Loss: -1118.146362\n",
      "Train Epoch: 715 [22528/101520 (22%)] Loss: -1127.713867\n",
      "Train Epoch: 715 [33792/101520 (33%)] Loss: -1122.703125\n",
      "Train Epoch: 715 [45056/101520 (44%)] Loss: -1125.345337\n",
      "Train Epoch: 715 [56320/101520 (55%)] Loss: -1122.278687\n",
      "Train Epoch: 715 [67584/101520 (67%)] Loss: -1128.512451\n",
      "Train Epoch: 715 [78848/101520 (78%)] Loss: -1123.944580\n",
      "Train Epoch: 715 [90112/101520 (89%)] Loss: -1120.387939\n",
      "Train Epoch: 715 [101376/101520 (100%)] Loss: -1121.044067\n",
      "    epoch          : 715\n",
      "    loss           : -1123.326588999686\n",
      "    ess            : 1.9684863743470542\n",
      "    log_marginal   : 1123.3545584367148\n",
      "    log_joint      : 1331.5890837242855\n",
      "    val_loss       : -1123.1528957201087\n",
      "    val_ess        : 1.9684912277304607\n",
      "    val_log_marginal: 1123.1788011633832\n",
      "    val_log_joint  : 1331.5032481317935\n",
      "Train Epoch: 716 [0/101520 (0%)] Loss: -1123.565674\n",
      "Train Epoch: 716 [11264/101520 (11%)] Loss: -1127.916870\n",
      "Train Epoch: 716 [22528/101520 (22%)] Loss: -1129.239380\n",
      "Train Epoch: 716 [33792/101520 (33%)] Loss: -1112.614014\n",
      "Train Epoch: 716 [45056/101520 (44%)] Loss: -1127.018188\n",
      "Train Epoch: 716 [56320/101520 (55%)] Loss: -1122.779907\n",
      "Train Epoch: 716 [67584/101520 (67%)] Loss: -1119.420898\n",
      "Train Epoch: 716 [78848/101520 (78%)] Loss: -1118.022827\n",
      "Train Epoch: 716 [90112/101520 (89%)] Loss: -1122.734619\n",
      "Train Epoch: 716 [101376/101520 (100%)] Loss: -1122.297363\n",
      "    epoch          : 716\n",
      "    loss           : -1123.6399398113615\n",
      "    ess            : 1.9687271747157802\n",
      "    log_marginal   : 1123.667590884108\n",
      "    log_joint      : 1331.9069713803392\n",
      "    val_loss       : -1122.3226424507473\n",
      "    val_ess        : 1.9697691720464956\n",
      "    val_log_marginal: 1122.3482294497283\n",
      "    val_log_joint  : 1330.290139903193\n",
      "Train Epoch: 717 [0/101520 (0%)] Loss: -1119.162598\n",
      "Train Epoch: 717 [11264/101520 (11%)] Loss: -1121.943481\n",
      "Train Epoch: 717 [22528/101520 (22%)] Loss: -1127.352783\n",
      "Train Epoch: 717 [33792/101520 (33%)] Loss: -1127.583496\n",
      "Train Epoch: 717 [45056/101520 (44%)] Loss: -1123.697998\n",
      "Train Epoch: 717 [56320/101520 (55%)] Loss: -1119.070068\n",
      "Train Epoch: 717 [67584/101520 (67%)] Loss: -1124.110840\n",
      "Train Epoch: 717 [78848/101520 (78%)] Loss: -1130.648193\n",
      "Train Epoch: 717 [90112/101520 (89%)] Loss: -1118.625000\n",
      "Train Epoch: 717 [101376/101520 (100%)] Loss: -1116.002808\n",
      "    epoch          : 717\n",
      "    loss           : -1123.5739040662295\n",
      "    ess            : 1.9688254900314102\n",
      "    log_marginal   : 1123.6010748321687\n",
      "    log_joint      : 1331.863372035961\n",
      "    val_loss       : -1120.0802001953125\n",
      "    val_ess        : 1.9700978631558625\n",
      "    val_log_marginal: 1120.1064134680707\n",
      "    val_log_joint  : 1328.5873174252717\n",
      "Train Epoch: 718 [0/101520 (0%)] Loss: -1128.199463\n",
      "Train Epoch: 718 [11264/101520 (11%)] Loss: -1126.794678\n",
      "Train Epoch: 718 [22528/101520 (22%)] Loss: -1130.021973\n",
      "Train Epoch: 718 [33792/101520 (33%)] Loss: -1121.633179\n",
      "Train Epoch: 718 [45056/101520 (44%)] Loss: -1126.866211\n",
      "Train Epoch: 718 [56320/101520 (55%)] Loss: -1120.360352\n",
      "Train Epoch: 718 [67584/101520 (67%)] Loss: -1120.891357\n",
      "Train Epoch: 718 [78848/101520 (78%)] Loss: -1124.806885\n",
      "Train Epoch: 718 [90112/101520 (89%)] Loss: -1120.551147\n",
      "Train Epoch: 718 [101376/101520 (100%)] Loss: -1115.163696\n",
      "    epoch          : 718\n",
      "    loss           : -1123.737208380771\n",
      "    ess            : 1.9683039793417083\n",
      "    log_marginal   : 1123.765505383362\n",
      "    log_joint      : 1331.9658534371074\n",
      "    val_loss       : -1124.3703188688858\n",
      "    val_ess        : 1.9686459043751592\n",
      "    val_log_marginal: 1124.3982039741848\n",
      "    val_log_joint  : 1332.3506708559783\n",
      "Train Epoch: 719 [0/101520 (0%)] Loss: -1125.289429\n",
      "Train Epoch: 719 [11264/101520 (11%)] Loss: -1114.695923\n",
      "Train Epoch: 719 [22528/101520 (22%)] Loss: -1123.452515\n",
      "Train Epoch: 719 [33792/101520 (33%)] Loss: -1124.036865\n",
      "Train Epoch: 719 [45056/101520 (44%)] Loss: -1129.464233\n",
      "Train Epoch: 719 [56320/101520 (55%)] Loss: -1133.678955\n",
      "Train Epoch: 719 [67584/101520 (67%)] Loss: -1124.327881\n",
      "Train Epoch: 719 [78848/101520 (78%)] Loss: -1124.967041\n",
      "Train Epoch: 719 [90112/101520 (89%)] Loss: -1126.441284\n",
      "Train Epoch: 719 [101376/101520 (100%)] Loss: -1122.926392\n",
      "    epoch          : 719\n",
      "    loss           : -1124.001263642431\n",
      "    ess            : 1.968080287003637\n",
      "    log_marginal   : 1124.0297005044756\n",
      "    log_joint      : 1332.3391794175957\n",
      "    val_loss       : -1122.5673456606658\n",
      "    val_ess        : 1.9683420761771824\n",
      "    val_log_marginal: 1122.5944664996603\n",
      "    val_log_joint  : 1330.6610160495925\n",
      "Train Epoch: 720 [0/101520 (0%)] Loss: -1117.534546\n",
      "Train Epoch: 720 [11264/101520 (11%)] Loss: -1123.323853\n",
      "Train Epoch: 720 [22528/101520 (22%)] Loss: -1116.949463\n",
      "Train Epoch: 720 [33792/101520 (33%)] Loss: -1130.461182\n",
      "Train Epoch: 720 [45056/101520 (44%)] Loss: -1121.328857\n",
      "Train Epoch: 720 [56320/101520 (55%)] Loss: -1117.240479\n",
      "Train Epoch: 720 [67584/101520 (67%)] Loss: -1127.451172\n",
      "Train Epoch: 720 [78848/101520 (78%)] Loss: -1120.996338\n",
      "Train Epoch: 720 [90112/101520 (89%)] Loss: -1129.211182\n",
      "Train Epoch: 720 [101376/101520 (100%)] Loss: -1121.807007\n",
      "    epoch          : 720\n",
      "    loss           : -1123.942915873312\n",
      "    ess            : 1.9691816658230883\n",
      "    log_marginal   : 1123.9695456039965\n",
      "    log_joint      : 1332.2178801723462\n",
      "    val_loss       : -1123.018851902174\n",
      "    val_ess        : 1.968455169511878\n",
      "    val_log_marginal: 1123.0470607591712\n",
      "    val_log_joint  : 1331.2153532608695\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch720.pth ...\n",
      "Train Epoch: 721 [0/101520 (0%)] Loss: -1129.005371\n",
      "Train Epoch: 721 [11264/101520 (11%)] Loss: -1134.327393\n",
      "Train Epoch: 721 [22528/101520 (22%)] Loss: -1129.837036\n",
      "Train Epoch: 721 [33792/101520 (33%)] Loss: -1122.887207\n",
      "Train Epoch: 721 [45056/101520 (44%)] Loss: -1122.801025\n",
      "Train Epoch: 721 [56320/101520 (55%)] Loss: -1123.780273\n",
      "Train Epoch: 721 [67584/101520 (67%)] Loss: -1125.869019\n",
      "Train Epoch: 721 [78848/101520 (78%)] Loss: -1132.914062\n",
      "Train Epoch: 721 [90112/101520 (89%)] Loss: -1118.655762\n",
      "Train Epoch: 721 [101376/101520 (100%)] Loss: -1137.481323\n",
      "    epoch          : 721\n",
      "    loss           : -1124.2336456452183\n",
      "    ess            : 1.9680628668722795\n",
      "    log_marginal   : 1124.2620763730763\n",
      "    log_joint      : 1332.473422409901\n",
      "    val_loss       : -1122.92894977072\n",
      "    val_ess        : 1.9659226148024849\n",
      "    val_log_marginal: 1122.9591382897418\n",
      "    val_log_joint  : 1331.137010657269\n",
      "Train Epoch: 722 [0/101520 (0%)] Loss: -1134.337158\n",
      "Train Epoch: 722 [11264/101520 (11%)] Loss: -1124.833252\n",
      "Train Epoch: 722 [22528/101520 (22%)] Loss: -1127.085938\n",
      "Train Epoch: 722 [33792/101520 (33%)] Loss: -1121.597168\n",
      "Train Epoch: 722 [45056/101520 (44%)] Loss: -1122.464478\n",
      "Train Epoch: 722 [56320/101520 (55%)] Loss: -1124.837158\n",
      "Train Epoch: 722 [67584/101520 (67%)] Loss: -1131.815552\n",
      "Train Epoch: 722 [78848/101520 (78%)] Loss: -1125.732910\n",
      "Train Epoch: 722 [90112/101520 (89%)] Loss: -1123.157715\n",
      "Train Epoch: 722 [101376/101520 (100%)] Loss: -1114.270996\n",
      "    epoch          : 722\n",
      "    loss           : -1124.3607128660883\n",
      "    ess            : 1.9682870204724259\n",
      "    log_marginal   : 1124.38844368326\n",
      "    log_joint      : 1332.582651416261\n",
      "    val_loss       : -1123.3346106487772\n",
      "    val_ess        : 1.968832052272299\n",
      "    val_log_marginal: 1123.3615616508152\n",
      "    val_log_joint  : 1331.7610075577445\n",
      "Train Epoch: 723 [0/101520 (0%)] Loss: -1125.538574\n",
      "Train Epoch: 723 [11264/101520 (11%)] Loss: -1117.943848\n",
      "Train Epoch: 723 [22528/101520 (22%)] Loss: -1124.815063\n",
      "Train Epoch: 723 [33792/101520 (33%)] Loss: -1123.274658\n",
      "Train Epoch: 723 [45056/101520 (44%)] Loss: -1125.829346\n",
      "Train Epoch: 723 [56320/101520 (55%)] Loss: -1127.333740\n",
      "Train Epoch: 723 [67584/101520 (67%)] Loss: -1126.978760\n",
      "Train Epoch: 723 [78848/101520 (78%)] Loss: -1122.129395\n",
      "Train Epoch: 723 [90112/101520 (89%)] Loss: -1124.921387\n",
      "Train Epoch: 723 [101376/101520 (100%)] Loss: -1109.353149\n",
      "    epoch          : 723\n",
      "    loss           : -1124.2751145866048\n",
      "    ess            : 1.9685343971204519\n",
      "    log_marginal   : 1124.3027570714903\n",
      "    log_joint      : 1332.5612406514997\n",
      "    val_loss       : -1124.8801004161005\n",
      "    val_ess        : 1.9693174465842869\n",
      "    val_log_marginal: 1124.9054804262908\n",
      "    val_log_joint  : 1333.2180865743885\n",
      "Train Epoch: 724 [0/101520 (0%)] Loss: -1122.909058\n",
      "Train Epoch: 724 [11264/101520 (11%)] Loss: -1127.230957\n",
      "Train Epoch: 724 [22528/101520 (22%)] Loss: -1125.976807\n",
      "Train Epoch: 724 [33792/101520 (33%)] Loss: -1119.927734\n",
      "Train Epoch: 724 [45056/101520 (44%)] Loss: -1122.579956\n",
      "Train Epoch: 724 [56320/101520 (55%)] Loss: -1126.339966\n",
      "Train Epoch: 724 [67584/101520 (67%)] Loss: -1126.309814\n",
      "Train Epoch: 724 [78848/101520 (78%)] Loss: -1119.106201\n",
      "Train Epoch: 724 [90112/101520 (89%)] Loss: -1132.662842\n",
      "Train Epoch: 724 [101376/101520 (100%)] Loss: -1125.252563\n",
      "    epoch          : 724\n",
      "    loss           : -1124.6264568693075\n",
      "    ess            : 1.9685350398921488\n",
      "    log_marginal   : 1124.6532608108903\n",
      "    log_joint      : 1332.9055053097518\n",
      "    val_loss       : -1124.8786780315897\n",
      "    val_ess        : 1.9691333874412205\n",
      "    val_log_marginal: 1124.9062659222147\n",
      "    val_log_joint  : 1333.399026621943\n",
      "Train Epoch: 725 [0/101520 (0%)] Loss: -1119.377563\n",
      "Train Epoch: 725 [11264/101520 (11%)] Loss: -1121.306152\n",
      "Train Epoch: 725 [22528/101520 (22%)] Loss: -1118.671143\n",
      "Train Epoch: 725 [33792/101520 (33%)] Loss: -1126.966553\n",
      "Train Epoch: 725 [45056/101520 (44%)] Loss: -1126.920166\n",
      "Train Epoch: 725 [56320/101520 (55%)] Loss: -1125.254150\n",
      "Train Epoch: 725 [67584/101520 (67%)] Loss: -1123.880737\n",
      "Train Epoch: 725 [78848/101520 (78%)] Loss: -1133.521484\n",
      "Train Epoch: 725 [90112/101520 (89%)] Loss: -1121.065918\n",
      "Train Epoch: 725 [101376/101520 (100%)] Loss: -1122.976318\n",
      "    epoch          : 725\n",
      "    loss           : -1124.805181302018\n",
      "    ess            : 1.9675851760797165\n",
      "    log_marginal   : 1124.834114419755\n",
      "    log_joint      : 1333.0366824356156\n",
      "    val_loss       : -1124.3138905400815\n",
      "    val_ess        : 1.9645828112311985\n",
      "    val_log_marginal: 1124.3450556216033\n",
      "    val_log_joint  : 1332.5760657269022\n",
      "Train Epoch: 726 [0/101520 (0%)] Loss: -1131.148926\n",
      "Train Epoch: 726 [11264/101520 (11%)] Loss: -1124.156738\n",
      "Train Epoch: 726 [22528/101520 (22%)] Loss: -1130.756592\n",
      "Train Epoch: 726 [33792/101520 (33%)] Loss: -1123.499390\n",
      "Train Epoch: 726 [45056/101520 (44%)] Loss: -1124.840088\n",
      "Train Epoch: 726 [56320/101520 (55%)] Loss: -1129.015869\n",
      "Train Epoch: 726 [67584/101520 (67%)] Loss: -1133.189453\n",
      "Train Epoch: 726 [78848/101520 (78%)] Loss: -1129.910034\n",
      "Train Epoch: 726 [90112/101520 (89%)] Loss: -1127.761963\n",
      "Train Epoch: 726 [101376/101520 (100%)] Loss: -1126.497803\n",
      "    epoch          : 726\n",
      "    loss           : -1124.8495695027873\n",
      "    ess            : 1.9683650833877486\n",
      "    log_marginal   : 1124.8772837576555\n",
      "    log_joint      : 1333.0933574120604\n",
      "    val_loss       : -1122.630418860394\n",
      "    val_ess        : 1.96885375354601\n",
      "    val_log_marginal: 1122.6566639775815\n",
      "    val_log_joint  : 1330.8636156165082\n",
      "Train Epoch: 727 [0/101520 (0%)] Loss: -1127.253906\n",
      "Train Epoch: 727 [11264/101520 (11%)] Loss: -1130.795898\n",
      "Train Epoch: 727 [22528/101520 (22%)] Loss: -1124.685059\n",
      "Train Epoch: 727 [33792/101520 (33%)] Loss: -1125.205566\n",
      "Train Epoch: 727 [45056/101520 (44%)] Loss: -1118.286621\n",
      "Train Epoch: 727 [56320/101520 (55%)] Loss: -1121.547241\n",
      "Train Epoch: 727 [67584/101520 (67%)] Loss: -1125.267700\n",
      "Train Epoch: 727 [78848/101520 (78%)] Loss: -1129.387207\n",
      "Train Epoch: 727 [90112/101520 (89%)] Loss: -1131.781738\n",
      "Train Epoch: 727 [101376/101520 (100%)] Loss: -1120.810791\n",
      "    epoch          : 727\n",
      "    loss           : -1124.966593833425\n",
      "    ess            : 1.968135101112289\n",
      "    log_marginal   : 1124.9951852769707\n",
      "    log_joint      : 1333.2441645483275\n",
      "    val_loss       : -1122.919046153193\n",
      "    val_ess        : 1.9689954311951348\n",
      "    val_log_marginal: 1122.9446543817935\n",
      "    val_log_joint  : 1331.1601774796195\n",
      "Train Epoch: 728 [0/101520 (0%)] Loss: -1117.268555\n",
      "Train Epoch: 728 [11264/101520 (11%)] Loss: -1124.716309\n",
      "Train Epoch: 728 [22528/101520 (22%)] Loss: -1125.261353\n",
      "Train Epoch: 728 [33792/101520 (33%)] Loss: -1122.125488\n",
      "Train Epoch: 728 [45056/101520 (44%)] Loss: -1125.163818\n",
      "Train Epoch: 728 [56320/101520 (55%)] Loss: -1126.405029\n",
      "Train Epoch: 728 [67584/101520 (67%)] Loss: -1130.377197\n",
      "Train Epoch: 728 [78848/101520 (78%)] Loss: -1123.295166\n",
      "Train Epoch: 728 [90112/101520 (89%)] Loss: -1121.428955\n",
      "Train Epoch: 728 [101376/101520 (100%)] Loss: -1134.969849\n",
      "    epoch          : 728\n",
      "    loss           : -1125.1578203517588\n",
      "    ess            : 1.9682571798113722\n",
      "    log_marginal   : 1125.185662811126\n",
      "    log_joint      : 1333.3937319654915\n",
      "    val_loss       : -1124.84302288553\n",
      "    val_ess        : 1.9693292638529902\n",
      "    val_log_marginal: 1124.8706213909647\n",
      "    val_log_joint  : 1333.214551842731\n",
      "Train Epoch: 729 [0/101520 (0%)] Loss: -1124.276367\n",
      "Train Epoch: 729 [11264/101520 (11%)] Loss: -1130.424927\n",
      "Train Epoch: 729 [22528/101520 (22%)] Loss: -1124.833008\n",
      "Train Epoch: 729 [33792/101520 (33%)] Loss: -1122.662598\n",
      "Train Epoch: 729 [45056/101520 (44%)] Loss: -1116.714355\n",
      "Train Epoch: 729 [56320/101520 (55%)] Loss: -1128.601929\n",
      "Train Epoch: 729 [67584/101520 (67%)] Loss: -1127.497437\n",
      "Train Epoch: 729 [78848/101520 (78%)] Loss: -1121.764648\n",
      "Train Epoch: 729 [90112/101520 (89%)] Loss: -1118.340454\n",
      "Train Epoch: 729 [101376/101520 (100%)] Loss: -1131.936523\n",
      "    epoch          : 729\n",
      "    loss           : -1125.130502978761\n",
      "    ess            : 1.969042401217935\n",
      "    log_marginal   : 1125.1574363516802\n",
      "    log_joint      : 1333.4506713253768\n",
      "    val_loss       : -1124.4347401494565\n",
      "    val_ess        : 1.968428223029427\n",
      "    val_log_marginal: 1124.4618450662365\n",
      "    val_log_joint  : 1332.2198645550272\n",
      "Train Epoch: 730 [0/101520 (0%)] Loss: -1125.751343\n",
      "Train Epoch: 730 [11264/101520 (11%)] Loss: -1123.401611\n",
      "Train Epoch: 730 [22528/101520 (22%)] Loss: -1120.298462\n",
      "Train Epoch: 730 [33792/101520 (33%)] Loss: -1118.356201\n",
      "Train Epoch: 730 [45056/101520 (44%)] Loss: -1130.139648\n",
      "Train Epoch: 730 [56320/101520 (55%)] Loss: -1118.914551\n",
      "Train Epoch: 730 [67584/101520 (67%)] Loss: -1132.102173\n",
      "Train Epoch: 730 [78848/101520 (78%)] Loss: -1123.706421\n",
      "Train Epoch: 730 [90112/101520 (89%)] Loss: -1122.987305\n",
      "Train Epoch: 730 [101376/101520 (100%)] Loss: -1145.307251\n",
      "    epoch          : 730\n",
      "    loss           : -1125.401144516528\n",
      "    ess            : 1.9679110355712661\n",
      "    log_marginal   : 1125.4292746692447\n",
      "    log_joint      : 1333.7133715452262\n",
      "    val_loss       : -1123.9733037533967\n",
      "    val_ess        : 1.9672152425931848\n",
      "    val_log_marginal: 1123.9992463485055\n",
      "    val_log_joint  : 1332.196676503057\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch730.pth ...\n",
      "Train Epoch: 731 [0/101520 (0%)] Loss: -1125.707031\n",
      "Train Epoch: 731 [11264/101520 (11%)] Loss: -1126.432617\n",
      "Train Epoch: 731 [22528/101520 (22%)] Loss: -1132.520874\n",
      "Train Epoch: 731 [33792/101520 (33%)] Loss: -1118.797363\n",
      "Train Epoch: 731 [45056/101520 (44%)] Loss: -1130.804932\n",
      "Train Epoch: 731 [56320/101520 (55%)] Loss: -1125.819092\n",
      "Train Epoch: 731 [67584/101520 (67%)] Loss: -1126.307617\n",
      "Train Epoch: 731 [78848/101520 (78%)] Loss: -1130.410400\n",
      "Train Epoch: 731 [90112/101520 (89%)] Loss: -1129.184082\n",
      "Train Epoch: 731 [101376/101520 (100%)] Loss: -1152.381470\n",
      "    epoch          : 731\n",
      "    loss           : -1125.5729121682632\n",
      "    ess            : 1.9692422840463457\n",
      "    log_marginal   : 1125.6003264614087\n",
      "    log_joint      : 1333.81929483845\n",
      "    val_loss       : -1125.262355638587\n",
      "    val_ess        : 1.9679945966471797\n",
      "    val_log_marginal: 1125.2892429517663\n",
      "    val_log_joint  : 1333.6614194123642\n",
      "Train Epoch: 732 [0/101520 (0%)] Loss: -1125.514771\n",
      "Train Epoch: 732 [11264/101520 (11%)] Loss: -1125.201050\n",
      "Train Epoch: 732 [22528/101520 (22%)] Loss: -1122.529297\n",
      "Train Epoch: 732 [33792/101520 (33%)] Loss: -1129.678711\n",
      "Train Epoch: 732 [45056/101520 (44%)] Loss: -1126.243286\n",
      "Train Epoch: 732 [56320/101520 (55%)] Loss: -1119.465576\n",
      "Train Epoch: 732 [67584/101520 (67%)] Loss: -1128.654053\n",
      "Train Epoch: 732 [78848/101520 (78%)] Loss: -1128.167969\n",
      "Train Epoch: 732 [90112/101520 (89%)] Loss: -1122.324219\n",
      "Train Epoch: 732 [101376/101520 (100%)] Loss: -1115.567261\n",
      "    epoch          : 732\n",
      "    loss           : -1125.488480611063\n",
      "    ess            : 1.9688952047022144\n",
      "    log_marginal   : 1125.5151042075613\n",
      "    log_joint      : 1333.7550196048603\n",
      "    val_loss       : -1125.7943699048913\n",
      "    val_ess        : 1.9715976300446882\n",
      "    val_log_marginal: 1125.817722486413\n",
      "    val_log_joint  : 1334.151271654212\n",
      "Train Epoch: 733 [0/101520 (0%)] Loss: -1120.515747\n",
      "Train Epoch: 733 [11264/101520 (11%)] Loss: -1129.298218\n",
      "Train Epoch: 733 [22528/101520 (22%)] Loss: -1128.922852\n",
      "Train Epoch: 733 [33792/101520 (33%)] Loss: -1126.827148\n",
      "Train Epoch: 733 [45056/101520 (44%)] Loss: -1132.949585\n",
      "Train Epoch: 733 [56320/101520 (55%)] Loss: -1126.568604\n",
      "Train Epoch: 733 [67584/101520 (67%)] Loss: -1124.416016\n",
      "Train Epoch: 733 [78848/101520 (78%)] Loss: -1129.430664\n",
      "Train Epoch: 733 [90112/101520 (89%)] Loss: -1133.356201\n",
      "Train Epoch: 733 [101376/101520 (100%)] Loss: -1115.540405\n",
      "    epoch          : 733\n",
      "    loss           : -1125.7463918714668\n",
      "    ess            : 1.9676410750528077\n",
      "    log_marginal   : 1125.7754642352386\n",
      "    log_joint      : 1333.9322847145886\n",
      "    val_loss       : -1125.1963580587635\n",
      "    val_ess        : 1.9643203745717588\n",
      "    val_log_marginal: 1125.228324558424\n",
      "    val_log_joint  : 1333.3573581861413\n",
      "Train Epoch: 734 [0/101520 (0%)] Loss: -1128.479370\n",
      "Train Epoch: 734 [11264/101520 (11%)] Loss: -1125.382812\n",
      "Train Epoch: 734 [22528/101520 (22%)] Loss: -1128.552246\n",
      "Train Epoch: 734 [33792/101520 (33%)] Loss: -1127.588867\n",
      "Train Epoch: 734 [45056/101520 (44%)] Loss: -1126.015381\n",
      "Train Epoch: 734 [56320/101520 (55%)] Loss: -1117.539307\n",
      "Train Epoch: 734 [67584/101520 (67%)] Loss: -1122.475342\n",
      "Train Epoch: 734 [78848/101520 (78%)] Loss: -1128.015625\n",
      "Train Epoch: 734 [90112/101520 (89%)] Loss: -1116.352783\n",
      "Train Epoch: 734 [101376/101520 (100%)] Loss: -1122.071777\n",
      "    epoch          : 734\n",
      "    loss           : -1125.7106264967415\n",
      "    ess            : 1.9680562468629386\n",
      "    log_marginal   : 1125.7389511031722\n",
      "    log_joint      : 1333.9893835633245\n",
      "    val_loss       : -1124.2803318189538\n",
      "    val_ess        : 1.9686724828637165\n",
      "    val_log_marginal: 1124.308641516644\n",
      "    val_log_joint  : 1332.5348749575408\n",
      "Train Epoch: 735 [0/101520 (0%)] Loss: -1124.779053\n",
      "Train Epoch: 735 [11264/101520 (11%)] Loss: -1127.637329\n",
      "Train Epoch: 735 [22528/101520 (22%)] Loss: -1133.053467\n",
      "Train Epoch: 735 [33792/101520 (33%)] Loss: -1121.232422\n",
      "Train Epoch: 735 [45056/101520 (44%)] Loss: -1123.821899\n",
      "Train Epoch: 735 [56320/101520 (55%)] Loss: -1127.646973\n",
      "Train Epoch: 735 [67584/101520 (67%)] Loss: -1124.853027\n",
      "Train Epoch: 735 [78848/101520 (78%)] Loss: -1120.570557\n",
      "Train Epoch: 735 [90112/101520 (89%)] Loss: -1123.238281\n",
      "Train Epoch: 735 [101376/101520 (100%)] Loss: -1132.877808\n",
      "    epoch          : 735\n",
      "    loss           : -1125.8364282349246\n",
      "    ess            : 1.9686110337175915\n",
      "    log_marginal   : 1125.8637474481784\n",
      "    log_joint      : 1334.135342851955\n",
      "    val_loss       : -1123.9789349099865\n",
      "    val_ess        : 1.9708534375480984\n",
      "    val_log_marginal: 1124.0041663128397\n",
      "    val_log_joint  : 1332.1745233950408\n",
      "Train Epoch: 736 [0/101520 (0%)] Loss: -1124.683838\n",
      "Train Epoch: 736 [11264/101520 (11%)] Loss: -1124.318115\n",
      "Train Epoch: 736 [22528/101520 (22%)] Loss: -1123.933350\n",
      "Train Epoch: 736 [33792/101520 (33%)] Loss: -1129.720947\n",
      "Train Epoch: 736 [45056/101520 (44%)] Loss: -1123.166504\n",
      "Train Epoch: 736 [56320/101520 (55%)] Loss: -1122.089600\n",
      "Train Epoch: 736 [67584/101520 (67%)] Loss: -1127.962646\n",
      "Train Epoch: 736 [78848/101520 (78%)] Loss: -1126.683960\n",
      "Train Epoch: 736 [90112/101520 (89%)] Loss: -1130.498291\n",
      "Train Epoch: 736 [101376/101520 (100%)] Loss: -1123.781860\n",
      "    epoch          : 736\n",
      "    loss           : -1126.0121211526382\n",
      "    ess            : 1.9681292047452688\n",
      "    log_marginal   : 1126.0401053117148\n",
      "    log_joint      : 1334.2209141410176\n",
      "    val_loss       : -1125.4687924592392\n",
      "    val_ess        : 1.969878735749618\n",
      "    val_log_marginal: 1125.4957434612772\n",
      "    val_log_joint  : 1333.509818699049\n",
      "Train Epoch: 737 [0/101520 (0%)] Loss: -1127.593018\n",
      "Train Epoch: 737 [11264/101520 (11%)] Loss: -1125.298950\n",
      "Train Epoch: 737 [22528/101520 (22%)] Loss: -1125.898193\n",
      "Train Epoch: 737 [33792/101520 (33%)] Loss: -1119.474121\n",
      "Train Epoch: 737 [45056/101520 (44%)] Loss: -1127.384033\n",
      "Train Epoch: 737 [56320/101520 (55%)] Loss: -1122.744629\n",
      "Train Epoch: 737 [67584/101520 (67%)] Loss: -1122.724365\n",
      "Train Epoch: 737 [78848/101520 (78%)] Loss: -1129.521973\n",
      "Train Epoch: 737 [90112/101520 (89%)] Loss: -1126.718750\n",
      "Train Epoch: 737 [101376/101520 (100%)] Loss: -1136.517334\n",
      "    epoch          : 737\n",
      "    loss           : -1126.025401053117\n",
      "    ess            : 1.9687294205229486\n",
      "    log_marginal   : 1126.052217263073\n",
      "    log_joint      : 1334.363908777285\n",
      "    val_loss       : -1123.0072870669158\n",
      "    val_ess        : 1.9685403212257053\n",
      "    val_log_marginal: 1123.0350925611413\n",
      "    val_log_joint  : 1331.4309188179348\n",
      "Train Epoch: 738 [0/101520 (0%)] Loss: -1126.427246\n",
      "Train Epoch: 738 [11264/101520 (11%)] Loss: -1134.032715\n",
      "Train Epoch: 738 [22528/101520 (22%)] Loss: -1130.269287\n",
      "Train Epoch: 738 [33792/101520 (33%)] Loss: -1129.415771\n",
      "Train Epoch: 738 [45056/101520 (44%)] Loss: -1123.142334\n",
      "Train Epoch: 738 [56320/101520 (55%)] Loss: -1127.900757\n",
      "Train Epoch: 738 [67584/101520 (67%)] Loss: -1124.476562\n",
      "Train Epoch: 738 [78848/101520 (78%)] Loss: -1133.741821\n",
      "Train Epoch: 738 [90112/101520 (89%)] Loss: -1131.334473\n",
      "Train Epoch: 738 [101376/101520 (100%)] Loss: -1129.697754\n",
      "    epoch          : 738\n",
      "    loss           : -1126.1051043793184\n",
      "    ess            : 1.9681669743216816\n",
      "    log_marginal   : 1126.1330682955795\n",
      "    log_joint      : 1334.4271270905308\n",
      "    val_loss       : -1125.278813901155\n",
      "    val_ess        : 1.9701365543448406\n",
      "    val_log_marginal: 1125.3046556555707\n",
      "    val_log_joint  : 1333.3390264096467\n",
      "Train Epoch: 739 [0/101520 (0%)] Loss: -1126.664795\n",
      "Train Epoch: 739 [11264/101520 (11%)] Loss: -1126.849121\n",
      "Train Epoch: 739 [22528/101520 (22%)] Loss: -1128.391113\n",
      "Train Epoch: 739 [33792/101520 (33%)] Loss: -1126.852539\n",
      "Train Epoch: 739 [45056/101520 (44%)] Loss: -1118.011963\n",
      "Train Epoch: 739 [56320/101520 (55%)] Loss: -1126.541992\n",
      "Train Epoch: 739 [67584/101520 (67%)] Loss: -1130.145508\n",
      "Train Epoch: 739 [78848/101520 (78%)] Loss: -1124.849365\n",
      "Train Epoch: 739 [90112/101520 (89%)] Loss: -1130.388916\n",
      "Train Epoch: 739 [101376/101520 (100%)] Loss: -1123.756958\n",
      "    epoch          : 739\n",
      "    loss           : -1126.3414361848304\n",
      "    ess            : 1.9686392839230484\n",
      "    log_marginal   : 1126.3689584396593\n",
      "    log_joint      : 1334.6831809192447\n",
      "    val_loss       : -1126.7859205163043\n",
      "    val_ess        : 1.9693504520084546\n",
      "    val_log_marginal: 1126.8139489215353\n",
      "    val_log_joint  : 1334.8988355553668\n",
      "Train Epoch: 740 [0/101520 (0%)] Loss: -1127.139648\n",
      "Train Epoch: 740 [11264/101520 (11%)] Loss: -1129.091919\n",
      "Train Epoch: 740 [22528/101520 (22%)] Loss: -1123.902588\n",
      "Train Epoch: 740 [33792/101520 (33%)] Loss: -1127.911621\n",
      "Train Epoch: 740 [45056/101520 (44%)] Loss: -1130.704224\n",
      "Train Epoch: 740 [56320/101520 (55%)] Loss: -1125.664551\n",
      "Train Epoch: 740 [67584/101520 (67%)] Loss: -1120.268311\n",
      "Train Epoch: 740 [78848/101520 (78%)] Loss: -1128.732666\n",
      "Train Epoch: 740 [90112/101520 (89%)] Loss: -1127.371582\n",
      "Train Epoch: 740 [101376/101520 (100%)] Loss: -1139.107910\n",
      "    epoch          : 740\n",
      "    loss           : -1126.4146734649812\n",
      "    ess            : 1.9686543084868235\n",
      "    log_marginal   : 1126.442831834956\n",
      "    log_joint      : 1334.7613384304334\n",
      "    val_loss       : -1122.495600161345\n",
      "    val_ess        : 1.9668595428052156\n",
      "    val_log_marginal: 1122.525050951087\n",
      "    val_log_joint  : 1330.817674719769\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch740.pth ...\n",
      "Train Epoch: 741 [0/101520 (0%)] Loss: -1131.398071\n",
      "Train Epoch: 741 [11264/101520 (11%)] Loss: -1128.467041\n",
      "Train Epoch: 741 [22528/101520 (22%)] Loss: -1127.695801\n",
      "Train Epoch: 741 [33792/101520 (33%)] Loss: -1133.111816\n",
      "Train Epoch: 741 [45056/101520 (44%)] Loss: -1119.832520\n",
      "Train Epoch: 741 [56320/101520 (55%)] Loss: -1128.157227\n",
      "Train Epoch: 741 [67584/101520 (67%)] Loss: -1124.830322\n",
      "Train Epoch: 741 [78848/101520 (78%)] Loss: -1129.407837\n",
      "Train Epoch: 741 [90112/101520 (89%)] Loss: -1130.041748\n",
      "Train Epoch: 741 [101376/101520 (100%)] Loss: -1117.586304\n",
      "    epoch          : 741\n",
      "    loss           : -1126.532252939502\n",
      "    ess            : 1.9689174638920692\n",
      "    log_marginal   : 1126.5594157309988\n",
      "    log_joint      : 1334.8231918871702\n",
      "    val_loss       : -1125.367574940557\n",
      "    val_ess        : 1.9721388298532236\n",
      "    val_log_marginal: 1125.3908797554348\n",
      "    val_log_joint  : 1333.5031525985055\n",
      "Train Epoch: 742 [0/101520 (0%)] Loss: -1116.558594\n",
      "Train Epoch: 742 [11264/101520 (11%)] Loss: -1125.962280\n",
      "Train Epoch: 742 [22528/101520 (22%)] Loss: -1125.344971\n",
      "Train Epoch: 742 [33792/101520 (33%)] Loss: -1129.169556\n",
      "Train Epoch: 742 [45056/101520 (44%)] Loss: -1124.986938\n",
      "Train Epoch: 742 [56320/101520 (55%)] Loss: -1122.518799\n",
      "Train Epoch: 742 [67584/101520 (67%)] Loss: -1127.100830\n",
      "Train Epoch: 742 [78848/101520 (78%)] Loss: -1127.888794\n",
      "Train Epoch: 742 [90112/101520 (89%)] Loss: -1137.979736\n",
      "Train Epoch: 742 [101376/101520 (100%)] Loss: -1110.446899\n",
      "    epoch          : 742\n",
      "    loss           : -1126.7983122399105\n",
      "    ess            : 1.9685927000477086\n",
      "    log_marginal   : 1126.8254639898712\n",
      "    log_joint      : 1335.0741880790672\n",
      "    val_loss       : -1126.3300940472147\n",
      "    val_ess        : 1.9685909177946008\n",
      "    val_log_marginal: 1126.3583665930707\n",
      "    val_log_joint  : 1334.6489629330842\n",
      "Train Epoch: 743 [0/101520 (0%)] Loss: -1132.565674\n",
      "Train Epoch: 743 [11264/101520 (11%)] Loss: -1123.539307\n",
      "Train Epoch: 743 [22528/101520 (22%)] Loss: -1126.846436\n",
      "Train Epoch: 743 [33792/101520 (33%)] Loss: -1126.445801\n",
      "Train Epoch: 743 [45056/101520 (44%)] Loss: -1132.319824\n",
      "Train Epoch: 743 [56320/101520 (55%)] Loss: -1128.554199\n",
      "Train Epoch: 743 [67584/101520 (67%)] Loss: -1128.824585\n",
      "Train Epoch: 743 [78848/101520 (78%)] Loss: -1122.759766\n",
      "Train Epoch: 743 [90112/101520 (89%)] Loss: -1129.239014\n",
      "Train Epoch: 743 [101376/101520 (100%)] Loss: -1122.945312\n",
      "    epoch          : 743\n",
      "    loss           : -1126.7822596871074\n",
      "    ess            : 1.9680965545788482\n",
      "    log_marginal   : 1126.810810645022\n",
      "    log_joint      : 1335.0567215962626\n",
      "    val_loss       : -1125.4924210258152\n",
      "    val_ess        : 1.9675115398738696\n",
      "    val_log_marginal: 1125.518846594769\n",
      "    val_log_joint  : 1333.670070482337\n",
      "Train Epoch: 744 [0/101520 (0%)] Loss: -1114.458740\n",
      "Train Epoch: 744 [11264/101520 (11%)] Loss: -1130.249512\n",
      "Train Epoch: 744 [22528/101520 (22%)] Loss: -1125.399048\n",
      "Train Epoch: 744 [33792/101520 (33%)] Loss: -1121.982422\n",
      "Train Epoch: 744 [45056/101520 (44%)] Loss: -1128.393555\n",
      "Train Epoch: 744 [56320/101520 (55%)] Loss: -1120.651489\n",
      "Train Epoch: 744 [67584/101520 (67%)] Loss: -1124.020996\n",
      "Train Epoch: 744 [78848/101520 (78%)] Loss: -1132.168335\n",
      "Train Epoch: 744 [90112/101520 (89%)] Loss: -1124.742676\n",
      "Train Epoch: 744 [101376/101520 (100%)] Loss: -1122.871948\n",
      "    epoch          : 744\n",
      "    loss           : -1126.850146116324\n",
      "    ess            : 1.9682385915487854\n",
      "    log_marginal   : 1126.8783836173052\n",
      "    log_joint      : 1335.1546434565405\n",
      "    val_loss       : -1126.036031971807\n",
      "    val_ess        : 1.9685822414315266\n",
      "    val_log_marginal: 1126.0623142408288\n",
      "    val_log_joint  : 1334.1485914147418\n",
      "Train Epoch: 745 [0/101520 (0%)] Loss: -1131.423218\n",
      "Train Epoch: 745 [11264/101520 (11%)] Loss: -1124.222412\n",
      "Train Epoch: 745 [22528/101520 (22%)] Loss: -1130.215088\n",
      "Train Epoch: 745 [33792/101520 (33%)] Loss: -1119.981201\n",
      "Train Epoch: 745 [45056/101520 (44%)] Loss: -1135.232178\n",
      "Train Epoch: 745 [56320/101520 (55%)] Loss: -1124.678223\n",
      "Train Epoch: 745 [67584/101520 (67%)] Loss: -1123.421387\n",
      "Train Epoch: 745 [78848/101520 (78%)] Loss: -1123.009033\n",
      "Train Epoch: 745 [90112/101520 (89%)] Loss: -1127.493896\n",
      "Train Epoch: 745 [101376/101520 (100%)] Loss: -1126.402588\n",
      "    epoch          : 745\n",
      "    loss           : -1127.0479448021356\n",
      "    ess            : 1.9681754537563227\n",
      "    log_marginal   : 1127.0755228780622\n",
      "    log_joint      : 1335.3682861328125\n",
      "    val_loss       : -1126.5477666440217\n",
      "    val_ess        : 1.9708459688269573\n",
      "    val_log_marginal: 1126.5722550101902\n",
      "    val_log_joint  : 1334.6532831606658\n",
      "Train Epoch: 746 [0/101520 (0%)] Loss: -1133.642334\n",
      "Train Epoch: 746 [11264/101520 (11%)] Loss: -1128.428467\n",
      "Train Epoch: 746 [22528/101520 (22%)] Loss: -1127.445068\n",
      "Train Epoch: 746 [33792/101520 (33%)] Loss: -1130.276123\n",
      "Train Epoch: 746 [45056/101520 (44%)] Loss: -1128.960083\n",
      "Train Epoch: 746 [56320/101520 (55%)] Loss: -1125.903076\n",
      "Train Epoch: 746 [67584/101520 (67%)] Loss: -1131.583984\n",
      "Train Epoch: 746 [78848/101520 (78%)] Loss: -1136.512817\n",
      "Train Epoch: 746 [90112/101520 (89%)] Loss: -1116.560791\n",
      "Train Epoch: 746 [101376/101520 (100%)] Loss: -1118.803345\n",
      "    epoch          : 746\n",
      "    loss           : -1127.142433971616\n",
      "    ess            : 1.968117248472856\n",
      "    log_marginal   : 1127.1708113320508\n",
      "    log_joint      : 1335.4080865754554\n",
      "    val_loss       : -1126.3720968495245\n",
      "    val_ess        : 1.9679171624390974\n",
      "    val_log_marginal: 1126.3987081776495\n",
      "    val_log_joint  : 1335.1358536430027\n",
      "Train Epoch: 747 [0/101520 (0%)] Loss: -1123.915283\n",
      "Train Epoch: 747 [11264/101520 (11%)] Loss: -1124.138184\n",
      "Train Epoch: 747 [22528/101520 (22%)] Loss: -1127.853027\n",
      "Train Epoch: 747 [33792/101520 (33%)] Loss: -1125.996704\n",
      "Train Epoch: 747 [45056/101520 (44%)] Loss: -1130.019043\n",
      "Train Epoch: 747 [56320/101520 (55%)] Loss: -1123.786865\n",
      "Train Epoch: 747 [67584/101520 (67%)] Loss: -1128.759277\n",
      "Train Epoch: 747 [78848/101520 (78%)] Loss: -1131.932129\n",
      "Train Epoch: 747 [90112/101520 (89%)] Loss: -1131.384277\n",
      "Train Epoch: 747 [101376/101520 (100%)] Loss: -1138.007812\n",
      "    epoch          : 747\n",
      "    loss           : -1127.268918444763\n",
      "    ess            : 1.9686050624703642\n",
      "    log_marginal   : 1127.296183677175\n",
      "    log_joint      : 1335.4653639290202\n",
      "    val_loss       : -1125.310838782269\n",
      "    val_ess        : 1.9668045510416445\n",
      "    val_log_marginal: 1125.3407566236413\n",
      "    val_log_joint  : 1333.6406143851902\n",
      "Train Epoch: 748 [0/101520 (0%)] Loss: -1126.802246\n",
      "Train Epoch: 748 [11264/101520 (11%)] Loss: -1129.327148\n",
      "Train Epoch: 748 [22528/101520 (22%)] Loss: -1129.514160\n",
      "Train Epoch: 748 [33792/101520 (33%)] Loss: -1128.145020\n",
      "Train Epoch: 748 [45056/101520 (44%)] Loss: -1123.949951\n",
      "Train Epoch: 748 [56320/101520 (55%)] Loss: -1128.809326\n",
      "Train Epoch: 748 [67584/101520 (67%)] Loss: -1122.864380\n",
      "Train Epoch: 748 [78848/101520 (78%)] Loss: -1129.117065\n",
      "Train Epoch: 748 [90112/101520 (89%)] Loss: -1127.901245\n",
      "Train Epoch: 748 [101376/101520 (100%)] Loss: -1133.776123\n",
      "    epoch          : 748\n",
      "    loss           : -1127.443778339942\n",
      "    ess            : 1.968256123101891\n",
      "    log_marginal   : 1127.4717575916693\n",
      "    log_joint      : 1335.679051384854\n",
      "    val_loss       : -1125.6746507727582\n",
      "    val_ess        : 1.9677491861840952\n",
      "    val_log_marginal: 1125.7010020380435\n",
      "    val_log_joint  : 1333.877781080163\n",
      "Train Epoch: 749 [0/101520 (0%)] Loss: -1135.303711\n",
      "Train Epoch: 749 [11264/101520 (11%)] Loss: -1127.864258\n",
      "Train Epoch: 749 [22528/101520 (22%)] Loss: -1122.057861\n",
      "Train Epoch: 749 [33792/101520 (33%)] Loss: -1125.661255\n",
      "Train Epoch: 749 [45056/101520 (44%)] Loss: -1121.841064\n",
      "Train Epoch: 749 [56320/101520 (55%)] Loss: -1130.398682\n",
      "Train Epoch: 749 [67584/101520 (67%)] Loss: -1134.155518\n",
      "Train Epoch: 749 [78848/101520 (78%)] Loss: -1134.711060\n",
      "Train Epoch: 749 [90112/101520 (89%)] Loss: -1124.115234\n",
      "Train Epoch: 749 [101376/101520 (100%)] Loss: -1130.645142\n",
      "    epoch          : 749\n",
      "    loss           : -1127.345006281407\n",
      "    ess            : 1.9686316551275589\n",
      "    log_marginal   : 1127.372334082522\n",
      "    log_joint      : 1335.6755242275833\n",
      "    val_loss       : -1125.2087720788043\n",
      "    val_ess        : 1.967996649120165\n",
      "    val_log_marginal: 1125.238673997962\n",
      "    val_log_joint  : 1333.5790272588315\n",
      "Train Epoch: 750 [0/101520 (0%)] Loss: -1125.571777\n",
      "Train Epoch: 750 [11264/101520 (11%)] Loss: -1123.732178\n",
      "Train Epoch: 750 [22528/101520 (22%)] Loss: -1134.474365\n",
      "Train Epoch: 750 [33792/101520 (33%)] Loss: -1127.997314\n",
      "Train Epoch: 750 [45056/101520 (44%)] Loss: -1130.649292\n",
      "Train Epoch: 750 [56320/101520 (55%)] Loss: -1132.561523\n",
      "Train Epoch: 750 [67584/101520 (67%)] Loss: -1129.065918\n",
      "Train Epoch: 750 [78848/101520 (78%)] Loss: -1131.024170\n",
      "Train Epoch: 750 [90112/101520 (89%)] Loss: -1125.238770\n",
      "Train Epoch: 750 [101376/101520 (100%)] Loss: -1124.060303\n",
      "    epoch          : 750\n",
      "    loss           : -1127.4785573374686\n",
      "    ess            : 1.9684567810902045\n",
      "    log_marginal   : 1127.506393662649\n",
      "    log_joint      : 1335.788483432789\n",
      "    val_loss       : -1127.7125881029212\n",
      "    val_ess        : 1.9690933486689692\n",
      "    val_log_marginal: 1127.7400592306385\n",
      "    val_log_joint  : 1335.9045781674592\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch750.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 751 [0/101520 (0%)] Loss: -1127.875000\n",
      "Train Epoch: 751 [11264/101520 (11%)] Loss: -1122.282593\n",
      "Train Epoch: 751 [22528/101520 (22%)] Loss: -1123.405029\n",
      "Train Epoch: 751 [33792/101520 (33%)] Loss: -1122.887939\n",
      "Train Epoch: 751 [45056/101520 (44%)] Loss: -1125.947876\n",
      "Train Epoch: 751 [56320/101520 (55%)] Loss: -1126.574951\n",
      "Train Epoch: 751 [67584/101520 (67%)] Loss: -1121.122437\n",
      "Train Epoch: 751 [78848/101520 (78%)] Loss: -1127.783447\n",
      "Train Epoch: 751 [90112/101520 (89%)] Loss: -1130.750244\n",
      "Train Epoch: 751 [101376/101520 (100%)] Loss: -1129.656006\n",
      "    epoch          : 751\n",
      "    loss           : -1127.695795260482\n",
      "    ess            : 1.9683643831080528\n",
      "    log_marginal   : 1127.7233819242697\n",
      "    log_joint      : 1336.0250765546482\n",
      "    val_loss       : -1126.7390402088995\n",
      "    val_ess        : 1.9717352131138677\n",
      "    val_log_marginal: 1126.7635073454483\n",
      "    val_log_joint  : 1335.1863217561142\n",
      "Train Epoch: 752 [0/101520 (0%)] Loss: -1129.521729\n",
      "Train Epoch: 752 [11264/101520 (11%)] Loss: -1123.757446\n",
      "Train Epoch: 752 [22528/101520 (22%)] Loss: -1132.280273\n",
      "Train Epoch: 752 [33792/101520 (33%)] Loss: -1130.625732\n",
      "Train Epoch: 752 [45056/101520 (44%)] Loss: -1128.837524\n",
      "Train Epoch: 752 [56320/101520 (55%)] Loss: -1132.554932\n",
      "Train Epoch: 752 [67584/101520 (67%)] Loss: -1126.260620\n",
      "Train Epoch: 752 [78848/101520 (78%)] Loss: -1133.595093\n",
      "Train Epoch: 752 [90112/101520 (89%)] Loss: -1127.171753\n",
      "Train Epoch: 752 [101376/101520 (100%)] Loss: -1132.829834\n",
      "    epoch          : 752\n",
      "    loss           : -1127.764882150008\n",
      "    ess            : 1.969083334932375\n",
      "    log_marginal   : 1127.7920909479035\n",
      "    log_joint      : 1336.0542808034313\n",
      "    val_loss       : -1124.7326023267663\n",
      "    val_ess        : 1.9675335365792979\n",
      "    val_log_marginal: 1124.761522376019\n",
      "    val_log_joint  : 1333.076166567595\n",
      "Train Epoch: 753 [0/101520 (0%)] Loss: -1133.362549\n",
      "Train Epoch: 753 [11264/101520 (11%)] Loss: -1133.078857\n",
      "Train Epoch: 753 [22528/101520 (22%)] Loss: -1131.100342\n",
      "Train Epoch: 753 [33792/101520 (33%)] Loss: -1122.062500\n",
      "Train Epoch: 753 [45056/101520 (44%)] Loss: -1124.671875\n",
      "Train Epoch: 753 [56320/101520 (55%)] Loss: -1129.137329\n",
      "Train Epoch: 753 [67584/101520 (67%)] Loss: -1130.747070\n",
      "Train Epoch: 753 [78848/101520 (78%)] Loss: -1129.047607\n",
      "Train Epoch: 753 [90112/101520 (89%)] Loss: -1138.034424\n",
      "Train Epoch: 753 [101376/101520 (100%)] Loss: -1111.202271\n",
      "    epoch          : 753\n",
      "    loss           : -1127.8109124725188\n",
      "    ess            : 1.9679005331729524\n",
      "    log_marginal   : 1127.8394548425722\n",
      "    log_joint      : 1336.10362051959\n",
      "    val_loss       : -1126.1068486752717\n",
      "    val_ess        : 1.9691600229429163\n",
      "    val_log_marginal: 1126.1322180706522\n",
      "    val_log_joint  : 1334.2086977751358\n",
      "Train Epoch: 754 [0/101520 (0%)] Loss: -1132.297729\n",
      "Train Epoch: 754 [11264/101520 (11%)] Loss: -1131.056885\n",
      "Train Epoch: 754 [22528/101520 (22%)] Loss: -1125.960205\n",
      "Train Epoch: 754 [33792/101520 (33%)] Loss: -1132.272217\n",
      "Train Epoch: 754 [45056/101520 (44%)] Loss: -1126.824951\n",
      "Train Epoch: 754 [56320/101520 (55%)] Loss: -1130.463989\n",
      "Train Epoch: 754 [67584/101520 (67%)] Loss: -1136.958618\n",
      "Train Epoch: 754 [78848/101520 (78%)] Loss: -1133.785645\n",
      "Train Epoch: 754 [90112/101520 (89%)] Loss: -1126.918701\n",
      "Train Epoch: 754 [101376/101520 (100%)] Loss: -1131.622192\n",
      "    epoch          : 754\n",
      "    loss           : -1128.0124787757145\n",
      "    ess            : 1.9686628544150884\n",
      "    log_marginal   : 1128.040472136071\n",
      "    log_joint      : 1336.35279915201\n",
      "    val_loss       : -1126.58373492697\n",
      "    val_ess        : 1.9682944028273872\n",
      "    val_log_marginal: 1126.6092423148777\n",
      "    val_log_joint  : 1335.0203910495925\n",
      "Train Epoch: 755 [0/101520 (0%)] Loss: -1132.176025\n",
      "Train Epoch: 755 [11264/101520 (11%)] Loss: -1124.039307\n",
      "Train Epoch: 755 [22528/101520 (22%)] Loss: -1127.699585\n",
      "Train Epoch: 755 [33792/101520 (33%)] Loss: -1128.364258\n",
      "Train Epoch: 755 [45056/101520 (44%)] Loss: -1127.270752\n",
      "Train Epoch: 755 [56320/101520 (55%)] Loss: -1128.142090\n",
      "Train Epoch: 755 [67584/101520 (67%)] Loss: -1130.688721\n",
      "Train Epoch: 755 [78848/101520 (78%)] Loss: -1126.506348\n",
      "Train Epoch: 755 [90112/101520 (89%)] Loss: -1132.302979\n",
      "Train Epoch: 755 [101376/101520 (100%)] Loss: -1125.590454\n",
      "    epoch          : 755\n",
      "    loss           : -1127.9433231832993\n",
      "    ess            : 1.968447131727209\n",
      "    log_marginal   : 1127.9712797385364\n",
      "    log_joint      : 1336.2628222901617\n",
      "    val_loss       : -1126.6032396399457\n",
      "    val_ess        : 1.9689988260683806\n",
      "    val_log_marginal: 1126.628465735394\n",
      "    val_log_joint  : 1334.7262599779212\n",
      "Train Epoch: 756 [0/101520 (0%)] Loss: -1127.317261\n",
      "Train Epoch: 756 [11264/101520 (11%)] Loss: -1121.620850\n",
      "Train Epoch: 756 [22528/101520 (22%)] Loss: -1134.114502\n",
      "Train Epoch: 756 [33792/101520 (33%)] Loss: -1131.634277\n",
      "Train Epoch: 756 [45056/101520 (44%)] Loss: -1129.732910\n",
      "Train Epoch: 756 [56320/101520 (55%)] Loss: -1128.940796\n",
      "Train Epoch: 756 [67584/101520 (67%)] Loss: -1134.455566\n",
      "Train Epoch: 756 [78848/101520 (78%)] Loss: -1131.103760\n",
      "Train Epoch: 756 [90112/101520 (89%)] Loss: -1130.771484\n",
      "Train Epoch: 756 [101376/101520 (100%)] Loss: -1131.310181\n",
      "    epoch          : 756\n",
      "    loss           : -1128.211928171129\n",
      "    ess            : 1.968244177612228\n",
      "    log_marginal   : 1128.2402711801194\n",
      "    log_joint      : 1336.496934746977\n",
      "    val_loss       : -1127.1343357252038\n",
      "    val_ess        : 1.9671710936919502\n",
      "    val_log_marginal: 1127.1616847826087\n",
      "    val_log_joint  : 1335.433933423913\n",
      "Train Epoch: 757 [0/101520 (0%)] Loss: -1134.663940\n",
      "Train Epoch: 757 [11264/101520 (11%)] Loss: -1126.406494\n",
      "Train Epoch: 757 [22528/101520 (22%)] Loss: -1129.027954\n",
      "Train Epoch: 757 [33792/101520 (33%)] Loss: -1123.185791\n",
      "Train Epoch: 757 [45056/101520 (44%)] Loss: -1125.562134\n",
      "Train Epoch: 757 [56320/101520 (55%)] Loss: -1131.747559\n",
      "Train Epoch: 757 [67584/101520 (67%)] Loss: -1136.251221\n",
      "Train Epoch: 757 [78848/101520 (78%)] Loss: -1124.870605\n",
      "Train Epoch: 757 [90112/101520 (89%)] Loss: -1125.597168\n",
      "Train Epoch: 757 [101376/101520 (100%)] Loss: -1123.461548\n",
      "    epoch          : 757\n",
      "    loss           : -1128.214982996035\n",
      "    ess            : 1.968266955572157\n",
      "    log_marginal   : 1128.242689889879\n",
      "    log_joint      : 1336.637158571176\n",
      "    val_loss       : -1128.0128704568615\n",
      "    val_ess        : 1.9647040885427725\n",
      "    val_log_marginal: 1128.0458772078805\n",
      "    val_log_joint  : 1336.1279031504755\n",
      "Train Epoch: 758 [0/101520 (0%)] Loss: -1129.646729\n",
      "Train Epoch: 758 [11264/101520 (11%)] Loss: -1129.890015\n",
      "Train Epoch: 758 [22528/101520 (22%)] Loss: -1129.392334\n",
      "Train Epoch: 758 [33792/101520 (33%)] Loss: -1127.349365\n",
      "Train Epoch: 758 [45056/101520 (44%)] Loss: -1134.060791\n",
      "Train Epoch: 758 [56320/101520 (55%)] Loss: -1126.171021\n",
      "Train Epoch: 758 [67584/101520 (67%)] Loss: -1125.421875\n",
      "Train Epoch: 758 [78848/101520 (78%)] Loss: -1130.033813\n",
      "Train Epoch: 758 [90112/101520 (89%)] Loss: -1129.308960\n",
      "Train Epoch: 758 [101376/101520 (100%)] Loss: -1146.740112\n",
      "    epoch          : 758\n",
      "    loss           : -1128.3952446558967\n",
      "    ess            : 1.9684642834879047\n",
      "    log_marginal   : 1128.42345393962\n",
      "    log_joint      : 1336.7076146111417\n",
      "    val_loss       : -1126.614401112432\n",
      "    val_ess        : 1.9708646432213162\n",
      "    val_log_marginal: 1126.6422437584918\n",
      "    val_log_joint  : 1335.1798732591712\n",
      "Train Epoch: 759 [0/101520 (0%)] Loss: -1128.963867\n",
      "Train Epoch: 759 [11264/101520 (11%)] Loss: -1130.491943\n",
      "Train Epoch: 759 [22528/101520 (22%)] Loss: -1131.242188\n",
      "Train Epoch: 759 [33792/101520 (33%)] Loss: -1133.248291\n",
      "Train Epoch: 759 [45056/101520 (44%)] Loss: -1123.358887\n",
      "Train Epoch: 759 [56320/101520 (55%)] Loss: -1128.464233\n",
      "Train Epoch: 759 [67584/101520 (67%)] Loss: -1126.534424\n",
      "Train Epoch: 759 [78848/101520 (78%)] Loss: -1122.060913\n",
      "Train Epoch: 759 [90112/101520 (89%)] Loss: -1122.967529\n",
      "Train Epoch: 759 [101376/101520 (100%)] Loss: -1130.943970\n",
      "    epoch          : 759\n",
      "    loss           : -1128.5390526853016\n",
      "    ess            : 1.9684523823273241\n",
      "    log_marginal   : 1128.5665412021042\n",
      "    log_joint      : 1336.8228226091394\n",
      "    val_loss       : -1127.9474460767663\n",
      "    val_ess        : 1.9701050934584245\n",
      "    val_log_marginal: 1127.9735425866168\n",
      "    val_log_joint  : 1336.3130095108695\n",
      "Train Epoch: 760 [0/101520 (0%)] Loss: -1137.280640\n",
      "Train Epoch: 760 [11264/101520 (11%)] Loss: -1133.205322\n",
      "Train Epoch: 760 [22528/101520 (22%)] Loss: -1130.768921\n",
      "Train Epoch: 760 [33792/101520 (33%)] Loss: -1130.942505\n",
      "Train Epoch: 760 [45056/101520 (44%)] Loss: -1131.527222\n",
      "Train Epoch: 760 [56320/101520 (55%)] Loss: -1133.455078\n",
      "Train Epoch: 760 [67584/101520 (67%)] Loss: -1124.959106\n",
      "Train Epoch: 760 [78848/101520 (78%)] Loss: -1122.863525\n",
      "Train Epoch: 760 [90112/101520 (89%)] Loss: -1119.812256\n",
      "Train Epoch: 760 [101376/101520 (100%)] Loss: -1134.398804\n",
      "    epoch          : 760\n",
      "    loss           : -1128.772722867266\n",
      "    ess            : 1.9676497216200708\n",
      "    log_marginal   : 1128.8013308731156\n",
      "    log_joint      : 1337.092889373626\n",
      "    val_loss       : -1129.9249639096467\n",
      "    val_ess        : 1.9683433408322542\n",
      "    val_log_marginal: 1129.960443911345\n",
      "    val_log_joint  : 1338.3195641559103\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch760.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 761 [0/101520 (0%)] Loss: -1125.558350\n",
      "Train Epoch: 761 [11264/101520 (11%)] Loss: -1129.454956\n",
      "Train Epoch: 761 [22528/101520 (22%)] Loss: -1127.817871\n",
      "Train Epoch: 761 [33792/101520 (33%)] Loss: -1127.724121\n",
      "Train Epoch: 761 [45056/101520 (44%)] Loss: -1131.126953\n",
      "Train Epoch: 761 [56320/101520 (55%)] Loss: -1125.263916\n",
      "Train Epoch: 761 [67584/101520 (67%)] Loss: -1122.375488\n",
      "Train Epoch: 761 [78848/101520 (78%)] Loss: -1125.704590\n",
      "Train Epoch: 761 [90112/101520 (89%)] Loss: -1134.074463\n",
      "Train Epoch: 761 [101376/101520 (100%)] Loss: -1117.330811\n",
      "    epoch          : 761\n",
      "    loss           : -1128.6524799289416\n",
      "    ess            : 1.9680287394691351\n",
      "    log_marginal   : 1128.6806573148947\n",
      "    log_joint      : 1336.9482949415044\n",
      "    val_loss       : -1127.33862835428\n",
      "    val_ess        : 1.9668083139087842\n",
      "    val_log_marginal: 1127.3691671620245\n",
      "    val_log_joint  : 1335.5908893087635\n",
      "Train Epoch: 762 [0/101520 (0%)] Loss: -1127.173340\n",
      "Train Epoch: 762 [11264/101520 (11%)] Loss: -1124.616699\n",
      "Train Epoch: 762 [22528/101520 (22%)] Loss: -1128.355957\n",
      "Train Epoch: 762 [33792/101520 (33%)] Loss: -1127.204834\n",
      "Train Epoch: 762 [45056/101520 (44%)] Loss: -1133.925293\n",
      "Train Epoch: 762 [56320/101520 (55%)] Loss: -1134.026611\n",
      "Train Epoch: 762 [67584/101520 (67%)] Loss: -1122.789307\n",
      "Train Epoch: 762 [78848/101520 (78%)] Loss: -1134.082764\n",
      "Train Epoch: 762 [90112/101520 (89%)] Loss: -1128.895508\n",
      "Train Epoch: 762 [101376/101520 (100%)] Loss: -1130.944946\n",
      "    epoch          : 762\n",
      "    loss           : -1128.904726268059\n",
      "    ess            : 1.9685446413318117\n",
      "    log_marginal   : 1128.9325178136778\n",
      "    log_joint      : 1337.2136911363457\n",
      "    val_loss       : -1128.3758491847825\n",
      "    val_ess        : 1.970365690148395\n",
      "    val_log_marginal: 1128.4004330842392\n",
      "    val_log_joint  : 1336.8016994310462\n",
      "Train Epoch: 763 [0/101520 (0%)] Loss: -1132.916992\n",
      "Train Epoch: 763 [11264/101520 (11%)] Loss: -1135.976318\n",
      "Train Epoch: 763 [22528/101520 (22%)] Loss: -1134.205078\n",
      "Train Epoch: 763 [33792/101520 (33%)] Loss: -1132.870239\n",
      "Train Epoch: 763 [45056/101520 (44%)] Loss: -1128.152344\n",
      "Train Epoch: 763 [56320/101520 (55%)] Loss: -1130.288330\n",
      "Train Epoch: 763 [67584/101520 (67%)] Loss: -1126.269653\n",
      "Train Epoch: 763 [78848/101520 (78%)] Loss: -1125.500854\n",
      "Train Epoch: 763 [90112/101520 (89%)] Loss: -1130.655762\n",
      "Train Epoch: 763 [101376/101520 (100%)] Loss: -1140.941406\n",
      "    epoch          : 763\n",
      "    loss           : -1129.113297198885\n",
      "    ess            : 1.9693224466026729\n",
      "    log_marginal   : 1129.1401305845634\n",
      "    log_joint      : 1337.4144372987987\n",
      "    val_loss       : -1127.6394414487092\n",
      "    val_ess        : 1.967273473739624\n",
      "    val_log_marginal: 1127.66820758322\n",
      "    val_log_joint  : 1335.9053848930027\n",
      "Train Epoch: 764 [0/101520 (0%)] Loss: -1132.532104\n",
      "Train Epoch: 764 [11264/101520 (11%)] Loss: -1132.123535\n",
      "Train Epoch: 764 [22528/101520 (22%)] Loss: -1132.052490\n",
      "Train Epoch: 764 [33792/101520 (33%)] Loss: -1127.156982\n",
      "Train Epoch: 764 [45056/101520 (44%)] Loss: -1130.874390\n",
      "Train Epoch: 764 [56320/101520 (55%)] Loss: -1120.008057\n",
      "Train Epoch: 764 [67584/101520 (67%)] Loss: -1127.655273\n",
      "Train Epoch: 764 [78848/101520 (78%)] Loss: -1129.640991\n",
      "Train Epoch: 764 [90112/101520 (89%)] Loss: -1129.618408\n",
      "Train Epoch: 764 [101376/101520 (100%)] Loss: -1128.928223\n",
      "    epoch          : 764\n",
      "    loss           : -1129.066445508794\n",
      "    ess            : 1.9689860673406017\n",
      "    log_marginal   : 1129.0934494248586\n",
      "    log_joint      : 1337.320233982412\n",
      "    val_loss       : -1128.2195408033288\n",
      "    val_ess        : 1.9662285421205603\n",
      "    val_log_marginal: 1128.2472826086957\n",
      "    val_log_joint  : 1336.4704749065897\n",
      "Train Epoch: 765 [0/101520 (0%)] Loss: -1126.660767\n",
      "Train Epoch: 765 [11264/101520 (11%)] Loss: -1125.348145\n",
      "Train Epoch: 765 [22528/101520 (22%)] Loss: -1123.254150\n",
      "Train Epoch: 765 [33792/101520 (33%)] Loss: -1130.160645\n",
      "Train Epoch: 765 [45056/101520 (44%)] Loss: -1129.312012\n",
      "Train Epoch: 765 [56320/101520 (55%)] Loss: -1126.885254\n",
      "Train Epoch: 765 [67584/101520 (67%)] Loss: -1128.860352\n",
      "Train Epoch: 765 [78848/101520 (78%)] Loss: -1131.946167\n",
      "Train Epoch: 765 [90112/101520 (89%)] Loss: -1125.552124\n",
      "Train Epoch: 765 [101376/101520 (100%)] Loss: -1115.709595\n",
      "    epoch          : 765\n",
      "    loss           : -1129.2029133705637\n",
      "    ess            : 1.9694193134355784\n",
      "    log_marginal   : 1129.2290492992306\n",
      "    log_joint      : 1337.495635526264\n",
      "    val_loss       : -1129.172071373981\n",
      "    val_ess        : 1.9680936284687207\n",
      "    val_log_marginal: 1129.202631411345\n",
      "    val_log_joint  : 1337.4545686141305\n",
      "Train Epoch: 766 [0/101520 (0%)] Loss: -1138.597290\n",
      "Train Epoch: 766 [11264/101520 (11%)] Loss: -1128.287598\n",
      "Train Epoch: 766 [22528/101520 (22%)] Loss: -1127.722168\n",
      "Train Epoch: 766 [33792/101520 (33%)] Loss: -1133.344849\n",
      "Train Epoch: 766 [45056/101520 (44%)] Loss: -1129.188721\n",
      "Train Epoch: 766 [56320/101520 (55%)] Loss: -1131.537109\n",
      "Train Epoch: 766 [67584/101520 (67%)] Loss: -1125.369629\n",
      "Train Epoch: 766 [78848/101520 (78%)] Loss: -1122.806641\n",
      "Train Epoch: 766 [90112/101520 (89%)] Loss: -1126.053467\n",
      "Train Epoch: 766 [101376/101520 (100%)] Loss: -1116.320068\n",
      "    epoch          : 766\n",
      "    loss           : -1129.3318932308025\n",
      "    ess            : 1.968631271740899\n",
      "    log_marginal   : 1129.3594682396356\n",
      "    log_joint      : 1337.627261674584\n",
      "    val_loss       : -1127.855028235394\n",
      "    val_ess        : 1.9704843396725862\n",
      "    val_log_marginal: 1127.881156589674\n",
      "    val_log_joint  : 1336.0783054517663\n",
      "Train Epoch: 767 [0/101520 (0%)] Loss: -1135.354492\n",
      "Train Epoch: 767 [11264/101520 (11%)] Loss: -1130.040649\n",
      "Train Epoch: 767 [22528/101520 (22%)] Loss: -1135.722534\n",
      "Train Epoch: 767 [33792/101520 (33%)] Loss: -1135.709106\n",
      "Train Epoch: 767 [45056/101520 (44%)] Loss: -1122.631836\n",
      "Train Epoch: 767 [56320/101520 (55%)] Loss: -1134.876709\n",
      "Train Epoch: 767 [67584/101520 (67%)] Loss: -1126.361938\n",
      "Train Epoch: 767 [78848/101520 (78%)] Loss: -1127.303711\n",
      "Train Epoch: 767 [90112/101520 (89%)] Loss: -1124.651001\n",
      "Train Epoch: 767 [101376/101520 (100%)] Loss: -1116.634521\n",
      "    epoch          : 767\n",
      "    loss           : -1129.342894280975\n",
      "    ess            : 1.9682993008263747\n",
      "    log_marginal   : 1129.3705030278345\n",
      "    log_joint      : 1337.6434737162374\n",
      "    val_loss       : -1129.9550940472147\n",
      "    val_ess        : 1.966603952905406\n",
      "    val_log_marginal: 1129.9848261294158\n",
      "    val_log_joint  : 1337.8383682914402\n",
      "Train Epoch: 768 [0/101520 (0%)] Loss: -1132.182007\n",
      "Train Epoch: 768 [11264/101520 (11%)] Loss: -1129.743896\n",
      "Train Epoch: 768 [22528/101520 (22%)] Loss: -1134.117432\n",
      "Train Epoch: 768 [33792/101520 (33%)] Loss: -1128.578735\n",
      "Train Epoch: 768 [45056/101520 (44%)] Loss: -1130.724121\n",
      "Train Epoch: 768 [56320/101520 (55%)] Loss: -1130.479004\n",
      "Train Epoch: 768 [67584/101520 (67%)] Loss: -1130.914062\n",
      "Train Epoch: 768 [78848/101520 (78%)] Loss: -1130.761719\n",
      "Train Epoch: 768 [90112/101520 (89%)] Loss: -1128.598999\n",
      "Train Epoch: 768 [101376/101520 (100%)] Loss: -1122.913330\n",
      "    epoch          : 768\n",
      "    loss           : -1129.600099496506\n",
      "    ess            : 1.9681900140628144\n",
      "    log_marginal   : 1129.6284560007066\n",
      "    log_joint      : 1337.8570857215766\n",
      "    val_loss       : -1128.9605129076087\n",
      "    val_ess        : 1.9671251151872717\n",
      "    val_log_marginal: 1128.9895337975543\n",
      "    val_log_joint  : 1337.252446713655\n",
      "Train Epoch: 769 [0/101520 (0%)] Loss: -1130.378174\n",
      "Train Epoch: 769 [11264/101520 (11%)] Loss: -1132.743408\n",
      "Train Epoch: 769 [22528/101520 (22%)] Loss: -1136.960938\n",
      "Train Epoch: 769 [33792/101520 (33%)] Loss: -1135.198975\n",
      "Train Epoch: 769 [45056/101520 (44%)] Loss: -1132.910156\n",
      "Train Epoch: 769 [56320/101520 (55%)] Loss: -1133.603149\n",
      "Train Epoch: 769 [67584/101520 (67%)] Loss: -1128.802612\n",
      "Train Epoch: 769 [78848/101520 (78%)] Loss: -1131.430420\n",
      "Train Epoch: 769 [90112/101520 (89%)] Loss: -1131.152710\n",
      "Train Epoch: 769 [101376/101520 (100%)] Loss: -1130.657959\n",
      "    epoch          : 769\n",
      "    loss           : -1129.7819143324043\n",
      "    ess            : 1.9677835223662794\n",
      "    log_marginal   : 1129.810237098579\n",
      "    log_joint      : 1338.0878587272298\n",
      "    val_loss       : -1129.2827997622283\n",
      "    val_ess        : 1.970298575318378\n",
      "    val_log_marginal: 1129.3077074133832\n",
      "    val_log_joint  : 1337.3342975118885\n",
      "Train Epoch: 770 [0/101520 (0%)] Loss: -1130.017334\n",
      "Train Epoch: 770 [11264/101520 (11%)] Loss: -1126.247803\n",
      "Train Epoch: 770 [22528/101520 (22%)] Loss: -1132.790283\n",
      "Train Epoch: 770 [33792/101520 (33%)] Loss: -1126.837036\n",
      "Train Epoch: 770 [45056/101520 (44%)] Loss: -1135.226562\n",
      "Train Epoch: 770 [56320/101520 (55%)] Loss: -1130.110596\n",
      "Train Epoch: 770 [67584/101520 (67%)] Loss: -1129.518311\n",
      "Train Epoch: 770 [78848/101520 (78%)] Loss: -1135.534668\n",
      "Train Epoch: 770 [90112/101520 (89%)] Loss: -1123.737305\n",
      "Train Epoch: 770 [101376/101520 (100%)] Loss: -1124.510742\n",
      "    epoch          : 770\n",
      "    loss           : -1129.7186868178785\n",
      "    ess            : 1.9678495002152332\n",
      "    log_marginal   : 1129.7469279993718\n",
      "    log_joint      : 1337.9774139250942\n",
      "    val_loss       : -1129.2143820057745\n",
      "    val_ess        : 1.9687594693640005\n",
      "    val_log_marginal: 1129.2412799337635\n",
      "    val_log_joint  : 1337.5973855723505\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch770.pth ...\n",
      "Train Epoch: 771 [0/101520 (0%)] Loss: -1126.438232\n",
      "Train Epoch: 771 [11264/101520 (11%)] Loss: -1134.108887\n",
      "Train Epoch: 771 [22528/101520 (22%)] Loss: -1132.025635\n",
      "Train Epoch: 771 [33792/101520 (33%)] Loss: -1139.208740\n",
      "Train Epoch: 771 [45056/101520 (44%)] Loss: -1127.720337\n",
      "Train Epoch: 771 [56320/101520 (55%)] Loss: -1126.747925\n",
      "Train Epoch: 771 [67584/101520 (67%)] Loss: -1131.219727\n",
      "Train Epoch: 771 [78848/101520 (78%)] Loss: -1129.779907\n",
      "Train Epoch: 771 [90112/101520 (89%)] Loss: -1127.108887\n",
      "Train Epoch: 771 [101376/101520 (100%)] Loss: -1126.769165\n",
      "    epoch          : 771\n",
      "    loss           : -1129.9101464353016\n",
      "    ess            : 1.967939679347091\n",
      "    log_marginal   : 1129.9379165112673\n",
      "    log_joint      : 1338.1803499921482\n",
      "    val_loss       : -1129.8956458050272\n",
      "    val_ess        : 1.9692873021830684\n",
      "    val_log_marginal: 1129.9244225543478\n",
      "    val_log_joint  : 1338.4391293733017\n",
      "Train Epoch: 772 [0/101520 (0%)] Loss: -1127.618408\n",
      "Train Epoch: 772 [11264/101520 (11%)] Loss: -1131.485352\n",
      "Train Epoch: 772 [22528/101520 (22%)] Loss: -1127.172119\n",
      "Train Epoch: 772 [33792/101520 (33%)] Loss: -1133.856567\n",
      "Train Epoch: 772 [45056/101520 (44%)] Loss: -1132.932373\n",
      "Train Epoch: 772 [56320/101520 (55%)] Loss: -1128.781494\n",
      "Train Epoch: 772 [67584/101520 (67%)] Loss: -1129.542847\n",
      "Train Epoch: 772 [78848/101520 (78%)] Loss: -1124.720703\n",
      "Train Epoch: 772 [90112/101520 (89%)] Loss: -1131.476807\n",
      "Train Epoch: 772 [101376/101520 (100%)] Loss: -1126.864868\n",
      "    epoch          : 772\n",
      "    loss           : -1130.0865349697708\n",
      "    ess            : 1.9680513221414844\n",
      "    log_marginal   : 1130.1146436528345\n",
      "    log_joint      : 1338.3739375588882\n",
      "    val_loss       : -1129.882913340693\n",
      "    val_ess        : 1.9707046073416006\n",
      "    val_log_marginal: 1129.9090735394022\n",
      "    val_log_joint  : 1338.2891049592392\n",
      "Train Epoch: 773 [0/101520 (0%)] Loss: -1131.771973\n",
      "Train Epoch: 773 [11264/101520 (11%)] Loss: -1131.632324\n",
      "Train Epoch: 773 [22528/101520 (22%)] Loss: -1129.305420\n",
      "Train Epoch: 773 [33792/101520 (33%)] Loss: -1134.061279\n",
      "Train Epoch: 773 [45056/101520 (44%)] Loss: -1128.592407\n",
      "Train Epoch: 773 [56320/101520 (55%)] Loss: -1130.788940\n",
      "Train Epoch: 773 [67584/101520 (67%)] Loss: -1138.521240\n",
      "Train Epoch: 773 [78848/101520 (78%)] Loss: -1131.650024\n",
      "Train Epoch: 773 [90112/101520 (89%)] Loss: -1134.376221\n",
      "Train Epoch: 773 [101376/101520 (100%)] Loss: -1128.786011\n",
      "    epoch          : 773\n",
      "    loss           : -1129.9588813206658\n",
      "    ess            : 1.968776275165117\n",
      "    log_marginal   : 1129.986367383794\n",
      "    log_joint      : 1338.1746151411353\n",
      "    val_loss       : -1128.4960141389267\n",
      "    val_ess        : 1.968798191651054\n",
      "    val_log_marginal: 1128.5232198963995\n",
      "    val_log_joint  : 1336.8860234799592\n",
      "Train Epoch: 774 [0/101520 (0%)] Loss: -1138.647095\n",
      "Train Epoch: 774 [11264/101520 (11%)] Loss: -1132.075439\n",
      "Train Epoch: 774 [22528/101520 (22%)] Loss: -1128.352539\n",
      "Train Epoch: 774 [33792/101520 (33%)] Loss: -1133.597656\n",
      "Train Epoch: 774 [45056/101520 (44%)] Loss: -1127.476074\n",
      "Train Epoch: 774 [56320/101520 (55%)] Loss: -1130.592163\n",
      "Train Epoch: 774 [67584/101520 (67%)] Loss: -1125.646606\n",
      "Train Epoch: 774 [78848/101520 (78%)] Loss: -1133.464600\n",
      "Train Epoch: 774 [90112/101520 (89%)] Loss: -1127.505127\n",
      "Train Epoch: 774 [101376/101520 (100%)] Loss: -1128.286743\n",
      "    epoch          : 774\n",
      "    loss           : -1130.2681381762327\n",
      "    ess            : 1.9677428600177094\n",
      "    log_marginal   : 1130.296969466473\n",
      "    log_joint      : 1338.5419578360552\n",
      "    val_loss       : -1128.62158203125\n",
      "    val_ess        : 1.9677841145059336\n",
      "    val_log_marginal: 1128.6507409137228\n",
      "    val_log_joint  : 1336.9409391983695\n",
      "Train Epoch: 775 [0/101520 (0%)] Loss: -1139.371826\n",
      "Train Epoch: 775 [11264/101520 (11%)] Loss: -1125.162842\n",
      "Train Epoch: 775 [22528/101520 (22%)] Loss: -1131.197510\n",
      "Train Epoch: 775 [33792/101520 (33%)] Loss: -1130.745972\n",
      "Train Epoch: 775 [45056/101520 (44%)] Loss: -1129.866577\n",
      "Train Epoch: 775 [56320/101520 (55%)] Loss: -1119.912109\n",
      "Train Epoch: 775 [67584/101520 (67%)] Loss: -1139.107666\n",
      "Train Epoch: 775 [78848/101520 (78%)] Loss: -1131.056519\n",
      "Train Epoch: 775 [90112/101520 (89%)] Loss: -1130.883789\n",
      "Train Epoch: 775 [101376/101520 (100%)] Loss: -1138.298828\n",
      "    epoch          : 775\n",
      "    loss           : -1130.431820356666\n",
      "    ess            : 1.9688225145915046\n",
      "    log_marginal   : 1130.4586114164572\n",
      "    log_joint      : 1338.738122374568\n",
      "    val_loss       : -1127.741264011549\n",
      "    val_ess        : 1.969137228053549\n",
      "    val_log_marginal: 1127.7691331946332\n",
      "    val_log_joint  : 1336.0321947180707\n",
      "Train Epoch: 776 [0/101520 (0%)] Loss: -1132.065186\n",
      "Train Epoch: 776 [11264/101520 (11%)] Loss: -1133.389771\n",
      "Train Epoch: 776 [22528/101520 (22%)] Loss: -1131.035156\n",
      "Train Epoch: 776 [33792/101520 (33%)] Loss: -1139.726196\n",
      "Train Epoch: 776 [45056/101520 (44%)] Loss: -1132.908813\n",
      "Train Epoch: 776 [56320/101520 (55%)] Loss: -1125.933105\n",
      "Train Epoch: 776 [67584/101520 (67%)] Loss: -1125.223389\n",
      "Train Epoch: 776 [78848/101520 (78%)] Loss: -1128.200439\n",
      "Train Epoch: 776 [90112/101520 (89%)] Loss: -1136.105835\n",
      "Train Epoch: 776 [101376/101520 (100%)] Loss: -1119.571899\n",
      "    epoch          : 776\n",
      "    loss           : -1130.3666660941426\n",
      "    ess            : 1.9684351520921717\n",
      "    log_marginal   : 1130.3940846812186\n",
      "    log_joint      : 1338.541181248037\n",
      "    val_loss       : -1129.7756931470788\n",
      "    val_ess        : 1.9691774326822031\n",
      "    val_log_marginal: 1129.8035517153533\n",
      "    val_log_joint  : 1338.1807277513587\n",
      "Train Epoch: 777 [0/101520 (0%)] Loss: -1130.694702\n",
      "Train Epoch: 777 [11264/101520 (11%)] Loss: -1130.746216\n",
      "Train Epoch: 777 [22528/101520 (22%)] Loss: -1131.750854\n",
      "Train Epoch: 777 [33792/101520 (33%)] Loss: -1130.478027\n",
      "Train Epoch: 777 [45056/101520 (44%)] Loss: -1134.896851\n",
      "Train Epoch: 777 [56320/101520 (55%)] Loss: -1128.639893\n",
      "Train Epoch: 777 [67584/101520 (67%)] Loss: -1125.828857\n",
      "Train Epoch: 777 [78848/101520 (78%)] Loss: -1127.290771\n",
      "Train Epoch: 777 [90112/101520 (89%)] Loss: -1130.831909\n",
      "Train Epoch: 777 [101376/101520 (100%)] Loss: -1119.723267\n",
      "    epoch          : 777\n",
      "    loss           : -1130.5302992010836\n",
      "    ess            : 1.9681742173343448\n",
      "    log_marginal   : 1130.5586299417007\n",
      "    log_joint      : 1338.824680654248\n",
      "    val_loss       : -1130.3892928413723\n",
      "    val_ess        : 1.9674545008203257\n",
      "    val_log_marginal: 1130.4157396399457\n",
      "    val_log_joint  : 1338.8775263247283\n",
      "Train Epoch: 778 [0/101520 (0%)] Loss: -1130.932861\n",
      "Train Epoch: 778 [11264/101520 (11%)] Loss: -1132.255127\n",
      "Train Epoch: 778 [22528/101520 (22%)] Loss: -1138.622070\n",
      "Train Epoch: 778 [33792/101520 (33%)] Loss: -1128.692139\n",
      "Train Epoch: 778 [45056/101520 (44%)] Loss: -1128.395508\n",
      "Train Epoch: 778 [56320/101520 (55%)] Loss: -1131.878906\n",
      "Train Epoch: 778 [67584/101520 (67%)] Loss: -1127.770752\n",
      "Train Epoch: 778 [78848/101520 (78%)] Loss: -1130.159180\n",
      "Train Epoch: 778 [90112/101520 (89%)] Loss: -1128.512695\n",
      "Train Epoch: 778 [101376/101520 (100%)] Loss: -1135.331421\n",
      "    epoch          : 778\n",
      "    loss           : -1130.613965211801\n",
      "    ess            : 1.9675881646985385\n",
      "    log_marginal   : 1130.6428504828832\n",
      "    log_joint      : 1338.9284551419205\n",
      "    val_loss       : -1128.940381920856\n",
      "    val_ess        : 1.9676160345906797\n",
      "    val_log_marginal: 1128.9711648692255\n",
      "    val_log_joint  : 1337.3643958050272\n",
      "Train Epoch: 779 [0/101520 (0%)] Loss: -1126.530273\n",
      "Train Epoch: 779 [11264/101520 (11%)] Loss: -1128.533447\n",
      "Train Epoch: 779 [22528/101520 (22%)] Loss: -1131.704468\n",
      "Train Epoch: 779 [33792/101520 (33%)] Loss: -1127.472656\n",
      "Train Epoch: 779 [45056/101520 (44%)] Loss: -1131.793701\n",
      "Train Epoch: 779 [56320/101520 (55%)] Loss: -1125.238403\n",
      "Train Epoch: 779 [67584/101520 (67%)] Loss: -1125.460449\n",
      "Train Epoch: 779 [78848/101520 (78%)] Loss: -1138.046509\n",
      "Train Epoch: 779 [90112/101520 (89%)] Loss: -1128.622437\n",
      "Train Epoch: 779 [101376/101520 (100%)] Loss: -1132.194702\n",
      "    epoch          : 779\n",
      "    loss           : -1130.6920012660962\n",
      "    ess            : 1.968468076020629\n",
      "    log_marginal   : 1130.7201357127435\n",
      "    log_joint      : 1338.995778452811\n",
      "    val_loss       : -1131.4843431555707\n",
      "    val_ess        : 1.970618875130363\n",
      "    val_log_marginal: 1131.510789954144\n",
      "    val_log_joint  : 1339.4766739555027\n",
      "Train Epoch: 780 [0/101520 (0%)] Loss: -1129.247314\n",
      "Train Epoch: 780 [11264/101520 (11%)] Loss: -1125.420166\n",
      "Train Epoch: 780 [22528/101520 (22%)] Loss: -1131.812378\n",
      "Train Epoch: 780 [33792/101520 (33%)] Loss: -1133.367920\n",
      "Train Epoch: 780 [45056/101520 (44%)] Loss: -1132.511475\n",
      "Train Epoch: 780 [56320/101520 (55%)] Loss: -1134.288818\n",
      "Train Epoch: 780 [67584/101520 (67%)] Loss: -1133.668823\n",
      "Train Epoch: 780 [78848/101520 (78%)] Loss: -1127.185181\n",
      "Train Epoch: 780 [90112/101520 (89%)] Loss: -1133.150757\n",
      "Train Epoch: 780 [101376/101520 (100%)] Loss: -1129.221558\n",
      "    epoch          : 780\n",
      "    loss           : -1130.967744606823\n",
      "    ess            : 1.968525991966976\n",
      "    log_marginal   : 1130.9949920500942\n",
      "    log_joint      : 1339.2075955951634\n",
      "    val_loss       : -1130.5191597316575\n",
      "    val_ess        : 1.9689144777215046\n",
      "    val_log_marginal: 1130.5458400560462\n",
      "    val_log_joint  : 1338.6810302734375\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch780.pth ...\n",
      "Train Epoch: 781 [0/101520 (0%)] Loss: -1129.102295\n",
      "Train Epoch: 781 [11264/101520 (11%)] Loss: -1129.287354\n",
      "Train Epoch: 781 [22528/101520 (22%)] Loss: -1131.721436\n",
      "Train Epoch: 781 [33792/101520 (33%)] Loss: -1135.249146\n",
      "Train Epoch: 781 [45056/101520 (44%)] Loss: -1134.019043\n",
      "Train Epoch: 781 [56320/101520 (55%)] Loss: -1130.635010\n",
      "Train Epoch: 781 [67584/101520 (67%)] Loss: -1137.957397\n",
      "Train Epoch: 781 [78848/101520 (78%)] Loss: -1128.856201\n",
      "Train Epoch: 781 [90112/101520 (89%)] Loss: -1129.387207\n",
      "Train Epoch: 781 [101376/101520 (100%)] Loss: -1132.638794\n",
      "    epoch          : 781\n",
      "    loss           : -1131.0209832119583\n",
      "    ess            : 1.9682665524171226\n",
      "    log_marginal   : 1131.048541045069\n",
      "    log_joint      : 1339.3195113752356\n",
      "    val_loss       : -1130.112941576087\n",
      "    val_ess        : 1.9681502425152322\n",
      "    val_log_marginal: 1130.1418350883152\n",
      "    val_log_joint  : 1338.1635105298913\n",
      "Train Epoch: 782 [0/101520 (0%)] Loss: -1135.734619\n",
      "Train Epoch: 782 [11264/101520 (11%)] Loss: -1126.128906\n",
      "Train Epoch: 782 [22528/101520 (22%)] Loss: -1138.388184\n",
      "Train Epoch: 782 [33792/101520 (33%)] Loss: -1129.474487\n",
      "Train Epoch: 782 [45056/101520 (44%)] Loss: -1128.453857\n",
      "Train Epoch: 782 [56320/101520 (55%)] Loss: -1135.291016\n",
      "Train Epoch: 782 [67584/101520 (67%)] Loss: -1131.253784\n",
      "Train Epoch: 782 [78848/101520 (78%)] Loss: -1124.664307\n",
      "Train Epoch: 782 [90112/101520 (89%)] Loss: -1132.643555\n",
      "Train Epoch: 782 [101376/101520 (100%)] Loss: -1132.677368\n",
      "    epoch          : 782\n",
      "    loss           : -1131.060521724835\n",
      "    ess            : 1.968757748004779\n",
      "    log_marginal   : 1131.0876759284704\n",
      "    log_joint      : 1339.35314143962\n",
      "    val_loss       : -1128.6074112601902\n",
      "    val_ess        : 1.965150791665782\n",
      "    val_log_marginal: 1128.6375573199728\n",
      "    val_log_joint  : 1337.115234375\n",
      "Train Epoch: 783 [0/101520 (0%)] Loss: -1126.131836\n",
      "Train Epoch: 783 [11264/101520 (11%)] Loss: -1128.873657\n",
      "Train Epoch: 783 [22528/101520 (22%)] Loss: -1131.813232\n",
      "Train Epoch: 783 [33792/101520 (33%)] Loss: -1133.112061\n",
      "Train Epoch: 783 [45056/101520 (44%)] Loss: -1144.869385\n",
      "Train Epoch: 783 [56320/101520 (55%)] Loss: -1128.651855\n",
      "Train Epoch: 783 [67584/101520 (67%)] Loss: -1133.330933\n",
      "Train Epoch: 783 [78848/101520 (78%)] Loss: -1126.749634\n",
      "Train Epoch: 783 [90112/101520 (89%)] Loss: -1124.633667\n",
      "Train Epoch: 783 [101376/101520 (100%)] Loss: -1118.267700\n",
      "    epoch          : 783\n",
      "    loss           : -1131.1306802567526\n",
      "    ess            : 1.967246909836429\n",
      "    log_marginal   : 1131.1600016684988\n",
      "    log_joint      : 1339.5037964480607\n",
      "    val_loss       : -1131.1141888162365\n",
      "    val_ess        : 1.9701294069704802\n",
      "    val_log_marginal: 1131.1419306216033\n",
      "    val_log_joint  : 1339.5388608186142\n",
      "Train Epoch: 784 [0/101520 (0%)] Loss: -1136.267090\n",
      "Train Epoch: 784 [11264/101520 (11%)] Loss: -1126.167725\n",
      "Train Epoch: 784 [22528/101520 (22%)] Loss: -1130.854492\n",
      "Train Epoch: 784 [33792/101520 (33%)] Loss: -1129.245239\n",
      "Train Epoch: 784 [45056/101520 (44%)] Loss: -1129.693604\n",
      "Train Epoch: 784 [56320/101520 (55%)] Loss: -1124.427002\n",
      "Train Epoch: 784 [67584/101520 (67%)] Loss: -1129.752319\n",
      "Train Epoch: 784 [78848/101520 (78%)] Loss: -1129.632812\n",
      "Train Epoch: 784 [90112/101520 (89%)] Loss: -1131.511719\n",
      "Train Epoch: 784 [101376/101520 (100%)] Loss: -1144.614380\n",
      "    epoch          : 784\n",
      "    loss           : -1131.34983756674\n",
      "    ess            : 1.9686017371901317\n",
      "    log_marginal   : 1131.377266581933\n",
      "    log_joint      : 1339.6277395277168\n",
      "    val_loss       : -1130.6069601307745\n",
      "    val_ess        : 1.9682828136112378\n",
      "    val_log_marginal: 1130.6345904806385\n",
      "    val_log_joint  : 1338.9316512398098\n",
      "Train Epoch: 785 [0/101520 (0%)] Loss: -1125.738037\n",
      "Train Epoch: 785 [11264/101520 (11%)] Loss: -1130.436035\n",
      "Train Epoch: 785 [22528/101520 (22%)] Loss: -1126.119629\n",
      "Train Epoch: 785 [33792/101520 (33%)] Loss: -1131.645142\n",
      "Train Epoch: 785 [45056/101520 (44%)] Loss: -1125.331055\n",
      "Train Epoch: 785 [56320/101520 (55%)] Loss: -1133.264404\n",
      "Train Epoch: 785 [67584/101520 (67%)] Loss: -1139.008179\n",
      "Train Epoch: 785 [78848/101520 (78%)] Loss: -1134.406860\n",
      "Train Epoch: 785 [90112/101520 (89%)] Loss: -1139.415771\n",
      "Train Epoch: 785 [101376/101520 (100%)] Loss: -1116.560425\n",
      "    epoch          : 785\n",
      "    loss           : -1131.2276200337626\n",
      "    ess            : 1.9683222872528\n",
      "    log_marginal   : 1131.2557268765704\n",
      "    log_joint      : 1339.5803308534862\n",
      "    val_loss       : -1130.2106615149457\n",
      "    val_ess        : 1.9708616629890774\n",
      "    val_log_marginal: 1130.2364183508832\n",
      "    val_log_joint  : 1338.4766792629075\n",
      "Train Epoch: 786 [0/101520 (0%)] Loss: -1132.032349\n",
      "Train Epoch: 786 [11264/101520 (11%)] Loss: -1136.517578\n",
      "Train Epoch: 786 [22528/101520 (22%)] Loss: -1128.927490\n",
      "Train Epoch: 786 [33792/101520 (33%)] Loss: -1131.651367\n",
      "Train Epoch: 786 [45056/101520 (44%)] Loss: -1130.443237\n",
      "Train Epoch: 786 [56320/101520 (55%)] Loss: -1137.867676\n",
      "Train Epoch: 786 [67584/101520 (67%)] Loss: -1128.838989\n",
      "Train Epoch: 786 [78848/101520 (78%)] Loss: -1131.637573\n",
      "Train Epoch: 786 [90112/101520 (89%)] Loss: -1128.256592\n",
      "Train Epoch: 786 [101376/101520 (100%)] Loss: -1129.050537\n",
      "    epoch          : 786\n",
      "    loss           : -1131.546062220281\n",
      "    ess            : 1.9680846258623517\n",
      "    log_marginal   : 1131.574994111181\n",
      "    log_joint      : 1339.8479175663474\n",
      "    val_loss       : -1130.4703050696332\n",
      "    val_ess        : 1.9715647023657095\n",
      "    val_log_marginal: 1130.4971923828125\n",
      "    val_log_joint  : 1338.7130286175272\n",
      "Train Epoch: 787 [0/101520 (0%)] Loss: -1138.050537\n",
      "Train Epoch: 787 [11264/101520 (11%)] Loss: -1137.024292\n",
      "Train Epoch: 787 [22528/101520 (22%)] Loss: -1125.835938\n",
      "Train Epoch: 787 [33792/101520 (33%)] Loss: -1134.705444\n",
      "Train Epoch: 787 [45056/101520 (44%)] Loss: -1128.780640\n",
      "Train Epoch: 787 [56320/101520 (55%)] Loss: -1132.106201\n",
      "Train Epoch: 787 [67584/101520 (67%)] Loss: -1133.273193\n",
      "Train Epoch: 787 [78848/101520 (78%)] Loss: -1130.696777\n",
      "Train Epoch: 787 [90112/101520 (89%)] Loss: -1138.078857\n",
      "Train Epoch: 787 [101376/101520 (100%)] Loss: -1135.074463\n",
      "    epoch          : 787\n",
      "    loss           : -1131.6267648054727\n",
      "    ess            : 1.9696175327253103\n",
      "    log_marginal   : 1131.6531080696452\n",
      "    log_joint      : 1339.9347942103093\n",
      "    val_loss       : -1132.2992898692255\n",
      "    val_ess        : 1.9690541650937952\n",
      "    val_log_marginal: 1132.3245796535325\n",
      "    val_log_joint  : 1340.4835841966712\n",
      "Train Epoch: 788 [0/101520 (0%)] Loss: -1137.275757\n",
      "Train Epoch: 788 [11264/101520 (11%)] Loss: -1132.345215\n",
      "Train Epoch: 788 [22528/101520 (22%)] Loss: -1130.703735\n",
      "Train Epoch: 788 [33792/101520 (33%)] Loss: -1131.933838\n",
      "Train Epoch: 788 [45056/101520 (44%)] Loss: -1134.275879\n",
      "Train Epoch: 788 [56320/101520 (55%)] Loss: -1135.228760\n",
      "Train Epoch: 788 [67584/101520 (67%)] Loss: -1136.326538\n",
      "Train Epoch: 788 [78848/101520 (78%)] Loss: -1124.328857\n",
      "Train Epoch: 788 [90112/101520 (89%)] Loss: -1143.018555\n",
      "Train Epoch: 788 [101376/101520 (100%)] Loss: -1138.060303\n",
      "    epoch          : 788\n",
      "    loss           : -1131.718785578282\n",
      "    ess            : 1.968583686866952\n",
      "    log_marginal   : 1131.7468592964824\n",
      "    log_joint      : 1339.9800362898477\n",
      "    val_loss       : -1129.773389733356\n",
      "    val_ess        : 1.9666502683059028\n",
      "    val_log_marginal: 1129.802633534307\n",
      "    val_log_joint  : 1338.1826012652853\n",
      "Train Epoch: 789 [0/101520 (0%)] Loss: -1132.404297\n",
      "Train Epoch: 789 [11264/101520 (11%)] Loss: -1131.337158\n",
      "Train Epoch: 789 [22528/101520 (22%)] Loss: -1130.816162\n",
      "Train Epoch: 789 [33792/101520 (33%)] Loss: -1129.007568\n",
      "Train Epoch: 789 [45056/101520 (44%)] Loss: -1129.349365\n",
      "Train Epoch: 789 [56320/101520 (55%)] Loss: -1126.179443\n",
      "Train Epoch: 789 [67584/101520 (67%)] Loss: -1130.546631\n",
      "Train Epoch: 789 [78848/101520 (78%)] Loss: -1132.235107\n",
      "Train Epoch: 789 [90112/101520 (89%)] Loss: -1130.362549\n",
      "Train Epoch: 789 [101376/101520 (100%)] Loss: -1135.832275\n",
      "    epoch          : 789\n",
      "    loss           : -1131.8885406034076\n",
      "    ess            : 1.9693899172634335\n",
      "    log_marginal   : 1131.9151537717887\n",
      "    log_joint      : 1340.1914590040044\n",
      "    val_loss       : -1131.1513034986413\n",
      "    val_ess        : 1.9677150353141453\n",
      "    val_log_marginal: 1131.1779944378397\n",
      "    val_log_joint  : 1339.5469970703125\n",
      "Train Epoch: 790 [0/101520 (0%)] Loss: -1131.681641\n",
      "Train Epoch: 790 [11264/101520 (11%)] Loss: -1135.398438\n",
      "Train Epoch: 790 [22528/101520 (22%)] Loss: -1132.637329\n",
      "Train Epoch: 790 [33792/101520 (33%)] Loss: -1128.433105\n",
      "Train Epoch: 790 [45056/101520 (44%)] Loss: -1136.163696\n",
      "Train Epoch: 790 [56320/101520 (55%)] Loss: -1128.911621\n",
      "Train Epoch: 790 [67584/101520 (67%)] Loss: -1132.989990\n",
      "Train Epoch: 790 [78848/101520 (78%)] Loss: -1135.488525\n",
      "Train Epoch: 790 [90112/101520 (89%)] Loss: -1129.537354\n",
      "Train Epoch: 790 [101376/101520 (100%)] Loss: -1122.657349\n",
      "    epoch          : 790\n",
      "    loss           : -1131.925547537492\n",
      "    ess            : 1.9686459991800127\n",
      "    log_marginal   : 1131.9529961820822\n",
      "    log_joint      : 1340.1541018078674\n",
      "    val_loss       : -1131.066109035326\n",
      "    val_ess        : 1.9666031132573667\n",
      "    val_log_marginal: 1131.09789508322\n",
      "    val_log_joint  : 1339.1915389351223\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch790.pth ...\n",
      "Train Epoch: 791 [0/101520 (0%)] Loss: -1136.117432\n",
      "Train Epoch: 791 [11264/101520 (11%)] Loss: -1127.687500\n",
      "Train Epoch: 791 [22528/101520 (22%)] Loss: -1130.068115\n",
      "Train Epoch: 791 [33792/101520 (33%)] Loss: -1130.203003\n",
      "Train Epoch: 791 [45056/101520 (44%)] Loss: -1129.928467\n",
      "Train Epoch: 791 [56320/101520 (55%)] Loss: -1132.860107\n",
      "Train Epoch: 791 [67584/101520 (67%)] Loss: -1132.066040\n",
      "Train Epoch: 791 [78848/101520 (78%)] Loss: -1136.216919\n",
      "Train Epoch: 791 [90112/101520 (89%)] Loss: -1127.420410\n",
      "Train Epoch: 791 [101376/101520 (100%)] Loss: -1143.996094\n",
      "    epoch          : 791\n",
      "    loss           : -1131.9903202536118\n",
      "    ess            : 1.9684849156806217\n",
      "    log_marginal   : 1132.018499479821\n",
      "    log_joint      : 1340.3297382910646\n",
      "    val_loss       : -1130.350294030231\n",
      "    val_ess        : 1.9673828197562175\n",
      "    val_log_marginal: 1130.3782905910325\n",
      "    val_log_joint  : 1338.88696819803\n",
      "Train Epoch: 792 [0/101520 (0%)] Loss: -1132.994141\n",
      "Train Epoch: 792 [11264/101520 (11%)] Loss: -1136.286499\n",
      "Train Epoch: 792 [22528/101520 (22%)] Loss: -1137.171875\n",
      "Train Epoch: 792 [33792/101520 (33%)] Loss: -1130.282227\n",
      "Train Epoch: 792 [45056/101520 (44%)] Loss: -1124.613037\n",
      "Train Epoch: 792 [56320/101520 (55%)] Loss: -1127.603027\n",
      "Train Epoch: 792 [67584/101520 (67%)] Loss: -1133.919922\n",
      "Train Epoch: 792 [78848/101520 (78%)] Loss: -1126.128906\n",
      "Train Epoch: 792 [90112/101520 (89%)] Loss: -1132.923340\n",
      "Train Epoch: 792 [101376/101520 (100%)] Loss: -1130.914307\n",
      "    epoch          : 792\n",
      "    loss           : -1132.1516432258952\n",
      "    ess            : 1.9682180743720663\n",
      "    log_marginal   : 1132.1806573148947\n",
      "    log_joint      : 1340.509173675997\n",
      "    val_loss       : -1129.5370775305707\n",
      "    val_ess        : 1.9667638281117314\n",
      "    val_log_marginal: 1129.564840565557\n",
      "    val_log_joint  : 1337.8456447435462\n",
      "Train Epoch: 793 [0/101520 (0%)] Loss: -1137.192749\n",
      "Train Epoch: 793 [11264/101520 (11%)] Loss: -1131.536133\n",
      "Train Epoch: 793 [22528/101520 (22%)] Loss: -1132.734619\n",
      "Train Epoch: 793 [33792/101520 (33%)] Loss: -1139.433472\n",
      "Train Epoch: 793 [45056/101520 (44%)] Loss: -1136.539307\n",
      "Train Epoch: 793 [56320/101520 (55%)] Loss: -1135.279297\n",
      "Train Epoch: 793 [67584/101520 (67%)] Loss: -1127.162109\n",
      "Train Epoch: 793 [78848/101520 (78%)] Loss: -1132.657715\n",
      "Train Epoch: 793 [90112/101520 (89%)] Loss: -1136.564209\n",
      "Train Epoch: 793 [101376/101520 (100%)] Loss: -1132.467407\n",
      "    epoch          : 793\n",
      "    loss           : -1132.218577015939\n",
      "    ess            : 1.9683305348583202\n",
      "    log_marginal   : 1132.2460612388113\n",
      "    log_joint      : 1340.4846860032585\n",
      "    val_loss       : -1131.2043032438858\n",
      "    val_ess        : 1.9687925007032312\n",
      "    val_log_marginal: 1131.230367909307\n",
      "    val_log_joint  : 1339.7174603006115\n",
      "Train Epoch: 794 [0/101520 (0%)] Loss: -1134.223877\n",
      "Train Epoch: 794 [11264/101520 (11%)] Loss: -1133.854614\n",
      "Train Epoch: 794 [22528/101520 (22%)] Loss: -1121.674072\n",
      "Train Epoch: 794 [33792/101520 (33%)] Loss: -1132.826660\n",
      "Train Epoch: 794 [45056/101520 (44%)] Loss: -1125.276123\n",
      "Train Epoch: 794 [56320/101520 (55%)] Loss: -1126.176636\n",
      "Train Epoch: 794 [67584/101520 (67%)] Loss: -1135.080078\n",
      "Train Epoch: 794 [78848/101520 (78%)] Loss: -1129.149170\n",
      "Train Epoch: 794 [90112/101520 (89%)] Loss: -1131.520752\n",
      "Train Epoch: 794 [101376/101520 (100%)] Loss: -1116.699341\n",
      "    epoch          : 794\n",
      "    loss           : -1132.2900844554806\n",
      "    ess            : 1.9675943575911785\n",
      "    log_marginal   : 1132.3189040907664\n",
      "    log_joint      : 1340.5535048288316\n",
      "    val_loss       : -1133.2102156929348\n",
      "    val_ess        : 1.9689966025559797\n",
      "    val_log_marginal: 1133.2392153532608\n",
      "    val_log_joint  : 1341.189161217731\n",
      "Train Epoch: 795 [0/101520 (0%)] Loss: -1135.823730\n",
      "Train Epoch: 795 [11264/101520 (11%)] Loss: -1126.609253\n",
      "Train Epoch: 795 [22528/101520 (22%)] Loss: -1138.963135\n",
      "Train Epoch: 795 [33792/101520 (33%)] Loss: -1133.782227\n",
      "Train Epoch: 795 [45056/101520 (44%)] Loss: -1134.098267\n",
      "Train Epoch: 795 [56320/101520 (55%)] Loss: -1141.058350\n",
      "Train Epoch: 795 [67584/101520 (67%)] Loss: -1128.380371\n",
      "Train Epoch: 795 [78848/101520 (78%)] Loss: -1137.777588\n",
      "Train Epoch: 795 [90112/101520 (89%)] Loss: -1130.585693\n",
      "Train Epoch: 795 [101376/101520 (100%)] Loss: -1136.395630\n",
      "    epoch          : 795\n",
      "    loss           : -1132.4928248419833\n",
      "    ess            : 1.9684160510499273\n",
      "    log_marginal   : 1132.520212144708\n",
      "    log_joint      : 1340.7972958051978\n",
      "    val_loss       : -1133.2851987092392\n",
      "    val_ess        : 1.969430016434711\n",
      "    val_log_marginal: 1133.3109608525815\n",
      "    val_log_joint  : 1341.304390285326\n",
      "Train Epoch: 796 [0/101520 (0%)] Loss: -1130.239624\n",
      "Train Epoch: 796 [11264/101520 (11%)] Loss: -1129.993164\n",
      "Train Epoch: 796 [22528/101520 (22%)] Loss: -1135.051270\n",
      "Train Epoch: 796 [33792/101520 (33%)] Loss: -1130.691406\n",
      "Train Epoch: 796 [45056/101520 (44%)] Loss: -1138.703369\n",
      "Train Epoch: 796 [56320/101520 (55%)] Loss: -1137.145264\n",
      "Train Epoch: 796 [67584/101520 (67%)] Loss: -1134.371826\n",
      "Train Epoch: 796 [78848/101520 (78%)] Loss: -1132.759521\n",
      "Train Epoch: 796 [90112/101520 (89%)] Loss: -1139.459961\n",
      "Train Epoch: 796 [101376/101520 (100%)] Loss: -1122.341431\n",
      "    epoch          : 796\n",
      "    loss           : -1132.4584838253768\n",
      "    ess            : 1.968247310600089\n",
      "    log_marginal   : 1132.487119435066\n",
      "    log_joint      : 1340.6951689600346\n",
      "    val_loss       : -1131.7092019786005\n",
      "    val_ess        : 1.9699354223583057\n",
      "    val_log_marginal: 1131.7376231317935\n",
      "    val_log_joint  : 1339.7759213654892\n",
      "Train Epoch: 797 [0/101520 (0%)] Loss: -1131.750977\n",
      "Train Epoch: 797 [11264/101520 (11%)] Loss: -1130.953125\n",
      "Train Epoch: 797 [22528/101520 (22%)] Loss: -1135.853516\n",
      "Train Epoch: 797 [33792/101520 (33%)] Loss: -1127.632812\n",
      "Train Epoch: 797 [45056/101520 (44%)] Loss: -1133.053101\n",
      "Train Epoch: 797 [56320/101520 (55%)] Loss: -1130.443115\n",
      "Train Epoch: 797 [67584/101520 (67%)] Loss: -1132.698975\n",
      "Train Epoch: 797 [78848/101520 (78%)] Loss: -1130.522827\n",
      "Train Epoch: 797 [90112/101520 (89%)] Loss: -1140.754395\n",
      "Train Epoch: 797 [101376/101520 (100%)] Loss: -1125.050659\n",
      "    epoch          : 797\n",
      "    loss           : -1132.6396515045933\n",
      "    ess            : 1.9682132796426515\n",
      "    log_marginal   : 1132.6677454656092\n",
      "    log_joint      : 1340.8630340422817\n",
      "    val_loss       : -1130.5756092900815\n",
      "    val_ess        : 1.9698971458103345\n",
      "    val_log_marginal: 1130.5998641304348\n",
      "    val_log_joint  : 1339.0455906080163\n",
      "Train Epoch: 798 [0/101520 (0%)] Loss: -1136.856934\n",
      "Train Epoch: 798 [11264/101520 (11%)] Loss: -1130.001099\n",
      "Train Epoch: 798 [22528/101520 (22%)] Loss: -1131.047852\n",
      "Train Epoch: 798 [33792/101520 (33%)] Loss: -1130.897949\n",
      "Train Epoch: 798 [45056/101520 (44%)] Loss: -1130.823975\n",
      "Train Epoch: 798 [56320/101520 (55%)] Loss: -1135.357910\n",
      "Train Epoch: 798 [67584/101520 (67%)] Loss: -1132.768555\n",
      "Train Epoch: 798 [78848/101520 (78%)] Loss: -1127.366455\n",
      "Train Epoch: 798 [90112/101520 (89%)] Loss: -1143.690796\n",
      "Train Epoch: 798 [101376/101520 (100%)] Loss: -1132.483276\n",
      "    epoch          : 798\n",
      "    loss           : -1132.6703948207837\n",
      "    ess            : 1.9685035932004151\n",
      "    log_marginal   : 1132.6989770630496\n",
      "    log_joint      : 1340.9692658850895\n",
      "    val_loss       : -1131.545065174932\n",
      "    val_ess        : 1.9683137924774834\n",
      "    val_log_marginal: 1131.5728441321332\n",
      "    val_log_joint  : 1339.7527067764945\n",
      "Train Epoch: 799 [0/101520 (0%)] Loss: -1130.531494\n",
      "Train Epoch: 799 [11264/101520 (11%)] Loss: -1138.250244\n",
      "Train Epoch: 799 [22528/101520 (22%)] Loss: -1138.954224\n",
      "Train Epoch: 799 [33792/101520 (33%)] Loss: -1134.228027\n",
      "Train Epoch: 799 [45056/101520 (44%)] Loss: -1134.366211\n",
      "Train Epoch: 799 [56320/101520 (55%)] Loss: -1127.376221\n",
      "Train Epoch: 799 [67584/101520 (67%)] Loss: -1130.856689\n",
      "Train Epoch: 799 [78848/101520 (78%)] Loss: -1133.169678\n",
      "Train Epoch: 799 [90112/101520 (89%)] Loss: -1129.419312\n",
      "Train Epoch: 799 [101376/101520 (100%)] Loss: -1138.434937\n",
      "    epoch          : 799\n",
      "    loss           : -1132.6806168292635\n",
      "    ess            : 1.9684834342506063\n",
      "    log_marginal   : 1132.7083507135285\n",
      "    log_joint      : 1341.0452433063756\n",
      "    val_loss       : -1131.861280358356\n",
      "    val_ess        : 1.9708627825197966\n",
      "    val_log_marginal: 1131.8858058763587\n",
      "    val_log_joint  : 1340.1133980129075\n",
      "Train Epoch: 800 [0/101520 (0%)] Loss: -1130.573853\n",
      "Train Epoch: 800 [11264/101520 (11%)] Loss: -1132.044922\n",
      "Train Epoch: 800 [22528/101520 (22%)] Loss: -1131.121582\n",
      "Train Epoch: 800 [33792/101520 (33%)] Loss: -1136.433350\n",
      "Train Epoch: 800 [45056/101520 (44%)] Loss: -1134.649414\n",
      "Train Epoch: 800 [56320/101520 (55%)] Loss: -1134.065796\n",
      "Train Epoch: 800 [67584/101520 (67%)] Loss: -1134.894043\n",
      "Train Epoch: 800 [78848/101520 (78%)] Loss: -1136.097778\n",
      "Train Epoch: 800 [90112/101520 (89%)] Loss: -1125.632812\n",
      "Train Epoch: 800 [101376/101520 (100%)] Loss: -1140.165771\n",
      "    epoch          : 800\n",
      "    loss           : -1132.841438638505\n",
      "    ess            : 1.9683088136078724\n",
      "    log_marginal   : 1132.8694583758636\n",
      "    log_joint      : 1341.1985335613615\n",
      "    val_loss       : -1132.5604248046875\n",
      "    val_ess        : 1.9697816786558733\n",
      "    val_log_marginal: 1132.5853218410325\n",
      "    val_log_joint  : 1340.7173223080842\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [0/101520 (0%)] Loss: -1130.238037\n",
      "Train Epoch: 801 [11264/101520 (11%)] Loss: -1137.202637\n",
      "Train Epoch: 801 [22528/101520 (22%)] Loss: -1132.984619\n",
      "Train Epoch: 801 [33792/101520 (33%)] Loss: -1133.193237\n",
      "Train Epoch: 801 [45056/101520 (44%)] Loss: -1130.125488\n",
      "Train Epoch: 801 [56320/101520 (55%)] Loss: -1123.817749\n",
      "Train Epoch: 801 [67584/101520 (67%)] Loss: -1130.631348\n",
      "Train Epoch: 801 [78848/101520 (78%)] Loss: -1138.632324\n",
      "Train Epoch: 801 [90112/101520 (89%)] Loss: -1126.938232\n",
      "Train Epoch: 801 [101376/101520 (100%)] Loss: -1121.134644\n",
      "    epoch          : 801\n",
      "    loss           : -1132.884958238458\n",
      "    ess            : 1.9684373757347988\n",
      "    log_marginal   : 1132.9130583336605\n",
      "    log_joint      : 1341.164837861181\n",
      "    val_loss       : -1130.186916185462\n",
      "    val_ess        : 1.9657663573389468\n",
      "    val_log_marginal: 1130.2150082795517\n",
      "    val_log_joint  : 1338.4065153702445\n",
      "Train Epoch: 802 [0/101520 (0%)] Loss: -1132.249634\n",
      "Train Epoch: 802 [11264/101520 (11%)] Loss: -1133.495361\n",
      "Train Epoch: 802 [22528/101520 (22%)] Loss: -1128.700195\n",
      "Train Epoch: 802 [33792/101520 (33%)] Loss: -1130.718628\n",
      "Train Epoch: 802 [45056/101520 (44%)] Loss: -1132.564087\n",
      "Train Epoch: 802 [56320/101520 (55%)] Loss: -1127.989502\n",
      "Train Epoch: 802 [67584/101520 (67%)] Loss: -1128.291870\n",
      "Train Epoch: 802 [78848/101520 (78%)] Loss: -1133.800049\n",
      "Train Epoch: 802 [90112/101520 (89%)] Loss: -1133.338989\n",
      "Train Epoch: 802 [101376/101520 (100%)] Loss: -1129.953613\n",
      "    epoch          : 802\n",
      "    loss           : -1132.933615219653\n",
      "    ess            : 1.9684674428335984\n",
      "    log_marginal   : 1132.9613055511934\n",
      "    log_joint      : 1341.2605575484847\n",
      "    val_loss       : -1131.7870456861413\n",
      "    val_ess        : 1.9694954312365989\n",
      "    val_log_marginal: 1131.813184655231\n",
      "    val_log_joint  : 1340.2763937245245\n",
      "Train Epoch: 803 [0/101520 (0%)] Loss: -1137.812012\n",
      "Train Epoch: 803 [11264/101520 (11%)] Loss: -1137.771484\n",
      "Train Epoch: 803 [22528/101520 (22%)] Loss: -1129.147705\n",
      "Train Epoch: 803 [33792/101520 (33%)] Loss: -1132.540649\n",
      "Train Epoch: 803 [45056/101520 (44%)] Loss: -1132.877197\n",
      "Train Epoch: 803 [56320/101520 (55%)] Loss: -1127.999268\n",
      "Train Epoch: 803 [67584/101520 (67%)] Loss: -1124.596313\n",
      "Train Epoch: 803 [78848/101520 (78%)] Loss: -1135.326050\n",
      "Train Epoch: 803 [90112/101520 (89%)] Loss: -1129.085205\n",
      "Train Epoch: 803 [101376/101520 (100%)] Loss: -1133.006104\n",
      "    epoch          : 803\n",
      "    loss           : -1133.1173218386857\n",
      "    ess            : 1.967452536875279\n",
      "    log_marginal   : 1133.1462034292556\n",
      "    log_joint      : 1341.4833193064935\n",
      "    val_loss       : -1131.5067032523777\n",
      "    val_ess        : 1.9670870355937793\n",
      "    val_log_marginal: 1131.534620202106\n",
      "    val_log_joint  : 1339.939994480299\n",
      "Train Epoch: 804 [0/101520 (0%)] Loss: -1130.553467\n",
      "Train Epoch: 804 [11264/101520 (11%)] Loss: -1138.888184\n",
      "Train Epoch: 804 [22528/101520 (22%)] Loss: -1128.958740\n",
      "Train Epoch: 804 [33792/101520 (33%)] Loss: -1134.746582\n",
      "Train Epoch: 804 [45056/101520 (44%)] Loss: -1131.900146\n",
      "Train Epoch: 804 [56320/101520 (55%)] Loss: -1133.612671\n",
      "Train Epoch: 804 [67584/101520 (67%)] Loss: -1130.971680\n",
      "Train Epoch: 804 [78848/101520 (78%)] Loss: -1136.583374\n",
      "Train Epoch: 804 [90112/101520 (89%)] Loss: -1132.563354\n",
      "Train Epoch: 804 [101376/101520 (100%)] Loss: -1133.123657\n",
      "    epoch          : 804\n",
      "    loss           : -1133.3054389378533\n",
      "    ess            : 1.9683573335858446\n",
      "    log_marginal   : 1133.3333869052292\n",
      "    log_joint      : 1341.6230082296247\n",
      "    val_loss       : -1131.5637366253397\n",
      "    val_ess        : 1.9626216370126475\n",
      "    val_log_marginal: 1131.5990786345108\n",
      "    val_log_joint  : 1339.8094641644022\n",
      "Train Epoch: 805 [0/101520 (0%)] Loss: -1131.483887\n",
      "Train Epoch: 805 [11264/101520 (11%)] Loss: -1132.978271\n",
      "Train Epoch: 805 [22528/101520 (22%)] Loss: -1139.879761\n",
      "Train Epoch: 805 [33792/101520 (33%)] Loss: -1128.327881\n",
      "Train Epoch: 805 [45056/101520 (44%)] Loss: -1129.663086\n",
      "Train Epoch: 805 [56320/101520 (55%)] Loss: -1136.613892\n",
      "Train Epoch: 805 [67584/101520 (67%)] Loss: -1133.821045\n",
      "Train Epoch: 805 [78848/101520 (78%)] Loss: -1132.024170\n",
      "Train Epoch: 805 [90112/101520 (89%)] Loss: -1138.550415\n",
      "Train Epoch: 805 [101376/101520 (100%)] Loss: -1130.210571\n",
      "    epoch          : 805\n",
      "    loss           : -1133.3855060213175\n",
      "    ess            : 1.9678345720971648\n",
      "    log_marginal   : 1133.4137570175094\n",
      "    log_joint      : 1341.6612837134894\n",
      "    val_loss       : -1131.4497813349185\n",
      "    val_ess        : 1.9678346333296404\n",
      "    val_log_marginal: 1131.476705799932\n",
      "    val_log_joint  : 1339.9521696671195\n",
      "Train Epoch: 806 [0/101520 (0%)] Loss: -1129.767578\n",
      "Train Epoch: 806 [11264/101520 (11%)] Loss: -1132.189209\n",
      "Train Epoch: 806 [22528/101520 (22%)] Loss: -1132.302246\n",
      "Train Epoch: 806 [33792/101520 (33%)] Loss: -1137.838501\n",
      "Train Epoch: 806 [45056/101520 (44%)] Loss: -1133.397949\n",
      "Train Epoch: 806 [56320/101520 (55%)] Loss: -1129.050537\n",
      "Train Epoch: 806 [67584/101520 (67%)] Loss: -1137.672852\n",
      "Train Epoch: 806 [78848/101520 (78%)] Loss: -1140.658569\n",
      "Train Epoch: 806 [90112/101520 (89%)] Loss: -1131.336548\n",
      "Train Epoch: 806 [101376/101520 (100%)] Loss: -1144.372192\n",
      "    epoch          : 806\n",
      "    loss           : -1133.5408831265704\n",
      "    ess            : 1.9683622397370075\n",
      "    log_marginal   : 1133.5692819566582\n",
      "    log_joint      : 1341.8543707306062\n",
      "    val_loss       : -1132.8654679008152\n",
      "    val_ess        : 1.9687981346379155\n",
      "    val_log_marginal: 1132.8925038213315\n",
      "    val_log_joint  : 1340.884914232337\n",
      "Train Epoch: 807 [0/101520 (0%)] Loss: -1128.607544\n",
      "Train Epoch: 807 [11264/101520 (11%)] Loss: -1139.875122\n",
      "Train Epoch: 807 [22528/101520 (22%)] Loss: -1127.647827\n",
      "Train Epoch: 807 [33792/101520 (33%)] Loss: -1134.575195\n",
      "Train Epoch: 807 [45056/101520 (44%)] Loss: -1129.670288\n",
      "Train Epoch: 807 [56320/101520 (55%)] Loss: -1145.699463\n",
      "Train Epoch: 807 [67584/101520 (67%)] Loss: -1126.781982\n",
      "Train Epoch: 807 [78848/101520 (78%)] Loss: -1137.469116\n",
      "Train Epoch: 807 [90112/101520 (89%)] Loss: -1130.281738\n",
      "Train Epoch: 807 [101376/101520 (100%)] Loss: -1125.532471\n",
      "    epoch          : 807\n",
      "    loss           : -1133.6408525783213\n",
      "    ess            : 1.9684160228949696\n",
      "    log_marginal   : 1133.6688950121702\n",
      "    log_joint      : 1341.9653921462782\n",
      "    val_loss       : -1132.2459186056385\n",
      "    val_ess        : 1.9674158199973728\n",
      "    val_log_marginal: 1132.27416461447\n",
      "    val_log_joint  : 1340.3828761888587\n",
      "Train Epoch: 808 [0/101520 (0%)] Loss: -1140.005249\n",
      "Train Epoch: 808 [11264/101520 (11%)] Loss: -1139.469727\n",
      "Train Epoch: 808 [22528/101520 (22%)] Loss: -1127.141235\n",
      "Train Epoch: 808 [33792/101520 (33%)] Loss: -1132.068481\n",
      "Train Epoch: 808 [45056/101520 (44%)] Loss: -1128.526123\n",
      "Train Epoch: 808 [56320/101520 (55%)] Loss: -1124.808716\n",
      "Train Epoch: 808 [67584/101520 (67%)] Loss: -1136.079102\n",
      "Train Epoch: 808 [78848/101520 (78%)] Loss: -1134.791260\n",
      "Train Epoch: 808 [90112/101520 (89%)] Loss: -1136.772705\n",
      "Train Epoch: 808 [101376/101520 (100%)] Loss: -1140.811890\n",
      "    epoch          : 808\n",
      "    loss           : -1133.7044082718278\n",
      "    ess            : 1.9683519709649397\n",
      "    log_marginal   : 1133.7319415681925\n",
      "    log_joint      : 1342.0309224344378\n",
      "    val_loss       : -1131.904052734375\n",
      "    val_ess        : 1.968842449395553\n",
      "    val_log_marginal: 1131.9327551800272\n",
      "    val_log_joint  : 1340.034572435462\n",
      "Train Epoch: 809 [0/101520 (0%)] Loss: -1137.453125\n",
      "Train Epoch: 809 [11264/101520 (11%)] Loss: -1129.250366\n",
      "Train Epoch: 809 [22528/101520 (22%)] Loss: -1142.585693\n",
      "Train Epoch: 809 [33792/101520 (33%)] Loss: -1135.963013\n",
      "Train Epoch: 809 [45056/101520 (44%)] Loss: -1131.499268\n",
      "Train Epoch: 809 [56320/101520 (55%)] Loss: -1132.745605\n",
      "Train Epoch: 809 [67584/101520 (67%)] Loss: -1139.014648\n",
      "Train Epoch: 809 [78848/101520 (78%)] Loss: -1132.929077\n",
      "Train Epoch: 809 [90112/101520 (89%)] Loss: -1125.180420\n",
      "Train Epoch: 809 [101376/101520 (100%)] Loss: -1137.046021\n",
      "    epoch          : 809\n",
      "    loss           : -1133.7968148849718\n",
      "    ess            : 1.9681049046204917\n",
      "    log_marginal   : 1133.8253327182788\n",
      "    log_joint      : 1342.124732549466\n",
      "    val_loss       : -1132.6632504670517\n",
      "    val_ess        : 1.9684991888377978\n",
      "    val_log_marginal: 1132.6925685716712\n",
      "    val_log_joint  : 1340.9985563858695\n",
      "Train Epoch: 810 [0/101520 (0%)] Loss: -1134.049683\n",
      "Train Epoch: 810 [11264/101520 (11%)] Loss: -1134.750122\n",
      "Train Epoch: 810 [22528/101520 (22%)] Loss: -1137.559570\n",
      "Train Epoch: 810 [33792/101520 (33%)] Loss: -1141.732788\n",
      "Train Epoch: 810 [45056/101520 (44%)] Loss: -1136.912231\n",
      "Train Epoch: 810 [56320/101520 (55%)] Loss: -1128.277832\n",
      "Train Epoch: 810 [67584/101520 (67%)] Loss: -1136.982178\n",
      "Train Epoch: 810 [78848/101520 (78%)] Loss: -1134.968628\n",
      "Train Epoch: 810 [90112/101520 (89%)] Loss: -1127.734497\n",
      "Train Epoch: 810 [101376/101520 (100%)] Loss: -1134.476929\n",
      "    epoch          : 810\n",
      "    loss           : -1133.7867738349953\n",
      "    ess            : 1.9671278916411663\n",
      "    log_marginal   : 1133.8162234512406\n",
      "    log_joint      : 1342.1888262111338\n",
      "    val_loss       : -1132.51732867697\n",
      "    val_ess        : 1.9670562795970752\n",
      "    val_log_marginal: 1132.5485627547555\n",
      "    val_log_joint  : 1340.8930929432745\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch810.pth ...\n",
      "Train Epoch: 811 [0/101520 (0%)] Loss: -1134.664062\n",
      "Train Epoch: 811 [11264/101520 (11%)] Loss: -1127.610107\n",
      "Train Epoch: 811 [22528/101520 (22%)] Loss: -1132.373413\n",
      "Train Epoch: 811 [33792/101520 (33%)] Loss: -1139.174316\n",
      "Train Epoch: 811 [45056/101520 (44%)] Loss: -1135.861084\n",
      "Train Epoch: 811 [56320/101520 (55%)] Loss: -1135.294189\n",
      "Train Epoch: 811 [67584/101520 (67%)] Loss: -1129.023315\n",
      "Train Epoch: 811 [78848/101520 (78%)] Loss: -1138.963501\n",
      "Train Epoch: 811 [90112/101520 (89%)] Loss: -1129.916504\n",
      "Train Epoch: 811 [101376/101520 (100%)] Loss: -1118.473999\n",
      "    epoch          : 811\n",
      "    loss           : -1133.9038368110082\n",
      "    ess            : 1.968428154087546\n",
      "    log_marginal   : 1133.9314743885443\n",
      "    log_joint      : 1342.265181498312\n",
      "    val_loss       : -1132.8246858016305\n",
      "    val_ess        : 1.968730532604715\n",
      "    val_log_marginal: 1132.8498800526495\n",
      "    val_log_joint  : 1340.9891888162365\n",
      "Train Epoch: 812 [0/101520 (0%)] Loss: -1125.230347\n",
      "Train Epoch: 812 [11264/101520 (11%)] Loss: -1136.790894\n",
      "Train Epoch: 812 [22528/101520 (22%)] Loss: -1132.114624\n",
      "Train Epoch: 812 [33792/101520 (33%)] Loss: -1136.053711\n",
      "Train Epoch: 812 [45056/101520 (44%)] Loss: -1132.424561\n",
      "Train Epoch: 812 [56320/101520 (55%)] Loss: -1133.505005\n",
      "Train Epoch: 812 [67584/101520 (67%)] Loss: -1136.114258\n",
      "Train Epoch: 812 [78848/101520 (78%)] Loss: -1134.126221\n",
      "Train Epoch: 812 [90112/101520 (89%)] Loss: -1135.403076\n",
      "Train Epoch: 812 [101376/101520 (100%)] Loss: -1132.145508\n",
      "    epoch          : 812\n",
      "    loss           : -1134.0533711035646\n",
      "    ess            : 1.9678082262451326\n",
      "    log_marginal   : 1134.0816631988066\n",
      "    log_joint      : 1342.400750701751\n",
      "    val_loss       : -1132.346822987432\n",
      "    val_ess        : 1.9704150790753572\n",
      "    val_log_marginal: 1132.3735139266305\n",
      "    val_log_joint  : 1340.6369788128397\n",
      "Train Epoch: 813 [0/101520 (0%)] Loss: -1134.608154\n",
      "Train Epoch: 813 [11264/101520 (11%)] Loss: -1138.250732\n",
      "Train Epoch: 813 [22528/101520 (22%)] Loss: -1133.964844\n",
      "Train Epoch: 813 [33792/101520 (33%)] Loss: -1135.759399\n",
      "Train Epoch: 813 [45056/101520 (44%)] Loss: -1126.406494\n",
      "Train Epoch: 813 [56320/101520 (55%)] Loss: -1139.664551\n",
      "Train Epoch: 813 [67584/101520 (67%)] Loss: -1138.026855\n",
      "Train Epoch: 813 [78848/101520 (78%)] Loss: -1136.894287\n",
      "Train Epoch: 813 [90112/101520 (89%)] Loss: -1140.541016\n",
      "Train Epoch: 813 [101376/101520 (100%)] Loss: -1130.441528\n",
      "    epoch          : 813\n",
      "    loss           : -1134.1170451868718\n",
      "    ess            : 1.9681543273542395\n",
      "    log_marginal   : 1134.1456016655543\n",
      "    log_joint      : 1342.46499894492\n",
      "    val_loss       : -1131.909620202106\n",
      "    val_ess        : 1.9712913502817568\n",
      "    val_log_marginal: 1131.933254076087\n",
      "    val_log_joint  : 1339.9989650560462\n",
      "Train Epoch: 814 [0/101520 (0%)] Loss: -1139.344360\n",
      "Train Epoch: 814 [11264/101520 (11%)] Loss: -1135.227295\n",
      "Train Epoch: 814 [22528/101520 (22%)] Loss: -1130.041870\n",
      "Train Epoch: 814 [33792/101520 (33%)] Loss: -1130.565796\n",
      "Train Epoch: 814 [45056/101520 (44%)] Loss: -1135.938354\n",
      "Train Epoch: 814 [56320/101520 (55%)] Loss: -1135.264893\n",
      "Train Epoch: 814 [67584/101520 (67%)] Loss: -1139.210327\n",
      "Train Epoch: 814 [78848/101520 (78%)] Loss: -1129.720703\n",
      "Train Epoch: 814 [90112/101520 (89%)] Loss: -1138.618164\n",
      "Train Epoch: 814 [101376/101520 (100%)] Loss: -1143.636963\n",
      "    epoch          : 814\n",
      "    loss           : -1134.303422017313\n",
      "    ess            : 1.96805035109496\n",
      "    log_marginal   : 1134.3313810262248\n",
      "    log_joint      : 1342.6419573453204\n",
      "    val_loss       : -1134.6454865828805\n",
      "    val_ess        : 1.9684528164241626\n",
      "    val_log_marginal: 1134.6745287024457\n",
      "    val_log_joint  : 1343.0561682659647\n",
      "Train Epoch: 815 [0/101520 (0%)] Loss: -1136.067871\n",
      "Train Epoch: 815 [11264/101520 (11%)] Loss: -1137.459473\n",
      "Train Epoch: 815 [22528/101520 (22%)] Loss: -1135.901611\n",
      "Train Epoch: 815 [33792/101520 (33%)] Loss: -1132.620117\n",
      "Train Epoch: 815 [45056/101520 (44%)] Loss: -1137.899780\n",
      "Train Epoch: 815 [56320/101520 (55%)] Loss: -1135.787720\n",
      "Train Epoch: 815 [67584/101520 (67%)] Loss: -1132.294922\n",
      "Train Epoch: 815 [78848/101520 (78%)] Loss: -1141.793213\n",
      "Train Epoch: 815 [90112/101520 (89%)] Loss: -1132.086670\n",
      "Train Epoch: 815 [101376/101520 (100%)] Loss: -1134.382446\n",
      "    epoch          : 815\n",
      "    loss           : -1134.4052225237515\n",
      "    ess            : 1.968923221880467\n",
      "    log_marginal   : 1134.43280428019\n",
      "    log_joint      : 1342.7628725904915\n",
      "    val_loss       : -1132.7203369140625\n",
      "    val_ess        : 1.9652291899142058\n",
      "    val_log_marginal: 1132.7543786090353\n",
      "    val_log_joint  : 1341.163579526155\n",
      "Train Epoch: 816 [0/101520 (0%)] Loss: -1140.197998\n",
      "Train Epoch: 816 [11264/101520 (11%)] Loss: -1131.992432\n",
      "Train Epoch: 816 [22528/101520 (22%)] Loss: -1137.339844\n",
      "Train Epoch: 816 [33792/101520 (33%)] Loss: -1134.386475\n",
      "Train Epoch: 816 [45056/101520 (44%)] Loss: -1134.541992\n",
      "Train Epoch: 816 [56320/101520 (55%)] Loss: -1135.695068\n",
      "Train Epoch: 816 [67584/101520 (67%)] Loss: -1136.091919\n",
      "Train Epoch: 816 [78848/101520 (78%)] Loss: -1130.656250\n",
      "Train Epoch: 816 [90112/101520 (89%)] Loss: -1129.112549\n",
      "Train Epoch: 816 [101376/101520 (100%)] Loss: -1128.738647\n",
      "    epoch          : 816\n",
      "    loss           : -1134.4850926998272\n",
      "    ess            : 1.968992487869071\n",
      "    log_marginal   : 1134.5122683731156\n",
      "    log_joint      : 1342.7463802165123\n",
      "    val_loss       : -1132.5642461362092\n",
      "    val_ess        : 1.9697873125905576\n",
      "    val_log_marginal: 1132.591451893682\n",
      "    val_log_joint  : 1340.8727868121603\n",
      "Train Epoch: 817 [0/101520 (0%)] Loss: -1135.348145\n",
      "Train Epoch: 817 [11264/101520 (11%)] Loss: -1128.544434\n",
      "Train Epoch: 817 [22528/101520 (22%)] Loss: -1135.674072\n",
      "Train Epoch: 817 [33792/101520 (33%)] Loss: -1132.968262\n",
      "Train Epoch: 817 [45056/101520 (44%)] Loss: -1135.249268\n",
      "Train Epoch: 817 [56320/101520 (55%)] Loss: -1131.272705\n",
      "Train Epoch: 817 [67584/101520 (67%)] Loss: -1129.618774\n",
      "Train Epoch: 817 [78848/101520 (78%)] Loss: -1136.170166\n",
      "Train Epoch: 817 [90112/101520 (89%)] Loss: -1131.840088\n",
      "Train Epoch: 817 [101376/101520 (100%)] Loss: -1134.971191\n",
      "    epoch          : 817\n",
      "    loss           : -1134.4658203125\n",
      "    ess            : 1.9690003904265974\n",
      "    log_marginal   : 1134.4931008803785\n",
      "    log_joint      : 1342.7954310124842\n",
      "    val_loss       : -1134.1094864555027\n",
      "    val_ess        : 1.9717966421790745\n",
      "    val_log_marginal: 1134.1334546959918\n",
      "    val_log_joint  : 1341.8548690132473\n",
      "Train Epoch: 818 [0/101520 (0%)] Loss: -1133.459229\n",
      "Train Epoch: 818 [11264/101520 (11%)] Loss: -1138.481445\n",
      "Train Epoch: 818 [22528/101520 (22%)] Loss: -1130.512695\n",
      "Train Epoch: 818 [33792/101520 (33%)] Loss: -1136.482056\n",
      "Train Epoch: 818 [45056/101520 (44%)] Loss: -1134.561035\n",
      "Train Epoch: 818 [56320/101520 (55%)] Loss: -1139.059448\n",
      "Train Epoch: 818 [67584/101520 (67%)] Loss: -1132.553223\n",
      "Train Epoch: 818 [78848/101520 (78%)] Loss: -1132.916748\n",
      "Train Epoch: 818 [90112/101520 (89%)] Loss: -1130.504272\n",
      "Train Epoch: 818 [101376/101520 (100%)] Loss: -1129.891113\n",
      "    epoch          : 818\n",
      "    loss           : -1134.6316598863457\n",
      "    ess            : 1.9684797441540052\n",
      "    log_marginal   : 1134.6597501668498\n",
      "    log_joint      : 1342.9376883195273\n",
      "    val_loss       : -1132.51049273947\n",
      "    val_ess        : 1.968539015106533\n",
      "    val_log_marginal: 1132.538377844769\n",
      "    val_log_joint  : 1340.8053774626358\n",
      "Train Epoch: 819 [0/101520 (0%)] Loss: -1134.391479\n",
      "Train Epoch: 819 [11264/101520 (11%)] Loss: -1133.053101\n",
      "Train Epoch: 819 [22528/101520 (22%)] Loss: -1131.161743\n",
      "Train Epoch: 819 [33792/101520 (33%)] Loss: -1136.852539\n",
      "Train Epoch: 819 [45056/101520 (44%)] Loss: -1134.640137\n",
      "Train Epoch: 819 [56320/101520 (55%)] Loss: -1129.894531\n",
      "Train Epoch: 819 [67584/101520 (67%)] Loss: -1132.569824\n",
      "Train Epoch: 819 [78848/101520 (78%)] Loss: -1134.073242\n",
      "Train Epoch: 819 [90112/101520 (89%)] Loss: -1141.504395\n",
      "Train Epoch: 819 [101376/101520 (100%)] Loss: -1111.172729\n",
      "    epoch          : 819\n",
      "    loss           : -1134.5967164926194\n",
      "    ess            : 1.9680751172741453\n",
      "    log_marginal   : 1134.6247190542556\n",
      "    log_joint      : 1342.9225080480528\n",
      "    val_loss       : -1132.3662905485733\n",
      "    val_ess        : 1.9699311826540076\n",
      "    val_log_marginal: 1132.3920208474865\n",
      "    val_log_joint  : 1340.9121199898098\n",
      "Train Epoch: 820 [0/101520 (0%)] Loss: -1140.293457\n",
      "Train Epoch: 820 [11264/101520 (11%)] Loss: -1134.811279\n",
      "Train Epoch: 820 [22528/101520 (22%)] Loss: -1134.555908\n",
      "Train Epoch: 820 [33792/101520 (33%)] Loss: -1137.863159\n",
      "Train Epoch: 820 [45056/101520 (44%)] Loss: -1139.965210\n",
      "Train Epoch: 820 [56320/101520 (55%)] Loss: -1123.146484\n",
      "Train Epoch: 820 [67584/101520 (67%)] Loss: -1138.262329\n",
      "Train Epoch: 820 [78848/101520 (78%)] Loss: -1142.240234\n",
      "Train Epoch: 820 [90112/101520 (89%)] Loss: -1133.179688\n",
      "Train Epoch: 820 [101376/101520 (100%)] Loss: -1133.485962\n",
      "    epoch          : 820\n",
      "    loss           : -1134.890152667635\n",
      "    ess            : 1.968289368116676\n",
      "    log_marginal   : 1134.9190299642744\n",
      "    log_joint      : 1343.1251539680827\n",
      "    val_loss       : -1132.3963941491168\n",
      "    val_ess        : 1.9694539515868477\n",
      "    val_log_marginal: 1132.4243588654892\n",
      "    val_log_joint  : 1340.721828294837\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch820.pth ...\n",
      "Train Epoch: 821 [0/101520 (0%)] Loss: -1138.817871\n",
      "Train Epoch: 821 [11264/101520 (11%)] Loss: -1131.037842\n",
      "Train Epoch: 821 [22528/101520 (22%)] Loss: -1137.689209\n",
      "Train Epoch: 821 [33792/101520 (33%)] Loss: -1138.858887\n",
      "Train Epoch: 821 [45056/101520 (44%)] Loss: -1130.501953\n",
      "Train Epoch: 821 [56320/101520 (55%)] Loss: -1137.157104\n",
      "Train Epoch: 821 [67584/101520 (67%)] Loss: -1128.208740\n",
      "Train Epoch: 821 [78848/101520 (78%)] Loss: -1132.750244\n",
      "Train Epoch: 821 [90112/101520 (89%)] Loss: -1135.637451\n",
      "Train Epoch: 821 [101376/101520 (100%)] Loss: -1127.829590\n",
      "    epoch          : 821\n",
      "    loss           : -1134.7572984551664\n",
      "    ess            : 1.966951578705754\n",
      "    log_marginal   : 1134.7866578988694\n",
      "    log_joint      : 1343.1688588204695\n",
      "    val_loss       : -1131.8634617017663\n",
      "    val_ess        : 1.9679746886958247\n",
      "    val_log_marginal: 1131.890524159307\n",
      "    val_log_joint  : 1339.9197573454483\n",
      "Train Epoch: 822 [0/101520 (0%)] Loss: -1135.022705\n",
      "Train Epoch: 822 [11264/101520 (11%)] Loss: -1139.169189\n",
      "Train Epoch: 822 [22528/101520 (22%)] Loss: -1137.262451\n",
      "Train Epoch: 822 [33792/101520 (33%)] Loss: -1134.114502\n",
      "Train Epoch: 822 [45056/101520 (44%)] Loss: -1136.669922\n",
      "Train Epoch: 822 [56320/101520 (55%)] Loss: -1135.801514\n",
      "Train Epoch: 822 [67584/101520 (67%)] Loss: -1137.653076\n",
      "Train Epoch: 822 [78848/101520 (78%)] Loss: -1122.801270\n",
      "Train Epoch: 822 [90112/101520 (89%)] Loss: -1138.893555\n",
      "Train Epoch: 822 [101376/101520 (100%)] Loss: -1126.327271\n",
      "    epoch          : 822\n",
      "    loss           : -1134.984954067211\n",
      "    ess            : 1.9684946602912405\n",
      "    log_marginal   : 1135.013281740735\n",
      "    log_joint      : 1343.1944941995132\n",
      "    val_loss       : -1133.948295261549\n",
      "    val_ess        : 1.9698567183121392\n",
      "    val_log_marginal: 1133.9736434273098\n",
      "    val_log_joint  : 1342.0694208559783\n",
      "Train Epoch: 823 [0/101520 (0%)] Loss: -1131.846191\n",
      "Train Epoch: 823 [11264/101520 (11%)] Loss: -1138.725586\n",
      "Train Epoch: 823 [22528/101520 (22%)] Loss: -1132.831665\n",
      "Train Epoch: 823 [33792/101520 (33%)] Loss: -1133.232178\n",
      "Train Epoch: 823 [45056/101520 (44%)] Loss: -1144.450073\n",
      "Train Epoch: 823 [56320/101520 (55%)] Loss: -1131.551025\n",
      "Train Epoch: 823 [67584/101520 (67%)] Loss: -1134.841309\n",
      "Train Epoch: 823 [78848/101520 (78%)] Loss: -1140.128052\n",
      "Train Epoch: 823 [90112/101520 (89%)] Loss: -1138.073730\n",
      "Train Epoch: 823 [101376/101520 (100%)] Loss: -1144.169556\n",
      "    epoch          : 823\n",
      "    loss           : -1135.1313605380417\n",
      "    ess            : 1.969103398035519\n",
      "    log_marginal   : 1135.15884660117\n",
      "    log_joint      : 1343.4800939512013\n",
      "    val_loss       : -1137.0205343495245\n",
      "    val_ess        : 1.9700903115065203\n",
      "    val_log_marginal: 1137.0456755264945\n",
      "    val_log_joint  : 1345.5145741338315\n",
      "Train Epoch: 824 [0/101520 (0%)] Loss: -1144.968750\n",
      "Train Epoch: 824 [11264/101520 (11%)] Loss: -1134.111572\n",
      "Train Epoch: 824 [22528/101520 (22%)] Loss: -1142.965820\n",
      "Train Epoch: 824 [33792/101520 (33%)] Loss: -1134.626221\n",
      "Train Epoch: 824 [45056/101520 (44%)] Loss: -1129.921631\n",
      "Train Epoch: 824 [56320/101520 (55%)] Loss: -1134.623291\n",
      "Train Epoch: 824 [67584/101520 (67%)] Loss: -1132.475830\n",
      "Train Epoch: 824 [78848/101520 (78%)] Loss: -1129.921631\n",
      "Train Epoch: 824 [90112/101520 (89%)] Loss: -1137.622070\n",
      "Train Epoch: 824 [101376/101520 (100%)] Loss: -1139.907837\n",
      "    epoch          : 824\n",
      "    loss           : -1135.2474113732726\n",
      "    ess            : 1.9684434566066493\n",
      "    log_marginal   : 1135.2747299731077\n",
      "    log_joint      : 1343.559607731038\n",
      "    val_loss       : -1134.732666015625\n",
      "    val_ess        : 1.966420899266782\n",
      "    val_log_marginal: 1134.7598558508832\n",
      "    val_log_joint  : 1342.7766007133152\n",
      "Train Epoch: 825 [0/101520 (0%)] Loss: -1137.467773\n",
      "Train Epoch: 825 [11264/101520 (11%)] Loss: -1136.557983\n",
      "Train Epoch: 825 [22528/101520 (22%)] Loss: -1130.831543\n",
      "Train Epoch: 825 [33792/101520 (33%)] Loss: -1136.016846\n",
      "Train Epoch: 825 [45056/101520 (44%)] Loss: -1139.021729\n",
      "Train Epoch: 825 [56320/101520 (55%)] Loss: -1134.774170\n",
      "Train Epoch: 825 [67584/101520 (67%)] Loss: -1138.278320\n",
      "Train Epoch: 825 [78848/101520 (78%)] Loss: -1131.222412\n",
      "Train Epoch: 825 [90112/101520 (89%)] Loss: -1137.958740\n",
      "Train Epoch: 825 [101376/101520 (100%)] Loss: -1147.592407\n",
      "    epoch          : 825\n",
      "    loss           : -1135.257732142156\n",
      "    ess            : 1.968326810017303\n",
      "    log_marginal   : 1135.2858604546168\n",
      "    log_joint      : 1343.5856798641646\n",
      "    val_loss       : -1133.6982634171195\n",
      "    val_ess        : 1.9686886020328687\n",
      "    val_log_marginal: 1133.7257186226223\n",
      "    val_log_joint  : 1341.9366136633832\n",
      "Train Epoch: 826 [0/101520 (0%)] Loss: -1142.819092\n",
      "Train Epoch: 826 [11264/101520 (11%)] Loss: -1138.150513\n",
      "Train Epoch: 826 [22528/101520 (22%)] Loss: -1138.245361\n",
      "Train Epoch: 826 [33792/101520 (33%)] Loss: -1134.227539\n",
      "Train Epoch: 826 [45056/101520 (44%)] Loss: -1142.310059\n",
      "Train Epoch: 826 [56320/101520 (55%)] Loss: -1126.333496\n",
      "Train Epoch: 826 [67584/101520 (67%)] Loss: -1127.317017\n",
      "Train Epoch: 826 [78848/101520 (78%)] Loss: -1137.656982\n",
      "Train Epoch: 826 [90112/101520 (89%)] Loss: -1137.164795\n",
      "Train Epoch: 826 [101376/101520 (100%)] Loss: -1124.522949\n",
      "    epoch          : 826\n",
      "    loss           : -1135.3453632910646\n",
      "    ess            : 1.9689179772707686\n",
      "    log_marginal   : 1135.3726272966394\n",
      "    log_joint      : 1343.617353123037\n",
      "    val_loss       : -1134.9721467391305\n",
      "    val_ess        : 1.9683088582494985\n",
      "    val_log_marginal: 1134.998975670856\n",
      "    val_log_joint  : 1343.2175505264945\n",
      "Train Epoch: 827 [0/101520 (0%)] Loss: -1134.823853\n",
      "Train Epoch: 827 [11264/101520 (11%)] Loss: -1134.583008\n",
      "Train Epoch: 827 [22528/101520 (22%)] Loss: -1136.764160\n",
      "Train Epoch: 827 [33792/101520 (33%)] Loss: -1132.062012\n",
      "Train Epoch: 827 [45056/101520 (44%)] Loss: -1132.937012\n",
      "Train Epoch: 827 [56320/101520 (55%)] Loss: -1145.289429\n",
      "Train Epoch: 827 [67584/101520 (67%)] Loss: -1137.969971\n",
      "Train Epoch: 827 [78848/101520 (78%)] Loss: -1135.699585\n",
      "Train Epoch: 827 [90112/101520 (89%)] Loss: -1135.504761\n",
      "Train Epoch: 827 [101376/101520 (100%)] Loss: -1138.582886\n",
      "    epoch          : 827\n",
      "    loss           : -1135.4795197913395\n",
      "    ess            : 1.9686144159067815\n",
      "    log_marginal   : 1135.5064543910962\n",
      "    log_joint      : 1343.804520036707\n",
      "    val_loss       : -1134.0245732846467\n",
      "    val_ess        : 1.9676664756691975\n",
      "    val_log_marginal: 1134.054491126019\n",
      "    val_log_joint  : 1342.2672702955163\n",
      "Train Epoch: 828 [0/101520 (0%)] Loss: -1134.520020\n",
      "Train Epoch: 828 [11264/101520 (11%)] Loss: -1138.559082\n",
      "Train Epoch: 828 [22528/101520 (22%)] Loss: -1133.205322\n",
      "Train Epoch: 828 [33792/101520 (33%)] Loss: -1138.731567\n",
      "Train Epoch: 828 [45056/101520 (44%)] Loss: -1132.338867\n",
      "Train Epoch: 828 [56320/101520 (55%)] Loss: -1135.175537\n",
      "Train Epoch: 828 [67584/101520 (67%)] Loss: -1123.840332\n",
      "Train Epoch: 828 [78848/101520 (78%)] Loss: -1137.026733\n",
      "Train Epoch: 828 [90112/101520 (89%)] Loss: -1136.871338\n",
      "Train Epoch: 828 [101376/101520 (100%)] Loss: -1136.610962\n",
      "    epoch          : 828\n",
      "    loss           : -1135.4955981077262\n",
      "    ess            : 1.9682474879164193\n",
      "    log_marginal   : 1135.5233203370367\n",
      "    log_joint      : 1343.801102067957\n",
      "    val_loss       : -1135.6759139351223\n",
      "    val_ess        : 1.9673652493435403\n",
      "    val_log_marginal: 1135.7063306725543\n",
      "    val_log_joint  : 1343.750143299932\n",
      "Train Epoch: 829 [0/101520 (0%)] Loss: -1140.080200\n",
      "Train Epoch: 829 [11264/101520 (11%)] Loss: -1129.837158\n",
      "Train Epoch: 829 [22528/101520 (22%)] Loss: -1135.172363\n",
      "Train Epoch: 829 [33792/101520 (33%)] Loss: -1136.945312\n",
      "Train Epoch: 829 [45056/101520 (44%)] Loss: -1133.756470\n",
      "Train Epoch: 829 [56320/101520 (55%)] Loss: -1137.015747\n",
      "Train Epoch: 829 [67584/101520 (67%)] Loss: -1135.492554\n",
      "Train Epoch: 829 [78848/101520 (78%)] Loss: -1130.464600\n",
      "Train Epoch: 829 [90112/101520 (89%)] Loss: -1139.516479\n",
      "Train Epoch: 829 [101376/101520 (100%)] Loss: -1132.740601\n",
      "    epoch          : 829\n",
      "    loss           : -1135.6452826878533\n",
      "    ess            : 1.9685925173400036\n",
      "    log_marginal   : 1135.673414680826\n",
      "    log_joint      : 1343.930952982687\n",
      "    val_loss       : -1134.6741094174592\n",
      "    val_ess        : 1.9703565473141877\n",
      "    val_log_marginal: 1134.7010551120925\n",
      "    val_log_joint  : 1342.9287693189538\n",
      "Train Epoch: 830 [0/101520 (0%)] Loss: -1131.189941\n",
      "Train Epoch: 830 [11264/101520 (11%)] Loss: -1137.060303\n",
      "Train Epoch: 830 [22528/101520 (22%)] Loss: -1134.365723\n",
      "Train Epoch: 830 [33792/101520 (33%)] Loss: -1142.327393\n",
      "Train Epoch: 830 [45056/101520 (44%)] Loss: -1142.595215\n",
      "Train Epoch: 830 [56320/101520 (55%)] Loss: -1131.293579\n",
      "Train Epoch: 830 [67584/101520 (67%)] Loss: -1133.540283\n",
      "Train Epoch: 830 [78848/101520 (78%)] Loss: -1136.392090\n",
      "Train Epoch: 830 [90112/101520 (89%)] Loss: -1134.068970\n",
      "Train Epoch: 830 [101376/101520 (100%)] Loss: -1141.990967\n",
      "    epoch          : 830\n",
      "    loss           : -1135.8601405464824\n",
      "    ess            : 1.9683576774357552\n",
      "    log_marginal   : 1135.8877192358275\n",
      "    log_joint      : 1344.1735569939542\n",
      "    val_loss       : -1135.2432914402175\n",
      "    val_ess        : 1.962843832762345\n",
      "    val_log_marginal: 1135.2753800101902\n",
      "    val_log_joint  : 1343.8265752377717\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch830.pth ...\n",
      "Train Epoch: 831 [0/101520 (0%)] Loss: -1132.067383\n",
      "Train Epoch: 831 [11264/101520 (11%)] Loss: -1130.496948\n",
      "Train Epoch: 831 [22528/101520 (22%)] Loss: -1142.229248\n",
      "Train Epoch: 831 [33792/101520 (33%)] Loss: -1136.692139\n",
      "Train Epoch: 831 [45056/101520 (44%)] Loss: -1136.273560\n",
      "Train Epoch: 831 [56320/101520 (55%)] Loss: -1141.588867\n",
      "Train Epoch: 831 [67584/101520 (67%)] Loss: -1127.395508\n",
      "Train Epoch: 831 [78848/101520 (78%)] Loss: -1136.760010\n",
      "Train Epoch: 831 [90112/101520 (89%)] Loss: -1139.274170\n",
      "Train Epoch: 831 [101376/101520 (100%)] Loss: -1136.409058\n",
      "    epoch          : 831\n",
      "    loss           : -1135.7303356381517\n",
      "    ess            : 1.9676112068358378\n",
      "    log_marginal   : 1135.7588964107647\n",
      "    log_joint      : 1344.0771962841552\n",
      "    val_loss       : -1134.0541355298913\n",
      "    val_ess        : 1.9680865277414736\n",
      "    val_log_marginal: 1134.0830661939538\n",
      "    val_log_joint  : 1342.6935929008152\n",
      "Train Epoch: 832 [0/101520 (0%)] Loss: -1135.072632\n",
      "Train Epoch: 832 [11264/101520 (11%)] Loss: -1140.562744\n",
      "Train Epoch: 832 [22528/101520 (22%)] Loss: -1136.185913\n",
      "Train Epoch: 832 [33792/101520 (33%)] Loss: -1133.511963\n",
      "Train Epoch: 832 [45056/101520 (44%)] Loss: -1134.230835\n",
      "Train Epoch: 832 [56320/101520 (55%)] Loss: -1132.784180\n",
      "Train Epoch: 832 [67584/101520 (67%)] Loss: -1134.048950\n",
      "Train Epoch: 832 [78848/101520 (78%)] Loss: -1135.082275\n",
      "Train Epoch: 832 [90112/101520 (89%)] Loss: -1135.298340\n",
      "Train Epoch: 832 [101376/101520 (100%)] Loss: -1128.035400\n",
      "    epoch          : 832\n",
      "    loss           : -1135.8839375098146\n",
      "    ess            : 1.9683841580721602\n",
      "    log_marginal   : 1135.911232799741\n",
      "    log_joint      : 1344.1302257135285\n",
      "    val_loss       : -1135.8925144361413\n",
      "    val_ess        : 1.967348160951034\n",
      "    val_log_marginal: 1135.9224800441575\n",
      "    val_log_joint  : 1344.3738960597825\n",
      "Train Epoch: 833 [0/101520 (0%)] Loss: -1142.213867\n",
      "Train Epoch: 833 [11264/101520 (11%)] Loss: -1133.310669\n",
      "Train Epoch: 833 [22528/101520 (22%)] Loss: -1135.476196\n",
      "Train Epoch: 833 [33792/101520 (33%)] Loss: -1128.170532\n",
      "Train Epoch: 833 [45056/101520 (44%)] Loss: -1128.758179\n",
      "Train Epoch: 833 [56320/101520 (55%)] Loss: -1131.596436\n",
      "Train Epoch: 833 [67584/101520 (67%)] Loss: -1133.481323\n",
      "Train Epoch: 833 [78848/101520 (78%)] Loss: -1132.566650\n",
      "Train Epoch: 833 [90112/101520 (89%)] Loss: -1138.111328\n",
      "Train Epoch: 833 [101376/101520 (100%)] Loss: -1147.705078\n",
      "    epoch          : 833\n",
      "    loss           : -1136.1105981567996\n",
      "    ess            : 1.968148687377048\n",
      "    log_marginal   : 1136.1391846929962\n",
      "    log_joint      : 1344.4814624882224\n",
      "    val_loss       : -1135.9971552309783\n",
      "    val_ess        : 1.9656642001608144\n",
      "    val_log_marginal: 1136.0323910920517\n",
      "    val_log_joint  : 1344.3156844429348\n",
      "Train Epoch: 834 [0/101520 (0%)] Loss: -1130.373901\n",
      "Train Epoch: 834 [11264/101520 (11%)] Loss: -1132.281982\n",
      "Train Epoch: 834 [22528/101520 (22%)] Loss: -1126.281372\n",
      "Train Epoch: 834 [33792/101520 (33%)] Loss: -1137.850342\n",
      "Train Epoch: 834 [45056/101520 (44%)] Loss: -1132.224854\n",
      "Train Epoch: 834 [56320/101520 (55%)] Loss: -1141.049805\n",
      "Train Epoch: 834 [67584/101520 (67%)] Loss: -1127.016113\n",
      "Train Epoch: 834 [78848/101520 (78%)] Loss: -1136.850098\n",
      "Train Epoch: 834 [90112/101520 (89%)] Loss: -1135.572998\n",
      "Train Epoch: 834 [101376/101520 (100%)] Loss: -1144.530396\n",
      "    epoch          : 834\n",
      "    loss           : -1136.0541844967022\n",
      "    ess            : 1.96777775299609\n",
      "    log_marginal   : 1136.0828145856235\n",
      "    log_joint      : 1344.3847306601367\n",
      "    val_loss       : -1135.7561831266983\n",
      "    val_ess        : 1.967606731083082\n",
      "    val_log_marginal: 1135.7840523097825\n",
      "    val_log_joint  : 1343.8700428838315\n",
      "Train Epoch: 835 [0/101520 (0%)] Loss: -1133.101929\n",
      "Train Epoch: 835 [11264/101520 (11%)] Loss: -1137.188721\n",
      "Train Epoch: 835 [22528/101520 (22%)] Loss: -1134.780273\n",
      "Train Epoch: 835 [33792/101520 (33%)] Loss: -1135.611206\n",
      "Train Epoch: 835 [45056/101520 (44%)] Loss: -1130.357422\n",
      "Train Epoch: 835 [56320/101520 (55%)] Loss: -1137.219849\n",
      "Train Epoch: 835 [67584/101520 (67%)] Loss: -1135.088867\n",
      "Train Epoch: 835 [78848/101520 (78%)] Loss: -1134.254395\n",
      "Train Epoch: 835 [90112/101520 (89%)] Loss: -1130.866943\n",
      "Train Epoch: 835 [101376/101520 (100%)] Loss: -1135.363037\n",
      "    epoch          : 835\n",
      "    loss           : -1136.1657690307004\n",
      "    ess            : 1.9681222594563086\n",
      "    log_marginal   : 1136.194255579656\n",
      "    log_joint      : 1344.4743413110475\n",
      "    val_loss       : -1133.8046131963315\n",
      "    val_ess        : 1.9658689032430234\n",
      "    val_log_marginal: 1133.8355606742527\n",
      "    val_log_joint  : 1342.3730840268342\n",
      "Train Epoch: 836 [0/101520 (0%)] Loss: -1125.028931\n",
      "Train Epoch: 836 [11264/101520 (11%)] Loss: -1135.648193\n",
      "Train Epoch: 836 [22528/101520 (22%)] Loss: -1137.761597\n",
      "Train Epoch: 836 [33792/101520 (33%)] Loss: -1135.848999\n",
      "Train Epoch: 836 [45056/101520 (44%)] Loss: -1133.527832\n",
      "Train Epoch: 836 [56320/101520 (55%)] Loss: -1134.739990\n",
      "Train Epoch: 836 [67584/101520 (67%)] Loss: -1135.751221\n",
      "Train Epoch: 836 [78848/101520 (78%)] Loss: -1140.979736\n",
      "Train Epoch: 836 [90112/101520 (89%)] Loss: -1135.157837\n",
      "Train Epoch: 836 [101376/101520 (100%)] Loss: -1140.402222\n",
      "    epoch          : 836\n",
      "    loss           : -1136.3208265448336\n",
      "    ess            : 1.9676532871160075\n",
      "    log_marginal   : 1136.3497026146356\n",
      "    log_joint      : 1344.6949051900126\n",
      "    val_loss       : -1134.891500721807\n",
      "    val_ess        : 1.9697342851887578\n",
      "    val_log_marginal: 1134.917379628057\n",
      "    val_log_joint  : 1343.0801577360733\n",
      "Train Epoch: 837 [0/101520 (0%)] Loss: -1142.327148\n",
      "Train Epoch: 837 [11264/101520 (11%)] Loss: -1138.794922\n",
      "Train Epoch: 837 [22528/101520 (22%)] Loss: -1139.474731\n",
      "Train Epoch: 837 [33792/101520 (33%)] Loss: -1143.246460\n",
      "Train Epoch: 837 [45056/101520 (44%)] Loss: -1137.688721\n",
      "Train Epoch: 837 [56320/101520 (55%)] Loss: -1131.492432\n",
      "Train Epoch: 837 [67584/101520 (67%)] Loss: -1133.031494\n",
      "Train Epoch: 837 [78848/101520 (78%)] Loss: -1135.813965\n",
      "Train Epoch: 837 [90112/101520 (89%)] Loss: -1132.162598\n",
      "Train Epoch: 837 [101376/101520 (100%)] Loss: -1121.257568\n",
      "    epoch          : 837\n",
      "    loss           : -1136.296743114989\n",
      "    ess            : 1.9688901428002208\n",
      "    log_marginal   : 1136.3240506732884\n",
      "    log_joint      : 1344.6585711761934\n",
      "    val_loss       : -1134.8668796705163\n",
      "    val_ess        : 1.9705974381902944\n",
      "    val_log_marginal: 1134.8927479619565\n",
      "    val_log_joint  : 1343.1620350713315\n",
      "Train Epoch: 838 [0/101520 (0%)] Loss: -1138.779297\n",
      "Train Epoch: 838 [11264/101520 (11%)] Loss: -1135.067871\n",
      "Train Epoch: 838 [22528/101520 (22%)] Loss: -1131.828125\n",
      "Train Epoch: 838 [33792/101520 (33%)] Loss: -1132.227051\n",
      "Train Epoch: 838 [45056/101520 (44%)] Loss: -1131.524536\n",
      "Train Epoch: 838 [56320/101520 (55%)] Loss: -1140.242676\n",
      "Train Epoch: 838 [67584/101520 (67%)] Loss: -1138.958008\n",
      "Train Epoch: 838 [78848/101520 (78%)] Loss: -1144.019775\n",
      "Train Epoch: 838 [90112/101520 (89%)] Loss: -1143.074463\n",
      "Train Epoch: 838 [101376/101520 (100%)] Loss: -1141.991577\n",
      "    epoch          : 838\n",
      "    loss           : -1136.4546168341708\n",
      "    ess            : 1.9691758766845242\n",
      "    log_marginal   : 1136.482083267902\n",
      "    log_joint      : 1344.80445930826\n",
      "    val_loss       : -1136.72974163553\n",
      "    val_ess        : 1.9691477184710295\n",
      "    val_log_marginal: 1136.7543626868207\n",
      "    val_log_joint  : 1345.0132048233695\n",
      "Train Epoch: 839 [0/101520 (0%)] Loss: -1142.004395\n",
      "Train Epoch: 839 [11264/101520 (11%)] Loss: -1139.696777\n",
      "Train Epoch: 839 [22528/101520 (22%)] Loss: -1132.630615\n",
      "Train Epoch: 839 [33792/101520 (33%)] Loss: -1133.595703\n",
      "Train Epoch: 839 [45056/101520 (44%)] Loss: -1134.401733\n",
      "Train Epoch: 839 [56320/101520 (55%)] Loss: -1134.264038\n",
      "Train Epoch: 839 [67584/101520 (67%)] Loss: -1139.260498\n",
      "Train Epoch: 839 [78848/101520 (78%)] Loss: -1134.943359\n",
      "Train Epoch: 839 [90112/101520 (89%)] Loss: -1138.715088\n",
      "Train Epoch: 839 [101376/101520 (100%)] Loss: -1139.985229\n",
      "    epoch          : 839\n",
      "    loss           : -1136.7461906701476\n",
      "    ess            : 1.968561861383256\n",
      "    log_marginal   : 1136.7743183691896\n",
      "    log_joint      : 1345.0450764565012\n",
      "    val_loss       : -1135.654832922894\n",
      "    val_ess        : 1.9617815536001455\n",
      "    val_log_marginal: 1135.6897874915082\n",
      "    val_log_joint  : 1343.8433519446332\n",
      "Train Epoch: 840 [0/101520 (0%)] Loss: -1140.149170\n",
      "Train Epoch: 840 [11264/101520 (11%)] Loss: -1138.680054\n",
      "Train Epoch: 840 [22528/101520 (22%)] Loss: -1141.877686\n",
      "Train Epoch: 840 [33792/101520 (33%)] Loss: -1134.026123\n",
      "Train Epoch: 840 [45056/101520 (44%)] Loss: -1141.061523\n",
      "Train Epoch: 840 [56320/101520 (55%)] Loss: -1141.127930\n",
      "Train Epoch: 840 [67584/101520 (67%)] Loss: -1136.793701\n",
      "Train Epoch: 840 [78848/101520 (78%)] Loss: -1138.586670\n",
      "Train Epoch: 840 [90112/101520 (89%)] Loss: -1131.865723\n",
      "Train Epoch: 840 [101376/101520 (100%)] Loss: -1154.192017\n",
      "    epoch          : 840\n",
      "    loss           : -1136.7294418871702\n",
      "    ess            : 1.9684282193830864\n",
      "    log_marginal   : 1136.757883656564\n",
      "    log_joint      : 1345.117258656564\n",
      "    val_loss       : -1137.0772864300272\n",
      "    val_ess        : 1.964984909347866\n",
      "    val_log_marginal: 1137.1091573963995\n",
      "    val_log_joint  : 1345.690185546875\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch840.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 841 [0/101520 (0%)] Loss: -1134.333374\n",
      "Train Epoch: 841 [11264/101520 (11%)] Loss: -1141.241455\n",
      "Train Epoch: 841 [22528/101520 (22%)] Loss: -1137.757080\n",
      "Train Epoch: 841 [33792/101520 (33%)] Loss: -1140.556396\n",
      "Train Epoch: 841 [45056/101520 (44%)] Loss: -1128.998291\n",
      "Train Epoch: 841 [56320/101520 (55%)] Loss: -1129.640137\n",
      "Train Epoch: 841 [67584/101520 (67%)] Loss: -1140.631470\n",
      "Train Epoch: 841 [78848/101520 (78%)] Loss: -1144.691162\n",
      "Train Epoch: 841 [90112/101520 (89%)] Loss: -1137.203369\n",
      "Train Epoch: 841 [101376/101520 (100%)] Loss: -1138.033203\n",
      "    epoch          : 841\n",
      "    loss           : -1136.6771589883008\n",
      "    ess            : 1.9679190651256235\n",
      "    log_marginal   : 1136.70586759481\n",
      "    log_joint      : 1345.1031046345006\n",
      "    val_loss       : -1136.193699048913\n",
      "    val_ess        : 1.9652609099512515\n",
      "    val_log_marginal: 1136.2227411684783\n",
      "    val_log_joint  : 1344.26050335428\n",
      "Train Epoch: 842 [0/101520 (0%)] Loss: -1137.721313\n",
      "Train Epoch: 842 [11264/101520 (11%)] Loss: -1135.631958\n",
      "Train Epoch: 842 [22528/101520 (22%)] Loss: -1134.125244\n",
      "Train Epoch: 842 [33792/101520 (33%)] Loss: -1143.153320\n",
      "Train Epoch: 842 [45056/101520 (44%)] Loss: -1136.369873\n",
      "Train Epoch: 842 [56320/101520 (55%)] Loss: -1141.347900\n",
      "Train Epoch: 842 [67584/101520 (67%)] Loss: -1130.471680\n",
      "Train Epoch: 842 [78848/101520 (78%)] Loss: -1135.404785\n",
      "Train Epoch: 842 [90112/101520 (89%)] Loss: -1140.921021\n",
      "Train Epoch: 842 [101376/101520 (100%)] Loss: -1135.393921\n",
      "    epoch          : 842\n",
      "    loss           : -1137.006780729821\n",
      "    ess            : 1.967328988127972\n",
      "    log_marginal   : 1137.036127291732\n",
      "    log_joint      : 1345.3389567466238\n",
      "    val_loss       : -1135.137987219769\n",
      "    val_ess        : 1.9679432122603706\n",
      "    val_log_marginal: 1135.1660050101902\n",
      "    val_log_joint  : 1343.3551503057065\n",
      "Train Epoch: 843 [0/101520 (0%)] Loss: -1140.161743\n",
      "Train Epoch: 843 [11264/101520 (11%)] Loss: -1146.866943\n",
      "Train Epoch: 843 [22528/101520 (22%)] Loss: -1135.391113\n",
      "Train Epoch: 843 [33792/101520 (33%)] Loss: -1135.390991\n",
      "Train Epoch: 843 [45056/101520 (44%)] Loss: -1136.160278\n",
      "Train Epoch: 843 [56320/101520 (55%)] Loss: -1142.026611\n",
      "Train Epoch: 843 [67584/101520 (67%)] Loss: -1138.597900\n",
      "Train Epoch: 843 [78848/101520 (78%)] Loss: -1129.077881\n",
      "Train Epoch: 843 [90112/101520 (89%)] Loss: -1140.800049\n",
      "Train Epoch: 843 [101376/101520 (100%)] Loss: -1148.872559\n",
      "    epoch          : 843\n",
      "    loss           : -1137.0118297787767\n",
      "    ess            : 1.9675993230474653\n",
      "    log_marginal   : 1137.040521822982\n",
      "    log_joint      : 1345.326661996506\n",
      "    val_loss       : -1133.990966796875\n",
      "    val_ess        : 1.9679343752239062\n",
      "    val_log_marginal: 1134.0197860054348\n",
      "    val_log_joint  : 1342.3306407099185\n",
      "Train Epoch: 844 [0/101520 (0%)] Loss: -1138.434570\n",
      "Train Epoch: 844 [11264/101520 (11%)] Loss: -1138.861328\n",
      "Train Epoch: 844 [22528/101520 (22%)] Loss: -1136.507202\n",
      "Train Epoch: 844 [33792/101520 (33%)] Loss: -1141.134155\n",
      "Train Epoch: 844 [45056/101520 (44%)] Loss: -1126.396973\n",
      "Train Epoch: 844 [56320/101520 (55%)] Loss: -1138.877197\n",
      "Train Epoch: 844 [67584/101520 (67%)] Loss: -1141.456787\n",
      "Train Epoch: 844 [78848/101520 (78%)] Loss: -1141.799683\n",
      "Train Epoch: 844 [90112/101520 (89%)] Loss: -1136.568359\n",
      "Train Epoch: 844 [101376/101520 (100%)] Loss: -1143.114380\n",
      "    epoch          : 844\n",
      "    loss           : -1137.1106202398712\n",
      "    ess            : 1.9683360130942646\n",
      "    log_marginal   : 1137.1383130250863\n",
      "    log_joint      : 1345.4109015536667\n",
      "    val_loss       : -1135.2272630774457\n",
      "    val_ess        : 1.9647918887760327\n",
      "    val_log_marginal: 1135.2620053498642\n",
      "    val_log_joint  : 1343.5885168987772\n",
      "Train Epoch: 845 [0/101520 (0%)] Loss: -1128.725830\n",
      "Train Epoch: 845 [11264/101520 (11%)] Loss: -1139.044922\n",
      "Train Epoch: 845 [22528/101520 (22%)] Loss: -1142.598633\n",
      "Train Epoch: 845 [33792/101520 (33%)] Loss: -1134.731445\n",
      "Train Epoch: 845 [45056/101520 (44%)] Loss: -1138.963867\n",
      "Train Epoch: 845 [56320/101520 (55%)] Loss: -1139.828369\n",
      "Train Epoch: 845 [67584/101520 (67%)] Loss: -1135.779053\n",
      "Train Epoch: 845 [78848/101520 (78%)] Loss: -1135.643433\n",
      "Train Epoch: 845 [90112/101520 (89%)] Loss: -1133.623413\n",
      "Train Epoch: 845 [101376/101520 (100%)] Loss: -1137.980225\n",
      "    epoch          : 845\n",
      "    loss           : -1137.3055119346734\n",
      "    ess            : 1.9682449084430484\n",
      "    log_marginal   : 1137.3332721959407\n",
      "    log_joint      : 1345.6089977475267\n",
      "    val_loss       : -1135.3918138586957\n",
      "    val_ess        : 1.9718379922535108\n",
      "    val_log_marginal: 1135.4157980213995\n",
      "    val_log_joint  : 1343.6369204313858\n",
      "Train Epoch: 846 [0/101520 (0%)] Loss: -1143.024414\n",
      "Train Epoch: 846 [11264/101520 (11%)] Loss: -1139.519775\n",
      "Train Epoch: 846 [22528/101520 (22%)] Loss: -1143.491333\n",
      "Train Epoch: 846 [33792/101520 (33%)] Loss: -1143.146240\n",
      "Train Epoch: 846 [45056/101520 (44%)] Loss: -1133.032227\n",
      "Train Epoch: 846 [56320/101520 (55%)] Loss: -1143.407715\n",
      "Train Epoch: 846 [67584/101520 (67%)] Loss: -1148.397949\n",
      "Train Epoch: 846 [78848/101520 (78%)] Loss: -1134.072388\n",
      "Train Epoch: 846 [90112/101520 (89%)] Loss: -1142.736572\n",
      "Train Epoch: 846 [101376/101520 (100%)] Loss: -1141.100708\n",
      "    epoch          : 846\n",
      "    loss           : -1137.215132670187\n",
      "    ess            : 1.9681007951947314\n",
      "    log_marginal   : 1137.2436437558888\n",
      "    log_joint      : 1345.5559609571294\n",
      "    val_loss       : -1136.9905952785325\n",
      "    val_ess        : 1.9689976650735606\n",
      "    val_log_marginal: 1137.0175622027853\n",
      "    val_log_joint  : 1345.2030560037365\n",
      "Train Epoch: 847 [0/101520 (0%)] Loss: -1146.288452\n",
      "Train Epoch: 847 [11264/101520 (11%)] Loss: -1140.150269\n",
      "Train Epoch: 847 [22528/101520 (22%)] Loss: -1132.782715\n",
      "Train Epoch: 847 [33792/101520 (33%)] Loss: -1135.752197\n",
      "Train Epoch: 847 [45056/101520 (44%)] Loss: -1130.700928\n",
      "Train Epoch: 847 [56320/101520 (55%)] Loss: -1139.898682\n",
      "Train Epoch: 847 [67584/101520 (67%)] Loss: -1136.018066\n",
      "Train Epoch: 847 [78848/101520 (78%)] Loss: -1139.651367\n",
      "Train Epoch: 847 [90112/101520 (89%)] Loss: -1134.470215\n",
      "Train Epoch: 847 [101376/101520 (100%)] Loss: -1149.856445\n",
      "    epoch          : 847\n",
      "    loss           : -1137.3625408536825\n",
      "    ess            : 1.9681912361077927\n",
      "    log_marginal   : 1137.3913187764997\n",
      "    log_joint      : 1345.7131090020414\n",
      "    val_loss       : -1134.4667119565217\n",
      "    val_ess        : 1.9691611010095347\n",
      "    val_log_marginal: 1134.4942945397418\n",
      "    val_log_joint  : 1342.6452742866848\n",
      "Train Epoch: 848 [0/101520 (0%)] Loss: -1135.782959\n",
      "Train Epoch: 848 [11264/101520 (11%)] Loss: -1128.032104\n",
      "Train Epoch: 848 [22528/101520 (22%)] Loss: -1139.202271\n",
      "Train Epoch: 848 [33792/101520 (33%)] Loss: -1140.735718\n",
      "Train Epoch: 848 [45056/101520 (44%)] Loss: -1139.002075\n",
      "Train Epoch: 848 [56320/101520 (55%)] Loss: -1135.894165\n",
      "Train Epoch: 848 [67584/101520 (67%)] Loss: -1139.062500\n",
      "Train Epoch: 848 [78848/101520 (78%)] Loss: -1139.937500\n",
      "Train Epoch: 848 [90112/101520 (89%)] Loss: -1139.049927\n",
      "Train Epoch: 848 [101376/101520 (100%)] Loss: -1138.285278\n",
      "    epoch          : 848\n",
      "    loss           : -1137.5106894334956\n",
      "    ess            : 1.968918794962629\n",
      "    log_marginal   : 1137.5377350620288\n",
      "    log_joint      : 1345.8391640821294\n",
      "    val_loss       : -1136.4966032608695\n",
      "    val_ess        : 1.966014053510583\n",
      "    val_log_marginal: 1136.5274976647418\n",
      "    val_log_joint  : 1344.714254628057\n",
      "Train Epoch: 849 [0/101520 (0%)] Loss: -1136.739014\n",
      "Train Epoch: 849 [11264/101520 (11%)] Loss: -1138.364502\n",
      "Train Epoch: 849 [22528/101520 (22%)] Loss: -1133.496704\n",
      "Train Epoch: 849 [33792/101520 (33%)] Loss: -1129.979126\n",
      "Train Epoch: 849 [45056/101520 (44%)] Loss: -1133.736328\n",
      "Train Epoch: 849 [56320/101520 (55%)] Loss: -1129.763184\n",
      "Train Epoch: 849 [67584/101520 (67%)] Loss: -1135.486816\n",
      "Train Epoch: 849 [78848/101520 (78%)] Loss: -1142.321899\n",
      "Train Epoch: 849 [90112/101520 (89%)] Loss: -1136.240356\n",
      "Train Epoch: 849 [101376/101520 (100%)] Loss: -1133.354004\n",
      "    epoch          : 849\n",
      "    loss           : -1137.4611374744818\n",
      "    ess            : 1.9676287695391095\n",
      "    log_marginal   : 1137.4892743748037\n",
      "    log_joint      : 1345.827114699474\n",
      "    val_loss       : -1139.0085873811142\n",
      "    val_ess        : 1.9679670800333438\n",
      "    val_log_marginal: 1139.0374755859375\n",
      "    val_log_joint  : 1347.4109258237092\n",
      "Train Epoch: 850 [0/101520 (0%)] Loss: -1142.346680\n",
      "Train Epoch: 850 [11264/101520 (11%)] Loss: -1137.351074\n",
      "Train Epoch: 850 [22528/101520 (22%)] Loss: -1131.450562\n",
      "Train Epoch: 850 [33792/101520 (33%)] Loss: -1138.127930\n",
      "Train Epoch: 850 [45056/101520 (44%)] Loss: -1138.572266\n",
      "Train Epoch: 850 [56320/101520 (55%)] Loss: -1136.245117\n",
      "Train Epoch: 850 [67584/101520 (67%)] Loss: -1141.006104\n",
      "Train Epoch: 850 [78848/101520 (78%)] Loss: -1136.431152\n",
      "Train Epoch: 850 [90112/101520 (89%)] Loss: -1137.389771\n",
      "Train Epoch: 850 [101376/101520 (100%)] Loss: -1151.287354\n",
      "    epoch          : 850\n",
      "    loss           : -1137.6345288453988\n",
      "    ess            : 1.9684837858880584\n",
      "    log_marginal   : 1137.6623473814384\n",
      "    log_joint      : 1345.968657373783\n",
      "    val_loss       : -1136.8953061311142\n",
      "    val_ess        : 1.968802218851836\n",
      "    val_log_marginal: 1136.9213973335598\n",
      "    val_log_joint  : 1345.1226647418478\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch850.pth ...\n",
      "Train Epoch: 851 [0/101520 (0%)] Loss: -1139.549561\n",
      "Train Epoch: 851 [11264/101520 (11%)] Loss: -1144.042236\n",
      "Train Epoch: 851 [22528/101520 (22%)] Loss: -1141.558350\n",
      "Train Epoch: 851 [33792/101520 (33%)] Loss: -1144.784912\n",
      "Train Epoch: 851 [45056/101520 (44%)] Loss: -1136.261475\n",
      "Train Epoch: 851 [56320/101520 (55%)] Loss: -1140.505737\n",
      "Train Epoch: 851 [67584/101520 (67%)] Loss: -1138.342163\n",
      "Train Epoch: 851 [78848/101520 (78%)] Loss: -1133.751465\n",
      "Train Epoch: 851 [90112/101520 (89%)] Loss: -1139.359131\n",
      "Train Epoch: 851 [101376/101520 (100%)] Loss: -1135.417603\n",
      "    epoch          : 851\n",
      "    loss           : -1137.7074814992934\n",
      "    ess            : 1.9681838049960496\n",
      "    log_marginal   : 1137.7357447638583\n",
      "    log_joint      : 1346.062551527167\n",
      "    val_loss       : -1136.8645125679348\n",
      "    val_ess        : 1.9649787208308345\n",
      "    val_log_marginal: 1136.8944251019022\n",
      "    val_log_joint  : 1345.0909583050272\n",
      "Train Epoch: 852 [0/101520 (0%)] Loss: -1134.267944\n",
      "Train Epoch: 852 [11264/101520 (11%)] Loss: -1139.822510\n",
      "Train Epoch: 852 [22528/101520 (22%)] Loss: -1138.707520\n",
      "Train Epoch: 852 [33792/101520 (33%)] Loss: -1139.314453\n",
      "Train Epoch: 852 [45056/101520 (44%)] Loss: -1136.386475\n",
      "Train Epoch: 852 [56320/101520 (55%)] Loss: -1134.402710\n",
      "Train Epoch: 852 [67584/101520 (67%)] Loss: -1136.436768\n",
      "Train Epoch: 852 [78848/101520 (78%)] Loss: -1133.901001\n",
      "Train Epoch: 852 [90112/101520 (89%)] Loss: -1143.378296\n",
      "Train Epoch: 852 [101376/101520 (100%)] Loss: -1137.953735\n",
      "    epoch          : 852\n",
      "    loss           : -1137.8117522426587\n",
      "    ess            : 1.968173100720698\n",
      "    log_marginal   : 1137.8399713410804\n",
      "    log_joint      : 1346.1027592797975\n",
      "    val_loss       : -1136.583543860394\n",
      "    val_ess        : 1.9682019016017085\n",
      "    val_log_marginal: 1136.6111423658288\n",
      "    val_log_joint  : 1344.9244862432065\n",
      "Train Epoch: 853 [0/101520 (0%)] Loss: -1141.713379\n",
      "Train Epoch: 853 [11264/101520 (11%)] Loss: -1137.276001\n",
      "Train Epoch: 853 [22528/101520 (22%)] Loss: -1147.069458\n",
      "Train Epoch: 853 [33792/101520 (33%)] Loss: -1137.930176\n",
      "Train Epoch: 853 [45056/101520 (44%)] Loss: -1141.222168\n",
      "Train Epoch: 853 [56320/101520 (55%)] Loss: -1139.537598\n",
      "Train Epoch: 853 [67584/101520 (67%)] Loss: -1142.692627\n",
      "Train Epoch: 853 [78848/101520 (78%)] Loss: -1132.844727\n",
      "Train Epoch: 853 [90112/101520 (89%)] Loss: -1139.161743\n",
      "Train Epoch: 853 [101376/101520 (100%)] Loss: -1143.463379\n",
      "    epoch          : 853\n",
      "    loss           : -1137.8188550172738\n",
      "    ess            : 1.9687832450147849\n",
      "    log_marginal   : 1137.8463005947708\n",
      "    log_joint      : 1346.153033232569\n",
      "    val_loss       : -1135.8459631878397\n",
      "    val_ess        : 1.9639124351999033\n",
      "    val_log_marginal: 1135.8797076681385\n",
      "    val_log_joint  : 1344.5714588994565\n",
      "Train Epoch: 854 [0/101520 (0%)] Loss: -1137.137939\n",
      "Train Epoch: 854 [11264/101520 (11%)] Loss: -1134.931030\n",
      "Train Epoch: 854 [22528/101520 (22%)] Loss: -1135.828125\n",
      "Train Epoch: 854 [33792/101520 (33%)] Loss: -1137.238525\n",
      "Train Epoch: 854 [45056/101520 (44%)] Loss: -1136.889771\n",
      "Train Epoch: 854 [56320/101520 (55%)] Loss: -1131.024048\n",
      "Train Epoch: 854 [67584/101520 (67%)] Loss: -1136.892090\n",
      "Train Epoch: 854 [78848/101520 (78%)] Loss: -1137.364380\n",
      "Train Epoch: 854 [90112/101520 (89%)] Loss: -1131.096680\n",
      "Train Epoch: 854 [101376/101520 (100%)] Loss: -1140.336792\n",
      "    epoch          : 854\n",
      "    loss           : -1138.022525346459\n",
      "    ess            : 1.967054014828936\n",
      "    log_marginal   : 1138.05235098834\n",
      "    log_joint      : 1346.311633852858\n",
      "    val_loss       : -1138.2698815387228\n",
      "    val_ess        : 1.968494731446971\n",
      "    val_log_marginal: 1138.2975171959918\n",
      "    val_log_joint  : 1346.4498397163723\n",
      "Train Epoch: 855 [0/101520 (0%)] Loss: -1142.178711\n",
      "Train Epoch: 855 [11264/101520 (11%)] Loss: -1144.708252\n",
      "Train Epoch: 855 [22528/101520 (22%)] Loss: -1137.718262\n",
      "Train Epoch: 855 [33792/101520 (33%)] Loss: -1133.929810\n",
      "Train Epoch: 855 [45056/101520 (44%)] Loss: -1137.540039\n",
      "Train Epoch: 855 [56320/101520 (55%)] Loss: -1138.308838\n",
      "Train Epoch: 855 [67584/101520 (67%)] Loss: -1139.495850\n",
      "Train Epoch: 855 [78848/101520 (78%)] Loss: -1135.899170\n",
      "Train Epoch: 855 [90112/101520 (89%)] Loss: -1139.336304\n",
      "Train Epoch: 855 [101376/101520 (100%)] Loss: -1142.947266\n",
      "    epoch          : 855\n",
      "    loss           : -1137.9798154591315\n",
      "    ess            : 1.96873150099462\n",
      "    log_marginal   : 1138.0071794519472\n",
      "    log_joint      : 1346.3027227200455\n",
      "    val_loss       : -1136.0908362347147\n",
      "    val_ess        : 1.9694395479948625\n",
      "    val_log_marginal: 1136.1189336362092\n",
      "    val_log_joint  : 1344.3348070227582\n",
      "Train Epoch: 856 [0/101520 (0%)] Loss: -1137.930908\n",
      "Train Epoch: 856 [11264/101520 (11%)] Loss: -1142.393799\n",
      "Train Epoch: 856 [22528/101520 (22%)] Loss: -1143.177002\n",
      "Train Epoch: 856 [33792/101520 (33%)] Loss: -1140.709961\n",
      "Train Epoch: 856 [45056/101520 (44%)] Loss: -1135.807007\n",
      "Train Epoch: 856 [56320/101520 (55%)] Loss: -1140.576294\n",
      "Train Epoch: 856 [67584/101520 (67%)] Loss: -1141.713623\n",
      "Train Epoch: 856 [78848/101520 (78%)] Loss: -1132.832520\n",
      "Train Epoch: 856 [90112/101520 (89%)] Loss: -1137.310547\n",
      "Train Epoch: 856 [101376/101520 (100%)] Loss: -1131.383789\n",
      "    epoch          : 856\n",
      "    loss           : -1138.052695116206\n",
      "    ess            : 1.9676907727466755\n",
      "    log_marginal   : 1138.0827035568468\n",
      "    log_joint      : 1346.3114259039337\n",
      "    val_loss       : -1137.877048658288\n",
      "    val_ess        : 1.9700437680534695\n",
      "    val_log_marginal: 1137.9005551545517\n",
      "    val_log_joint  : 1345.9976382048233\n",
      "Train Epoch: 857 [0/101520 (0%)] Loss: -1138.272949\n",
      "Train Epoch: 857 [11264/101520 (11%)] Loss: -1138.371216\n",
      "Train Epoch: 857 [22528/101520 (22%)] Loss: -1136.458862\n",
      "Train Epoch: 857 [33792/101520 (33%)] Loss: -1135.949463\n",
      "Train Epoch: 857 [45056/101520 (44%)] Loss: -1144.062500\n",
      "Train Epoch: 857 [56320/101520 (55%)] Loss: -1135.806396\n",
      "Train Epoch: 857 [67584/101520 (67%)] Loss: -1133.120728\n",
      "Train Epoch: 857 [78848/101520 (78%)] Loss: -1139.026367\n",
      "Train Epoch: 857 [90112/101520 (89%)] Loss: -1135.161133\n",
      "Train Epoch: 857 [101376/101520 (100%)] Loss: -1125.875732\n",
      "    epoch          : 857\n",
      "    loss           : -1138.123257277599\n",
      "    ess            : 1.9681559729216687\n",
      "    log_marginal   : 1138.151860989518\n",
      "    log_joint      : 1346.437813456933\n",
      "    val_loss       : -1135.3713591202445\n",
      "    val_ess        : 1.965469292972399\n",
      "    val_log_marginal: 1135.4028214164402\n",
      "    val_log_joint  : 1343.7163935122283\n",
      "Train Epoch: 858 [0/101520 (0%)] Loss: -1135.663330\n",
      "Train Epoch: 858 [11264/101520 (11%)] Loss: -1136.875488\n",
      "Train Epoch: 858 [22528/101520 (22%)] Loss: -1133.104004\n",
      "Train Epoch: 858 [33792/101520 (33%)] Loss: -1140.344849\n",
      "Train Epoch: 858 [45056/101520 (44%)] Loss: -1133.675415\n",
      "Train Epoch: 858 [56320/101520 (55%)] Loss: -1132.036743\n",
      "Train Epoch: 858 [67584/101520 (67%)] Loss: -1134.151489\n",
      "Train Epoch: 858 [78848/101520 (78%)] Loss: -1134.759033\n",
      "Train Epoch: 858 [90112/101520 (89%)] Loss: -1138.221802\n",
      "Train Epoch: 858 [101376/101520 (100%)] Loss: -1144.449829\n",
      "    epoch          : 858\n",
      "    loss           : -1138.2729160941426\n",
      "    ess            : 1.9691910492115883\n",
      "    log_marginal   : 1138.3005683937265\n",
      "    log_joint      : 1346.5940487348853\n",
      "    val_loss       : -1137.1609470533288\n",
      "    val_ess        : 1.9666341854178386\n",
      "    val_log_marginal: 1137.19115680197\n",
      "    val_log_joint  : 1345.3776271654212\n",
      "Train Epoch: 859 [0/101520 (0%)] Loss: -1142.004639\n",
      "Train Epoch: 859 [11264/101520 (11%)] Loss: -1135.826416\n",
      "Train Epoch: 859 [22528/101520 (22%)] Loss: -1128.616821\n",
      "Train Epoch: 859 [33792/101520 (33%)] Loss: -1132.520020\n",
      "Train Epoch: 859 [45056/101520 (44%)] Loss: -1138.787476\n",
      "Train Epoch: 859 [56320/101520 (55%)] Loss: -1135.808472\n",
      "Train Epoch: 859 [67584/101520 (67%)] Loss: -1136.164673\n",
      "Train Epoch: 859 [78848/101520 (78%)] Loss: -1140.298340\n",
      "Train Epoch: 859 [90112/101520 (89%)] Loss: -1134.101318\n",
      "Train Epoch: 859 [101376/101520 (100%)] Loss: -1148.722534\n",
      "    epoch          : 859\n",
      "    loss           : -1138.28011824258\n",
      "    ess            : 1.9676827863233173\n",
      "    log_marginal   : 1138.3098708876414\n",
      "    log_joint      : 1346.6591870485238\n",
      "    val_loss       : -1137.9715629245925\n",
      "    val_ess        : 1.96990270199983\n",
      "    val_log_marginal: 1137.9979194972825\n",
      "    val_log_joint  : 1346.276075280231\n",
      "Train Epoch: 860 [0/101520 (0%)] Loss: -1144.184692\n",
      "Train Epoch: 860 [11264/101520 (11%)] Loss: -1141.735107\n",
      "Train Epoch: 860 [22528/101520 (22%)] Loss: -1132.442383\n",
      "Train Epoch: 860 [33792/101520 (33%)] Loss: -1139.430908\n",
      "Train Epoch: 860 [45056/101520 (44%)] Loss: -1142.452637\n",
      "Train Epoch: 860 [56320/101520 (55%)] Loss: -1136.914062\n",
      "Train Epoch: 860 [67584/101520 (67%)] Loss: -1140.703857\n",
      "Train Epoch: 860 [78848/101520 (78%)] Loss: -1143.387817\n",
      "Train Epoch: 860 [90112/101520 (89%)] Loss: -1135.778809\n",
      "Train Epoch: 860 [101376/101520 (100%)] Loss: -1146.271729\n",
      "    epoch          : 860\n",
      "    loss           : -1138.5795732814463\n",
      "    ess            : 1.9685388605798309\n",
      "    log_marginal   : 1138.6076476130654\n",
      "    log_joint      : 1346.9257843170933\n",
      "    val_loss       : -1137.4657937754755\n",
      "    val_ess        : 1.9673548418542612\n",
      "    val_log_marginal: 1137.4961521314538\n",
      "    val_log_joint  : 1346.2700407608695\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch860.pth ...\n",
      "Train Epoch: 861 [0/101520 (0%)] Loss: -1142.347168\n",
      "Train Epoch: 861 [11264/101520 (11%)] Loss: -1145.856934\n",
      "Train Epoch: 861 [22528/101520 (22%)] Loss: -1137.272217\n",
      "Train Epoch: 861 [33792/101520 (33%)] Loss: -1138.031738\n",
      "Train Epoch: 861 [45056/101520 (44%)] Loss: -1145.974854\n",
      "Train Epoch: 861 [56320/101520 (55%)] Loss: -1135.273682\n",
      "Train Epoch: 861 [67584/101520 (67%)] Loss: -1136.476318\n",
      "Train Epoch: 861 [78848/101520 (78%)] Loss: -1137.554932\n",
      "Train Epoch: 861 [90112/101520 (89%)] Loss: -1139.194336\n",
      "Train Epoch: 861 [101376/101520 (100%)] Loss: -1139.797607\n",
      "    epoch          : 861\n",
      "    loss           : -1138.5545562284076\n",
      "    ess            : 1.9682369555660228\n",
      "    log_marginal   : 1138.5830532054806\n",
      "    log_joint      : 1346.8532389731863\n",
      "    val_loss       : -1137.44995647928\n",
      "    val_ess        : 1.971348482629527\n",
      "    val_log_marginal: 1137.4738875679348\n",
      "    val_log_joint  : 1346.0080194887908\n",
      "Train Epoch: 862 [0/101520 (0%)] Loss: -1135.295776\n",
      "Train Epoch: 862 [11264/101520 (11%)] Loss: -1138.901123\n",
      "Train Epoch: 862 [22528/101520 (22%)] Loss: -1142.726685\n",
      "Train Epoch: 862 [33792/101520 (33%)] Loss: -1141.895752\n",
      "Train Epoch: 862 [45056/101520 (44%)] Loss: -1135.031006\n",
      "Train Epoch: 862 [56320/101520 (55%)] Loss: -1142.312988\n",
      "Train Epoch: 862 [67584/101520 (67%)] Loss: -1138.734131\n",
      "Train Epoch: 862 [78848/101520 (78%)] Loss: -1138.350952\n",
      "Train Epoch: 862 [90112/101520 (89%)] Loss: -1148.894775\n",
      "Train Epoch: 862 [101376/101520 (100%)] Loss: -1137.605225\n",
      "    epoch          : 862\n",
      "    loss           : -1138.6121580804413\n",
      "    ess            : 1.9682554761369024\n",
      "    log_marginal   : 1138.6403422139997\n",
      "    log_joint      : 1346.9441476179727\n",
      "    val_loss       : -1137.0227422299592\n",
      "    val_ess        : 1.9667718151341314\n",
      "    val_log_marginal: 1137.0520603345788\n",
      "    val_log_joint  : 1345.364555027174\n",
      "Train Epoch: 863 [0/101520 (0%)] Loss: -1144.260254\n",
      "Train Epoch: 863 [11264/101520 (11%)] Loss: -1143.071777\n",
      "Train Epoch: 863 [22528/101520 (22%)] Loss: -1141.896362\n",
      "Train Epoch: 863 [33792/101520 (33%)] Loss: -1139.047852\n",
      "Train Epoch: 863 [45056/101520 (44%)] Loss: -1138.633057\n",
      "Train Epoch: 863 [56320/101520 (55%)] Loss: -1135.341064\n",
      "Train Epoch: 863 [67584/101520 (67%)] Loss: -1144.278076\n",
      "Train Epoch: 863 [78848/101520 (78%)] Loss: -1141.988281\n",
      "Train Epoch: 863 [90112/101520 (89%)] Loss: -1134.669556\n",
      "Train Epoch: 863 [101376/101520 (100%)] Loss: -1127.588501\n",
      "    epoch          : 863\n",
      "    loss           : -1138.693056346184\n",
      "    ess            : 1.9680992131257178\n",
      "    log_marginal   : 1138.720990204931\n",
      "    log_joint      : 1347.0367542055983\n",
      "    val_loss       : -1137.6877865998642\n",
      "    val_ess        : 1.9716009212576824\n",
      "    val_log_marginal: 1137.7127048658288\n",
      "    val_log_joint  : 1346.413866126019\n",
      "Train Epoch: 864 [0/101520 (0%)] Loss: -1145.125000\n",
      "Train Epoch: 864 [11264/101520 (11%)] Loss: -1132.847656\n",
      "Train Epoch: 864 [22528/101520 (22%)] Loss: -1143.625244\n",
      "Train Epoch: 864 [33792/101520 (33%)] Loss: -1138.384277\n",
      "Train Epoch: 864 [45056/101520 (44%)] Loss: -1131.244385\n",
      "Train Epoch: 864 [56320/101520 (55%)] Loss: -1135.560791\n",
      "Train Epoch: 864 [67584/101520 (67%)] Loss: -1134.160400\n",
      "Train Epoch: 864 [78848/101520 (78%)] Loss: -1135.439209\n",
      "Train Epoch: 864 [90112/101520 (89%)] Loss: -1141.503540\n",
      "Train Epoch: 864 [101376/101520 (100%)] Loss: -1138.430908\n",
      "    epoch          : 864\n",
      "    loss           : -1138.7537492148242\n",
      "    ess            : 1.9686578062910531\n",
      "    log_marginal   : 1138.7818756870288\n",
      "    log_joint      : 1347.1218844466473\n",
      "    val_loss       : -1137.08056640625\n",
      "    val_ess        : 1.972133087075275\n",
      "    val_log_marginal: 1137.1039614470108\n",
      "    val_log_joint  : 1345.273288892663\n",
      "Train Epoch: 865 [0/101520 (0%)] Loss: -1144.654663\n",
      "Train Epoch: 865 [11264/101520 (11%)] Loss: -1137.628418\n",
      "Train Epoch: 865 [22528/101520 (22%)] Loss: -1141.979004\n",
      "Train Epoch: 865 [33792/101520 (33%)] Loss: -1137.845215\n",
      "Train Epoch: 865 [45056/101520 (44%)] Loss: -1141.492432\n",
      "Train Epoch: 865 [56320/101520 (55%)] Loss: -1142.871338\n",
      "Train Epoch: 865 [67584/101520 (67%)] Loss: -1136.247559\n",
      "Train Epoch: 865 [78848/101520 (78%)] Loss: -1135.338623\n",
      "Train Epoch: 865 [90112/101520 (89%)] Loss: -1141.309570\n",
      "Train Epoch: 865 [101376/101520 (100%)] Loss: -1135.530396\n",
      "    epoch          : 865\n",
      "    loss           : -1138.851450244386\n",
      "    ess            : 1.9679712614222387\n",
      "    log_marginal   : 1138.879947221459\n",
      "    log_joint      : 1347.203485076751\n",
      "    val_loss       : -1138.9205853006115\n",
      "    val_ess        : 1.969615770422894\n",
      "    val_log_marginal: 1138.9469631029212\n",
      "    val_log_joint  : 1347.3565620754075\n",
      "Train Epoch: 866 [0/101520 (0%)] Loss: -1144.168945\n",
      "Train Epoch: 866 [11264/101520 (11%)] Loss: -1139.698975\n",
      "Train Epoch: 866 [22528/101520 (22%)] Loss: -1140.331177\n",
      "Train Epoch: 866 [33792/101520 (33%)] Loss: -1140.006348\n",
      "Train Epoch: 866 [45056/101520 (44%)] Loss: -1135.164429\n",
      "Train Epoch: 866 [56320/101520 (55%)] Loss: -1134.356445\n",
      "Train Epoch: 866 [67584/101520 (67%)] Loss: -1137.225708\n",
      "Train Epoch: 866 [78848/101520 (78%)] Loss: -1142.040771\n",
      "Train Epoch: 866 [90112/101520 (89%)] Loss: -1141.607910\n",
      "Train Epoch: 866 [101376/101520 (100%)] Loss: -1119.309814\n",
      "    epoch          : 866\n",
      "    loss           : -1138.9698093740185\n",
      "    ess            : 1.9684495069273753\n",
      "    log_marginal   : 1138.9979609963882\n",
      "    log_joint      : 1347.3488567103093\n",
      "    val_loss       : -1135.9923891813858\n",
      "    val_ess        : 1.9692809685416843\n",
      "    val_log_marginal: 1136.0280124830163\n",
      "    val_log_joint  : 1344.1985234799592\n",
      "Train Epoch: 867 [0/101520 (0%)] Loss: -1140.158813\n",
      "Train Epoch: 867 [11264/101520 (11%)] Loss: -1146.062500\n",
      "Train Epoch: 867 [22528/101520 (22%)] Loss: -1136.990479\n",
      "Train Epoch: 867 [33792/101520 (33%)] Loss: -1137.389404\n",
      "Train Epoch: 867 [45056/101520 (44%)] Loss: -1136.885498\n",
      "Train Epoch: 867 [56320/101520 (55%)] Loss: -1135.608521\n",
      "Train Epoch: 867 [67584/101520 (67%)] Loss: -1144.056641\n",
      "Train Epoch: 867 [78848/101520 (78%)] Loss: -1137.220825\n",
      "Train Epoch: 867 [90112/101520 (89%)] Loss: -1134.109131\n",
      "Train Epoch: 867 [101376/101520 (100%)] Loss: -1147.450562\n",
      "    epoch          : 867\n",
      "    loss           : -1139.0415854909313\n",
      "    ess            : 1.9677322210378982\n",
      "    log_marginal   : 1139.0705498930197\n",
      "    log_joint      : 1347.4591898702497\n",
      "    val_loss       : -1136.9405995244565\n",
      "    val_ess        : 1.9654466172923213\n",
      "    val_log_marginal: 1136.980325450068\n",
      "    val_log_joint  : 1345.3290219514267\n",
      "Train Epoch: 868 [0/101520 (0%)] Loss: -1139.531250\n",
      "Train Epoch: 868 [11264/101520 (11%)] Loss: -1141.292480\n",
      "Train Epoch: 868 [22528/101520 (22%)] Loss: -1132.882568\n",
      "Train Epoch: 868 [33792/101520 (33%)] Loss: -1139.093750\n",
      "Train Epoch: 868 [45056/101520 (44%)] Loss: -1141.411011\n",
      "Train Epoch: 868 [56320/101520 (55%)] Loss: -1132.022949\n",
      "Train Epoch: 868 [67584/101520 (67%)] Loss: -1132.357178\n",
      "Train Epoch: 868 [78848/101520 (78%)] Loss: -1136.471558\n",
      "Train Epoch: 868 [90112/101520 (89%)] Loss: -1133.585693\n",
      "Train Epoch: 868 [101376/101520 (100%)] Loss: -1145.493896\n",
      "    epoch          : 868\n",
      "    loss           : -1139.2053106106705\n",
      "    ess            : 1.96847637454469\n",
      "    log_marginal   : 1139.232844520454\n",
      "    log_joint      : 1347.575562750275\n",
      "    val_loss       : -1138.4778415845788\n",
      "    val_ess        : 1.9681977914727253\n",
      "    val_log_marginal: 1138.50610882303\n",
      "    val_log_joint  : 1346.5939198369565\n",
      "Train Epoch: 869 [0/101520 (0%)] Loss: -1143.691650\n",
      "Train Epoch: 869 [11264/101520 (11%)] Loss: -1140.740723\n",
      "Train Epoch: 869 [22528/101520 (22%)] Loss: -1138.870850\n",
      "Train Epoch: 869 [33792/101520 (33%)] Loss: -1136.845703\n",
      "Train Epoch: 869 [45056/101520 (44%)] Loss: -1137.445312\n",
      "Train Epoch: 869 [56320/101520 (55%)] Loss: -1147.661621\n",
      "Train Epoch: 869 [67584/101520 (67%)] Loss: -1133.541504\n",
      "Train Epoch: 869 [78848/101520 (78%)] Loss: -1149.831055\n",
      "Train Epoch: 869 [90112/101520 (89%)] Loss: -1147.817139\n",
      "Train Epoch: 869 [101376/101520 (100%)] Loss: -1152.398682\n",
      "    epoch          : 869\n",
      "    loss           : -1139.3425795972048\n",
      "    ess            : 1.9674057607075677\n",
      "    log_marginal   : 1139.371966644747\n",
      "    log_joint      : 1347.7331193320117\n",
      "    val_loss       : -1137.515964673913\n",
      "    val_ess        : 1.9693226347798887\n",
      "    val_log_marginal: 1137.5432341202445\n",
      "    val_log_joint  : 1345.933646824049\n",
      "Train Epoch: 870 [0/101520 (0%)] Loss: -1139.966553\n",
      "Train Epoch: 870 [11264/101520 (11%)] Loss: -1139.954956\n",
      "Train Epoch: 870 [22528/101520 (22%)] Loss: -1130.903076\n",
      "Train Epoch: 870 [33792/101520 (33%)] Loss: -1144.617310\n",
      "Train Epoch: 870 [45056/101520 (44%)] Loss: -1139.969238\n",
      "Train Epoch: 870 [56320/101520 (55%)] Loss: -1142.264893\n",
      "Train Epoch: 870 [67584/101520 (67%)] Loss: -1139.832520\n",
      "Train Epoch: 870 [78848/101520 (78%)] Loss: -1147.040771\n",
      "Train Epoch: 870 [90112/101520 (89%)] Loss: -1144.462280\n",
      "Train Epoch: 870 [101376/101520 (100%)] Loss: -1127.395630\n",
      "    epoch          : 870\n",
      "    loss           : -1139.3762538277324\n",
      "    ess            : 1.9678252851543714\n",
      "    log_marginal   : 1139.404729948571\n",
      "    log_joint      : 1347.72972037924\n",
      "    val_loss       : -1139.044778575068\n",
      "    val_ess        : 1.9694654008616572\n",
      "    val_log_marginal: 1139.0728441321332\n",
      "    val_log_joint  : 1347.3947541610055\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch870.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 871 [0/101520 (0%)] Loss: -1136.801758\n",
      "Train Epoch: 871 [11264/101520 (11%)] Loss: -1137.250977\n",
      "Train Epoch: 871 [22528/101520 (22%)] Loss: -1142.830811\n",
      "Train Epoch: 871 [33792/101520 (33%)] Loss: -1137.026611\n",
      "Train Epoch: 871 [45056/101520 (44%)] Loss: -1136.268066\n",
      "Train Epoch: 871 [56320/101520 (55%)] Loss: -1140.095215\n",
      "Train Epoch: 871 [67584/101520 (67%)] Loss: -1143.128662\n",
      "Train Epoch: 871 [78848/101520 (78%)] Loss: -1137.795044\n",
      "Train Epoch: 871 [90112/101520 (89%)] Loss: -1140.240723\n",
      "Train Epoch: 871 [101376/101520 (100%)] Loss: -1152.195923\n",
      "    epoch          : 871\n",
      "    loss           : -1139.5528711673603\n",
      "    ess            : 1.9680019203741945\n",
      "    log_marginal   : 1139.5816153521514\n",
      "    log_joint      : 1347.8629861956265\n",
      "    val_loss       : -1139.3079674762228\n",
      "    val_ess        : 1.971596448317818\n",
      "    val_log_marginal: 1139.3314845872962\n",
      "    val_log_joint  : 1347.722560716712\n",
      "Train Epoch: 872 [0/101520 (0%)] Loss: -1139.542236\n",
      "Train Epoch: 872 [11264/101520 (11%)] Loss: -1139.471313\n",
      "Train Epoch: 872 [22528/101520 (22%)] Loss: -1140.211060\n",
      "Train Epoch: 872 [33792/101520 (33%)] Loss: -1133.630371\n",
      "Train Epoch: 872 [45056/101520 (44%)] Loss: -1134.241089\n",
      "Train Epoch: 872 [56320/101520 (55%)] Loss: -1145.722900\n",
      "Train Epoch: 872 [67584/101520 (67%)] Loss: -1138.788208\n",
      "Train Epoch: 872 [78848/101520 (78%)] Loss: -1141.651367\n",
      "Train Epoch: 872 [90112/101520 (89%)] Loss: -1145.349609\n",
      "Train Epoch: 872 [101376/101520 (100%)] Loss: -1150.136963\n",
      "    epoch          : 872\n",
      "    loss           : -1139.5741396189935\n",
      "    ess            : 1.9685915073557714\n",
      "    log_marginal   : 1139.6019728770807\n",
      "    log_joint      : 1347.952266213882\n",
      "    val_loss       : -1138.5146325152853\n",
      "    val_ess        : 1.9675294938294783\n",
      "    val_log_marginal: 1138.546094811481\n",
      "    val_log_joint  : 1346.6899095618207\n",
      "Train Epoch: 873 [0/101520 (0%)] Loss: -1141.885620\n",
      "Train Epoch: 873 [11264/101520 (11%)] Loss: -1137.373291\n",
      "Train Epoch: 873 [22528/101520 (22%)] Loss: -1139.293945\n",
      "Train Epoch: 873 [33792/101520 (33%)] Loss: -1135.680908\n",
      "Train Epoch: 873 [45056/101520 (44%)] Loss: -1136.945923\n",
      "Train Epoch: 873 [56320/101520 (55%)] Loss: -1145.103394\n",
      "Train Epoch: 873 [67584/101520 (67%)] Loss: -1138.325317\n",
      "Train Epoch: 873 [78848/101520 (78%)] Loss: -1141.748413\n",
      "Train Epoch: 873 [90112/101520 (89%)] Loss: -1142.939087\n",
      "Train Epoch: 873 [101376/101520 (100%)] Loss: -1131.934204\n",
      "    epoch          : 873\n",
      "    loss           : -1139.662871854389\n",
      "    ess            : 1.9684411892339813\n",
      "    log_marginal   : 1139.6905658664416\n",
      "    log_joint      : 1347.98312914671\n",
      "    val_loss       : -1138.0081415591033\n",
      "    val_ess        : 1.9688292845435764\n",
      "    val_log_marginal: 1138.0359682829483\n",
      "    val_log_joint  : 1346.4423191236413\n",
      "Train Epoch: 874 [0/101520 (0%)] Loss: -1136.423584\n",
      "Train Epoch: 874 [11264/101520 (11%)] Loss: -1138.458374\n",
      "Train Epoch: 874 [22528/101520 (22%)] Loss: -1144.574707\n",
      "Train Epoch: 874 [33792/101520 (33%)] Loss: -1141.071289\n",
      "Train Epoch: 874 [45056/101520 (44%)] Loss: -1141.446777\n",
      "Train Epoch: 874 [56320/101520 (55%)] Loss: -1142.841309\n",
      "Train Epoch: 874 [67584/101520 (67%)] Loss: -1146.768066\n",
      "Train Epoch: 874 [78848/101520 (78%)] Loss: -1140.541016\n",
      "Train Epoch: 874 [90112/101520 (89%)] Loss: -1139.879272\n",
      "Train Epoch: 874 [101376/101520 (100%)] Loss: -1119.035889\n",
      "    epoch          : 874\n",
      "    loss           : -1139.6980090884108\n",
      "    ess            : 1.9679709367416611\n",
      "    log_marginal   : 1139.7273206854586\n",
      "    log_joint      : 1348.0266708297347\n",
      "    val_loss       : -1139.3454165251358\n",
      "    val_ess        : 1.9683490680611653\n",
      "    val_log_marginal: 1139.374416185462\n",
      "    val_log_joint  : 1347.737936268682\n",
      "Train Epoch: 875 [0/101520 (0%)] Loss: -1137.403564\n",
      "Train Epoch: 875 [11264/101520 (11%)] Loss: -1139.388184\n",
      "Train Epoch: 875 [22528/101520 (22%)] Loss: -1138.914917\n",
      "Train Epoch: 875 [33792/101520 (33%)] Loss: -1137.745605\n",
      "Train Epoch: 875 [45056/101520 (44%)] Loss: -1140.542358\n",
      "Train Epoch: 875 [56320/101520 (55%)] Loss: -1135.480469\n",
      "Train Epoch: 875 [67584/101520 (67%)] Loss: -1138.217041\n",
      "Train Epoch: 875 [78848/101520 (78%)] Loss: -1144.958008\n",
      "Train Epoch: 875 [90112/101520 (89%)] Loss: -1137.267700\n",
      "Train Epoch: 875 [101376/101520 (100%)] Loss: -1138.297485\n",
      "    epoch          : 875\n",
      "    loss           : -1139.8940619847283\n",
      "    ess            : 1.9687532474048173\n",
      "    log_marginal   : 1139.921051792164\n",
      "    log_joint      : 1348.219578728604\n",
      "    val_loss       : -1136.762690005095\n",
      "    val_ess        : 1.9698150935380354\n",
      "    val_log_marginal: 1136.788966966712\n",
      "    val_log_joint  : 1345.1169327445652\n",
      "Train Epoch: 876 [0/101520 (0%)] Loss: -1144.855469\n",
      "Train Epoch: 876 [11264/101520 (11%)] Loss: -1140.008179\n",
      "Train Epoch: 876 [22528/101520 (22%)] Loss: -1142.507935\n",
      "Train Epoch: 876 [33792/101520 (33%)] Loss: -1130.476440\n",
      "Train Epoch: 876 [45056/101520 (44%)] Loss: -1145.174316\n",
      "Train Epoch: 876 [56320/101520 (55%)] Loss: -1134.519531\n",
      "Train Epoch: 876 [67584/101520 (67%)] Loss: -1140.639648\n",
      "Train Epoch: 876 [78848/101520 (78%)] Loss: -1144.010742\n",
      "Train Epoch: 876 [90112/101520 (89%)] Loss: -1137.195557\n",
      "Train Epoch: 876 [101376/101520 (100%)] Loss: -1143.101562\n",
      "    epoch          : 876\n",
      "    loss           : -1139.9972052646042\n",
      "    ess            : 1.9682233249721814\n",
      "    log_marginal   : 1140.02500723834\n",
      "    log_joint      : 1348.332076029562\n",
      "    val_loss       : -1138.9740467900815\n",
      "    val_ess        : 1.9706066587696904\n",
      "    val_log_marginal: 1138.9996815557065\n",
      "    val_log_joint  : 1347.311916185462\n",
      "Train Epoch: 877 [0/101520 (0%)] Loss: -1138.005859\n",
      "Train Epoch: 877 [11264/101520 (11%)] Loss: -1143.050781\n",
      "Train Epoch: 877 [22528/101520 (22%)] Loss: -1134.852783\n",
      "Train Epoch: 877 [33792/101520 (33%)] Loss: -1142.616455\n",
      "Train Epoch: 877 [45056/101520 (44%)] Loss: -1139.822021\n",
      "Train Epoch: 877 [56320/101520 (55%)] Loss: -1136.055420\n",
      "Train Epoch: 877 [67584/101520 (67%)] Loss: -1142.078491\n",
      "Train Epoch: 877 [78848/101520 (78%)] Loss: -1139.223022\n",
      "Train Epoch: 877 [90112/101520 (89%)] Loss: -1138.853882\n",
      "Train Epoch: 877 [101376/101520 (100%)] Loss: -1149.907959\n",
      "    epoch          : 877\n",
      "    loss           : -1140.155567265036\n",
      "    ess            : 1.9681053634864003\n",
      "    log_marginal   : 1140.1834072707286\n",
      "    log_joint      : 1348.4806809928548\n",
      "    val_loss       : -1138.9464960512908\n",
      "    val_ess        : 1.965311076330102\n",
      "    val_log_marginal: 1138.9769658627717\n",
      "    val_log_joint  : 1347.1467603600543\n",
      "Train Epoch: 878 [0/101520 (0%)] Loss: -1135.815918\n",
      "Train Epoch: 878 [11264/101520 (11%)] Loss: -1144.572021\n",
      "Train Epoch: 878 [22528/101520 (22%)] Loss: -1138.263184\n",
      "Train Epoch: 878 [33792/101520 (33%)] Loss: -1141.038330\n",
      "Train Epoch: 878 [45056/101520 (44%)] Loss: -1146.093506\n",
      "Train Epoch: 878 [56320/101520 (55%)] Loss: -1135.409912\n",
      "Train Epoch: 878 [67584/101520 (67%)] Loss: -1143.120728\n",
      "Train Epoch: 878 [78848/101520 (78%)] Loss: -1143.917725\n",
      "Train Epoch: 878 [90112/101520 (89%)] Loss: -1149.932129\n",
      "Train Epoch: 878 [101376/101520 (100%)] Loss: -1136.588379\n",
      "    epoch          : 878\n",
      "    loss           : -1140.2032624057788\n",
      "    ess            : 1.9673072926363153\n",
      "    log_marginal   : 1140.2333389358903\n",
      "    log_joint      : 1348.543139893805\n",
      "    val_loss       : -1140.5674995754075\n",
      "    val_ess        : 1.9682994199835735\n",
      "    val_log_marginal: 1140.5951033882473\n",
      "    val_log_joint  : 1348.717141856318\n",
      "Train Epoch: 879 [0/101520 (0%)] Loss: -1140.382446\n",
      "Train Epoch: 879 [11264/101520 (11%)] Loss: -1135.853271\n",
      "Train Epoch: 879 [22528/101520 (22%)] Loss: -1141.227295\n",
      "Train Epoch: 879 [33792/101520 (33%)] Loss: -1137.122070\n",
      "Train Epoch: 879 [45056/101520 (44%)] Loss: -1125.983887\n",
      "Train Epoch: 879 [56320/101520 (55%)] Loss: -1141.946289\n",
      "Train Epoch: 879 [67584/101520 (67%)] Loss: -1144.668457\n",
      "Train Epoch: 879 [78848/101520 (78%)] Loss: -1132.010376\n",
      "Train Epoch: 879 [90112/101520 (89%)] Loss: -1138.127441\n",
      "Train Epoch: 879 [101376/101520 (100%)] Loss: -1157.953979\n",
      "    epoch          : 879\n",
      "    loss           : -1140.3392916732098\n",
      "    ess            : 1.9686395942266264\n",
      "    log_marginal   : 1140.3673242923603\n",
      "    log_joint      : 1348.713897245014\n",
      "    val_loss       : -1139.241895592731\n",
      "    val_ess        : 1.9622154598650725\n",
      "    val_log_marginal: 1139.2763671875\n",
      "    val_log_joint  : 1347.5727379840353\n",
      "Train Epoch: 880 [0/101520 (0%)] Loss: -1140.443848\n",
      "Train Epoch: 880 [11264/101520 (11%)] Loss: -1135.749512\n",
      "Train Epoch: 880 [22528/101520 (22%)] Loss: -1143.766113\n",
      "Train Epoch: 880 [33792/101520 (33%)] Loss: -1143.729004\n",
      "Train Epoch: 880 [45056/101520 (44%)] Loss: -1138.739258\n",
      "Train Epoch: 880 [56320/101520 (55%)] Loss: -1138.365479\n",
      "Train Epoch: 880 [67584/101520 (67%)] Loss: -1143.943970\n",
      "Train Epoch: 880 [78848/101520 (78%)] Loss: -1144.995605\n",
      "Train Epoch: 880 [90112/101520 (89%)] Loss: -1140.042969\n",
      "Train Epoch: 880 [101376/101520 (100%)] Loss: -1143.121094\n",
      "    epoch          : 880\n",
      "    loss           : -1140.4361320763976\n",
      "    ess            : 1.9693419178526606\n",
      "    log_marginal   : 1140.463405283252\n",
      "    log_joint      : 1348.822620180983\n",
      "    val_loss       : -1139.5077381963315\n",
      "    val_ess        : 1.967658281326294\n",
      "    val_log_marginal: 1139.5363981827445\n",
      "    val_log_joint  : 1347.7620690387228\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch880.pth ...\n",
      "Train Epoch: 881 [0/101520 (0%)] Loss: -1142.362427\n",
      "Train Epoch: 881 [11264/101520 (11%)] Loss: -1142.331055\n",
      "Train Epoch: 881 [22528/101520 (22%)] Loss: -1141.815918\n",
      "Train Epoch: 881 [33792/101520 (33%)] Loss: -1137.011963\n",
      "Train Epoch: 881 [45056/101520 (44%)] Loss: -1141.359375\n",
      "Train Epoch: 881 [56320/101520 (55%)] Loss: -1141.195312\n",
      "Train Epoch: 881 [67584/101520 (67%)] Loss: -1141.778564\n",
      "Train Epoch: 881 [78848/101520 (78%)] Loss: -1141.885742\n",
      "Train Epoch: 881 [90112/101520 (89%)] Loss: -1140.164429\n",
      "Train Epoch: 881 [101376/101520 (100%)] Loss: -1138.679810\n",
      "    epoch          : 881\n",
      "    loss           : -1140.4740965570038\n",
      "    ess            : 1.967868304731858\n",
      "    log_marginal   : 1140.50305298465\n",
      "    log_joint      : 1348.8163995023947\n",
      "    val_loss       : -1139.2614109205163\n",
      "    val_ess        : 1.9671135933502861\n",
      "    val_log_marginal: 1139.2928307574728\n",
      "    val_log_joint  : 1347.8382886803668\n",
      "Train Epoch: 882 [0/101520 (0%)] Loss: -1137.583008\n",
      "Train Epoch: 882 [11264/101520 (11%)] Loss: -1138.193359\n",
      "Train Epoch: 882 [22528/101520 (22%)] Loss: -1146.421143\n",
      "Train Epoch: 882 [33792/101520 (33%)] Loss: -1140.919922\n",
      "Train Epoch: 882 [45056/101520 (44%)] Loss: -1140.663330\n",
      "Train Epoch: 882 [56320/101520 (55%)] Loss: -1145.414795\n",
      "Train Epoch: 882 [67584/101520 (67%)] Loss: -1147.103271\n",
      "Train Epoch: 882 [78848/101520 (78%)] Loss: -1141.336670\n",
      "Train Epoch: 882 [90112/101520 (89%)] Loss: -1138.265869\n",
      "Train Epoch: 882 [101376/101520 (100%)] Loss: -1142.356323\n",
      "    epoch          : 882\n",
      "    loss           : -1140.5577950789102\n",
      "    ess            : 1.9679514067855912\n",
      "    log_marginal   : 1140.586290215727\n",
      "    log_joint      : 1348.9530599776224\n",
      "    val_loss       : -1137.936815344769\n",
      "    val_ess        : 1.9657939983450847\n",
      "    val_log_marginal: 1137.9656451681385\n",
      "    val_log_joint  : 1346.2856286090353\n",
      "Train Epoch: 883 [0/101520 (0%)] Loss: -1144.144287\n",
      "Train Epoch: 883 [11264/101520 (11%)] Loss: -1141.446777\n",
      "Train Epoch: 883 [22528/101520 (22%)] Loss: -1145.711670\n",
      "Train Epoch: 883 [33792/101520 (33%)] Loss: -1138.171631\n",
      "Train Epoch: 883 [45056/101520 (44%)] Loss: -1138.007324\n",
      "Train Epoch: 883 [56320/101520 (55%)] Loss: -1139.396484\n",
      "Train Epoch: 883 [67584/101520 (67%)] Loss: -1152.996216\n",
      "Train Epoch: 883 [78848/101520 (78%)] Loss: -1141.519409\n",
      "Train Epoch: 883 [90112/101520 (89%)] Loss: -1143.688843\n",
      "Train Epoch: 883 [101376/101520 (100%)] Loss: -1130.567383\n",
      "    epoch          : 883\n",
      "    loss           : -1140.7583909537923\n",
      "    ess            : 1.967739224433899\n",
      "    log_marginal   : 1140.7868173877198\n",
      "    log_joint      : 1349.0673515281485\n",
      "    val_loss       : -1139.4049496858017\n",
      "    val_ess        : 1.9657073072765185\n",
      "    val_log_marginal: 1139.4341987941575\n",
      "    val_log_joint  : 1347.4137122112772\n",
      "Train Epoch: 884 [0/101520 (0%)] Loss: -1141.389404\n",
      "Train Epoch: 884 [11264/101520 (11%)] Loss: -1146.841553\n",
      "Train Epoch: 884 [22528/101520 (22%)] Loss: -1145.334595\n",
      "Train Epoch: 884 [33792/101520 (33%)] Loss: -1145.972168\n",
      "Train Epoch: 884 [45056/101520 (44%)] Loss: -1134.671631\n",
      "Train Epoch: 884 [56320/101520 (55%)] Loss: -1142.640259\n",
      "Train Epoch: 884 [67584/101520 (67%)] Loss: -1139.757080\n",
      "Train Epoch: 884 [78848/101520 (78%)] Loss: -1138.682495\n",
      "Train Epoch: 884 [90112/101520 (89%)] Loss: -1141.743652\n",
      "Train Epoch: 884 [101376/101520 (100%)] Loss: -1133.578491\n",
      "    epoch          : 884\n",
      "    loss           : -1140.7611212802292\n",
      "    ess            : 1.9675398609746042\n",
      "    log_marginal   : 1140.7901948708386\n",
      "    log_joint      : 1349.089426625314\n",
      "    val_loss       : -1139.7986211362092\n",
      "    val_ess        : 1.9678726040798684\n",
      "    val_log_marginal: 1139.8280400815217\n",
      "    val_log_joint  : 1348.2540177055027\n",
      "Train Epoch: 885 [0/101520 (0%)] Loss: -1144.548218\n",
      "Train Epoch: 885 [11264/101520 (11%)] Loss: -1143.603760\n",
      "Train Epoch: 885 [22528/101520 (22%)] Loss: -1153.859375\n",
      "Train Epoch: 885 [33792/101520 (33%)] Loss: -1132.478149\n",
      "Train Epoch: 885 [45056/101520 (44%)] Loss: -1143.107422\n",
      "Train Epoch: 885 [56320/101520 (55%)] Loss: -1148.080078\n",
      "Train Epoch: 885 [67584/101520 (67%)] Loss: -1143.435425\n",
      "Train Epoch: 885 [78848/101520 (78%)] Loss: -1142.810303\n",
      "Train Epoch: 885 [90112/101520 (89%)] Loss: -1139.345947\n",
      "Train Epoch: 885 [101376/101520 (100%)] Loss: -1134.432739\n",
      "    epoch          : 885\n",
      "    loss           : -1140.7968670255575\n",
      "    ess            : 1.9676433484158924\n",
      "    log_marginal   : 1140.8261675810695\n",
      "    log_joint      : 1349.1759174289416\n",
      "    val_loss       : -1139.2746741253397\n",
      "    val_ess        : 1.9663796113884968\n",
      "    val_log_marginal: 1139.3030209748642\n",
      "    val_log_joint  : 1347.6001666525135\n",
      "Train Epoch: 886 [0/101520 (0%)] Loss: -1138.585938\n",
      "Train Epoch: 886 [11264/101520 (11%)] Loss: -1134.060059\n",
      "Train Epoch: 886 [22528/101520 (22%)] Loss: -1144.649658\n",
      "Train Epoch: 886 [33792/101520 (33%)] Loss: -1135.472900\n",
      "Train Epoch: 886 [45056/101520 (44%)] Loss: -1139.387817\n",
      "Train Epoch: 886 [56320/101520 (55%)] Loss: -1132.760498\n",
      "Train Epoch: 886 [67584/101520 (67%)] Loss: -1142.335693\n",
      "Train Epoch: 886 [78848/101520 (78%)] Loss: -1140.979248\n",
      "Train Epoch: 886 [90112/101520 (89%)] Loss: -1133.700806\n",
      "Train Epoch: 886 [101376/101520 (100%)] Loss: -1135.539917\n",
      "    epoch          : 886\n",
      "    loss           : -1141.0349360327025\n",
      "    ess            : 1.9677881325908642\n",
      "    log_marginal   : 1141.0633526519316\n",
      "    log_joint      : 1349.3178201800015\n",
      "    val_loss       : -1139.467279848845\n",
      "    val_ess        : 1.966445109118586\n",
      "    val_log_marginal: 1139.4990552819293\n",
      "    val_log_joint  : 1347.7017769191575\n",
      "Train Epoch: 887 [0/101520 (0%)] Loss: -1143.995117\n",
      "Train Epoch: 887 [11264/101520 (11%)] Loss: -1140.866455\n",
      "Train Epoch: 887 [22528/101520 (22%)] Loss: -1137.540649\n",
      "Train Epoch: 887 [33792/101520 (33%)] Loss: -1148.327637\n",
      "Train Epoch: 887 [45056/101520 (44%)] Loss: -1140.514648\n",
      "Train Epoch: 887 [56320/101520 (55%)] Loss: -1140.068115\n",
      "Train Epoch: 887 [67584/101520 (67%)] Loss: -1142.190186\n",
      "Train Epoch: 887 [78848/101520 (78%)] Loss: -1131.259644\n",
      "Train Epoch: 887 [90112/101520 (89%)] Loss: -1142.819214\n",
      "Train Epoch: 887 [101376/101520 (100%)] Loss: -1144.716064\n",
      "    epoch          : 887\n",
      "    loss           : -1141.026302778541\n",
      "    ess            : 1.96800480895306\n",
      "    log_marginal   : 1141.055191730135\n",
      "    log_joint      : 1349.3620936714824\n",
      "    val_loss       : -1141.5841382897418\n",
      "    val_ess        : 1.9674692879552427\n",
      "    val_log_marginal: 1141.611667798913\n",
      "    val_log_joint  : 1349.7291206691575\n",
      "Train Epoch: 888 [0/101520 (0%)] Loss: -1141.829834\n",
      "Train Epoch: 888 [11264/101520 (11%)] Loss: -1139.200684\n",
      "Train Epoch: 888 [22528/101520 (22%)] Loss: -1138.083374\n",
      "Train Epoch: 888 [33792/101520 (33%)] Loss: -1144.808105\n",
      "Train Epoch: 888 [45056/101520 (44%)] Loss: -1142.312866\n",
      "Train Epoch: 888 [56320/101520 (55%)] Loss: -1146.057007\n",
      "Train Epoch: 888 [67584/101520 (67%)] Loss: -1139.588257\n",
      "Train Epoch: 888 [78848/101520 (78%)] Loss: -1134.592529\n",
      "Train Epoch: 888 [90112/101520 (89%)] Loss: -1139.123657\n",
      "Train Epoch: 888 [101376/101520 (100%)] Loss: -1133.785400\n",
      "    epoch          : 888\n",
      "    loss           : -1141.1164109119818\n",
      "    ess            : 1.9674061063546031\n",
      "    log_marginal   : 1141.1459433397456\n",
      "    log_joint      : 1349.4639518392744\n",
      "    val_loss       : -1142.0212614639945\n",
      "    val_ess        : 1.9675513402275417\n",
      "    val_log_marginal: 1142.0506591796875\n",
      "    val_log_joint  : 1350.2584387737772\n",
      "Train Epoch: 889 [0/101520 (0%)] Loss: -1147.600220\n",
      "Train Epoch: 889 [11264/101520 (11%)] Loss: -1136.886963\n",
      "Train Epoch: 889 [22528/101520 (22%)] Loss: -1139.969849\n",
      "Train Epoch: 889 [33792/101520 (33%)] Loss: -1133.661621\n",
      "Train Epoch: 889 [45056/101520 (44%)] Loss: -1133.613892\n",
      "Train Epoch: 889 [56320/101520 (55%)] Loss: -1148.260620\n",
      "Train Epoch: 889 [67584/101520 (67%)] Loss: -1144.743042\n",
      "Train Epoch: 889 [78848/101520 (78%)] Loss: -1146.236328\n",
      "Train Epoch: 889 [90112/101520 (89%)] Loss: -1149.242432\n",
      "Train Epoch: 889 [101376/101520 (100%)] Loss: -1133.853516\n",
      "    epoch          : 889\n",
      "    loss           : -1141.2180611308495\n",
      "    ess            : 1.9682151294832853\n",
      "    log_marginal   : 1141.2459808809674\n",
      "    log_joint      : 1349.513971223304\n",
      "    val_loss       : -1138.7861434273098\n",
      "    val_ess        : 1.969183958095053\n",
      "    val_log_marginal: 1138.8132111922555\n",
      "    val_log_joint  : 1346.781196925951\n",
      "Train Epoch: 890 [0/101520 (0%)] Loss: -1134.489258\n",
      "Train Epoch: 890 [11264/101520 (11%)] Loss: -1144.760620\n",
      "Train Epoch: 890 [22528/101520 (22%)] Loss: -1153.635498\n",
      "Train Epoch: 890 [33792/101520 (33%)] Loss: -1137.418091\n",
      "Train Epoch: 890 [45056/101520 (44%)] Loss: -1141.317627\n",
      "Train Epoch: 890 [56320/101520 (55%)] Loss: -1142.978027\n",
      "Train Epoch: 890 [67584/101520 (67%)] Loss: -1136.038940\n",
      "Train Epoch: 890 [78848/101520 (78%)] Loss: -1138.202637\n",
      "Train Epoch: 890 [90112/101520 (89%)] Loss: -1142.702393\n",
      "Train Epoch: 890 [101376/101520 (100%)] Loss: -1134.481567\n",
      "    epoch          : 890\n",
      "    loss           : -1141.2405208415123\n",
      "    ess            : 1.9675400376918928\n",
      "    log_marginal   : 1141.2700121702262\n",
      "    log_joint      : 1349.5821539337312\n",
      "    val_loss       : -1140.7717179008152\n",
      "    val_ess        : 1.9683682296587073\n",
      "    val_log_marginal: 1140.7990032693615\n",
      "    val_log_joint  : 1349.189208984375\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch890.pth ...\n",
      "Train Epoch: 891 [0/101520 (0%)] Loss: -1141.790161\n",
      "Train Epoch: 891 [11264/101520 (11%)] Loss: -1138.640503\n",
      "Train Epoch: 891 [22528/101520 (22%)] Loss: -1136.890625\n",
      "Train Epoch: 891 [33792/101520 (33%)] Loss: -1145.596558\n",
      "Train Epoch: 891 [45056/101520 (44%)] Loss: -1142.668457\n",
      "Train Epoch: 891 [56320/101520 (55%)] Loss: -1150.674561\n",
      "Train Epoch: 891 [67584/101520 (67%)] Loss: -1138.901978\n",
      "Train Epoch: 891 [78848/101520 (78%)] Loss: -1140.780640\n",
      "Train Epoch: 891 [90112/101520 (89%)] Loss: -1138.586426\n",
      "Train Epoch: 891 [101376/101520 (100%)] Loss: -1144.316162\n",
      "    epoch          : 891\n",
      "    loss           : -1141.2987410195508\n",
      "    ess            : 1.968051792988226\n",
      "    log_marginal   : 1141.3277998881124\n",
      "    log_joint      : 1349.6657089156722\n",
      "    val_loss       : -1139.4809994904892\n",
      "    val_ess        : 1.9673824102982231\n",
      "    val_log_marginal: 1139.5122229534647\n",
      "    val_log_joint  : 1348.0407555621603\n",
      "Train Epoch: 892 [0/101520 (0%)] Loss: -1142.251343\n",
      "Train Epoch: 892 [11264/101520 (11%)] Loss: -1139.126831\n",
      "Train Epoch: 892 [22528/101520 (22%)] Loss: -1136.437988\n",
      "Train Epoch: 892 [33792/101520 (33%)] Loss: -1137.323853\n",
      "Train Epoch: 892 [45056/101520 (44%)] Loss: -1145.723511\n",
      "Train Epoch: 892 [56320/101520 (55%)] Loss: -1137.787720\n",
      "Train Epoch: 892 [67584/101520 (67%)] Loss: -1147.487915\n",
      "Train Epoch: 892 [78848/101520 (78%)] Loss: -1147.965820\n",
      "Train Epoch: 892 [90112/101520 (89%)] Loss: -1142.179443\n",
      "Train Epoch: 892 [101376/101520 (100%)] Loss: -1141.596191\n",
      "    epoch          : 892\n",
      "    loss           : -1141.4009795069096\n",
      "    ess            : 1.9679296334185192\n",
      "    log_marginal   : 1141.4308223245132\n",
      "    log_joint      : 1349.7663942269944\n",
      "    val_loss       : -1142.9984290081522\n",
      "    val_ess        : 1.9688372819320015\n",
      "    val_log_marginal: 1143.0259001358695\n",
      "    val_log_joint  : 1351.1668701171875\n",
      "Train Epoch: 893 [0/101520 (0%)] Loss: -1134.436401\n",
      "Train Epoch: 893 [11264/101520 (11%)] Loss: -1139.110840\n",
      "Train Epoch: 893 [22528/101520 (22%)] Loss: -1142.416260\n",
      "Train Epoch: 893 [33792/101520 (33%)] Loss: -1134.504395\n",
      "Train Epoch: 893 [45056/101520 (44%)] Loss: -1144.678345\n",
      "Train Epoch: 893 [56320/101520 (55%)] Loss: -1143.230957\n",
      "Train Epoch: 893 [67584/101520 (67%)] Loss: -1142.383911\n",
      "Train Epoch: 893 [78848/101520 (78%)] Loss: -1135.866211\n",
      "Train Epoch: 893 [90112/101520 (89%)] Loss: -1142.702637\n",
      "Train Epoch: 893 [101376/101520 (100%)] Loss: -1125.532959\n",
      "    epoch          : 893\n",
      "    loss           : -1141.4793805453046\n",
      "    ess            : 1.9688561807325737\n",
      "    log_marginal   : 1141.506392435812\n",
      "    log_joint      : 1349.7651060478172\n",
      "    val_loss       : -1141.0017514436142\n",
      "    val_ess        : 1.9682139780210413\n",
      "    val_log_marginal: 1141.0300611413043\n",
      "    val_log_joint  : 1349.0541567595108\n",
      "Train Epoch: 894 [0/101520 (0%)] Loss: -1145.686279\n",
      "Train Epoch: 894 [11264/101520 (11%)] Loss: -1137.112793\n",
      "Train Epoch: 894 [22528/101520 (22%)] Loss: -1146.919434\n",
      "Train Epoch: 894 [33792/101520 (33%)] Loss: -1144.423340\n",
      "Train Epoch: 894 [45056/101520 (44%)] Loss: -1146.400391\n",
      "Train Epoch: 894 [56320/101520 (55%)] Loss: -1142.996216\n",
      "Train Epoch: 894 [67584/101520 (67%)] Loss: -1138.614502\n",
      "Train Epoch: 894 [78848/101520 (78%)] Loss: -1133.587646\n",
      "Train Epoch: 894 [90112/101520 (89%)] Loss: -1142.131836\n",
      "Train Epoch: 894 [101376/101520 (100%)] Loss: -1127.747192\n",
      "    epoch          : 894\n",
      "    loss           : -1141.5058747104663\n",
      "    ess            : 1.9688243117164728\n",
      "    log_marginal   : 1141.5330546776854\n",
      "    log_joint      : 1349.8438315846813\n",
      "    val_loss       : -1140.3900783372962\n",
      "    val_ess        : 1.9710107989933179\n",
      "    val_log_marginal: 1140.415235436481\n",
      "    val_log_joint  : 1348.6709408967392\n",
      "Train Epoch: 895 [0/101520 (0%)] Loss: -1146.662598\n",
      "Train Epoch: 895 [11264/101520 (11%)] Loss: -1139.801758\n",
      "Train Epoch: 895 [22528/101520 (22%)] Loss: -1144.239258\n",
      "Train Epoch: 895 [33792/101520 (33%)] Loss: -1139.392090\n",
      "Train Epoch: 895 [45056/101520 (44%)] Loss: -1144.636230\n",
      "Train Epoch: 895 [56320/101520 (55%)] Loss: -1139.691650\n",
      "Train Epoch: 895 [67584/101520 (67%)] Loss: -1143.638184\n",
      "Train Epoch: 895 [78848/101520 (78%)] Loss: -1140.794678\n",
      "Train Epoch: 895 [90112/101520 (89%)] Loss: -1145.027344\n",
      "Train Epoch: 895 [101376/101520 (100%)] Loss: -1137.553345\n",
      "    epoch          : 895\n",
      "    loss           : -1141.6512162865106\n",
      "    ess            : 1.9684721195518071\n",
      "    log_marginal   : 1141.6797654041693\n",
      "    log_joint      : 1349.927011154405\n",
      "    val_loss       : -1141.5977358610733\n",
      "    val_ess        : 1.9689729006394097\n",
      "    val_log_marginal: 1141.624660326087\n",
      "    val_log_joint  : 1349.6866402004075\n",
      "Train Epoch: 896 [0/101520 (0%)] Loss: -1147.798218\n",
      "Train Epoch: 896 [11264/101520 (11%)] Loss: -1133.772217\n",
      "Train Epoch: 896 [22528/101520 (22%)] Loss: -1143.759277\n",
      "Train Epoch: 896 [33792/101520 (33%)] Loss: -1135.830688\n",
      "Train Epoch: 896 [45056/101520 (44%)] Loss: -1138.314087\n",
      "Train Epoch: 896 [56320/101520 (55%)] Loss: -1137.793945\n",
      "Train Epoch: 896 [67584/101520 (67%)] Loss: -1148.240112\n",
      "Train Epoch: 896 [78848/101520 (78%)] Loss: -1143.161377\n",
      "Train Epoch: 896 [90112/101520 (89%)] Loss: -1138.303833\n",
      "Train Epoch: 896 [101376/101520 (100%)] Loss: -1140.224976\n",
      "    epoch          : 896\n",
      "    loss           : -1141.7224967611494\n",
      "    ess            : 1.968127591526089\n",
      "    log_marginal   : 1141.750773520925\n",
      "    log_joint      : 1350.0544335446766\n",
      "    val_loss       : -1141.160400390625\n",
      "    val_ess        : 1.968572683956312\n",
      "    val_log_marginal: 1141.1896176545517\n",
      "    val_log_joint  : 1349.4578485903533\n",
      "Train Epoch: 897 [0/101520 (0%)] Loss: -1136.510010\n",
      "Train Epoch: 897 [11264/101520 (11%)] Loss: -1135.685059\n",
      "Train Epoch: 897 [22528/101520 (22%)] Loss: -1137.904541\n",
      "Train Epoch: 897 [33792/101520 (33%)] Loss: -1147.302002\n",
      "Train Epoch: 897 [45056/101520 (44%)] Loss: -1137.799561\n",
      "Train Epoch: 897 [56320/101520 (55%)] Loss: -1137.848389\n",
      "Train Epoch: 897 [67584/101520 (67%)] Loss: -1142.328003\n",
      "Train Epoch: 897 [78848/101520 (78%)] Loss: -1139.365967\n",
      "Train Epoch: 897 [90112/101520 (89%)] Loss: -1149.283691\n",
      "Train Epoch: 897 [101376/101520 (100%)] Loss: -1144.872314\n",
      "    epoch          : 897\n",
      "    loss           : -1141.7864186655936\n",
      "    ess            : 1.9680663137579684\n",
      "    log_marginal   : 1141.8143224668263\n",
      "    log_joint      : 1350.1534497438363\n",
      "    val_loss       : -1139.07788616678\n",
      "    val_ess        : 1.9672367054483164\n",
      "    val_log_marginal: 1139.1066204568615\n",
      "    val_log_joint  : 1347.4004808508832\n",
      "Train Epoch: 898 [0/101520 (0%)] Loss: -1140.418701\n",
      "Train Epoch: 898 [11264/101520 (11%)] Loss: -1139.079590\n",
      "Train Epoch: 898 [22528/101520 (22%)] Loss: -1147.002441\n",
      "Train Epoch: 898 [33792/101520 (33%)] Loss: -1138.341553\n",
      "Train Epoch: 898 [45056/101520 (44%)] Loss: -1139.821411\n",
      "Train Epoch: 898 [56320/101520 (55%)] Loss: -1141.796631\n",
      "Train Epoch: 898 [67584/101520 (67%)] Loss: -1137.099365\n",
      "Train Epoch: 898 [78848/101520 (78%)] Loss: -1141.005371\n",
      "Train Epoch: 898 [90112/101520 (89%)] Loss: -1136.213135\n",
      "Train Epoch: 898 [101376/101520 (100%)] Loss: -1141.871826\n",
      "    epoch          : 898\n",
      "    loss           : -1141.8824407682946\n",
      "    ess            : 1.967295301020445\n",
      "    log_marginal   : 1141.9123682376728\n",
      "    log_joint      : 1350.2220268824592\n",
      "    val_loss       : -1140.2493259595788\n",
      "    val_ess        : 1.9702645540237427\n",
      "    val_log_marginal: 1140.2781398607337\n",
      "    val_log_joint  : 1348.5792713994565\n",
      "Train Epoch: 899 [0/101520 (0%)] Loss: -1146.570801\n",
      "Train Epoch: 899 [11264/101520 (11%)] Loss: -1147.221924\n",
      "Train Epoch: 899 [22528/101520 (22%)] Loss: -1141.755371\n",
      "Train Epoch: 899 [33792/101520 (33%)] Loss: -1140.957520\n",
      "Train Epoch: 899 [45056/101520 (44%)] Loss: -1141.331055\n",
      "Train Epoch: 899 [56320/101520 (55%)] Loss: -1142.772217\n",
      "Train Epoch: 899 [67584/101520 (67%)] Loss: -1150.363037\n",
      "Train Epoch: 899 [78848/101520 (78%)] Loss: -1136.659424\n",
      "Train Epoch: 899 [90112/101520 (89%)] Loss: -1145.990967\n",
      "Train Epoch: 899 [101376/101520 (100%)] Loss: -1141.514404\n",
      "    epoch          : 899\n",
      "    loss           : -1141.9037865106784\n",
      "    ess            : 1.967463430447794\n",
      "    log_marginal   : 1141.9327300565326\n",
      "    log_joint      : 1350.288869886542\n",
      "    val_loss       : -1141.2777046535325\n",
      "    val_ess        : 1.971080842225448\n",
      "    val_log_marginal: 1141.3041089928668\n",
      "    val_log_joint  : 1349.7905485733695\n",
      "Train Epoch: 900 [0/101520 (0%)] Loss: -1147.054932\n",
      "Train Epoch: 900 [11264/101520 (11%)] Loss: -1140.739380\n",
      "Train Epoch: 900 [22528/101520 (22%)] Loss: -1140.518066\n",
      "Train Epoch: 900 [33792/101520 (33%)] Loss: -1145.100586\n",
      "Train Epoch: 900 [45056/101520 (44%)] Loss: -1141.161377\n",
      "Train Epoch: 900 [56320/101520 (55%)] Loss: -1133.927002\n",
      "Train Epoch: 900 [67584/101520 (67%)] Loss: -1141.660645\n",
      "Train Epoch: 900 [78848/101520 (78%)] Loss: -1146.912598\n",
      "Train Epoch: 900 [90112/101520 (89%)] Loss: -1138.744141\n",
      "Train Epoch: 900 [101376/101520 (100%)] Loss: -1146.922485\n",
      "    epoch          : 900\n",
      "    loss           : -1141.9630335515467\n",
      "    ess            : 1.9681129359719742\n",
      "    log_marginal   : 1141.9913422090924\n",
      "    log_joint      : 1350.3480561989636\n",
      "    val_loss       : -1141.2017397673233\n",
      "    val_ess        : 1.9710623803346052\n",
      "    val_log_marginal: 1141.2263926630435\n",
      "    val_log_joint  : 1349.785103175951\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [0/101520 (0%)] Loss: -1146.106934\n",
      "Train Epoch: 901 [11264/101520 (11%)] Loss: -1145.497437\n",
      "Train Epoch: 901 [22528/101520 (22%)] Loss: -1140.591553\n",
      "Train Epoch: 901 [33792/101520 (33%)] Loss: -1142.571533\n",
      "Train Epoch: 901 [45056/101520 (44%)] Loss: -1133.552979\n",
      "Train Epoch: 901 [56320/101520 (55%)] Loss: -1139.888916\n",
      "Train Epoch: 901 [67584/101520 (67%)] Loss: -1143.029785\n",
      "Train Epoch: 901 [78848/101520 (78%)] Loss: -1140.277954\n",
      "Train Epoch: 901 [90112/101520 (89%)] Loss: -1143.069092\n",
      "Train Epoch: 901 [101376/101520 (100%)] Loss: -1144.027100\n",
      "    epoch          : 901\n",
      "    loss           : -1142.144607927332\n",
      "    ess            : 1.9684621245417762\n",
      "    log_marginal   : 1142.1730135050252\n",
      "    log_joint      : 1350.5226179726758\n",
      "    val_loss       : -1144.4670516304348\n",
      "    val_ess        : 1.969039595645407\n",
      "    val_log_marginal: 1144.4968420940897\n",
      "    val_log_joint  : 1352.6809241253397\n",
      "Train Epoch: 902 [0/101520 (0%)] Loss: -1147.461670\n",
      "Train Epoch: 902 [11264/101520 (11%)] Loss: -1150.465820\n",
      "Train Epoch: 902 [22528/101520 (22%)] Loss: -1148.177490\n",
      "Train Epoch: 902 [33792/101520 (33%)] Loss: -1146.485474\n",
      "Train Epoch: 902 [45056/101520 (44%)] Loss: -1144.533203\n",
      "Train Epoch: 902 [56320/101520 (55%)] Loss: -1141.682373\n",
      "Train Epoch: 902 [67584/101520 (67%)] Loss: -1137.513428\n",
      "Train Epoch: 902 [78848/101520 (78%)] Loss: -1135.986206\n",
      "Train Epoch: 902 [90112/101520 (89%)] Loss: -1143.327759\n",
      "Train Epoch: 902 [101376/101520 (100%)] Loss: -1134.867676\n",
      "    epoch          : 902\n",
      "    loss           : -1142.1866473480684\n",
      "    ess            : 1.9683516145351545\n",
      "    log_marginal   : 1142.214747443271\n",
      "    log_joint      : 1350.5545783114792\n",
      "    val_loss       : -1141.7882080078125\n",
      "    val_ess        : 1.972494996112326\n",
      "    val_log_marginal: 1141.8144265879755\n",
      "    val_log_joint  : 1349.9634213654892\n",
      "Train Epoch: 903 [0/101520 (0%)] Loss: -1146.649658\n",
      "Train Epoch: 903 [11264/101520 (11%)] Loss: -1145.585327\n",
      "Train Epoch: 903 [22528/101520 (22%)] Loss: -1145.167480\n",
      "Train Epoch: 903 [33792/101520 (33%)] Loss: -1145.686401\n",
      "Train Epoch: 903 [45056/101520 (44%)] Loss: -1141.355957\n",
      "Train Epoch: 903 [56320/101520 (55%)] Loss: -1137.110596\n",
      "Train Epoch: 903 [67584/101520 (67%)] Loss: -1147.583618\n",
      "Train Epoch: 903 [78848/101520 (78%)] Loss: -1148.135254\n",
      "Train Epoch: 903 [90112/101520 (89%)] Loss: -1140.556641\n",
      "Train Epoch: 903 [101376/101520 (100%)] Loss: -1134.999146\n",
      "    epoch          : 903\n",
      "    loss           : -1142.2523585947315\n",
      "    ess            : 1.9682913659205987\n",
      "    log_marginal   : 1142.280312696294\n",
      "    log_joint      : 1350.5813086428234\n",
      "    val_loss       : -1140.6470628821332\n",
      "    val_ess        : 1.97008987095045\n",
      "    val_log_marginal: 1140.67309039572\n",
      "    val_log_joint  : 1349.2159583050272\n",
      "Train Epoch: 904 [0/101520 (0%)] Loss: -1138.438232\n",
      "Train Epoch: 904 [11264/101520 (11%)] Loss: -1132.384888\n",
      "Train Epoch: 904 [22528/101520 (22%)] Loss: -1133.287476\n",
      "Train Epoch: 904 [33792/101520 (33%)] Loss: -1140.711304\n",
      "Train Epoch: 904 [45056/101520 (44%)] Loss: -1139.684082\n",
      "Train Epoch: 904 [56320/101520 (55%)] Loss: -1147.743408\n",
      "Train Epoch: 904 [67584/101520 (67%)] Loss: -1141.307617\n",
      "Train Epoch: 904 [78848/101520 (78%)] Loss: -1142.642822\n",
      "Train Epoch: 904 [90112/101520 (89%)] Loss: -1142.981201\n",
      "Train Epoch: 904 [101376/101520 (100%)] Loss: -1151.950317\n",
      "    epoch          : 904\n",
      "    loss           : -1142.4005034940326\n",
      "    ess            : 1.967911183534555\n",
      "    log_marginal   : 1142.4290808289495\n",
      "    log_joint      : 1350.7631222518844\n",
      "    val_loss       : -1140.6198412024457\n",
      "    val_ess        : 1.9688283723333608\n",
      "    val_log_marginal: 1140.6445577870245\n",
      "    val_log_joint  : 1349.1062595533288\n",
      "Train Epoch: 905 [0/101520 (0%)] Loss: -1138.114868\n",
      "Train Epoch: 905 [11264/101520 (11%)] Loss: -1140.866455\n",
      "Train Epoch: 905 [22528/101520 (22%)] Loss: -1141.494385\n",
      "Train Epoch: 905 [33792/101520 (33%)] Loss: -1144.974854\n",
      "Train Epoch: 905 [45056/101520 (44%)] Loss: -1143.833008\n",
      "Train Epoch: 905 [56320/101520 (55%)] Loss: -1147.301758\n",
      "Train Epoch: 905 [67584/101520 (67%)] Loss: -1138.762939\n",
      "Train Epoch: 905 [78848/101520 (78%)] Loss: -1143.981445\n",
      "Train Epoch: 905 [90112/101520 (89%)] Loss: -1143.912842\n",
      "Train Epoch: 905 [101376/101520 (100%)] Loss: -1129.332642\n",
      "    epoch          : 905\n",
      "    loss           : -1142.4497113251805\n",
      "    ess            : 1.9674434661865234\n",
      "    log_marginal   : 1142.4790664749528\n",
      "    log_joint      : 1350.7976319586212\n",
      "    val_loss       : -1140.7198433254075\n",
      "    val_ess        : 1.9663087129592896\n",
      "    val_log_marginal: 1140.7482060971467\n",
      "    val_log_joint  : 1349.0456277598505\n",
      "Train Epoch: 906 [0/101520 (0%)] Loss: -1145.218872\n",
      "Train Epoch: 906 [11264/101520 (11%)] Loss: -1148.897583\n",
      "Train Epoch: 906 [22528/101520 (22%)] Loss: -1139.202881\n",
      "Train Epoch: 906 [33792/101520 (33%)] Loss: -1149.307739\n",
      "Train Epoch: 906 [45056/101520 (44%)] Loss: -1144.708984\n",
      "Train Epoch: 906 [56320/101520 (55%)] Loss: -1141.817139\n",
      "Train Epoch: 906 [67584/101520 (67%)] Loss: -1141.676514\n",
      "Train Epoch: 906 [78848/101520 (78%)] Loss: -1145.472778\n",
      "Train Epoch: 906 [90112/101520 (89%)] Loss: -1148.484009\n",
      "Train Epoch: 906 [101376/101520 (100%)] Loss: -1151.077759\n",
      "    epoch          : 906\n",
      "    loss           : -1142.5265229958386\n",
      "    ess            : 1.9675894987643066\n",
      "    log_marginal   : 1142.5559112702183\n",
      "    log_joint      : 1350.9347776480056\n",
      "    val_loss       : -1142.5104821246603\n",
      "    val_ess        : 1.9712295480396436\n",
      "    val_log_marginal: 1142.5369236158288\n",
      "    val_log_joint  : 1350.97045367697\n",
      "Train Epoch: 907 [0/101520 (0%)] Loss: -1140.084595\n",
      "Train Epoch: 907 [11264/101520 (11%)] Loss: -1145.141846\n",
      "Train Epoch: 907 [22528/101520 (22%)] Loss: -1142.583740\n",
      "Train Epoch: 907 [33792/101520 (33%)] Loss: -1134.150269\n",
      "Train Epoch: 907 [45056/101520 (44%)] Loss: -1142.770142\n",
      "Train Epoch: 907 [56320/101520 (55%)] Loss: -1137.242920\n",
      "Train Epoch: 907 [67584/101520 (67%)] Loss: -1133.959717\n",
      "Train Epoch: 907 [78848/101520 (78%)] Loss: -1139.181641\n",
      "Train Epoch: 907 [90112/101520 (89%)] Loss: -1146.616455\n",
      "Train Epoch: 907 [101376/101520 (100%)] Loss: -1156.612671\n",
      "    epoch          : 907\n",
      "    loss           : -1142.6992929736573\n",
      "    ess            : 1.9679311244332012\n",
      "    log_marginal   : 1142.7281009539886\n",
      "    log_joint      : 1351.0065316818468\n",
      "    val_loss       : -1144.0090597401495\n",
      "    val_ess        : 1.9708284709764563\n",
      "    val_log_marginal: 1144.0362018087635\n",
      "    val_log_joint  : 1352.5277948794158\n",
      "Train Epoch: 908 [0/101520 (0%)] Loss: -1145.345581\n",
      "Train Epoch: 908 [11264/101520 (11%)] Loss: -1142.527954\n",
      "Train Epoch: 908 [22528/101520 (22%)] Loss: -1142.722046\n",
      "Train Epoch: 908 [33792/101520 (33%)] Loss: -1136.064453\n",
      "Train Epoch: 908 [45056/101520 (44%)] Loss: -1139.531006\n",
      "Train Epoch: 908 [56320/101520 (55%)] Loss: -1140.634277\n",
      "Train Epoch: 908 [67584/101520 (67%)] Loss: -1143.810059\n",
      "Train Epoch: 908 [78848/101520 (78%)] Loss: -1133.082642\n",
      "Train Epoch: 908 [90112/101520 (89%)] Loss: -1140.569580\n",
      "Train Epoch: 908 [101376/101520 (100%)] Loss: -1138.255493\n",
      "    epoch          : 908\n",
      "    loss           : -1142.7298651460428\n",
      "    ess            : 1.9685925580748362\n",
      "    log_marginal   : 1142.758076270022\n",
      "    log_joint      : 1351.020807160804\n",
      "    val_loss       : -1142.0736402428668\n",
      "    val_ess        : 1.9685786236887393\n",
      "    val_log_marginal: 1142.1011379076087\n",
      "    val_log_joint  : 1350.4364119819973\n",
      "Train Epoch: 909 [0/101520 (0%)] Loss: -1152.442749\n",
      "Train Epoch: 909 [11264/101520 (11%)] Loss: -1138.991211\n",
      "Train Epoch: 909 [22528/101520 (22%)] Loss: -1143.822998\n",
      "Train Epoch: 909 [33792/101520 (33%)] Loss: -1142.522461\n",
      "Train Epoch: 909 [45056/101520 (44%)] Loss: -1141.371338\n",
      "Train Epoch: 909 [56320/101520 (55%)] Loss: -1143.742432\n",
      "Train Epoch: 909 [67584/101520 (67%)] Loss: -1137.798096\n",
      "Train Epoch: 909 [78848/101520 (78%)] Loss: -1136.249512\n",
      "Train Epoch: 909 [90112/101520 (89%)] Loss: -1141.048584\n",
      "Train Epoch: 909 [101376/101520 (100%)] Loss: -1138.612549\n",
      "    epoch          : 909\n",
      "    loss           : -1142.7088340874293\n",
      "    ess            : 1.9684634352449197\n",
      "    log_marginal   : 1142.7367753072\n",
      "    log_joint      : 1351.0968563520728\n",
      "    val_loss       : -1141.3386336616848\n",
      "    val_ess        : 1.9659252788709558\n",
      "    val_log_marginal: 1141.3717943274457\n",
      "    val_log_joint  : 1349.6270698879075\n",
      "Train Epoch: 910 [0/101520 (0%)] Loss: -1140.217163\n",
      "Train Epoch: 910 [11264/101520 (11%)] Loss: -1138.399414\n",
      "Train Epoch: 910 [22528/101520 (22%)] Loss: -1140.460205\n",
      "Train Epoch: 910 [33792/101520 (33%)] Loss: -1139.280029\n",
      "Train Epoch: 910 [45056/101520 (44%)] Loss: -1144.091675\n",
      "Train Epoch: 910 [56320/101520 (55%)] Loss: -1140.478027\n",
      "Train Epoch: 910 [67584/101520 (67%)] Loss: -1145.147461\n",
      "Train Epoch: 910 [78848/101520 (78%)] Loss: -1144.796265\n",
      "Train Epoch: 910 [90112/101520 (89%)] Loss: -1144.983398\n",
      "Train Epoch: 910 [101376/101520 (100%)] Loss: -1136.390869\n",
      "    epoch          : 910\n",
      "    loss           : -1142.8908734345555\n",
      "    ess            : 1.9683577301514208\n",
      "    log_marginal   : 1142.9192084691033\n",
      "    log_joint      : 1351.2396602151382\n",
      "    val_loss       : -1141.2504086701767\n",
      "    val_ess        : 1.967143644457278\n",
      "    val_log_marginal: 1141.2804538892663\n",
      "    val_log_joint  : 1349.622659434443\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch910.pth ...\n",
      "Train Epoch: 911 [0/101520 (0%)] Loss: -1142.485352\n",
      "Train Epoch: 911 [11264/101520 (11%)] Loss: -1142.030151\n",
      "Train Epoch: 911 [22528/101520 (22%)] Loss: -1142.252808\n",
      "Train Epoch: 911 [33792/101520 (33%)] Loss: -1143.407227\n",
      "Train Epoch: 911 [45056/101520 (44%)] Loss: -1143.413940\n",
      "Train Epoch: 911 [56320/101520 (55%)] Loss: -1148.221680\n",
      "Train Epoch: 911 [67584/101520 (67%)] Loss: -1143.660034\n",
      "Train Epoch: 911 [78848/101520 (78%)] Loss: -1143.465820\n",
      "Train Epoch: 911 [90112/101520 (89%)] Loss: -1134.373047\n",
      "Train Epoch: 911 [101376/101520 (100%)] Loss: -1143.116821\n",
      "    epoch          : 911\n",
      "    loss           : -1142.9816373125393\n",
      "    ess            : 1.9678214201376067\n",
      "    log_marginal   : 1143.0106446539337\n",
      "    log_joint      : 1351.2909125706658\n",
      "    val_loss       : -1142.8009670091712\n",
      "    val_ess        : 1.969232289687447\n",
      "    val_log_marginal: 1142.8297702955163\n",
      "    val_log_joint  : 1351.20068359375\n",
      "Train Epoch: 912 [0/101520 (0%)] Loss: -1144.509033\n",
      "Train Epoch: 912 [11264/101520 (11%)] Loss: -1143.696777\n",
      "Train Epoch: 912 [22528/101520 (22%)] Loss: -1153.955322\n",
      "Train Epoch: 912 [33792/101520 (33%)] Loss: -1139.257324\n",
      "Train Epoch: 912 [45056/101520 (44%)] Loss: -1143.974121\n",
      "Train Epoch: 912 [56320/101520 (55%)] Loss: -1144.764648\n",
      "Train Epoch: 912 [67584/101520 (67%)] Loss: -1141.811279\n",
      "Train Epoch: 912 [78848/101520 (78%)] Loss: -1145.904785\n",
      "Train Epoch: 912 [90112/101520 (89%)] Loss: -1142.087402\n",
      "Train Epoch: 912 [101376/101520 (100%)] Loss: -1150.286255\n",
      "    epoch          : 912\n",
      "    loss           : -1143.0052852151382\n",
      "    ess            : 1.9686847583732414\n",
      "    log_marginal   : 1143.0332951377984\n",
      "    log_joint      : 1351.360137479389\n",
      "    val_loss       : -1143.44262164572\n",
      "    val_ess        : 1.969074451405069\n",
      "    val_log_marginal: 1143.4709897248642\n",
      "    val_log_joint  : 1351.6951745074728\n",
      "Train Epoch: 913 [0/101520 (0%)] Loss: -1145.162598\n",
      "Train Epoch: 913 [11264/101520 (11%)] Loss: -1140.358643\n",
      "Train Epoch: 913 [22528/101520 (22%)] Loss: -1142.308594\n",
      "Train Epoch: 913 [33792/101520 (33%)] Loss: -1143.730957\n",
      "Train Epoch: 913 [45056/101520 (44%)] Loss: -1140.188843\n",
      "Train Epoch: 913 [56320/101520 (55%)] Loss: -1146.946289\n",
      "Train Epoch: 913 [67584/101520 (67%)] Loss: -1135.109741\n",
      "Train Epoch: 913 [78848/101520 (78%)] Loss: -1142.571655\n",
      "Train Epoch: 913 [90112/101520 (89%)] Loss: -1145.883545\n",
      "Train Epoch: 913 [101376/101520 (100%)] Loss: -1147.304321\n",
      "    epoch          : 913\n",
      "    loss           : -1143.187649674152\n",
      "    ess            : 1.9679270725154399\n",
      "    log_marginal   : 1143.217269207365\n",
      "    log_joint      : 1351.4718299750707\n",
      "    val_loss       : -1142.9334822944973\n",
      "    val_ess        : 1.9701881253201028\n",
      "    val_log_marginal: 1142.9600936226223\n",
      "    val_log_joint  : 1351.0737729279892\n",
      "Train Epoch: 914 [0/101520 (0%)] Loss: -1140.606201\n",
      "Train Epoch: 914 [11264/101520 (11%)] Loss: -1141.829834\n",
      "Train Epoch: 914 [22528/101520 (22%)] Loss: -1144.072998\n",
      "Train Epoch: 914 [33792/101520 (33%)] Loss: -1144.987305\n",
      "Train Epoch: 914 [45056/101520 (44%)] Loss: -1143.046387\n",
      "Train Epoch: 914 [56320/101520 (55%)] Loss: -1152.118164\n",
      "Train Epoch: 914 [67584/101520 (67%)] Loss: -1147.986328\n",
      "Train Epoch: 914 [78848/101520 (78%)] Loss: -1141.910400\n",
      "Train Epoch: 914 [90112/101520 (89%)] Loss: -1138.341797\n",
      "Train Epoch: 914 [101376/101520 (100%)] Loss: -1143.675293\n",
      "    epoch          : 914\n",
      "    loss           : -1143.1787379279208\n",
      "    ess            : 1.968174164618679\n",
      "    log_marginal   : 1143.2075618571373\n",
      "    log_joint      : 1351.487511409587\n",
      "    val_loss       : -1142.5268979279892\n",
      "    val_ess        : 1.9685087514960247\n",
      "    val_log_marginal: 1142.5548679517663\n",
      "    val_log_joint  : 1350.6282799762228\n",
      "Train Epoch: 915 [0/101520 (0%)] Loss: -1149.704102\n",
      "Train Epoch: 915 [11264/101520 (11%)] Loss: -1143.421387\n",
      "Train Epoch: 915 [22528/101520 (22%)] Loss: -1142.122437\n",
      "Train Epoch: 915 [33792/101520 (33%)] Loss: -1147.586426\n",
      "Train Epoch: 915 [45056/101520 (44%)] Loss: -1132.731079\n",
      "Train Epoch: 915 [56320/101520 (55%)] Loss: -1142.148926\n",
      "Train Epoch: 915 [67584/101520 (67%)] Loss: -1141.072998\n",
      "Train Epoch: 915 [78848/101520 (78%)] Loss: -1139.434448\n",
      "Train Epoch: 915 [90112/101520 (89%)] Loss: -1142.687744\n",
      "Train Epoch: 915 [101376/101520 (100%)] Loss: -1153.971313\n",
      "    epoch          : 915\n",
      "    loss           : -1143.288166295344\n",
      "    ess            : 1.9687746697334787\n",
      "    log_marginal   : 1143.3163289592494\n",
      "    log_joint      : 1351.629604933849\n",
      "    val_loss       : -1140.5410315472147\n",
      "    val_ess        : 1.9684844690820444\n",
      "    val_log_marginal: 1140.569479237432\n",
      "    val_log_joint  : 1349.0682691491168\n",
      "Train Epoch: 916 [0/101520 (0%)] Loss: -1146.231445\n",
      "Train Epoch: 916 [11264/101520 (11%)] Loss: -1139.962524\n",
      "Train Epoch: 916 [22528/101520 (22%)] Loss: -1137.519775\n",
      "Train Epoch: 916 [33792/101520 (33%)] Loss: -1131.727661\n",
      "Train Epoch: 916 [45056/101520 (44%)] Loss: -1148.167236\n",
      "Train Epoch: 916 [56320/101520 (55%)] Loss: -1145.483643\n",
      "Train Epoch: 916 [67584/101520 (67%)] Loss: -1141.802734\n",
      "Train Epoch: 916 [78848/101520 (78%)] Loss: -1143.517334\n",
      "Train Epoch: 916 [90112/101520 (89%)] Loss: -1140.031372\n",
      "Train Epoch: 916 [101376/101520 (100%)] Loss: -1157.995850\n",
      "    epoch          : 916\n",
      "    loss           : -1143.3801527167086\n",
      "    ess            : 1.9676996571334762\n",
      "    log_marginal   : 1143.4097311508715\n",
      "    log_joint      : 1351.725781618051\n",
      "    val_loss       : -1141.6610956606658\n",
      "    val_ess        : 1.9707498965056047\n",
      "    val_log_marginal: 1141.6871444038723\n",
      "    val_log_joint  : 1349.9266038977582\n",
      "Train Epoch: 917 [0/101520 (0%)] Loss: -1142.046265\n",
      "Train Epoch: 917 [11264/101520 (11%)] Loss: -1147.438721\n",
      "Train Epoch: 917 [22528/101520 (22%)] Loss: -1146.732544\n",
      "Train Epoch: 917 [33792/101520 (33%)] Loss: -1141.564697\n",
      "Train Epoch: 917 [45056/101520 (44%)] Loss: -1142.364380\n",
      "Train Epoch: 917 [56320/101520 (55%)] Loss: -1144.696899\n",
      "Train Epoch: 917 [67584/101520 (67%)] Loss: -1142.869629\n",
      "Train Epoch: 917 [78848/101520 (78%)] Loss: -1147.191406\n",
      "Train Epoch: 917 [90112/101520 (89%)] Loss: -1138.358276\n",
      "Train Epoch: 917 [101376/101520 (100%)] Loss: -1142.367554\n",
      "    epoch          : 917\n",
      "    loss           : -1143.3883731401147\n",
      "    ess            : 1.9691661374652805\n",
      "    log_marginal   : 1143.415602180826\n",
      "    log_joint      : 1351.7158104978016\n",
      "    val_loss       : -1142.6332689368207\n",
      "    val_ess        : 1.9632300086643384\n",
      "    val_log_marginal: 1142.6710205078125\n",
      "    val_log_joint  : 1350.9022641389267\n",
      "Train Epoch: 918 [0/101520 (0%)] Loss: -1139.115356\n",
      "Train Epoch: 918 [11264/101520 (11%)] Loss: -1137.360840\n",
      "Train Epoch: 918 [22528/101520 (22%)] Loss: -1152.411377\n",
      "Train Epoch: 918 [33792/101520 (33%)] Loss: -1142.206421\n",
      "Train Epoch: 918 [45056/101520 (44%)] Loss: -1146.739502\n",
      "Train Epoch: 918 [56320/101520 (55%)] Loss: -1136.304199\n",
      "Train Epoch: 918 [67584/101520 (67%)] Loss: -1144.230225\n",
      "Train Epoch: 918 [78848/101520 (78%)] Loss: -1148.619141\n",
      "Train Epoch: 918 [90112/101520 (89%)] Loss: -1144.859253\n",
      "Train Epoch: 918 [101376/101520 (100%)] Loss: -1154.485474\n",
      "    epoch          : 918\n",
      "    loss           : -1143.4289421963333\n",
      "    ess            : 1.9679545289907023\n",
      "    log_marginal   : 1143.4574373331502\n",
      "    log_joint      : 1351.7889612859217\n",
      "    val_loss       : -1141.6284391983695\n",
      "    val_ess        : 1.968309480211009\n",
      "    val_log_marginal: 1141.6577360733695\n",
      "    val_log_joint  : 1350.2688625169837\n",
      "Train Epoch: 919 [0/101520 (0%)] Loss: -1140.646118\n",
      "Train Epoch: 919 [11264/101520 (11%)] Loss: -1144.527344\n",
      "Train Epoch: 919 [22528/101520 (22%)] Loss: -1147.832642\n",
      "Train Epoch: 919 [33792/101520 (33%)] Loss: -1144.898193\n",
      "Train Epoch: 919 [45056/101520 (44%)] Loss: -1136.509033\n",
      "Train Epoch: 919 [56320/101520 (55%)] Loss: -1143.297119\n",
      "Train Epoch: 919 [67584/101520 (67%)] Loss: -1149.615967\n",
      "Train Epoch: 919 [78848/101520 (78%)] Loss: -1142.428345\n",
      "Train Epoch: 919 [90112/101520 (89%)] Loss: -1146.495605\n",
      "Train Epoch: 919 [101376/101520 (100%)] Loss: -1145.136108\n",
      "    epoch          : 919\n",
      "    loss           : -1143.6303349020493\n",
      "    ess            : 1.9681469567457037\n",
      "    log_marginal   : 1143.6588281986103\n",
      "    log_joint      : 1351.949069075848\n",
      "    val_loss       : -1141.2083421790082\n",
      "    val_ess        : 1.9672878980636597\n",
      "    val_log_marginal: 1141.239406419837\n",
      "    val_log_joint  : 1349.5218134341033\n",
      "Train Epoch: 920 [0/101520 (0%)] Loss: -1147.488281\n",
      "Train Epoch: 920 [11264/101520 (11%)] Loss: -1144.567261\n",
      "Train Epoch: 920 [22528/101520 (22%)] Loss: -1146.910767\n",
      "Train Epoch: 920 [33792/101520 (33%)] Loss: -1141.193359\n",
      "Train Epoch: 920 [45056/101520 (44%)] Loss: -1139.258179\n",
      "Train Epoch: 920 [56320/101520 (55%)] Loss: -1146.637817\n",
      "Train Epoch: 920 [67584/101520 (67%)] Loss: -1141.228760\n",
      "Train Epoch: 920 [78848/101520 (78%)] Loss: -1141.171265\n",
      "Train Epoch: 920 [90112/101520 (89%)] Loss: -1144.510864\n",
      "Train Epoch: 920 [101376/101520 (100%)] Loss: -1142.313599\n",
      "    epoch          : 920\n",
      "    loss           : -1143.5836568094378\n",
      "    ess            : 1.968517216006715\n",
      "    log_marginal   : 1143.611611524419\n",
      "    log_joint      : 1351.9934891743876\n",
      "    val_loss       : -1141.2562255859375\n",
      "    val_ess        : 1.9631382123283718\n",
      "    val_log_marginal: 1141.2902460512908\n",
      "    val_log_joint  : 1349.7915782099185\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch920.pth ...\n",
      "Train Epoch: 921 [0/101520 (0%)] Loss: -1147.944580\n",
      "Train Epoch: 921 [11264/101520 (11%)] Loss: -1143.426758\n",
      "Train Epoch: 921 [22528/101520 (22%)] Loss: -1139.950928\n",
      "Train Epoch: 921 [33792/101520 (33%)] Loss: -1143.153076\n",
      "Train Epoch: 921 [45056/101520 (44%)] Loss: -1151.116333\n",
      "Train Epoch: 921 [56320/101520 (55%)] Loss: -1144.512085\n",
      "Train Epoch: 921 [67584/101520 (67%)] Loss: -1143.289795\n",
      "Train Epoch: 921 [78848/101520 (78%)] Loss: -1143.510742\n",
      "Train Epoch: 921 [90112/101520 (89%)] Loss: -1146.774170\n",
      "Train Epoch: 921 [101376/101520 (100%)] Loss: -1143.782715\n",
      "    epoch          : 921\n",
      "    loss           : -1143.6330738163474\n",
      "    ess            : 1.9676074562360293\n",
      "    log_marginal   : 1143.662798857569\n",
      "    log_joint      : 1352.018675530975\n",
      "    val_loss       : -1142.0044263756793\n",
      "    val_ess        : 1.9705923381059065\n",
      "    val_log_marginal: 1142.0291323454483\n",
      "    val_log_joint  : 1350.5901621942935\n",
      "Train Epoch: 922 [0/101520 (0%)] Loss: -1149.152832\n",
      "Train Epoch: 922 [11264/101520 (11%)] Loss: -1139.163818\n",
      "Train Epoch: 922 [22528/101520 (22%)] Loss: -1141.037598\n",
      "Train Epoch: 922 [33792/101520 (33%)] Loss: -1145.341675\n",
      "Train Epoch: 922 [45056/101520 (44%)] Loss: -1146.621338\n",
      "Train Epoch: 922 [56320/101520 (55%)] Loss: -1142.808594\n",
      "Train Epoch: 922 [67584/101520 (67%)] Loss: -1140.852905\n",
      "Train Epoch: 922 [78848/101520 (78%)] Loss: -1137.848877\n",
      "Train Epoch: 922 [90112/101520 (89%)] Loss: -1150.847900\n",
      "Train Epoch: 922 [101376/101520 (100%)] Loss: -1157.119629\n",
      "    epoch          : 922\n",
      "    loss           : -1143.8978578193703\n",
      "    ess            : 1.9679234848549618\n",
      "    log_marginal   : 1143.9263719721655\n",
      "    log_joint      : 1352.2175501531092\n",
      "    val_loss       : -1142.9995011039402\n",
      "    val_ess        : 1.969048779943715\n",
      "    val_log_marginal: 1143.02709430197\n",
      "    val_log_joint  : 1351.3592900815217\n",
      "Train Epoch: 923 [0/101520 (0%)] Loss: -1145.028320\n",
      "Train Epoch: 923 [11264/101520 (11%)] Loss: -1141.338867\n",
      "Train Epoch: 923 [22528/101520 (22%)] Loss: -1145.684570\n",
      "Train Epoch: 923 [33792/101520 (33%)] Loss: -1147.527344\n",
      "Train Epoch: 923 [45056/101520 (44%)] Loss: -1143.396729\n",
      "Train Epoch: 923 [56320/101520 (55%)] Loss: -1148.836304\n",
      "Train Epoch: 923 [67584/101520 (67%)] Loss: -1138.508667\n",
      "Train Epoch: 923 [78848/101520 (78%)] Loss: -1144.453735\n",
      "Train Epoch: 923 [90112/101520 (89%)] Loss: -1135.812744\n",
      "Train Epoch: 923 [101376/101520 (100%)] Loss: -1144.534302\n",
      "    epoch          : 923\n",
      "    loss           : -1143.8017639466866\n",
      "    ess            : 1.968377712982983\n",
      "    log_marginal   : 1143.8304253199592\n",
      "    log_joint      : 1352.1983102769707\n",
      "    val_loss       : -1140.9629277768342\n",
      "    val_ess        : 1.9665287577587625\n",
      "    val_log_marginal: 1140.993015455163\n",
      "    val_log_joint  : 1349.6270380434783\n",
      "Train Epoch: 924 [0/101520 (0%)] Loss: -1139.329102\n",
      "Train Epoch: 924 [11264/101520 (11%)] Loss: -1137.126953\n",
      "Train Epoch: 924 [22528/101520 (22%)] Loss: -1140.156982\n",
      "Train Epoch: 924 [33792/101520 (33%)] Loss: -1140.684326\n",
      "Train Epoch: 924 [45056/101520 (44%)] Loss: -1144.063232\n",
      "Train Epoch: 924 [56320/101520 (55%)] Loss: -1148.932373\n",
      "Train Epoch: 924 [67584/101520 (67%)] Loss: -1143.087158\n",
      "Train Epoch: 924 [78848/101520 (78%)] Loss: -1144.108398\n",
      "Train Epoch: 924 [90112/101520 (89%)] Loss: -1143.356689\n",
      "Train Epoch: 924 [101376/101520 (100%)] Loss: -1122.080322\n",
      "    epoch          : 924\n",
      "    loss           : -1143.897099020493\n",
      "    ess            : 1.9688911138467453\n",
      "    log_marginal   : 1143.924278987712\n",
      "    log_joint      : 1352.1654015929255\n",
      "    val_loss       : -1142.1297766644022\n",
      "    val_ess        : 1.9685713933861775\n",
      "    val_log_marginal: 1142.1559156334918\n",
      "    val_log_joint  : 1350.6289168648098\n",
      "Train Epoch: 925 [0/101520 (0%)] Loss: -1149.477661\n",
      "Train Epoch: 925 [11264/101520 (11%)] Loss: -1145.183228\n",
      "Train Epoch: 925 [22528/101520 (22%)] Loss: -1137.550293\n",
      "Train Epoch: 925 [33792/101520 (33%)] Loss: -1143.893555\n",
      "Train Epoch: 925 [45056/101520 (44%)] Loss: -1140.682617\n",
      "Train Epoch: 925 [56320/101520 (55%)] Loss: -1147.211914\n",
      "Train Epoch: 925 [67584/101520 (67%)] Loss: -1148.025635\n",
      "Train Epoch: 925 [78848/101520 (78%)] Loss: -1149.836914\n",
      "Train Epoch: 925 [90112/101520 (89%)] Loss: -1145.000488\n",
      "Train Epoch: 925 [101376/101520 (100%)] Loss: -1141.734497\n",
      "    epoch          : 925\n",
      "    loss           : -1144.0444930953597\n",
      "    ess            : 1.9676811563309713\n",
      "    log_marginal   : 1144.0742156829067\n",
      "    log_joint      : 1352.431067078557\n",
      "    val_loss       : -1142.6888905400815\n",
      "    val_ess        : 1.9710755451865818\n",
      "    val_log_marginal: 1142.713723887568\n",
      "    val_log_joint  : 1351.0507175611413\n",
      "Train Epoch: 926 [0/101520 (0%)] Loss: -1153.517822\n",
      "Train Epoch: 926 [11264/101520 (11%)] Loss: -1143.745972\n",
      "Train Epoch: 926 [22528/101520 (22%)] Loss: -1146.005127\n",
      "Train Epoch: 926 [33792/101520 (33%)] Loss: -1150.385376\n",
      "Train Epoch: 926 [45056/101520 (44%)] Loss: -1152.044189\n",
      "Train Epoch: 926 [56320/101520 (55%)] Loss: -1144.323242\n",
      "Train Epoch: 926 [67584/101520 (67%)] Loss: -1143.299561\n",
      "Train Epoch: 926 [78848/101520 (78%)] Loss: -1144.015381\n",
      "Train Epoch: 926 [90112/101520 (89%)] Loss: -1143.095459\n",
      "Train Epoch: 926 [101376/101520 (100%)] Loss: -1136.620728\n",
      "    epoch          : 926\n",
      "    loss           : -1144.0621368561558\n",
      "    ess            : 1.9679451108577863\n",
      "    log_marginal   : 1144.0905283252198\n",
      "    log_joint      : 1352.4007304589354\n",
      "    val_loss       : -1141.9348887567935\n",
      "    val_ess        : 1.9696293084517769\n",
      "    val_log_marginal: 1141.96312945822\n",
      "    val_log_joint  : 1350.2531844429348\n",
      "Train Epoch: 927 [0/101520 (0%)] Loss: -1150.801758\n",
      "Train Epoch: 927 [11264/101520 (11%)] Loss: -1141.772949\n",
      "Train Epoch: 927 [22528/101520 (22%)] Loss: -1147.311035\n",
      "Train Epoch: 927 [33792/101520 (33%)] Loss: -1142.147949\n",
      "Train Epoch: 927 [45056/101520 (44%)] Loss: -1149.170288\n",
      "Train Epoch: 927 [56320/101520 (55%)] Loss: -1141.703247\n",
      "Train Epoch: 927 [67584/101520 (67%)] Loss: -1143.949341\n",
      "Train Epoch: 927 [78848/101520 (78%)] Loss: -1144.588135\n",
      "Train Epoch: 927 [90112/101520 (89%)] Loss: -1155.468262\n",
      "Train Epoch: 927 [101376/101520 (100%)] Loss: -1146.521484\n",
      "    epoch          : 927\n",
      "    loss           : -1144.2083795442054\n",
      "    ess            : 1.9676910812531285\n",
      "    log_marginal   : 1144.2379573649498\n",
      "    log_joint      : 1352.5815975630103\n",
      "    val_loss       : -1141.8296110733695\n",
      "    val_ess        : 1.9681636768838633\n",
      "    val_log_marginal: 1141.8569919752038\n",
      "    val_log_joint  : 1350.0556746773098\n",
      "Train Epoch: 928 [0/101520 (0%)] Loss: -1148.293213\n",
      "Train Epoch: 928 [11264/101520 (11%)] Loss: -1142.472290\n",
      "Train Epoch: 928 [22528/101520 (22%)] Loss: -1142.852173\n",
      "Train Epoch: 928 [33792/101520 (33%)] Loss: -1141.466919\n",
      "Train Epoch: 928 [45056/101520 (44%)] Loss: -1137.453857\n",
      "Train Epoch: 928 [56320/101520 (55%)] Loss: -1145.818115\n",
      "Train Epoch: 928 [67584/101520 (67%)] Loss: -1131.022949\n",
      "Train Epoch: 928 [78848/101520 (78%)] Loss: -1143.092163\n",
      "Train Epoch: 928 [90112/101520 (89%)] Loss: -1145.604492\n",
      "Train Epoch: 928 [101376/101520 (100%)] Loss: -1141.894287\n",
      "    epoch          : 928\n",
      "    loss           : -1144.3171037070117\n",
      "    ess            : 1.9679406000741164\n",
      "    log_marginal   : 1144.345995725699\n",
      "    log_joint      : 1352.6831987083856\n",
      "    val_loss       : -1142.8486434273098\n",
      "    val_ess        : 1.9687146052070286\n",
      "    val_log_marginal: 1142.8767832880435\n",
      "    val_log_joint  : 1350.8589769446332\n",
      "Train Epoch: 929 [0/101520 (0%)] Loss: -1135.037842\n",
      "Train Epoch: 929 [11264/101520 (11%)] Loss: -1147.619019\n",
      "Train Epoch: 929 [22528/101520 (22%)] Loss: -1145.394531\n",
      "Train Epoch: 929 [33792/101520 (33%)] Loss: -1148.306641\n",
      "Train Epoch: 929 [45056/101520 (44%)] Loss: -1141.235352\n",
      "Train Epoch: 929 [56320/101520 (55%)] Loss: -1145.852539\n",
      "Train Epoch: 929 [67584/101520 (67%)] Loss: -1143.101074\n",
      "Train Epoch: 929 [78848/101520 (78%)] Loss: -1146.357178\n",
      "Train Epoch: 929 [90112/101520 (89%)] Loss: -1150.478882\n",
      "Train Epoch: 929 [101376/101520 (100%)] Loss: -1143.945801\n",
      "    epoch          : 929\n",
      "    loss           : -1144.3368490401224\n",
      "    ess            : 1.9685325035497772\n",
      "    log_marginal   : 1144.364952815837\n",
      "    log_joint      : 1352.657666383676\n",
      "    val_loss       : -1141.9390709918478\n",
      "    val_ess        : 1.9679529044939124\n",
      "    val_log_marginal: 1141.966552734375\n",
      "    val_log_joint  : 1349.9241359544837\n",
      "Train Epoch: 930 [0/101520 (0%)] Loss: -1146.089844\n",
      "Train Epoch: 930 [11264/101520 (11%)] Loss: -1138.802979\n",
      "Train Epoch: 930 [22528/101520 (22%)] Loss: -1145.641602\n",
      "Train Epoch: 930 [33792/101520 (33%)] Loss: -1142.269165\n",
      "Train Epoch: 930 [45056/101520 (44%)] Loss: -1143.165649\n",
      "Train Epoch: 930 [56320/101520 (55%)] Loss: -1139.514526\n",
      "Train Epoch: 930 [67584/101520 (67%)] Loss: -1143.912720\n",
      "Train Epoch: 930 [78848/101520 (78%)] Loss: -1143.458862\n",
      "Train Epoch: 930 [90112/101520 (89%)] Loss: -1139.532227\n",
      "Train Epoch: 930 [101376/101520 (100%)] Loss: -1149.827759\n",
      "    epoch          : 930\n",
      "    loss           : -1144.421089210702\n",
      "    ess            : 1.9674634502161688\n",
      "    log_marginal   : 1144.4503615489557\n",
      "    log_joint      : 1352.8044924328674\n",
      "    val_loss       : -1142.982570482337\n",
      "    val_ess        : 1.9702231884002686\n",
      "    val_log_marginal: 1143.0079027258832\n",
      "    val_log_joint  : 1351.3654519786005\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch930.pth ...\n",
      "Train Epoch: 931 [0/101520 (0%)] Loss: -1147.529297\n",
      "Train Epoch: 931 [11264/101520 (11%)] Loss: -1147.023193\n",
      "Train Epoch: 931 [22528/101520 (22%)] Loss: -1144.347656\n",
      "Train Epoch: 931 [33792/101520 (33%)] Loss: -1147.673218\n",
      "Train Epoch: 931 [45056/101520 (44%)] Loss: -1139.542114\n",
      "Train Epoch: 931 [56320/101520 (55%)] Loss: -1139.121582\n",
      "Train Epoch: 931 [67584/101520 (67%)] Loss: -1148.086548\n",
      "Train Epoch: 931 [78848/101520 (78%)] Loss: -1146.270020\n",
      "Train Epoch: 931 [90112/101520 (89%)] Loss: -1151.029053\n",
      "Train Epoch: 931 [101376/101520 (100%)] Loss: -1150.016479\n",
      "    epoch          : 931\n",
      "    loss           : -1144.6187461968043\n",
      "    ess            : 1.9683025769851914\n",
      "    log_marginal   : 1144.6464322344143\n",
      "    log_joint      : 1352.9156635226916\n",
      "    val_loss       : -1143.3886877972147\n",
      "    val_ess        : 1.968135911485423\n",
      "    val_log_marginal: 1143.4185950237772\n",
      "    val_log_joint  : 1351.9398671025815\n",
      "Train Epoch: 932 [0/101520 (0%)] Loss: -1149.950684\n",
      "Train Epoch: 932 [11264/101520 (11%)] Loss: -1153.379395\n",
      "Train Epoch: 932 [22528/101520 (22%)] Loss: -1143.060669\n",
      "Train Epoch: 932 [33792/101520 (33%)] Loss: -1136.007324\n",
      "Train Epoch: 932 [45056/101520 (44%)] Loss: -1140.104126\n",
      "Train Epoch: 932 [56320/101520 (55%)] Loss: -1139.994751\n",
      "Train Epoch: 932 [67584/101520 (67%)] Loss: -1149.975098\n",
      "Train Epoch: 932 [78848/101520 (78%)] Loss: -1140.616699\n",
      "Train Epoch: 932 [90112/101520 (89%)] Loss: -1140.076416\n",
      "Train Epoch: 932 [101376/101520 (100%)] Loss: -1157.379639\n",
      "    epoch          : 932\n",
      "    loss           : -1144.5216273015467\n",
      "    ess            : 1.9680204798827818\n",
      "    log_marginal   : 1144.5502015693703\n",
      "    log_joint      : 1352.9016444527324\n",
      "    val_loss       : -1146.2848218834918\n",
      "    val_ess        : 1.9706870732100115\n",
      "    val_log_marginal: 1146.3086521314538\n",
      "    val_log_joint  : 1354.5889892578125\n",
      "Train Epoch: 933 [0/101520 (0%)] Loss: -1151.472534\n",
      "Train Epoch: 933 [11264/101520 (11%)] Loss: -1146.937500\n",
      "Train Epoch: 933 [22528/101520 (22%)] Loss: -1145.236084\n",
      "Train Epoch: 933 [33792/101520 (33%)] Loss: -1153.244873\n",
      "Train Epoch: 933 [45056/101520 (44%)] Loss: -1144.338135\n",
      "Train Epoch: 933 [56320/101520 (55%)] Loss: -1148.316895\n",
      "Train Epoch: 933 [67584/101520 (67%)] Loss: -1140.387695\n",
      "Train Epoch: 933 [78848/101520 (78%)] Loss: -1151.313721\n",
      "Train Epoch: 933 [90112/101520 (89%)] Loss: -1142.307007\n",
      "Train Epoch: 933 [101376/101520 (100%)] Loss: -1127.982666\n",
      "    epoch          : 933\n",
      "    loss           : -1144.4773219122958\n",
      "    ess            : 1.9676340938213484\n",
      "    log_marginal   : 1144.5065764614087\n",
      "    log_joint      : 1352.9366025685065\n",
      "    val_loss       : -1143.1196607506793\n",
      "    val_ess        : 1.967460036277771\n",
      "    val_log_marginal: 1143.1477634595788\n",
      "    val_log_joint  : 1351.583400560462\n",
      "Train Epoch: 934 [0/101520 (0%)] Loss: -1150.176514\n",
      "Train Epoch: 934 [11264/101520 (11%)] Loss: -1140.342773\n",
      "Train Epoch: 934 [22528/101520 (22%)] Loss: -1141.498047\n",
      "Train Epoch: 934 [33792/101520 (33%)] Loss: -1147.446289\n",
      "Train Epoch: 934 [45056/101520 (44%)] Loss: -1147.354248\n",
      "Train Epoch: 934 [56320/101520 (55%)] Loss: -1134.970459\n",
      "Train Epoch: 934 [67584/101520 (67%)] Loss: -1142.562256\n",
      "Train Epoch: 934 [78848/101520 (78%)] Loss: -1137.381348\n",
      "Train Epoch: 934 [90112/101520 (89%)] Loss: -1144.738281\n",
      "Train Epoch: 934 [101376/101520 (100%)] Loss: -1145.857422\n",
      "    epoch          : 934\n",
      "    loss           : -1144.753553534273\n",
      "    ess            : 1.967585854793913\n",
      "    log_marginal   : 1144.783288390193\n",
      "    log_joint      : 1353.0895057563207\n",
      "    val_loss       : -1144.0100416100543\n",
      "    val_ess        : 1.9653505657030188\n",
      "    val_log_marginal: 1144.0474747367527\n",
      "    val_log_joint  : 1352.773681640625\n",
      "Train Epoch: 935 [0/101520 (0%)] Loss: -1153.267090\n",
      "Train Epoch: 935 [11264/101520 (11%)] Loss: -1143.641846\n",
      "Train Epoch: 935 [22528/101520 (22%)] Loss: -1143.331543\n",
      "Train Epoch: 935 [33792/101520 (33%)] Loss: -1143.093506\n",
      "Train Epoch: 935 [45056/101520 (44%)] Loss: -1143.036377\n",
      "Train Epoch: 935 [56320/101520 (55%)] Loss: -1144.795898\n",
      "Train Epoch: 935 [67584/101520 (67%)] Loss: -1140.565308\n",
      "Train Epoch: 935 [78848/101520 (78%)] Loss: -1140.414185\n",
      "Train Epoch: 935 [90112/101520 (89%)] Loss: -1140.880493\n",
      "Train Epoch: 935 [101376/101520 (100%)] Loss: -1144.572510\n",
      "    epoch          : 935\n",
      "    loss           : -1144.8587640350188\n",
      "    ess            : 1.9681242243129404\n",
      "    log_marginal   : 1144.8880296256673\n",
      "    log_joint      : 1353.159532403227\n",
      "    val_loss       : -1144.3817457116168\n",
      "    val_ess        : 1.968284264854763\n",
      "    val_log_marginal: 1144.4104322350543\n",
      "    val_log_joint  : 1352.5935483186142\n",
      "Train Epoch: 936 [0/101520 (0%)] Loss: -1145.698975\n",
      "Train Epoch: 936 [11264/101520 (11%)] Loss: -1137.696411\n",
      "Train Epoch: 936 [22528/101520 (22%)] Loss: -1141.618774\n",
      "Train Epoch: 936 [33792/101520 (33%)] Loss: -1142.139404\n",
      "Train Epoch: 936 [45056/101520 (44%)] Loss: -1142.793091\n",
      "Train Epoch: 936 [56320/101520 (55%)] Loss: -1149.201294\n",
      "Train Epoch: 936 [67584/101520 (67%)] Loss: -1139.832031\n",
      "Train Epoch: 936 [78848/101520 (78%)] Loss: -1141.956299\n",
      "Train Epoch: 936 [90112/101520 (89%)] Loss: -1145.948486\n",
      "Train Epoch: 936 [101376/101520 (100%)] Loss: -1148.553955\n",
      "    epoch          : 936\n",
      "    loss           : -1144.901203404719\n",
      "    ess            : 1.9689025423634592\n",
      "    log_marginal   : 1144.9282214294126\n",
      "    log_joint      : 1353.25\n",
      "    val_loss       : -1145.4608525815217\n",
      "    val_ess        : 1.9703239668970522\n",
      "    val_log_marginal: 1145.4891410495925\n",
      "    val_log_joint  : 1353.5606636379075\n",
      "Train Epoch: 937 [0/101520 (0%)] Loss: -1145.681152\n",
      "Train Epoch: 937 [11264/101520 (11%)] Loss: -1145.397339\n",
      "Train Epoch: 937 [22528/101520 (22%)] Loss: -1144.645386\n",
      "Train Epoch: 937 [33792/101520 (33%)] Loss: -1145.904785\n",
      "Train Epoch: 937 [45056/101520 (44%)] Loss: -1144.522095\n",
      "Train Epoch: 937 [56320/101520 (55%)] Loss: -1138.980225\n",
      "Train Epoch: 937 [67584/101520 (67%)] Loss: -1136.210449\n",
      "Train Epoch: 937 [78848/101520 (78%)] Loss: -1139.300781\n",
      "Train Epoch: 937 [90112/101520 (89%)] Loss: -1149.753418\n",
      "Train Epoch: 937 [101376/101520 (100%)] Loss: -1139.821899\n",
      "    epoch          : 937\n",
      "    loss           : -1144.9604731420775\n",
      "    ess            : 1.9680016813565737\n",
      "    log_marginal   : 1144.9892283684046\n",
      "    log_joint      : 1353.3469348696608\n",
      "    val_loss       : -1143.1317191745925\n",
      "    val_ess        : 1.9666120176729949\n",
      "    val_log_marginal: 1143.1624915081522\n",
      "    val_log_joint  : 1351.415033755095\n",
      "Train Epoch: 938 [0/101520 (0%)] Loss: -1151.335693\n",
      "Train Epoch: 938 [11264/101520 (11%)] Loss: -1138.055908\n",
      "Train Epoch: 938 [22528/101520 (22%)] Loss: -1146.583008\n",
      "Train Epoch: 938 [33792/101520 (33%)] Loss: -1139.312012\n",
      "Train Epoch: 938 [45056/101520 (44%)] Loss: -1141.860596\n",
      "Train Epoch: 938 [56320/101520 (55%)] Loss: -1144.358154\n",
      "Train Epoch: 938 [67584/101520 (67%)] Loss: -1141.797607\n",
      "Train Epoch: 938 [78848/101520 (78%)] Loss: -1146.661499\n",
      "Train Epoch: 938 [90112/101520 (89%)] Loss: -1160.105713\n",
      "Train Epoch: 938 [101376/101520 (100%)] Loss: -1146.876343\n",
      "    epoch          : 938\n",
      "    loss           : -1145.068623145022\n",
      "    ess            : 1.9683664018784337\n",
      "    log_marginal   : 1145.096858192329\n",
      "    log_joint      : 1353.3929516969613\n",
      "    val_loss       : -1144.2318062160325\n",
      "    val_ess        : 1.9701132411542146\n",
      "    val_log_marginal: 1144.2586722995925\n",
      "    val_log_joint  : 1352.7380530315897\n",
      "Train Epoch: 939 [0/101520 (0%)] Loss: -1137.200439\n",
      "Train Epoch: 939 [11264/101520 (11%)] Loss: -1142.239746\n",
      "Train Epoch: 939 [22528/101520 (22%)] Loss: -1147.425415\n",
      "Train Epoch: 939 [33792/101520 (33%)] Loss: -1135.279663\n",
      "Train Epoch: 939 [45056/101520 (44%)] Loss: -1149.747925\n",
      "Train Epoch: 939 [56320/101520 (55%)] Loss: -1150.760986\n",
      "Train Epoch: 939 [67584/101520 (67%)] Loss: -1140.912842\n",
      "Train Epoch: 939 [78848/101520 (78%)] Loss: -1141.242310\n",
      "Train Epoch: 939 [90112/101520 (89%)] Loss: -1140.886230\n",
      "Train Epoch: 939 [101376/101520 (100%)] Loss: -1138.779297\n",
      "    epoch          : 939\n",
      "    loss           : -1145.2200160961056\n",
      "    ess            : 1.968181687982837\n",
      "    log_marginal   : 1145.2484909901068\n",
      "    log_joint      : 1353.526572069331\n",
      "    val_loss       : -1146.10669476053\n",
      "    val_ess        : 1.9631003659704458\n",
      "    val_log_marginal: 1146.141845703125\n",
      "    val_log_joint  : 1354.420893130095\n",
      "Train Epoch: 940 [0/101520 (0%)] Loss: -1149.067505\n",
      "Train Epoch: 940 [11264/101520 (11%)] Loss: -1146.371338\n",
      "Train Epoch: 940 [22528/101520 (22%)] Loss: -1140.643066\n",
      "Train Epoch: 940 [33792/101520 (33%)] Loss: -1148.836426\n",
      "Train Epoch: 940 [45056/101520 (44%)] Loss: -1136.728271\n",
      "Train Epoch: 940 [56320/101520 (55%)] Loss: -1143.648315\n",
      "Train Epoch: 940 [67584/101520 (67%)] Loss: -1141.410400\n",
      "Train Epoch: 940 [78848/101520 (78%)] Loss: -1146.727661\n",
      "Train Epoch: 940 [90112/101520 (89%)] Loss: -1148.529419\n",
      "Train Epoch: 940 [101376/101520 (100%)] Loss: -1145.636597\n",
      "    epoch          : 940\n",
      "    loss           : -1145.269734291575\n",
      "    ess            : 1.967384486941237\n",
      "    log_marginal   : 1145.2998672562028\n",
      "    log_joint      : 1353.6176162796403\n",
      "    val_loss       : -1143.68530804178\n",
      "    val_ess        : 1.9673374217489492\n",
      "    val_log_marginal: 1143.7133682914402\n",
      "    val_log_joint  : 1351.9856593919837\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch940.pth ...\n",
      "Train Epoch: 941 [0/101520 (0%)] Loss: -1149.699829\n",
      "Train Epoch: 941 [11264/101520 (11%)] Loss: -1142.351196\n",
      "Train Epoch: 941 [22528/101520 (22%)] Loss: -1148.672607\n",
      "Train Epoch: 941 [33792/101520 (33%)] Loss: -1145.624268\n",
      "Train Epoch: 941 [45056/101520 (44%)] Loss: -1145.481689\n",
      "Train Epoch: 941 [56320/101520 (55%)] Loss: -1139.505615\n",
      "Train Epoch: 941 [67584/101520 (67%)] Loss: -1147.210449\n",
      "Train Epoch: 941 [78848/101520 (78%)] Loss: -1144.029663\n",
      "Train Epoch: 941 [90112/101520 (89%)] Loss: -1144.326538\n",
      "Train Epoch: 941 [101376/101520 (100%)] Loss: -1140.465942\n",
      "    epoch          : 941\n",
      "    loss           : -1145.3220668773556\n",
      "    ess            : 1.9675746862612777\n",
      "    log_marginal   : 1145.351471714039\n",
      "    log_joint      : 1353.6465923366834\n",
      "    val_loss       : -1144.8934697690217\n",
      "    val_ess        : 1.9677415516065515\n",
      "    val_log_marginal: 1144.9222093665082\n",
      "    val_log_joint  : 1353.2401441491168\n",
      "Train Epoch: 942 [0/101520 (0%)] Loss: -1145.254883\n",
      "Train Epoch: 942 [11264/101520 (11%)] Loss: -1149.194092\n",
      "Train Epoch: 942 [22528/101520 (22%)] Loss: -1142.011475\n",
      "Train Epoch: 942 [33792/101520 (33%)] Loss: -1150.145752\n",
      "Train Epoch: 942 [45056/101520 (44%)] Loss: -1143.214111\n",
      "Train Epoch: 942 [56320/101520 (55%)] Loss: -1148.354004\n",
      "Train Epoch: 942 [67584/101520 (67%)] Loss: -1143.750732\n",
      "Train Epoch: 942 [78848/101520 (78%)] Loss: -1145.570190\n",
      "Train Epoch: 942 [90112/101520 (89%)] Loss: -1143.676514\n",
      "Train Epoch: 942 [101376/101520 (100%)] Loss: -1153.117310\n",
      "    epoch          : 942\n",
      "    loss           : -1145.341360120917\n",
      "    ess            : 1.9683554304305033\n",
      "    log_marginal   : 1145.3692283438677\n",
      "    log_joint      : 1353.7088708925487\n",
      "    val_loss       : -1144.1304878566575\n",
      "    val_ess        : 1.9658239509748376\n",
      "    val_log_marginal: 1144.1619713824728\n",
      "    val_log_joint  : 1352.479837168818\n",
      "Train Epoch: 943 [0/101520 (0%)] Loss: -1151.895630\n",
      "Train Epoch: 943 [11264/101520 (11%)] Loss: -1142.632324\n",
      "Train Epoch: 943 [22528/101520 (22%)] Loss: -1144.213623\n",
      "Train Epoch: 943 [33792/101520 (33%)] Loss: -1144.909180\n",
      "Train Epoch: 943 [45056/101520 (44%)] Loss: -1147.391235\n",
      "Train Epoch: 943 [56320/101520 (55%)] Loss: -1143.920288\n",
      "Train Epoch: 943 [67584/101520 (67%)] Loss: -1146.336304\n",
      "Train Epoch: 943 [78848/101520 (78%)] Loss: -1139.730957\n",
      "Train Epoch: 943 [90112/101520 (89%)] Loss: -1140.758057\n",
      "Train Epoch: 943 [101376/101520 (100%)] Loss: -1147.085083\n",
      "    epoch          : 943\n",
      "    loss           : -1145.4414583905857\n",
      "    ess            : 1.9680107095133719\n",
      "    log_marginal   : 1145.470587188874\n",
      "    log_joint      : 1353.840590280504\n",
      "    val_loss       : -1144.7786759086277\n",
      "    val_ess        : 1.965368099834608\n",
      "    val_log_marginal: 1144.8081903872283\n",
      "    val_log_joint  : 1353.1213644276495\n",
      "Train Epoch: 944 [0/101520 (0%)] Loss: -1144.537842\n",
      "Train Epoch: 944 [11264/101520 (11%)] Loss: -1143.782349\n",
      "Train Epoch: 944 [22528/101520 (22%)] Loss: -1149.361450\n",
      "Train Epoch: 944 [33792/101520 (33%)] Loss: -1148.756226\n",
      "Train Epoch: 944 [45056/101520 (44%)] Loss: -1142.547119\n",
      "Train Epoch: 944 [56320/101520 (55%)] Loss: -1145.642090\n",
      "Train Epoch: 944 [67584/101520 (67%)] Loss: -1143.031860\n",
      "Train Epoch: 944 [78848/101520 (78%)] Loss: -1143.302734\n",
      "Train Epoch: 944 [90112/101520 (89%)] Loss: -1144.218750\n",
      "Train Epoch: 944 [101376/101520 (100%)] Loss: -1146.805298\n",
      "    epoch          : 944\n",
      "    loss           : -1145.5550113850502\n",
      "    ess            : 1.9677883075110276\n",
      "    log_marginal   : 1145.5831801831423\n",
      "    log_joint      : 1353.898408669323\n",
      "    val_loss       : -1142.7874862007473\n",
      "    val_ess        : 1.966284850369329\n",
      "    val_log_marginal: 1142.8159445057745\n",
      "    val_log_joint  : 1351.482857082201\n",
      "Train Epoch: 945 [0/101520 (0%)] Loss: -1144.772583\n",
      "Train Epoch: 945 [11264/101520 (11%)] Loss: -1143.004395\n",
      "Train Epoch: 945 [22528/101520 (22%)] Loss: -1142.669678\n",
      "Train Epoch: 945 [33792/101520 (33%)] Loss: -1150.411133\n",
      "Train Epoch: 945 [45056/101520 (44%)] Loss: -1147.093750\n",
      "Train Epoch: 945 [56320/101520 (55%)] Loss: -1149.513062\n",
      "Train Epoch: 945 [67584/101520 (67%)] Loss: -1138.524048\n",
      "Train Epoch: 945 [78848/101520 (78%)] Loss: -1148.070312\n",
      "Train Epoch: 945 [90112/101520 (89%)] Loss: -1141.335938\n",
      "Train Epoch: 945 [101376/101520 (100%)] Loss: -1137.087524\n",
      "    epoch          : 945\n",
      "    loss           : -1145.576193344653\n",
      "    ess            : 1.9677319484739448\n",
      "    log_marginal   : 1145.6050810694096\n",
      "    log_joint      : 1353.9518214853565\n",
      "    val_loss       : -1142.5289625084918\n",
      "    val_ess        : 1.966680366060008\n",
      "    val_log_marginal: 1142.5585831351902\n",
      "    val_log_joint  : 1351.047856869905\n",
      "Train Epoch: 946 [0/101520 (0%)] Loss: -1149.962402\n",
      "Train Epoch: 946 [11264/101520 (11%)] Loss: -1141.450806\n",
      "Train Epoch: 946 [22528/101520 (22%)] Loss: -1148.838623\n",
      "Train Epoch: 946 [33792/101520 (33%)] Loss: -1153.551758\n",
      "Train Epoch: 946 [45056/101520 (44%)] Loss: -1137.845215\n",
      "Train Epoch: 946 [56320/101520 (55%)] Loss: -1155.007812\n",
      "Train Epoch: 946 [67584/101520 (67%)] Loss: -1152.354370\n",
      "Train Epoch: 946 [78848/101520 (78%)] Loss: -1141.367310\n",
      "Train Epoch: 946 [90112/101520 (89%)] Loss: -1142.036011\n",
      "Train Epoch: 946 [101376/101520 (100%)] Loss: -1132.660156\n",
      "    epoch          : 946\n",
      "    loss           : -1145.758993944331\n",
      "    ess            : 1.9677624073459874\n",
      "    log_marginal   : 1145.7883178097518\n",
      "    log_joint      : 1354.1043523280464\n",
      "    val_loss       : -1144.400783372962\n",
      "    val_ess        : 1.9706634334895923\n",
      "    val_log_marginal: 1144.4287746263587\n",
      "    val_log_joint  : 1352.631841244905\n",
      "Train Epoch: 947 [0/101520 (0%)] Loss: -1138.535156\n",
      "Train Epoch: 947 [11264/101520 (11%)] Loss: -1139.347900\n",
      "Train Epoch: 947 [22528/101520 (22%)] Loss: -1149.124634\n",
      "Train Epoch: 947 [33792/101520 (33%)] Loss: -1142.933594\n",
      "Train Epoch: 947 [45056/101520 (44%)] Loss: -1138.854126\n",
      "Train Epoch: 947 [56320/101520 (55%)] Loss: -1147.566650\n",
      "Train Epoch: 947 [67584/101520 (67%)] Loss: -1145.246338\n",
      "Train Epoch: 947 [78848/101520 (78%)] Loss: -1143.763428\n",
      "Train Epoch: 947 [90112/101520 (89%)] Loss: -1147.435303\n",
      "Train Epoch: 947 [101376/101520 (100%)] Loss: -1145.500000\n",
      "    epoch          : 947\n",
      "    loss           : -1145.8179275282664\n",
      "    ess            : 1.967727639567313\n",
      "    log_marginal   : 1145.8468980645414\n",
      "    log_joint      : 1354.181617315091\n",
      "    val_loss       : -1145.119968580163\n",
      "    val_ess        : 1.9695632613223533\n",
      "    val_log_marginal: 1145.146436608356\n",
      "    val_log_joint  : 1353.5663478685462\n",
      "Train Epoch: 948 [0/101520 (0%)] Loss: -1128.750977\n",
      "Train Epoch: 948 [11264/101520 (11%)] Loss: -1138.485596\n",
      "Train Epoch: 948 [22528/101520 (22%)] Loss: -1146.688965\n",
      "Train Epoch: 948 [33792/101520 (33%)] Loss: -1153.008911\n",
      "Train Epoch: 948 [45056/101520 (44%)] Loss: -1147.353027\n",
      "Train Epoch: 948 [56320/101520 (55%)] Loss: -1150.437134\n",
      "Train Epoch: 948 [67584/101520 (67%)] Loss: -1150.477295\n",
      "Train Epoch: 948 [78848/101520 (78%)] Loss: -1142.614380\n",
      "Train Epoch: 948 [90112/101520 (89%)] Loss: -1141.914551\n",
      "Train Epoch: 948 [101376/101520 (100%)] Loss: -1156.828125\n",
      "    epoch          : 948\n",
      "    loss           : -1145.8529230625786\n",
      "    ess            : 1.9677424598578832\n",
      "    log_marginal   : 1145.8823248321687\n",
      "    log_joint      : 1354.2218404031878\n",
      "    val_loss       : -1147.2290782099185\n",
      "    val_ess        : 1.9684355725412783\n",
      "    val_log_marginal: 1147.2574993631115\n",
      "    val_log_joint  : 1355.286918308424\n",
      "Train Epoch: 949 [0/101520 (0%)] Loss: -1146.168213\n",
      "Train Epoch: 949 [11264/101520 (11%)] Loss: -1142.924805\n",
      "Train Epoch: 949 [22528/101520 (22%)] Loss: -1142.638428\n",
      "Train Epoch: 949 [33792/101520 (33%)] Loss: -1151.996582\n",
      "Train Epoch: 949 [45056/101520 (44%)] Loss: -1150.313477\n",
      "Train Epoch: 949 [56320/101520 (55%)] Loss: -1152.322021\n",
      "Train Epoch: 949 [67584/101520 (67%)] Loss: -1147.789551\n",
      "Train Epoch: 949 [78848/101520 (78%)] Loss: -1147.570557\n",
      "Train Epoch: 949 [90112/101520 (89%)] Loss: -1145.403687\n",
      "Train Epoch: 949 [101376/101520 (100%)] Loss: -1149.838135\n",
      "    epoch          : 949\n",
      "    loss           : -1145.92743318644\n",
      "    ess            : 1.968631801293723\n",
      "    log_marginal   : 1145.9547131409\n",
      "    log_joint      : 1354.2936361494974\n",
      "    val_loss       : -1145.2383502462635\n",
      "    val_ess        : 1.9651515017385068\n",
      "    val_log_marginal: 1145.2729863705842\n",
      "    val_log_joint  : 1353.5240531589675\n",
      "Train Epoch: 950 [0/101520 (0%)] Loss: -1152.384033\n",
      "Train Epoch: 950 [11264/101520 (11%)] Loss: -1149.830444\n",
      "Train Epoch: 950 [22528/101520 (22%)] Loss: -1144.178955\n",
      "Train Epoch: 950 [33792/101520 (33%)] Loss: -1143.632568\n",
      "Train Epoch: 950 [45056/101520 (44%)] Loss: -1139.974731\n",
      "Train Epoch: 950 [56320/101520 (55%)] Loss: -1142.864624\n",
      "Train Epoch: 950 [67584/101520 (67%)] Loss: -1138.212402\n",
      "Train Epoch: 950 [78848/101520 (78%)] Loss: -1138.719238\n",
      "Train Epoch: 950 [90112/101520 (89%)] Loss: -1138.305664\n",
      "Train Epoch: 950 [101376/101520 (100%)] Loss: -1139.521240\n",
      "    epoch          : 950\n",
      "    loss           : -1145.9743173877198\n",
      "    ess            : 1.9685429586238\n",
      "    log_marginal   : 1146.0021782496467\n",
      "    log_joint      : 1354.2890545255575\n",
      "    val_loss       : -1145.2585343070652\n",
      "    val_ess        : 1.9675360503404036\n",
      "    val_log_marginal: 1145.2875286599865\n",
      "    val_log_joint  : 1353.5615022078805\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch950.pth ...\n",
      "Train Epoch: 951 [0/101520 (0%)] Loss: -1141.353027\n",
      "Train Epoch: 951 [11264/101520 (11%)] Loss: -1151.604980\n",
      "Train Epoch: 951 [22528/101520 (22%)] Loss: -1151.708008\n",
      "Train Epoch: 951 [33792/101520 (33%)] Loss: -1146.084595\n",
      "Train Epoch: 951 [45056/101520 (44%)] Loss: -1143.274414\n",
      "Train Epoch: 951 [56320/101520 (55%)] Loss: -1139.750854\n",
      "Train Epoch: 951 [67584/101520 (67%)] Loss: -1146.795776\n",
      "Train Epoch: 951 [78848/101520 (78%)] Loss: -1142.725342\n",
      "Train Epoch: 951 [90112/101520 (89%)] Loss: -1148.791992\n",
      "Train Epoch: 951 [101376/101520 (100%)] Loss: -1157.862183\n",
      "    epoch          : 951\n",
      "    loss           : -1146.053876560537\n",
      "    ess            : 1.9674503743349008\n",
      "    log_marginal   : 1146.0832077869818\n",
      "    log_joint      : 1354.4635009765625\n",
      "    val_loss       : -1144.4382005774457\n",
      "    val_ess        : 1.966118335723877\n",
      "    val_log_marginal: 1144.4669189453125\n",
      "    val_log_joint  : 1352.6365860648777\n",
      "Train Epoch: 952 [0/101520 (0%)] Loss: -1139.779053\n",
      "Train Epoch: 952 [11264/101520 (11%)] Loss: -1150.795288\n",
      "Train Epoch: 952 [22528/101520 (22%)] Loss: -1151.804077\n",
      "Train Epoch: 952 [33792/101520 (33%)] Loss: -1144.119385\n",
      "Train Epoch: 952 [45056/101520 (44%)] Loss: -1144.830322\n",
      "Train Epoch: 952 [56320/101520 (55%)] Loss: -1143.439209\n",
      "Train Epoch: 952 [67584/101520 (67%)] Loss: -1145.125977\n",
      "Train Epoch: 952 [78848/101520 (78%)] Loss: -1148.117554\n",
      "Train Epoch: 952 [90112/101520 (89%)] Loss: -1143.162354\n",
      "Train Epoch: 952 [101376/101520 (100%)] Loss: -1154.331543\n",
      "    epoch          : 952\n",
      "    loss           : -1146.187868664612\n",
      "    ess            : 1.9674122459325358\n",
      "    log_marginal   : 1146.217509667478\n",
      "    log_joint      : 1354.520876477112\n",
      "    val_loss       : -1144.857321034307\n",
      "    val_ess        : 1.967136237932288\n",
      "    val_log_marginal: 1144.8959854789402\n",
      "    val_log_joint  : 1353.2400486158288\n",
      "Train Epoch: 953 [0/101520 (0%)] Loss: -1147.817139\n",
      "Train Epoch: 953 [11264/101520 (11%)] Loss: -1136.753418\n",
      "Train Epoch: 953 [22528/101520 (22%)] Loss: -1153.048828\n",
      "Train Epoch: 953 [33792/101520 (33%)] Loss: -1140.630005\n",
      "Train Epoch: 953 [45056/101520 (44%)] Loss: -1144.581909\n",
      "Train Epoch: 953 [56320/101520 (55%)] Loss: -1146.132690\n",
      "Train Epoch: 953 [67584/101520 (67%)] Loss: -1147.056152\n",
      "Train Epoch: 953 [78848/101520 (78%)] Loss: -1145.516113\n",
      "Train Epoch: 953 [90112/101520 (89%)] Loss: -1152.226807\n",
      "Train Epoch: 953 [101376/101520 (100%)] Loss: -1146.085327\n",
      "    epoch          : 953\n",
      "    loss           : -1146.1613916751728\n",
      "    ess            : 1.96798697188871\n",
      "    log_marginal   : 1146.1904303009187\n",
      "    log_joint      : 1354.5675864674938\n",
      "    val_loss       : -1144.5925133746603\n",
      "    val_ess        : 1.9693003478257551\n",
      "    val_log_marginal: 1144.623482082201\n",
      "    val_log_joint  : 1352.8790283203125\n",
      "Train Epoch: 954 [0/101520 (0%)] Loss: -1145.307861\n",
      "Train Epoch: 954 [11264/101520 (11%)] Loss: -1150.958740\n",
      "Train Epoch: 954 [22528/101520 (22%)] Loss: -1142.294434\n",
      "Train Epoch: 954 [33792/101520 (33%)] Loss: -1145.921875\n",
      "Train Epoch: 954 [45056/101520 (44%)] Loss: -1143.360107\n",
      "Train Epoch: 954 [56320/101520 (55%)] Loss: -1140.935303\n",
      "Train Epoch: 954 [67584/101520 (67%)] Loss: -1144.651733\n",
      "Train Epoch: 954 [78848/101520 (78%)] Loss: -1143.789551\n",
      "Train Epoch: 954 [90112/101520 (89%)] Loss: -1142.205078\n",
      "Train Epoch: 954 [101376/101520 (100%)] Loss: -1125.280884\n",
      "    epoch          : 954\n",
      "    loss           : -1146.1900542752826\n",
      "    ess            : 1.9679190231927077\n",
      "    log_marginal   : 1146.2182175526068\n",
      "    log_joint      : 1354.596476645925\n",
      "    val_loss       : -1144.0905920940897\n",
      "    val_ess        : 1.9632297806117847\n",
      "    val_log_marginal: 1144.121438731318\n",
      "    val_log_joint  : 1352.508497155231\n",
      "Train Epoch: 955 [0/101520 (0%)] Loss: -1149.637451\n",
      "Train Epoch: 955 [11264/101520 (11%)] Loss: -1150.661133\n",
      "Train Epoch: 955 [22528/101520 (22%)] Loss: -1144.784546\n",
      "Train Epoch: 955 [33792/101520 (33%)] Loss: -1153.625000\n",
      "Train Epoch: 955 [45056/101520 (44%)] Loss: -1142.923584\n",
      "Train Epoch: 955 [56320/101520 (55%)] Loss: -1147.195923\n",
      "Train Epoch: 955 [67584/101520 (67%)] Loss: -1144.684570\n",
      "Train Epoch: 955 [78848/101520 (78%)] Loss: -1152.751587\n",
      "Train Epoch: 955 [90112/101520 (89%)] Loss: -1139.850098\n",
      "Train Epoch: 955 [101376/101520 (100%)] Loss: -1151.715088\n",
      "    epoch          : 955\n",
      "    loss           : -1146.4520797346106\n",
      "    ess            : 1.9683846169380685\n",
      "    log_marginal   : 1146.4798154591315\n",
      "    log_joint      : 1354.7511059938363\n",
      "    val_loss       : -1145.401759935462\n",
      "    val_ess        : 1.9680091038994167\n",
      "    val_log_marginal: 1145.4278617527175\n",
      "    val_log_joint  : 1353.6478377632473\n",
      "Train Epoch: 956 [0/101520 (0%)] Loss: -1149.943359\n",
      "Train Epoch: 956 [11264/101520 (11%)] Loss: -1145.914185\n",
      "Train Epoch: 956 [22528/101520 (22%)] Loss: -1146.574829\n",
      "Train Epoch: 956 [33792/101520 (33%)] Loss: -1150.857422\n",
      "Train Epoch: 956 [45056/101520 (44%)] Loss: -1149.300537\n",
      "Train Epoch: 956 [56320/101520 (55%)] Loss: -1135.540039\n",
      "Train Epoch: 956 [67584/101520 (67%)] Loss: -1146.540894\n",
      "Train Epoch: 956 [78848/101520 (78%)] Loss: -1150.661743\n",
      "Train Epoch: 956 [90112/101520 (89%)] Loss: -1143.669800\n",
      "Train Epoch: 956 [101376/101520 (100%)] Loss: -1154.003784\n",
      "    epoch          : 956\n",
      "    loss           : -1146.438852588136\n",
      "    ess            : 1.9684380760144948\n",
      "    log_marginal   : 1146.4668017823492\n",
      "    log_joint      : 1354.751475271867\n",
      "    val_loss       : -1144.2407067340353\n",
      "    val_ess        : 1.9692738989125127\n",
      "    val_log_marginal: 1144.2671269955842\n",
      "    val_log_joint  : 1352.743700110394\n",
      "Train Epoch: 957 [0/101520 (0%)] Loss: -1144.907593\n",
      "Train Epoch: 957 [11264/101520 (11%)] Loss: -1154.434814\n",
      "Train Epoch: 957 [22528/101520 (22%)] Loss: -1139.741943\n",
      "Train Epoch: 957 [33792/101520 (33%)] Loss: -1146.385010\n",
      "Train Epoch: 957 [45056/101520 (44%)] Loss: -1147.660645\n",
      "Train Epoch: 957 [56320/101520 (55%)] Loss: -1151.632202\n",
      "Train Epoch: 957 [67584/101520 (67%)] Loss: -1151.076172\n",
      "Train Epoch: 957 [78848/101520 (78%)] Loss: -1146.545410\n",
      "Train Epoch: 957 [90112/101520 (89%)] Loss: -1147.146484\n",
      "Train Epoch: 957 [101376/101520 (100%)] Loss: -1148.497681\n",
      "    epoch          : 957\n",
      "    loss           : -1146.4248789111573\n",
      "    ess            : 1.967998474686589\n",
      "    log_marginal   : 1146.454067824474\n",
      "    log_joint      : 1354.8336819596027\n",
      "    val_loss       : -1145.9723431131115\n",
      "    val_ess        : 1.9689786641494087\n",
      "    val_log_marginal: 1146.000928795856\n",
      "    val_log_joint  : 1354.3319728685462\n",
      "Train Epoch: 958 [0/101520 (0%)] Loss: -1143.797974\n",
      "Train Epoch: 958 [11264/101520 (11%)] Loss: -1145.480713\n",
      "Train Epoch: 958 [22528/101520 (22%)] Loss: -1145.033325\n",
      "Train Epoch: 958 [33792/101520 (33%)] Loss: -1148.168457\n",
      "Train Epoch: 958 [45056/101520 (44%)] Loss: -1146.596436\n",
      "Train Epoch: 958 [56320/101520 (55%)] Loss: -1143.279297\n",
      "Train Epoch: 958 [67584/101520 (67%)] Loss: -1148.601685\n",
      "Train Epoch: 958 [78848/101520 (78%)] Loss: -1151.333496\n",
      "Train Epoch: 958 [90112/101520 (89%)] Loss: -1146.811035\n",
      "Train Epoch: 958 [101376/101520 (100%)] Loss: -1153.123169\n",
      "    epoch          : 958\n",
      "    loss           : -1146.6335154777796\n",
      "    ess            : 1.9682078265664567\n",
      "    log_marginal   : 1146.662155381399\n",
      "    log_joint      : 1354.9831457090138\n",
      "    val_loss       : -1143.179878566576\n",
      "    val_ess        : 1.9686336465503858\n",
      "    val_log_marginal: 1143.207079016644\n",
      "    val_log_joint  : 1352.000583814538\n",
      "Train Epoch: 959 [0/101520 (0%)] Loss: -1150.942383\n",
      "Train Epoch: 959 [11264/101520 (11%)] Loss: -1147.646973\n",
      "Train Epoch: 959 [22528/101520 (22%)] Loss: -1150.706543\n",
      "Train Epoch: 959 [33792/101520 (33%)] Loss: -1146.237915\n",
      "Train Epoch: 959 [45056/101520 (44%)] Loss: -1145.065308\n",
      "Train Epoch: 959 [56320/101520 (55%)] Loss: -1141.606689\n",
      "Train Epoch: 959 [67584/101520 (67%)] Loss: -1147.378052\n",
      "Train Epoch: 959 [78848/101520 (78%)] Loss: -1146.781982\n",
      "Train Epoch: 959 [90112/101520 (89%)] Loss: -1146.728149\n",
      "Train Epoch: 959 [101376/101520 (100%)] Loss: -1139.159912\n",
      "    epoch          : 959\n",
      "    loss           : -1146.6523744209328\n",
      "    ess            : 1.9672358778852914\n",
      "    log_marginal   : 1146.6818767911825\n",
      "    log_joint      : 1354.988917365146\n",
      "    val_loss       : -1145.4470851732337\n",
      "    val_ess        : 1.966486967128256\n",
      "    val_log_marginal: 1145.4790410580842\n",
      "    val_log_joint  : 1353.5506697944973\n",
      "Train Epoch: 960 [0/101520 (0%)] Loss: -1144.982178\n",
      "Train Epoch: 960 [11264/101520 (11%)] Loss: -1132.179199\n",
      "Train Epoch: 960 [22528/101520 (22%)] Loss: -1144.923584\n",
      "Train Epoch: 960 [33792/101520 (33%)] Loss: -1144.593628\n",
      "Train Epoch: 960 [45056/101520 (44%)] Loss: -1145.450317\n",
      "Train Epoch: 960 [56320/101520 (55%)] Loss: -1143.664062\n",
      "Train Epoch: 960 [67584/101520 (67%)] Loss: -1155.750122\n",
      "Train Epoch: 960 [78848/101520 (78%)] Loss: -1149.192383\n",
      "Train Epoch: 960 [90112/101520 (89%)] Loss: -1145.333984\n",
      "Train Epoch: 960 [101376/101520 (100%)] Loss: -1153.380615\n",
      "    epoch          : 960\n",
      "    loss           : -1146.7709261640232\n",
      "    ess            : 1.9673836686503348\n",
      "    log_marginal   : 1146.8006076525205\n",
      "    log_joint      : 1355.11013257204\n",
      "    val_loss       : -1145.7113461701767\n",
      "    val_ess        : 1.9668603150740913\n",
      "    val_log_marginal: 1145.7429570737092\n",
      "    val_log_joint  : 1354.1787109375\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch960.pth ...\n",
      "Train Epoch: 961 [0/101520 (0%)] Loss: -1146.097534\n",
      "Train Epoch: 961 [11264/101520 (11%)] Loss: -1143.381958\n",
      "Train Epoch: 961 [22528/101520 (22%)] Loss: -1146.627808\n",
      "Train Epoch: 961 [33792/101520 (33%)] Loss: -1142.932861\n",
      "Train Epoch: 961 [45056/101520 (44%)] Loss: -1148.548096\n",
      "Train Epoch: 961 [56320/101520 (55%)] Loss: -1141.643555\n",
      "Train Epoch: 961 [67584/101520 (67%)] Loss: -1146.594360\n",
      "Train Epoch: 961 [78848/101520 (78%)] Loss: -1150.528809\n",
      "Train Epoch: 961 [90112/101520 (89%)] Loss: -1141.448730\n",
      "Train Epoch: 961 [101376/101520 (100%)] Loss: -1148.630615\n",
      "    epoch          : 961\n",
      "    loss           : -1146.8238887307632\n",
      "    ess            : 1.9676802589665705\n",
      "    log_marginal   : 1146.853925388662\n",
      "    log_joint      : 1355.2462980184123\n",
      "    val_loss       : -1146.3950036090353\n",
      "    val_ess        : 1.9682575775229412\n",
      "    val_log_marginal: 1146.4241571841033\n",
      "    val_log_joint  : 1354.8702976392663\n",
      "Train Epoch: 962 [0/101520 (0%)] Loss: -1145.734131\n",
      "Train Epoch: 962 [11264/101520 (11%)] Loss: -1152.547119\n",
      "Train Epoch: 962 [22528/101520 (22%)] Loss: -1143.419678\n",
      "Train Epoch: 962 [33792/101520 (33%)] Loss: -1146.372314\n",
      "Train Epoch: 962 [45056/101520 (44%)] Loss: -1145.235962\n",
      "Train Epoch: 962 [56320/101520 (55%)] Loss: -1145.322510\n",
      "Train Epoch: 962 [67584/101520 (67%)] Loss: -1142.828247\n",
      "Train Epoch: 962 [78848/101520 (78%)] Loss: -1148.550171\n",
      "Train Epoch: 962 [90112/101520 (89%)] Loss: -1146.830322\n",
      "Train Epoch: 962 [101376/101520 (100%)] Loss: -1140.176147\n",
      "    epoch          : 962\n",
      "    loss           : -1146.8990128866992\n",
      "    ess            : 1.9679197875698606\n",
      "    log_marginal   : 1146.9282839981156\n",
      "    log_joint      : 1355.201467542792\n",
      "    val_loss       : -1146.3914211107337\n",
      "    val_ess        : 1.9691452461740244\n",
      "    val_log_marginal: 1146.4189028532608\n",
      "    val_log_joint  : 1354.4966987941575\n",
      "Train Epoch: 963 [0/101520 (0%)] Loss: -1153.953613\n",
      "Train Epoch: 963 [11264/101520 (11%)] Loss: -1144.432373\n",
      "Train Epoch: 963 [22528/101520 (22%)] Loss: -1148.140015\n",
      "Train Epoch: 963 [33792/101520 (33%)] Loss: -1143.254639\n",
      "Train Epoch: 963 [45056/101520 (44%)] Loss: -1151.713379\n",
      "Train Epoch: 963 [56320/101520 (55%)] Loss: -1146.951172\n",
      "Train Epoch: 963 [67584/101520 (67%)] Loss: -1149.525757\n",
      "Train Epoch: 963 [78848/101520 (78%)] Loss: -1146.817749\n",
      "Train Epoch: 963 [90112/101520 (89%)] Loss: -1146.685913\n",
      "Train Epoch: 963 [101376/101520 (100%)] Loss: -1142.148315\n",
      "    epoch          : 963\n",
      "    loss           : -1146.8190991578988\n",
      "    ess            : 1.9682419581628925\n",
      "    log_marginal   : 1146.847610243601\n",
      "    log_joint      : 1355.230641734061\n",
      "    val_loss       : -1144.309379245924\n",
      "    val_ess        : 1.9681970917660256\n",
      "    val_log_marginal: 1144.339944590693\n",
      "    val_log_joint  : 1352.924947987432\n",
      "Train Epoch: 964 [0/101520 (0%)] Loss: -1145.246582\n",
      "Train Epoch: 964 [11264/101520 (11%)] Loss: -1153.963867\n",
      "Train Epoch: 964 [22528/101520 (22%)] Loss: -1143.987305\n",
      "Train Epoch: 964 [33792/101520 (33%)] Loss: -1144.384033\n",
      "Train Epoch: 964 [45056/101520 (44%)] Loss: -1153.284424\n",
      "Train Epoch: 964 [56320/101520 (55%)] Loss: -1152.514893\n",
      "Train Epoch: 964 [67584/101520 (67%)] Loss: -1151.306885\n",
      "Train Epoch: 964 [78848/101520 (78%)] Loss: -1158.602295\n",
      "Train Epoch: 964 [90112/101520 (89%)] Loss: -1144.764160\n",
      "Train Epoch: 964 [101376/101520 (100%)] Loss: -1150.929932\n",
      "    epoch          : 964\n",
      "    loss           : -1147.0150986867934\n",
      "    ess            : 1.9680652917929031\n",
      "    log_marginal   : 1147.0439140281485\n",
      "    log_joint      : 1355.419061248626\n",
      "    val_loss       : -1145.3933264690897\n",
      "    val_ess        : 1.9688886248547097\n",
      "    val_log_marginal: 1145.4223898182745\n",
      "    val_log_joint  : 1353.7366518766983\n",
      "Train Epoch: 965 [0/101520 (0%)] Loss: -1139.476807\n",
      "Train Epoch: 965 [11264/101520 (11%)] Loss: -1146.090576\n",
      "Train Epoch: 965 [22528/101520 (22%)] Loss: -1145.345093\n",
      "Train Epoch: 965 [33792/101520 (33%)] Loss: -1149.306641\n",
      "Train Epoch: 965 [45056/101520 (44%)] Loss: -1144.783569\n",
      "Train Epoch: 965 [56320/101520 (55%)] Loss: -1147.526611\n",
      "Train Epoch: 965 [67584/101520 (67%)] Loss: -1150.451172\n",
      "Train Epoch: 965 [78848/101520 (78%)] Loss: -1151.070190\n",
      "Train Epoch: 965 [90112/101520 (89%)] Loss: -1144.670410\n",
      "Train Epoch: 965 [101376/101520 (100%)] Loss: -1150.458984\n",
      "    epoch          : 965\n",
      "    loss           : -1147.1061134530073\n",
      "    ess            : 1.9678571919100967\n",
      "    log_marginal   : 1147.1353992864715\n",
      "    log_joint      : 1355.4839376324985\n",
      "    val_loss       : -1145.1119278617527\n",
      "    val_ess        : 1.9711214563120967\n",
      "    val_log_marginal: 1145.137212338655\n",
      "    val_log_joint  : 1353.5071172299592\n",
      "Train Epoch: 966 [0/101520 (0%)] Loss: -1150.701416\n",
      "Train Epoch: 966 [11264/101520 (11%)] Loss: -1144.289062\n",
      "Train Epoch: 966 [22528/101520 (22%)] Loss: -1147.464111\n",
      "Train Epoch: 966 [33792/101520 (33%)] Loss: -1148.768555\n",
      "Train Epoch: 966 [45056/101520 (44%)] Loss: -1143.964111\n",
      "Train Epoch: 966 [56320/101520 (55%)] Loss: -1145.795898\n",
      "Train Epoch: 966 [67584/101520 (67%)] Loss: -1147.110596\n",
      "Train Epoch: 966 [78848/101520 (78%)] Loss: -1145.942749\n",
      "Train Epoch: 966 [90112/101520 (89%)] Loss: -1146.370728\n",
      "Train Epoch: 966 [101376/101520 (100%)] Loss: -1153.042969\n",
      "    epoch          : 966\n",
      "    loss           : -1147.205398942957\n",
      "    ess            : 1.9674270500489814\n",
      "    log_marginal   : 1147.2350454665907\n",
      "    log_joint      : 1355.5350163905464\n",
      "    val_loss       : -1145.336277173913\n",
      "    val_ess        : 1.9683723657027534\n",
      "    val_log_marginal: 1145.363865064538\n",
      "    val_log_joint  : 1353.810446034307\n",
      "Train Epoch: 967 [0/101520 (0%)] Loss: -1152.053955\n",
      "Train Epoch: 967 [11264/101520 (11%)] Loss: -1149.726807\n",
      "Train Epoch: 967 [22528/101520 (22%)] Loss: -1140.090454\n",
      "Train Epoch: 967 [33792/101520 (33%)] Loss: -1150.777344\n",
      "Train Epoch: 967 [45056/101520 (44%)] Loss: -1154.119873\n",
      "Train Epoch: 967 [56320/101520 (55%)] Loss: -1148.822754\n",
      "Train Epoch: 967 [67584/101520 (67%)] Loss: -1136.191650\n",
      "Train Epoch: 967 [78848/101520 (78%)] Loss: -1150.913452\n",
      "Train Epoch: 967 [90112/101520 (89%)] Loss: -1153.261475\n",
      "Train Epoch: 967 [101376/101520 (100%)] Loss: -1143.994141\n",
      "    epoch          : 967\n",
      "    loss           : -1147.1874754632538\n",
      "    ess            : 1.9683061646456694\n",
      "    log_marginal   : 1147.2163000058888\n",
      "    log_joint      : 1355.5783918371153\n",
      "    val_loss       : -1146.5880763841712\n",
      "    val_ess        : 1.9668587653533272\n",
      "    val_log_marginal: 1146.6181534476902\n",
      "    val_log_joint  : 1355.047119140625\n",
      "Train Epoch: 968 [0/101520 (0%)] Loss: -1151.100586\n",
      "Train Epoch: 968 [11264/101520 (11%)] Loss: -1153.932861\n",
      "Train Epoch: 968 [22528/101520 (22%)] Loss: -1142.026367\n",
      "Train Epoch: 968 [33792/101520 (33%)] Loss: -1140.072876\n",
      "Train Epoch: 968 [45056/101520 (44%)] Loss: -1147.707031\n",
      "Train Epoch: 968 [56320/101520 (55%)] Loss: -1152.787842\n",
      "Train Epoch: 968 [67584/101520 (67%)] Loss: -1142.022705\n",
      "Train Epoch: 968 [78848/101520 (78%)] Loss: -1150.616943\n",
      "Train Epoch: 968 [90112/101520 (89%)] Loss: -1142.741699\n",
      "Train Epoch: 968 [101376/101520 (100%)] Loss: -1144.068481\n",
      "    epoch          : 968\n",
      "    loss           : -1147.3403847852544\n",
      "    ess            : 1.9680665168330898\n",
      "    log_marginal   : 1147.3693221969222\n",
      "    log_joint      : 1355.6619253494032\n",
      "    val_loss       : -1145.8781897503397\n",
      "    val_ess        : 1.9679988415344902\n",
      "    val_log_marginal: 1145.9080757472825\n",
      "    val_log_joint  : 1354.1238111413043\n",
      "Train Epoch: 969 [0/101520 (0%)] Loss: -1153.801758\n",
      "Train Epoch: 969 [11264/101520 (11%)] Loss: -1139.391479\n",
      "Train Epoch: 969 [22528/101520 (22%)] Loss: -1156.330811\n",
      "Train Epoch: 969 [33792/101520 (33%)] Loss: -1151.101318\n",
      "Train Epoch: 969 [45056/101520 (44%)] Loss: -1139.266479\n",
      "Train Epoch: 969 [56320/101520 (55%)] Loss: -1144.962646\n",
      "Train Epoch: 969 [67584/101520 (67%)] Loss: -1149.225098\n",
      "Train Epoch: 969 [78848/101520 (78%)] Loss: -1142.998535\n",
      "Train Epoch: 969 [90112/101520 (89%)] Loss: -1148.736816\n",
      "Train Epoch: 969 [101376/101520 (100%)] Loss: -1151.583618\n",
      "    epoch          : 969\n",
      "    loss           : -1147.33680916791\n",
      "    ess            : 1.9679680050317965\n",
      "    log_marginal   : 1147.3656159214038\n",
      "    log_joint      : 1355.752868959053\n",
      "    val_loss       : -1144.0999118970788\n",
      "    val_ess        : 1.9646564359250276\n",
      "    val_log_marginal: 1144.1316342561142\n",
      "    val_log_joint  : 1352.5307033372962\n",
      "Train Epoch: 970 [0/101520 (0%)] Loss: -1139.683105\n",
      "Train Epoch: 970 [11264/101520 (11%)] Loss: -1145.070923\n",
      "Train Epoch: 970 [22528/101520 (22%)] Loss: -1149.648438\n",
      "Train Epoch: 970 [33792/101520 (33%)] Loss: -1153.441162\n",
      "Train Epoch: 970 [45056/101520 (44%)] Loss: -1138.959229\n",
      "Train Epoch: 970 [56320/101520 (55%)] Loss: -1144.381592\n",
      "Train Epoch: 970 [67584/101520 (67%)] Loss: -1142.465576\n",
      "Train Epoch: 970 [78848/101520 (78%)] Loss: -1150.236084\n",
      "Train Epoch: 970 [90112/101520 (89%)] Loss: -1143.551758\n",
      "Train Epoch: 970 [101376/101520 (100%)] Loss: -1145.776001\n",
      "    epoch          : 970\n",
      "    loss           : -1147.3969751099246\n",
      "    ess            : 1.9676235620699936\n",
      "    log_marginal   : 1147.4266608923524\n",
      "    log_joint      : 1355.744024688874\n",
      "    val_loss       : -1144.3254023012908\n",
      "    val_ess        : 1.9677017875339673\n",
      "    val_log_marginal: 1144.3538871433425\n",
      "    val_log_joint  : 1352.7643299932065\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch970.pth ...\n",
      "Train Epoch: 971 [0/101520 (0%)] Loss: -1146.119629\n",
      "Train Epoch: 971 [11264/101520 (11%)] Loss: -1145.727417\n",
      "Train Epoch: 971 [22528/101520 (22%)] Loss: -1146.920776\n",
      "Train Epoch: 971 [33792/101520 (33%)] Loss: -1149.907959\n",
      "Train Epoch: 971 [45056/101520 (44%)] Loss: -1146.085938\n",
      "Train Epoch: 971 [56320/101520 (55%)] Loss: -1150.862061\n",
      "Train Epoch: 971 [67584/101520 (67%)] Loss: -1149.122803\n",
      "Train Epoch: 971 [78848/101520 (78%)] Loss: -1152.662842\n",
      "Train Epoch: 971 [90112/101520 (89%)] Loss: -1146.673096\n",
      "Train Epoch: 971 [101376/101520 (100%)] Loss: -1147.410156\n",
      "    epoch          : 971\n",
      "    loss           : -1147.4949221203674\n",
      "    ess            : 1.9673144050578975\n",
      "    log_marginal   : 1147.5245594427215\n",
      "    log_joint      : 1355.894779071137\n",
      "    val_loss       : -1146.6530177904212\n",
      "    val_ess        : 1.9687548357507456\n",
      "    val_log_marginal: 1146.6806322180707\n",
      "    val_log_joint  : 1355.1121720023777\n",
      "Train Epoch: 972 [0/101520 (0%)] Loss: -1147.476074\n",
      "Train Epoch: 972 [11264/101520 (11%)] Loss: -1149.306030\n",
      "Train Epoch: 972 [22528/101520 (22%)] Loss: -1146.557373\n",
      "Train Epoch: 972 [33792/101520 (33%)] Loss: -1153.966064\n",
      "Train Epoch: 972 [45056/101520 (44%)] Loss: -1147.750244\n",
      "Train Epoch: 972 [56320/101520 (55%)] Loss: -1140.236572\n",
      "Train Epoch: 972 [67584/101520 (67%)] Loss: -1152.146240\n",
      "Train Epoch: 972 [78848/101520 (78%)] Loss: -1143.833984\n",
      "Train Epoch: 972 [90112/101520 (89%)] Loss: -1147.370605\n",
      "Train Epoch: 972 [101376/101520 (100%)] Loss: -1133.872314\n",
      "    epoch          : 972\n",
      "    loss           : -1147.476688864243\n",
      "    ess            : 1.9678811673542962\n",
      "    log_marginal   : 1147.5056655347048\n",
      "    log_joint      : 1355.8487751256282\n",
      "    val_loss       : -1147.2308402683425\n",
      "    val_ess        : 1.9677077998285708\n",
      "    val_log_marginal: 1147.2570429262908\n",
      "    val_log_joint  : 1355.2748227326767\n",
      "Train Epoch: 973 [0/101520 (0%)] Loss: -1146.651978\n",
      "Train Epoch: 973 [11264/101520 (11%)] Loss: -1145.183105\n",
      "Train Epoch: 973 [22528/101520 (22%)] Loss: -1147.254028\n",
      "Train Epoch: 973 [33792/101520 (33%)] Loss: -1152.786743\n",
      "Train Epoch: 973 [45056/101520 (44%)] Loss: -1145.343262\n",
      "Train Epoch: 973 [56320/101520 (55%)] Loss: -1148.288818\n",
      "Train Epoch: 973 [67584/101520 (67%)] Loss: -1147.573486\n",
      "Train Epoch: 973 [78848/101520 (78%)] Loss: -1150.479492\n",
      "Train Epoch: 973 [90112/101520 (89%)] Loss: -1142.729248\n",
      "Train Epoch: 973 [101376/101520 (100%)] Loss: -1146.819092\n",
      "    epoch          : 973\n",
      "    loss           : -1147.5933696804334\n",
      "    ess            : 1.9678050872668549\n",
      "    log_marginal   : 1147.6218415073413\n",
      "    log_joint      : 1355.9613000304255\n",
      "    val_loss       : -1145.8098038383152\n",
      "    val_ess        : 1.9697623719339785\n",
      "    val_log_marginal: 1145.8370255180027\n",
      "    val_log_joint  : 1354.3525655995245\n",
      "Train Epoch: 974 [0/101520 (0%)] Loss: -1148.385010\n",
      "Train Epoch: 974 [11264/101520 (11%)] Loss: -1145.769287\n",
      "Train Epoch: 974 [22528/101520 (22%)] Loss: -1147.047852\n",
      "Train Epoch: 974 [33792/101520 (33%)] Loss: -1145.386963\n",
      "Train Epoch: 974 [45056/101520 (44%)] Loss: -1152.466064\n",
      "Train Epoch: 974 [56320/101520 (55%)] Loss: -1148.007446\n",
      "Train Epoch: 974 [67584/101520 (67%)] Loss: -1149.414917\n",
      "Train Epoch: 974 [78848/101520 (78%)] Loss: -1145.631592\n",
      "Train Epoch: 974 [90112/101520 (89%)] Loss: -1146.114258\n",
      "Train Epoch: 974 [101376/101520 (100%)] Loss: -1148.190186\n",
      "    epoch          : 974\n",
      "    loss           : -1147.7398632567133\n",
      "    ess            : 1.967571012937843\n",
      "    log_marginal   : 1147.769383416104\n",
      "    log_joint      : 1356.1368371398005\n",
      "    val_loss       : -1148.058981190557\n",
      "    val_ess        : 1.9634143476900847\n",
      "    val_log_marginal: 1148.0949017068615\n",
      "    val_log_joint  : 1356.30859375\n",
      "Train Epoch: 975 [0/101520 (0%)] Loss: -1145.014160\n",
      "Train Epoch: 975 [11264/101520 (11%)] Loss: -1150.869873\n",
      "Train Epoch: 975 [22528/101520 (22%)] Loss: -1145.207275\n",
      "Train Epoch: 975 [33792/101520 (33%)] Loss: -1143.688843\n",
      "Train Epoch: 975 [45056/101520 (44%)] Loss: -1144.128662\n",
      "Train Epoch: 975 [56320/101520 (55%)] Loss: -1142.648193\n",
      "Train Epoch: 975 [67584/101520 (67%)] Loss: -1147.550781\n",
      "Train Epoch: 975 [78848/101520 (78%)] Loss: -1154.958008\n",
      "Train Epoch: 975 [90112/101520 (89%)] Loss: -1148.120483\n",
      "Train Epoch: 975 [101376/101520 (100%)] Loss: -1157.046021\n",
      "    epoch          : 975\n",
      "    loss           : -1147.766346993758\n",
      "    ess            : 1.9674060350686462\n",
      "    log_marginal   : 1147.796032162767\n",
      "    log_joint      : 1356.1131450710584\n",
      "    val_loss       : -1145.845851732337\n",
      "    val_ess        : 1.9660036408382913\n",
      "    val_log_marginal: 1145.8799624235733\n",
      "    val_log_joint  : 1354.004638671875\n",
      "Train Epoch: 976 [0/101520 (0%)] Loss: -1153.112793\n",
      "Train Epoch: 976 [11264/101520 (11%)] Loss: -1152.524414\n",
      "Train Epoch: 976 [22528/101520 (22%)] Loss: -1147.547241\n",
      "Train Epoch: 976 [33792/101520 (33%)] Loss: -1150.083496\n",
      "Train Epoch: 976 [45056/101520 (44%)] Loss: -1152.639160\n",
      "Train Epoch: 976 [56320/101520 (55%)] Loss: -1154.511719\n",
      "Train Epoch: 976 [67584/101520 (67%)] Loss: -1150.564453\n",
      "Train Epoch: 976 [78848/101520 (78%)] Loss: -1147.721313\n",
      "Train Epoch: 976 [90112/101520 (89%)] Loss: -1151.623291\n",
      "Train Epoch: 976 [101376/101520 (100%)] Loss: -1149.901978\n",
      "    epoch          : 976\n",
      "    loss           : -1147.8067841649654\n",
      "    ess            : 1.966849656560313\n",
      "    log_marginal   : 1147.8373176919756\n",
      "    log_joint      : 1356.2104565797738\n",
      "    val_loss       : -1146.6726817255435\n",
      "    val_ess        : 1.9699796179066533\n",
      "    val_log_marginal: 1146.7001740828805\n",
      "    val_log_joint  : 1354.961330247962\n",
      "Train Epoch: 977 [0/101520 (0%)] Loss: -1150.875732\n",
      "Train Epoch: 977 [11264/101520 (11%)] Loss: -1146.055054\n",
      "Train Epoch: 977 [22528/101520 (22%)] Loss: -1146.388428\n",
      "Train Epoch: 977 [33792/101520 (33%)] Loss: -1151.954590\n",
      "Train Epoch: 977 [45056/101520 (44%)] Loss: -1149.914185\n",
      "Train Epoch: 977 [56320/101520 (55%)] Loss: -1152.849365\n",
      "Train Epoch: 977 [67584/101520 (67%)] Loss: -1147.817505\n",
      "Train Epoch: 977 [78848/101520 (78%)] Loss: -1155.303711\n",
      "Train Epoch: 977 [90112/101520 (89%)] Loss: -1147.170654\n",
      "Train Epoch: 977 [101376/101520 (100%)] Loss: -1154.078979\n",
      "    epoch          : 977\n",
      "    loss           : -1147.9330300182553\n",
      "    ess            : 1.967554418884929\n",
      "    log_marginal   : 1147.9617883116755\n",
      "    log_joint      : 1356.3192641675173\n",
      "    val_loss       : -1146.4856806216033\n",
      "    val_ess        : 1.9680223620456199\n",
      "    val_log_marginal: 1146.5160548997962\n",
      "    val_log_joint  : 1354.8665718410325\n",
      "Train Epoch: 978 [0/101520 (0%)] Loss: -1149.420898\n",
      "Train Epoch: 978 [11264/101520 (11%)] Loss: -1148.653809\n",
      "Train Epoch: 978 [22528/101520 (22%)] Loss: -1144.336792\n",
      "Train Epoch: 978 [33792/101520 (33%)] Loss: -1146.326660\n",
      "Train Epoch: 978 [45056/101520 (44%)] Loss: -1146.525757\n",
      "Train Epoch: 978 [56320/101520 (55%)] Loss: -1147.789795\n",
      "Train Epoch: 978 [67584/101520 (67%)] Loss: -1150.180176\n",
      "Train Epoch: 978 [78848/101520 (78%)] Loss: -1152.166016\n",
      "Train Epoch: 978 [90112/101520 (89%)] Loss: -1150.574951\n",
      "Train Epoch: 978 [101376/101520 (100%)] Loss: -1169.053833\n",
      "    epoch          : 978\n",
      "    loss           : -1148.042763868169\n",
      "    ess            : 1.9680703039744392\n",
      "    log_marginal   : 1148.0713215736887\n",
      "    log_joint      : 1356.4441267617383\n",
      "    val_loss       : -1145.7346775220788\n",
      "    val_ess        : 1.9689636696939883\n",
      "    val_log_marginal: 1145.759717858356\n",
      "    val_log_joint  : 1354.0485680621603\n",
      "Train Epoch: 979 [0/101520 (0%)] Loss: -1143.452759\n",
      "Train Epoch: 979 [11264/101520 (11%)] Loss: -1148.061768\n",
      "Train Epoch: 979 [22528/101520 (22%)] Loss: -1151.848511\n",
      "Train Epoch: 979 [33792/101520 (33%)] Loss: -1148.974365\n",
      "Train Epoch: 979 [45056/101520 (44%)] Loss: -1147.220459\n",
      "Train Epoch: 979 [56320/101520 (55%)] Loss: -1146.924805\n",
      "Train Epoch: 979 [67584/101520 (67%)] Loss: -1145.518188\n",
      "Train Epoch: 979 [78848/101520 (78%)] Loss: -1150.259766\n",
      "Train Epoch: 979 [90112/101520 (89%)] Loss: -1143.760986\n",
      "Train Epoch: 979 [101376/101520 (100%)] Loss: -1150.565674\n",
      "    epoch          : 979\n",
      "    loss           : -1148.0993443781408\n",
      "    ess            : 1.9676552058464318\n",
      "    log_marginal   : 1148.127765904719\n",
      "    log_joint      : 1356.487877007106\n",
      "    val_loss       : -1145.5614544412365\n",
      "    val_ess        : 1.9712809583415156\n",
      "    val_log_marginal: 1145.5863461701767\n",
      "    val_log_joint  : 1353.8148511803668\n",
      "Train Epoch: 980 [0/101520 (0%)] Loss: -1147.443359\n",
      "Train Epoch: 980 [11264/101520 (11%)] Loss: -1149.529297\n",
      "Train Epoch: 980 [22528/101520 (22%)] Loss: -1149.147461\n",
      "Train Epoch: 980 [33792/101520 (33%)] Loss: -1147.057861\n",
      "Train Epoch: 980 [45056/101520 (44%)] Loss: -1150.144165\n",
      "Train Epoch: 980 [56320/101520 (55%)] Loss: -1153.950928\n",
      "Train Epoch: 980 [67584/101520 (67%)] Loss: -1154.011963\n",
      "Train Epoch: 980 [78848/101520 (78%)] Loss: -1150.237305\n",
      "Train Epoch: 980 [90112/101520 (89%)] Loss: -1146.954102\n",
      "Train Epoch: 980 [101376/101520 (100%)] Loss: -1146.234985\n",
      "    epoch          : 980\n",
      "    loss           : -1148.1659788198806\n",
      "    ess            : 1.9676998470296811\n",
      "    log_marginal   : 1148.1956621486338\n",
      "    log_joint      : 1356.508628960231\n",
      "    val_loss       : -1146.9661971382473\n",
      "    val_ess        : 1.9697740285292915\n",
      "    val_log_marginal: 1146.9920335852582\n",
      "    val_log_joint  : 1355.29467242697\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch980.pth ...\n",
      "Train Epoch: 981 [0/101520 (0%)] Loss: -1147.292725\n",
      "Train Epoch: 981 [11264/101520 (11%)] Loss: -1151.473877\n",
      "Train Epoch: 981 [22528/101520 (22%)] Loss: -1153.157471\n",
      "Train Epoch: 981 [33792/101520 (33%)] Loss: -1149.093018\n",
      "Train Epoch: 981 [45056/101520 (44%)] Loss: -1151.256348\n",
      "Train Epoch: 981 [56320/101520 (55%)] Loss: -1148.472168\n",
      "Train Epoch: 981 [67584/101520 (67%)] Loss: -1147.449707\n",
      "Train Epoch: 981 [78848/101520 (78%)] Loss: -1145.016724\n",
      "Train Epoch: 981 [90112/101520 (89%)] Loss: -1153.932373\n",
      "Train Epoch: 981 [101376/101520 (100%)] Loss: -1140.628052\n",
      "    epoch          : 981\n",
      "    loss           : -1148.2095879789572\n",
      "    ess            : 1.967707191280384\n",
      "    log_marginal   : 1148.2390112182004\n",
      "    log_joint      : 1356.523000745917\n",
      "    val_loss       : -1146.5085555366848\n",
      "    val_ess        : 1.968162458875905\n",
      "    val_log_marginal: 1146.5357029127038\n",
      "    val_log_joint  : 1355.0947849439538\n",
      "Train Epoch: 982 [0/101520 (0%)] Loss: -1150.320068\n",
      "Train Epoch: 982 [11264/101520 (11%)] Loss: -1150.324585\n",
      "Train Epoch: 982 [22528/101520 (22%)] Loss: -1149.063843\n",
      "Train Epoch: 982 [33792/101520 (33%)] Loss: -1151.699707\n",
      "Train Epoch: 982 [45056/101520 (44%)] Loss: -1154.750244\n",
      "Train Epoch: 982 [56320/101520 (55%)] Loss: -1152.218750\n",
      "Train Epoch: 982 [67584/101520 (67%)] Loss: -1146.509155\n",
      "Train Epoch: 982 [78848/101520 (78%)] Loss: -1143.918213\n",
      "Train Epoch: 982 [90112/101520 (89%)] Loss: -1154.834473\n",
      "Train Epoch: 982 [101376/101520 (100%)] Loss: -1139.224487\n",
      "    epoch          : 982\n",
      "    loss           : -1148.2282653501884\n",
      "    ess            : 1.9672668357590335\n",
      "    log_marginal   : 1148.2580235160176\n",
      "    log_joint      : 1356.5972397387327\n",
      "    val_loss       : -1145.9218537703805\n",
      "    val_ess        : 1.9685886994652126\n",
      "    val_log_marginal: 1145.950391686481\n",
      "    val_log_joint  : 1354.2249915081522\n",
      "Train Epoch: 983 [0/101520 (0%)] Loss: -1154.916748\n",
      "Train Epoch: 983 [11264/101520 (11%)] Loss: -1145.984863\n",
      "Train Epoch: 983 [22528/101520 (22%)] Loss: -1143.196533\n",
      "Train Epoch: 983 [33792/101520 (33%)] Loss: -1149.446533\n",
      "Train Epoch: 983 [45056/101520 (44%)] Loss: -1153.867920\n",
      "Train Epoch: 983 [56320/101520 (55%)] Loss: -1144.846069\n",
      "Train Epoch: 983 [67584/101520 (67%)] Loss: -1145.302979\n",
      "Train Epoch: 983 [78848/101520 (78%)] Loss: -1146.285156\n",
      "Train Epoch: 983 [90112/101520 (89%)] Loss: -1150.899536\n",
      "Train Epoch: 983 [101376/101520 (100%)] Loss: -1152.024048\n",
      "    epoch          : 983\n",
      "    loss           : -1148.2527125372958\n",
      "    ess            : 1.9674974230665658\n",
      "    log_marginal   : 1148.281709450573\n",
      "    log_joint      : 1356.706261409587\n",
      "    val_loss       : -1146.6264011548913\n",
      "    val_ess        : 1.9672306102255117\n",
      "    val_log_marginal: 1146.6553902004075\n",
      "    val_log_joint  : 1355.0303636633832\n",
      "Train Epoch: 984 [0/101520 (0%)] Loss: -1147.285156\n",
      "Train Epoch: 984 [11264/101520 (11%)] Loss: -1147.427734\n",
      "Train Epoch: 984 [22528/101520 (22%)] Loss: -1151.580811\n",
      "Train Epoch: 984 [33792/101520 (33%)] Loss: -1144.178955\n",
      "Train Epoch: 984 [45056/101520 (44%)] Loss: -1150.638794\n",
      "Train Epoch: 984 [56320/101520 (55%)] Loss: -1150.822021\n",
      "Train Epoch: 984 [67584/101520 (67%)] Loss: -1147.675049\n",
      "Train Epoch: 984 [78848/101520 (78%)] Loss: -1150.809204\n",
      "Train Epoch: 984 [90112/101520 (89%)] Loss: -1149.438721\n",
      "Train Epoch: 984 [101376/101520 (100%)] Loss: -1144.620361\n",
      "    epoch          : 984\n",
      "    loss           : -1148.371824331619\n",
      "    ess            : 1.9675584953633984\n",
      "    log_marginal   : 1148.4010868551743\n",
      "    log_joint      : 1356.7770321333228\n",
      "    val_loss       : -1145.897121263587\n",
      "    val_ess        : 1.967937417652296\n",
      "    val_log_marginal: 1145.9249639096467\n",
      "    val_log_joint  : 1354.260153065557\n",
      "Train Epoch: 985 [0/101520 (0%)] Loss: -1146.554810\n",
      "Train Epoch: 985 [11264/101520 (11%)] Loss: -1148.082764\n",
      "Train Epoch: 985 [22528/101520 (22%)] Loss: -1146.114990\n",
      "Train Epoch: 985 [33792/101520 (33%)] Loss: -1150.742798\n",
      "Train Epoch: 985 [45056/101520 (44%)] Loss: -1153.068726\n",
      "Train Epoch: 985 [56320/101520 (55%)] Loss: -1148.161499\n",
      "Train Epoch: 985 [67584/101520 (67%)] Loss: -1151.024902\n",
      "Train Epoch: 985 [78848/101520 (78%)] Loss: -1152.231689\n",
      "Train Epoch: 985 [90112/101520 (89%)] Loss: -1146.503662\n",
      "Train Epoch: 985 [101376/101520 (100%)] Loss: -1156.162109\n",
      "    epoch          : 985\n",
      "    loss           : -1148.583640247134\n",
      "    ess            : 1.9673206314968703\n",
      "    log_marginal   : 1148.612913812225\n",
      "    log_joint      : 1356.9311566376805\n",
      "    val_loss       : -1146.2765370244565\n",
      "    val_ess        : 1.9697622060775757\n",
      "    val_log_marginal: 1146.3041089928668\n",
      "    val_log_joint  : 1354.5772280485733\n",
      "Train Epoch: 986 [0/101520 (0%)] Loss: -1156.166260\n",
      "Train Epoch: 986 [11264/101520 (11%)] Loss: -1148.165405\n",
      "Train Epoch: 986 [22528/101520 (22%)] Loss: -1148.677612\n",
      "Train Epoch: 986 [33792/101520 (33%)] Loss: -1143.560059\n",
      "Train Epoch: 986 [45056/101520 (44%)] Loss: -1151.753662\n",
      "Train Epoch: 986 [56320/101520 (55%)] Loss: -1152.706055\n",
      "Train Epoch: 986 [67584/101520 (67%)] Loss: -1151.627930\n",
      "Train Epoch: 986 [78848/101520 (78%)] Loss: -1158.625732\n",
      "Train Epoch: 986 [90112/101520 (89%)] Loss: -1154.322266\n",
      "Train Epoch: 986 [101376/101520 (100%)] Loss: -1159.180664\n",
      "    epoch          : 986\n",
      "    loss           : -1148.590435699003\n",
      "    ess            : 1.9670071290366014\n",
      "    log_marginal   : 1148.6201674878298\n",
      "    log_joint      : 1356.989649787021\n",
      "    val_loss       : -1147.0606052564538\n",
      "    val_ess        : 1.9687522183293882\n",
      "    val_log_marginal: 1147.0891219429348\n",
      "    val_log_joint  : 1355.5553296959918\n",
      "Train Epoch: 987 [0/101520 (0%)] Loss: -1149.556641\n",
      "Train Epoch: 987 [11264/101520 (11%)] Loss: -1143.465942\n",
      "Train Epoch: 987 [22528/101520 (22%)] Loss: -1148.343018\n",
      "Train Epoch: 987 [33792/101520 (33%)] Loss: -1144.235107\n",
      "Train Epoch: 987 [45056/101520 (44%)] Loss: -1149.693848\n",
      "Train Epoch: 987 [56320/101520 (55%)] Loss: -1150.728638\n",
      "Train Epoch: 987 [67584/101520 (67%)] Loss: -1142.436768\n",
      "Train Epoch: 987 [78848/101520 (78%)] Loss: -1151.734741\n",
      "Train Epoch: 987 [90112/101520 (89%)] Loss: -1150.882812\n",
      "Train Epoch: 987 [101376/101520 (100%)] Loss: -1147.854004\n",
      "    epoch          : 987\n",
      "    loss           : -1148.6255465560223\n",
      "    ess            : 1.967633975810142\n",
      "    log_marginal   : 1148.6548360699985\n",
      "    log_joint      : 1356.9548861249607\n",
      "    val_loss       : -1147.4107825237772\n",
      "    val_ess        : 1.9689059153847073\n",
      "    val_log_marginal: 1147.4373938519022\n",
      "    val_log_joint  : 1355.5597773012908\n",
      "Train Epoch: 988 [0/101520 (0%)] Loss: -1149.541626\n",
      "Train Epoch: 988 [11264/101520 (11%)] Loss: -1150.234131\n",
      "Train Epoch: 988 [22528/101520 (22%)] Loss: -1149.029297\n",
      "Train Epoch: 988 [33792/101520 (33%)] Loss: -1148.974121\n",
      "Train Epoch: 988 [45056/101520 (44%)] Loss: -1151.140381\n",
      "Train Epoch: 988 [56320/101520 (55%)] Loss: -1150.145630\n",
      "Train Epoch: 988 [67584/101520 (67%)] Loss: -1148.765991\n",
      "Train Epoch: 988 [78848/101520 (78%)] Loss: -1147.797852\n",
      "Train Epoch: 988 [90112/101520 (89%)] Loss: -1154.846191\n",
      "Train Epoch: 988 [101376/101520 (100%)] Loss: -1149.222900\n",
      "    epoch          : 988\n",
      "    loss           : -1148.707767352387\n",
      "    ess            : 1.9678383412672646\n",
      "    log_marginal   : 1148.73726358845\n",
      "    log_joint      : 1357.065162236966\n",
      "    val_loss       : -1147.0292756453805\n",
      "    val_ess        : 1.9680803599564924\n",
      "    val_log_marginal: 1147.058545983356\n",
      "    val_log_joint  : 1355.3077923318615\n",
      "Train Epoch: 989 [0/101520 (0%)] Loss: -1150.271118\n",
      "Train Epoch: 989 [11264/101520 (11%)] Loss: -1149.141846\n",
      "Train Epoch: 989 [22528/101520 (22%)] Loss: -1143.230713\n",
      "Train Epoch: 989 [33792/101520 (33%)] Loss: -1149.683228\n",
      "Train Epoch: 989 [45056/101520 (44%)] Loss: -1144.207397\n",
      "Train Epoch: 989 [56320/101520 (55%)] Loss: -1146.311401\n",
      "Train Epoch: 989 [67584/101520 (67%)] Loss: -1146.097778\n",
      "Train Epoch: 989 [78848/101520 (78%)] Loss: -1145.993652\n",
      "Train Epoch: 989 [90112/101520 (89%)] Loss: -1147.686890\n",
      "Train Epoch: 989 [101376/101520 (100%)] Loss: -1146.380493\n",
      "    epoch          : 989\n",
      "    loss           : -1148.7437738006438\n",
      "    ess            : 1.9675141075747695\n",
      "    log_marginal   : 1148.7728774487673\n",
      "    log_joint      : 1357.123216178549\n",
      "    val_loss       : -1146.4768278702445\n",
      "    val_ess        : 1.966126130974811\n",
      "    val_log_marginal: 1146.5080831776495\n",
      "    val_log_joint  : 1354.8433784816575\n",
      "Train Epoch: 990 [0/101520 (0%)] Loss: -1151.696045\n",
      "Train Epoch: 990 [11264/101520 (11%)] Loss: -1147.144653\n",
      "Train Epoch: 990 [22528/101520 (22%)] Loss: -1149.515137\n",
      "Train Epoch: 990 [33792/101520 (33%)] Loss: -1150.171143\n",
      "Train Epoch: 990 [45056/101520 (44%)] Loss: -1152.830078\n",
      "Train Epoch: 990 [56320/101520 (55%)] Loss: -1143.655029\n",
      "Train Epoch: 990 [67584/101520 (67%)] Loss: -1148.792603\n",
      "Train Epoch: 990 [78848/101520 (78%)] Loss: -1145.183594\n",
      "Train Epoch: 990 [90112/101520 (89%)] Loss: -1147.920044\n",
      "Train Epoch: 990 [101376/101520 (100%)] Loss: -1148.558960\n",
      "    epoch          : 990\n",
      "    loss           : -1148.8268662649184\n",
      "    ess            : 1.967353904666613\n",
      "    log_marginal   : 1148.8567796256673\n",
      "    log_joint      : 1357.1775840874293\n",
      "    val_loss       : -1145.9659742272418\n",
      "    val_ess        : 1.968664718710858\n",
      "    val_log_marginal: 1145.995308254076\n",
      "    val_log_joint  : 1354.4623174252717\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch990.pth ...\n",
      "Train Epoch: 991 [0/101520 (0%)] Loss: -1148.645264\n",
      "Train Epoch: 991 [11264/101520 (11%)] Loss: -1142.300659\n",
      "Train Epoch: 991 [22528/101520 (22%)] Loss: -1144.870361\n",
      "Train Epoch: 991 [33792/101520 (33%)] Loss: -1155.423828\n",
      "Train Epoch: 991 [45056/101520 (44%)] Loss: -1149.268921\n",
      "Train Epoch: 991 [56320/101520 (55%)] Loss: -1150.726318\n",
      "Train Epoch: 991 [67584/101520 (67%)] Loss: -1149.250366\n",
      "Train Epoch: 991 [78848/101520 (78%)] Loss: -1150.908203\n",
      "Train Epoch: 991 [90112/101520 (89%)] Loss: -1150.153076\n",
      "Train Epoch: 991 [101376/101520 (100%)] Loss: -1155.703613\n",
      "    epoch          : 991\n",
      "    loss           : -1148.9074768373116\n",
      "    ess            : 1.96799062544377\n",
      "    log_marginal   : 1148.9358756673994\n",
      "    log_joint      : 1357.3313209111966\n",
      "    val_loss       : -1147.9998779296875\n",
      "    val_ess        : 1.9679623012957366\n",
      "    val_log_marginal: 1148.03051227072\n",
      "    val_log_joint  : 1356.2860160495925\n",
      "Train Epoch: 992 [0/101520 (0%)] Loss: -1152.543945\n",
      "Train Epoch: 992 [11264/101520 (11%)] Loss: -1157.322510\n",
      "Train Epoch: 992 [22528/101520 (22%)] Loss: -1144.620605\n",
      "Train Epoch: 992 [33792/101520 (33%)] Loss: -1142.711670\n",
      "Train Epoch: 992 [45056/101520 (44%)] Loss: -1144.582031\n",
      "Train Epoch: 992 [56320/101520 (55%)] Loss: -1144.953613\n",
      "Train Epoch: 992 [67584/101520 (67%)] Loss: -1149.107056\n",
      "Train Epoch: 992 [78848/101520 (78%)] Loss: -1151.369751\n",
      "Train Epoch: 992 [90112/101520 (89%)] Loss: -1148.185913\n",
      "Train Epoch: 992 [101376/101520 (100%)] Loss: -1161.328369\n",
      "    epoch          : 992\n",
      "    loss           : -1149.087364925212\n",
      "    ess            : 1.9673152670788405\n",
      "    log_marginal   : 1149.1170249440563\n",
      "    log_joint      : 1357.429758656564\n",
      "    val_loss       : -1148.4376167629075\n",
      "    val_ess        : 1.966840816580731\n",
      "    val_log_marginal: 1148.4690365998642\n",
      "    val_log_joint  : 1356.5318762737772\n",
      "Train Epoch: 993 [0/101520 (0%)] Loss: -1155.538330\n",
      "Train Epoch: 993 [11264/101520 (11%)] Loss: -1147.071289\n",
      "Train Epoch: 993 [22528/101520 (22%)] Loss: -1147.871338\n",
      "Train Epoch: 993 [33792/101520 (33%)] Loss: -1143.396851\n",
      "Train Epoch: 993 [45056/101520 (44%)] Loss: -1149.461182\n",
      "Train Epoch: 993 [56320/101520 (55%)] Loss: -1149.004639\n",
      "Train Epoch: 993 [67584/101520 (67%)] Loss: -1143.356445\n",
      "Train Epoch: 993 [78848/101520 (78%)] Loss: -1149.737549\n",
      "Train Epoch: 993 [90112/101520 (89%)] Loss: -1151.070190\n",
      "Train Epoch: 993 [101376/101520 (100%)] Loss: -1150.454834\n",
      "    epoch          : 993\n",
      "    loss           : -1148.935676919755\n",
      "    ess            : 1.9683741486851294\n",
      "    log_marginal   : 1148.9637892833307\n",
      "    log_joint      : 1357.323794877709\n",
      "    val_loss       : -1147.6227868121603\n",
      "    val_ess        : 1.9679214539735213\n",
      "    val_log_marginal: 1147.6511124320652\n",
      "    val_log_joint  : 1356.2765211022418\n",
      "Train Epoch: 994 [0/101520 (0%)] Loss: -1147.748169\n",
      "Train Epoch: 994 [11264/101520 (11%)] Loss: -1150.587524\n",
      "Train Epoch: 994 [22528/101520 (22%)] Loss: -1154.358521\n",
      "Train Epoch: 994 [33792/101520 (33%)] Loss: -1153.130615\n",
      "Train Epoch: 994 [45056/101520 (44%)] Loss: -1147.267700\n",
      "Train Epoch: 994 [56320/101520 (55%)] Loss: -1148.475830\n",
      "Train Epoch: 994 [67584/101520 (67%)] Loss: -1150.290039\n",
      "Train Epoch: 994 [78848/101520 (78%)] Loss: -1154.876099\n",
      "Train Epoch: 994 [90112/101520 (89%)] Loss: -1148.705078\n",
      "Train Epoch: 994 [101376/101520 (100%)] Loss: -1146.724976\n",
      "    epoch          : 994\n",
      "    loss           : -1149.1177874234454\n",
      "    ess            : 1.9676634923896599\n",
      "    log_marginal   : 1149.1467935380026\n",
      "    log_joint      : 1357.516859811754\n",
      "    val_loss       : -1148.604200280231\n",
      "    val_ess        : 1.9689459386079207\n",
      "    val_log_marginal: 1148.6296015200408\n",
      "    val_log_joint  : 1356.6226647418478\n",
      "Train Epoch: 995 [0/101520 (0%)] Loss: -1156.625000\n",
      "Train Epoch: 995 [11264/101520 (11%)] Loss: -1151.191895\n",
      "Train Epoch: 995 [22528/101520 (22%)] Loss: -1148.582764\n",
      "Train Epoch: 995 [33792/101520 (33%)] Loss: -1150.471680\n",
      "Train Epoch: 995 [45056/101520 (44%)] Loss: -1147.829834\n",
      "Train Epoch: 995 [56320/101520 (55%)] Loss: -1143.250244\n",
      "Train Epoch: 995 [67584/101520 (67%)] Loss: -1149.535889\n",
      "Train Epoch: 995 [78848/101520 (78%)] Loss: -1153.180420\n",
      "Train Epoch: 995 [90112/101520 (89%)] Loss: -1151.327637\n",
      "Train Epoch: 995 [101376/101520 (100%)] Loss: -1154.361572\n",
      "    epoch          : 995\n",
      "    loss           : -1149.1592398025282\n",
      "    ess            : 1.9676378863540727\n",
      "    log_marginal   : 1149.1881821215452\n",
      "    log_joint      : 1357.5844413718985\n",
      "    val_loss       : -1147.8021824048913\n",
      "    val_ess        : 1.9706672119057698\n",
      "    val_log_marginal: 1147.8271219004755\n",
      "    val_log_joint  : 1356.4461192255435\n",
      "Train Epoch: 996 [0/101520 (0%)] Loss: -1154.634033\n",
      "Train Epoch: 996 [11264/101520 (11%)] Loss: -1140.214111\n",
      "Train Epoch: 996 [22528/101520 (22%)] Loss: -1148.674072\n",
      "Train Epoch: 996 [33792/101520 (33%)] Loss: -1153.645142\n",
      "Train Epoch: 996 [45056/101520 (44%)] Loss: -1150.452515\n",
      "Train Epoch: 996 [56320/101520 (55%)] Loss: -1149.377686\n",
      "Train Epoch: 996 [67584/101520 (67%)] Loss: -1154.383301\n",
      "Train Epoch: 996 [78848/101520 (78%)] Loss: -1147.724854\n",
      "Train Epoch: 996 [90112/101520 (89%)] Loss: -1139.002197\n",
      "Train Epoch: 996 [101376/101520 (100%)] Loss: -1141.246582\n",
      "    epoch          : 996\n",
      "    loss           : -1149.2484253543107\n",
      "    ess            : 1.967816675727691\n",
      "    log_marginal   : 1149.2780185105214\n",
      "    log_joint      : 1357.5567093278894\n",
      "    val_loss       : -1148.0686725118885\n",
      "    val_ess        : 1.9672653156778086\n",
      "    val_log_marginal: 1148.1006496263587\n",
      "    val_log_joint  : 1356.410740064538\n",
      "Train Epoch: 997 [0/101520 (0%)] Loss: -1149.624268\n",
      "Train Epoch: 997 [11264/101520 (11%)] Loss: -1148.380005\n",
      "Train Epoch: 997 [22528/101520 (22%)] Loss: -1148.837402\n",
      "Train Epoch: 997 [33792/101520 (33%)] Loss: -1144.859131\n",
      "Train Epoch: 997 [45056/101520 (44%)] Loss: -1150.077759\n",
      "Train Epoch: 997 [56320/101520 (55%)] Loss: -1152.842529\n",
      "Train Epoch: 997 [67584/101520 (67%)] Loss: -1144.603027\n",
      "Train Epoch: 997 [78848/101520 (78%)] Loss: -1152.174194\n",
      "Train Epoch: 997 [90112/101520 (89%)] Loss: -1153.999512\n",
      "Train Epoch: 997 [101376/101520 (100%)] Loss: -1151.260498\n",
      "    epoch          : 997\n",
      "    loss           : -1149.4314633470085\n",
      "    ess            : 1.968423247936383\n",
      "    log_marginal   : 1149.4594125412218\n",
      "    log_joint      : 1357.751361175997\n",
      "    val_loss       : -1148.7147269870925\n",
      "    val_ess        : 1.96654358635778\n",
      "    val_log_marginal: 1148.7451861837635\n",
      "    val_log_joint  : 1356.983892026155\n",
      "Train Epoch: 998 [0/101520 (0%)] Loss: -1153.414551\n",
      "Train Epoch: 998 [11264/101520 (11%)] Loss: -1152.256592\n",
      "Train Epoch: 998 [22528/101520 (22%)] Loss: -1149.030884\n",
      "Train Epoch: 998 [33792/101520 (33%)] Loss: -1157.700439\n",
      "Train Epoch: 998 [45056/101520 (44%)] Loss: -1153.577637\n",
      "Train Epoch: 998 [56320/101520 (55%)] Loss: -1155.542236\n",
      "Train Epoch: 998 [67584/101520 (67%)] Loss: -1149.465942\n",
      "Train Epoch: 998 [78848/101520 (78%)] Loss: -1149.568848\n",
      "Train Epoch: 998 [90112/101520 (89%)] Loss: -1154.506836\n",
      "Train Epoch: 998 [101376/101520 (100%)] Loss: -1160.622803\n",
      "    epoch          : 998\n",
      "    loss           : -1149.4364314247016\n",
      "    ess            : 1.967439020698394\n",
      "    log_marginal   : 1149.4660534115892\n",
      "    log_joint      : 1357.826719044441\n",
      "    val_loss       : -1149.4765041185462\n",
      "    val_ess        : 1.965610535248466\n",
      "    val_log_marginal: 1149.50903851053\n",
      "    val_log_joint  : 1358.1142471976902\n",
      "Train Epoch: 999 [0/101520 (0%)] Loss: -1146.402832\n",
      "Train Epoch: 999 [11264/101520 (11%)] Loss: -1147.812012\n",
      "Train Epoch: 999 [22528/101520 (22%)] Loss: -1147.338867\n",
      "Train Epoch: 999 [33792/101520 (33%)] Loss: -1143.995605\n",
      "Train Epoch: 999 [45056/101520 (44%)] Loss: -1146.371826\n",
      "Train Epoch: 999 [56320/101520 (55%)] Loss: -1145.031372\n",
      "Train Epoch: 999 [67584/101520 (67%)] Loss: -1151.053955\n",
      "Train Epoch: 999 [78848/101520 (78%)] Loss: -1154.987671\n",
      "Train Epoch: 999 [90112/101520 (89%)] Loss: -1155.479736\n",
      "Train Epoch: 999 [101376/101520 (100%)] Loss: -1157.043213\n",
      "    epoch          : 999\n",
      "    loss           : -1149.518103824788\n",
      "    ess            : 1.967881347066793\n",
      "    log_marginal   : 1149.5480840481705\n",
      "    log_joint      : 1357.905307175526\n",
      "    val_loss       : -1149.1710788892663\n",
      "    val_ess        : 1.9615172873372617\n",
      "    val_log_marginal: 1149.204982591712\n",
      "    val_log_joint  : 1357.900438391644\n",
      "Train Epoch: 1000 [0/101520 (0%)] Loss: -1154.753540\n",
      "Train Epoch: 1000 [11264/101520 (11%)] Loss: -1147.153687\n",
      "Train Epoch: 1000 [22528/101520 (22%)] Loss: -1151.807861\n",
      "Train Epoch: 1000 [33792/101520 (33%)] Loss: -1149.866455\n",
      "Train Epoch: 1000 [45056/101520 (44%)] Loss: -1146.139404\n",
      "Train Epoch: 1000 [56320/101520 (55%)] Loss: -1148.725098\n",
      "Train Epoch: 1000 [67584/101520 (67%)] Loss: -1155.046753\n",
      "Train Epoch: 1000 [78848/101520 (78%)] Loss: -1156.199097\n",
      "Train Epoch: 1000 [90112/101520 (89%)] Loss: -1147.531128\n",
      "Train Epoch: 1000 [101376/101520 (100%)] Loss: -1143.144165\n",
      "    epoch          : 1000\n",
      "    loss           : -1149.536111342847\n",
      "    ess            : 1.9684840590510535\n",
      "    log_marginal   : 1149.564597278384\n",
      "    log_joint      : 1357.9267713077104\n",
      "    val_loss       : -1148.62572711447\n",
      "    val_ess        : 1.97061766748843\n",
      "    val_log_marginal: 1148.6520412279212\n",
      "    val_log_joint  : 1357.0415198284647\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1000.pth ...\n",
      "Train Epoch: 1001 [0/101520 (0%)] Loss: -1150.819336\n",
      "Train Epoch: 1001 [11264/101520 (11%)] Loss: -1152.017212\n",
      "Train Epoch: 1001 [22528/101520 (22%)] Loss: -1152.709839\n",
      "Train Epoch: 1001 [33792/101520 (33%)] Loss: -1148.234863\n",
      "Train Epoch: 1001 [45056/101520 (44%)] Loss: -1152.882568\n",
      "Train Epoch: 1001 [56320/101520 (55%)] Loss: -1150.704346\n",
      "Train Epoch: 1001 [67584/101520 (67%)] Loss: -1148.250732\n",
      "Train Epoch: 1001 [78848/101520 (78%)] Loss: -1140.186035\n",
      "Train Epoch: 1001 [90112/101520 (89%)] Loss: -1145.592773\n",
      "Train Epoch: 1001 [101376/101520 (100%)] Loss: -1170.603271\n",
      "    epoch          : 1001\n",
      "    loss           : -1149.7944164180276\n",
      "    ess            : 1.9678172909434717\n",
      "    log_marginal   : 1149.823664832954\n",
      "    log_joint      : 1358.1266666584877\n",
      "    val_loss       : -1148.468994140625\n",
      "    val_ess        : 1.9699172818142434\n",
      "    val_log_marginal: 1148.4942149286685\n",
      "    val_log_joint  : 1356.457418690557\n",
      "Train Epoch: 1002 [0/101520 (0%)] Loss: -1152.878296\n",
      "Train Epoch: 1002 [11264/101520 (11%)] Loss: -1149.433228\n",
      "Train Epoch: 1002 [22528/101520 (22%)] Loss: -1151.894287\n",
      "Train Epoch: 1002 [33792/101520 (33%)] Loss: -1146.953613\n",
      "Train Epoch: 1002 [45056/101520 (44%)] Loss: -1152.299805\n",
      "Train Epoch: 1002 [56320/101520 (55%)] Loss: -1144.619629\n",
      "Train Epoch: 1002 [67584/101520 (67%)] Loss: -1152.461304\n",
      "Train Epoch: 1002 [78848/101520 (78%)] Loss: -1151.932373\n",
      "Train Epoch: 1002 [90112/101520 (89%)] Loss: -1147.646729\n",
      "Train Epoch: 1002 [101376/101520 (100%)] Loss: -1141.167847\n",
      "    epoch          : 1002\n",
      "    loss           : -1149.7174679550094\n",
      "    ess            : 1.967381024480465\n",
      "    log_marginal   : 1149.7473672071294\n",
      "    log_joint      : 1358.079290495446\n",
      "    val_loss       : -1148.6668382727582\n",
      "    val_ess        : 1.9687841923340508\n",
      "    val_log_marginal: 1148.6949462890625\n",
      "    val_log_joint  : 1357.1716414741848\n",
      "Train Epoch: 1003 [0/101520 (0%)] Loss: -1150.072510\n",
      "Train Epoch: 1003 [11264/101520 (11%)] Loss: -1152.065063\n",
      "Train Epoch: 1003 [22528/101520 (22%)] Loss: -1152.195312\n",
      "Train Epoch: 1003 [33792/101520 (33%)] Loss: -1155.977295\n",
      "Train Epoch: 1003 [45056/101520 (44%)] Loss: -1151.312988\n",
      "Train Epoch: 1003 [56320/101520 (55%)] Loss: -1147.611450\n",
      "Train Epoch: 1003 [67584/101520 (67%)] Loss: -1157.828491\n",
      "Train Epoch: 1003 [78848/101520 (78%)] Loss: -1142.166992\n",
      "Train Epoch: 1003 [90112/101520 (89%)] Loss: -1144.780151\n",
      "Train Epoch: 1003 [101376/101520 (100%)] Loss: -1150.678223\n",
      "    epoch          : 1003\n",
      "    loss           : -1149.7934005967336\n",
      "    ess            : 1.968216655841425\n",
      "    log_marginal   : 1149.8222417016725\n",
      "    log_joint      : 1358.1428621378377\n",
      "    val_loss       : -1148.921243418818\n",
      "    val_ess        : 1.9667762932570085\n",
      "    val_log_marginal: 1148.9515805451767\n",
      "    val_log_joint  : 1357.4571480129075\n",
      "Train Epoch: 1004 [0/101520 (0%)] Loss: -1151.239502\n",
      "Train Epoch: 1004 [11264/101520 (11%)] Loss: -1149.737793\n",
      "Train Epoch: 1004 [22528/101520 (22%)] Loss: -1156.872803\n",
      "Train Epoch: 1004 [33792/101520 (33%)] Loss: -1147.897583\n",
      "Train Epoch: 1004 [45056/101520 (44%)] Loss: -1148.152100\n",
      "Train Epoch: 1004 [56320/101520 (55%)] Loss: -1144.228638\n",
      "Train Epoch: 1004 [67584/101520 (67%)] Loss: -1147.213867\n",
      "Train Epoch: 1004 [78848/101520 (78%)] Loss: -1148.621460\n",
      "Train Epoch: 1004 [90112/101520 (89%)] Loss: -1142.137085\n",
      "Train Epoch: 1004 [101376/101520 (100%)] Loss: -1169.806641\n",
      "    epoch          : 1004\n",
      "    loss           : -1149.9605835574357\n",
      "    ess            : 1.9678280748913635\n",
      "    log_marginal   : 1149.9894387710035\n",
      "    log_joint      : 1358.3214344427215\n",
      "    val_loss       : -1148.6032608695652\n",
      "    val_ess        : 1.9674360233804453\n",
      "    val_log_marginal: 1148.6334069293478\n",
      "    val_log_joint  : 1356.9564845872962\n",
      "Train Epoch: 1005 [0/101520 (0%)] Loss: -1153.465454\n",
      "Train Epoch: 1005 [11264/101520 (11%)] Loss: -1147.633057\n",
      "Train Epoch: 1005 [22528/101520 (22%)] Loss: -1150.776123\n",
      "Train Epoch: 1005 [33792/101520 (33%)] Loss: -1149.955811\n",
      "Train Epoch: 1005 [45056/101520 (44%)] Loss: -1149.844238\n",
      "Train Epoch: 1005 [56320/101520 (55%)] Loss: -1158.283081\n",
      "Train Epoch: 1005 [67584/101520 (67%)] Loss: -1152.955322\n",
      "Train Epoch: 1005 [78848/101520 (78%)] Loss: -1146.215576\n",
      "Train Epoch: 1005 [90112/101520 (89%)] Loss: -1145.469238\n",
      "Train Epoch: 1005 [101376/101520 (100%)] Loss: -1148.872803\n",
      "    epoch          : 1005\n",
      "    loss           : -1149.970439354978\n",
      "    ess            : 1.967068476892596\n",
      "    log_marginal   : 1150.0008655337233\n",
      "    log_joint      : 1358.3336193909\n",
      "    val_loss       : -1148.0890529466712\n",
      "    val_ess        : 1.9694764873255854\n",
      "    val_log_marginal: 1148.1174157184103\n",
      "    val_log_joint  : 1356.7756613026495\n",
      "Train Epoch: 1006 [0/101520 (0%)] Loss: -1152.934448\n",
      "Train Epoch: 1006 [11264/101520 (11%)] Loss: -1145.265503\n",
      "Train Epoch: 1006 [22528/101520 (22%)] Loss: -1152.666260\n",
      "Train Epoch: 1006 [33792/101520 (33%)] Loss: -1148.906616\n",
      "Train Epoch: 1006 [45056/101520 (44%)] Loss: -1152.584106\n",
      "Train Epoch: 1006 [56320/101520 (55%)] Loss: -1150.219971\n",
      "Train Epoch: 1006 [67584/101520 (67%)] Loss: -1144.017822\n",
      "Train Epoch: 1006 [78848/101520 (78%)] Loss: -1153.183228\n",
      "Train Epoch: 1006 [90112/101520 (89%)] Loss: -1148.811401\n",
      "Train Epoch: 1006 [101376/101520 (100%)] Loss: -1147.836060\n",
      "    epoch          : 1006\n",
      "    loss           : -1150.0022641282585\n",
      "    ess            : 1.9680053432982172\n",
      "    log_marginal   : 1150.0313346517744\n",
      "    log_joint      : 1358.3533708581972\n",
      "    val_loss       : -1151.3555271314538\n",
      "    val_ess        : 1.9683628030445264\n",
      "    val_log_marginal: 1151.3829239555027\n",
      "    val_log_joint  : 1359.5199558423913\n",
      "Train Epoch: 1007 [0/101520 (0%)] Loss: -1147.198242\n",
      "Train Epoch: 1007 [11264/101520 (11%)] Loss: -1149.729492\n",
      "Train Epoch: 1007 [22528/101520 (22%)] Loss: -1147.473267\n",
      "Train Epoch: 1007 [33792/101520 (33%)] Loss: -1152.283203\n",
      "Train Epoch: 1007 [45056/101520 (44%)] Loss: -1149.689575\n",
      "Train Epoch: 1007 [56320/101520 (55%)] Loss: -1144.052856\n",
      "Train Epoch: 1007 [67584/101520 (67%)] Loss: -1146.018311\n",
      "Train Epoch: 1007 [78848/101520 (78%)] Loss: -1149.169189\n",
      "Train Epoch: 1007 [90112/101520 (89%)] Loss: -1151.509888\n",
      "Train Epoch: 1007 [101376/101520 (100%)] Loss: -1152.699219\n",
      "    epoch          : 1007\n",
      "    loss           : -1150.0243913660097\n",
      "    ess            : 1.9673974525988402\n",
      "    log_marginal   : 1150.0544580814228\n",
      "    log_joint      : 1358.4435648702497\n",
      "    val_loss       : -1147.2296248726223\n",
      "    val_ess        : 1.9696221247963284\n",
      "    val_log_marginal: 1147.2566607931385\n",
      "    val_log_joint  : 1355.4922671110733\n",
      "Train Epoch: 1008 [0/101520 (0%)] Loss: -1145.856689\n",
      "Train Epoch: 1008 [11264/101520 (11%)] Loss: -1147.847778\n",
      "Train Epoch: 1008 [22528/101520 (22%)] Loss: -1146.375366\n",
      "Train Epoch: 1008 [33792/101520 (33%)] Loss: -1151.738403\n",
      "Train Epoch: 1008 [45056/101520 (44%)] Loss: -1147.547241\n",
      "Train Epoch: 1008 [56320/101520 (55%)] Loss: -1151.608887\n",
      "Train Epoch: 1008 [67584/101520 (67%)] Loss: -1152.277466\n",
      "Train Epoch: 1008 [78848/101520 (78%)] Loss: -1142.542847\n",
      "Train Epoch: 1008 [90112/101520 (89%)] Loss: -1146.252197\n",
      "Train Epoch: 1008 [101376/101520 (100%)] Loss: -1154.845337\n",
      "    epoch          : 1008\n",
      "    loss           : -1150.1844040760443\n",
      "    ess            : 1.9677163847726793\n",
      "    log_marginal   : 1150.2131568486966\n",
      "    log_joint      : 1358.5061630172347\n",
      "    val_loss       : -1148.650539232337\n",
      "    val_ess        : 1.9643679909084155\n",
      "    val_log_marginal: 1148.6817680027175\n",
      "    val_log_joint  : 1356.8111094599185\n",
      "Train Epoch: 1009 [0/101520 (0%)] Loss: -1149.154785\n",
      "Train Epoch: 1009 [11264/101520 (11%)] Loss: -1160.798584\n",
      "Train Epoch: 1009 [22528/101520 (22%)] Loss: -1147.567505\n",
      "Train Epoch: 1009 [33792/101520 (33%)] Loss: -1149.358276\n",
      "Train Epoch: 1009 [45056/101520 (44%)] Loss: -1150.450928\n",
      "Train Epoch: 1009 [56320/101520 (55%)] Loss: -1149.197144\n",
      "Train Epoch: 1009 [67584/101520 (67%)] Loss: -1153.036865\n",
      "Train Epoch: 1009 [78848/101520 (78%)] Loss: -1146.385010\n",
      "Train Epoch: 1009 [90112/101520 (89%)] Loss: -1148.419434\n",
      "Train Epoch: 1009 [101376/101520 (100%)] Loss: -1149.682861\n",
      "    epoch          : 1009\n",
      "    loss           : -1150.1646783723304\n",
      "    ess            : 1.9683074957162292\n",
      "    log_marginal   : 1150.193010339785\n",
      "    log_joint      : 1358.5754915937107\n",
      "    val_loss       : -1148.21361773947\n",
      "    val_ess        : 1.970407765844594\n",
      "    val_log_marginal: 1148.2403511379075\n",
      "    val_log_joint  : 1356.572806980299\n",
      "Train Epoch: 1010 [0/101520 (0%)] Loss: -1152.229492\n",
      "Train Epoch: 1010 [11264/101520 (11%)] Loss: -1145.885132\n",
      "Train Epoch: 1010 [22528/101520 (22%)] Loss: -1148.952148\n",
      "Train Epoch: 1010 [33792/101520 (33%)] Loss: -1146.990723\n",
      "Train Epoch: 1010 [45056/101520 (44%)] Loss: -1153.617920\n",
      "Train Epoch: 1010 [56320/101520 (55%)] Loss: -1149.705078\n",
      "Train Epoch: 1010 [67584/101520 (67%)] Loss: -1153.622559\n",
      "Train Epoch: 1010 [78848/101520 (78%)] Loss: -1146.740112\n",
      "Train Epoch: 1010 [90112/101520 (89%)] Loss: -1151.770020\n",
      "Train Epoch: 1010 [101376/101520 (100%)] Loss: -1151.099609\n",
      "    epoch          : 1010\n",
      "    loss           : -1150.327146597244\n",
      "    ess            : 1.9676216684993189\n",
      "    log_marginal   : 1150.3567869866913\n",
      "    log_joint      : 1358.6602433554492\n",
      "    val_loss       : -1148.4106286090353\n",
      "    val_ess        : 1.9670954631722493\n",
      "    val_log_marginal: 1148.439548658288\n",
      "    val_log_joint  : 1356.9517397673233\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1010.pth ...\n",
      "Train Epoch: 1011 [0/101520 (0%)] Loss: -1155.359131\n",
      "Train Epoch: 1011 [11264/101520 (11%)] Loss: -1152.187866\n",
      "Train Epoch: 1011 [22528/101520 (22%)] Loss: -1152.323730\n",
      "Train Epoch: 1011 [33792/101520 (33%)] Loss: -1149.541992\n",
      "Train Epoch: 1011 [45056/101520 (44%)] Loss: -1147.917480\n",
      "Train Epoch: 1011 [56320/101520 (55%)] Loss: -1150.261719\n",
      "Train Epoch: 1011 [67584/101520 (67%)] Loss: -1149.771973\n",
      "Train Epoch: 1011 [78848/101520 (78%)] Loss: -1152.757324\n",
      "Train Epoch: 1011 [90112/101520 (89%)] Loss: -1152.108154\n",
      "Train Epoch: 1011 [101376/101520 (100%)] Loss: -1139.807373\n",
      "    epoch          : 1011\n",
      "    loss           : -1150.2781730920226\n",
      "    ess            : 1.9674424699802495\n",
      "    log_marginal   : 1150.30750309163\n",
      "    log_joint      : 1358.7486523192133\n",
      "    val_loss       : -1148.3457455842392\n",
      "    val_ess        : 1.9668876917465874\n",
      "    val_log_marginal: 1148.3767939028533\n",
      "    val_log_joint  : 1356.8786833389945\n",
      "Train Epoch: 1012 [0/101520 (0%)] Loss: -1149.141724\n",
      "Train Epoch: 1012 [11264/101520 (11%)] Loss: -1153.373291\n",
      "Train Epoch: 1012 [22528/101520 (22%)] Loss: -1153.628906\n",
      "Train Epoch: 1012 [33792/101520 (33%)] Loss: -1148.744385\n",
      "Train Epoch: 1012 [45056/101520 (44%)] Loss: -1156.583496\n",
      "Train Epoch: 1012 [56320/101520 (55%)] Loss: -1145.962036\n",
      "Train Epoch: 1012 [67584/101520 (67%)] Loss: -1152.235596\n",
      "Train Epoch: 1012 [78848/101520 (78%)] Loss: -1148.844482\n",
      "Train Epoch: 1012 [90112/101520 (89%)] Loss: -1151.606201\n",
      "Train Epoch: 1012 [101376/101520 (100%)] Loss: -1140.951660\n",
      "    epoch          : 1012\n",
      "    loss           : -1150.3310411922896\n",
      "    ess            : 1.9672666596407866\n",
      "    log_marginal   : 1150.3606122654287\n",
      "    log_joint      : 1358.7540338410804\n",
      "    val_loss       : -1149.8210714588995\n",
      "    val_ess        : 1.965803623199463\n",
      "    val_log_marginal: 1149.851026452106\n",
      "    val_log_joint  : 1358.3003301205842\n",
      "Train Epoch: 1013 [0/101520 (0%)] Loss: -1152.148682\n",
      "Train Epoch: 1013 [11264/101520 (11%)] Loss: -1149.613525\n",
      "Train Epoch: 1013 [22528/101520 (22%)] Loss: -1147.441040\n",
      "Train Epoch: 1013 [33792/101520 (33%)] Loss: -1149.129395\n",
      "Train Epoch: 1013 [45056/101520 (44%)] Loss: -1147.229980\n",
      "Train Epoch: 1013 [56320/101520 (55%)] Loss: -1148.145630\n",
      "Train Epoch: 1013 [67584/101520 (67%)] Loss: -1145.631592\n",
      "Train Epoch: 1013 [78848/101520 (78%)] Loss: -1151.609863\n",
      "Train Epoch: 1013 [90112/101520 (89%)] Loss: -1148.779541\n",
      "Train Epoch: 1013 [101376/101520 (100%)] Loss: -1152.019531\n",
      "    epoch          : 1013\n",
      "    loss           : -1150.5096006153815\n",
      "    ess            : 1.96773146564637\n",
      "    log_marginal   : 1150.5389753945508\n",
      "    log_joint      : 1358.982301031525\n",
      "    val_loss       : -1148.876417077106\n",
      "    val_ess        : 1.96725114532139\n",
      "    val_log_marginal: 1148.9071522588315\n",
      "    val_log_joint  : 1357.167581309443\n",
      "Train Epoch: 1014 [0/101520 (0%)] Loss: -1150.813477\n",
      "Train Epoch: 1014 [11264/101520 (11%)] Loss: -1153.957031\n",
      "Train Epoch: 1014 [22528/101520 (22%)] Loss: -1152.933838\n",
      "Train Epoch: 1014 [33792/101520 (33%)] Loss: -1150.483643\n",
      "Train Epoch: 1014 [45056/101520 (44%)] Loss: -1150.644287\n",
      "Train Epoch: 1014 [56320/101520 (55%)] Loss: -1155.472656\n",
      "Train Epoch: 1014 [67584/101520 (67%)] Loss: -1155.004395\n",
      "Train Epoch: 1014 [78848/101520 (78%)] Loss: -1147.707275\n",
      "Train Epoch: 1014 [90112/101520 (89%)] Loss: -1151.855713\n",
      "Train Epoch: 1014 [101376/101520 (100%)] Loss: -1145.706421\n",
      "    epoch          : 1014\n",
      "    loss           : -1150.5836463813207\n",
      "    ess            : 1.9669647013122713\n",
      "    log_marginal   : 1150.6138983491678\n",
      "    log_joint      : 1358.9370129455874\n",
      "    val_loss       : -1149.9929305366848\n",
      "    val_ess        : 1.9684750826462456\n",
      "    val_log_marginal: 1150.022360096807\n",
      "    val_log_joint  : 1358.680812669837\n",
      "Train Epoch: 1015 [0/101520 (0%)] Loss: -1153.512207\n",
      "Train Epoch: 1015 [11264/101520 (11%)] Loss: -1150.277832\n",
      "Train Epoch: 1015 [22528/101520 (22%)] Loss: -1153.087402\n",
      "Train Epoch: 1015 [33792/101520 (33%)] Loss: -1148.710938\n",
      "Train Epoch: 1015 [45056/101520 (44%)] Loss: -1157.147217\n",
      "Train Epoch: 1015 [56320/101520 (55%)] Loss: -1147.284546\n",
      "Train Epoch: 1015 [67584/101520 (67%)] Loss: -1151.621338\n",
      "Train Epoch: 1015 [78848/101520 (78%)] Loss: -1152.346069\n",
      "Train Epoch: 1015 [90112/101520 (89%)] Loss: -1158.284180\n",
      "Train Epoch: 1015 [101376/101520 (100%)] Loss: -1164.425049\n",
      "    epoch          : 1015\n",
      "    loss           : -1150.706694483158\n",
      "    ess            : 1.967871697104756\n",
      "    log_marginal   : 1150.735727588136\n",
      "    log_joint      : 1359.0874152255417\n",
      "    val_loss       : -1148.0787671959918\n",
      "    val_ess        : 1.9662634341613106\n",
      "    val_log_marginal: 1148.112548828125\n",
      "    val_log_joint  : 1356.5988079568615\n",
      "Train Epoch: 1016 [0/101520 (0%)] Loss: -1149.385742\n",
      "Train Epoch: 1016 [11264/101520 (11%)] Loss: -1153.951538\n",
      "Train Epoch: 1016 [22528/101520 (22%)] Loss: -1153.276123\n",
      "Train Epoch: 1016 [33792/101520 (33%)] Loss: -1146.551636\n",
      "Train Epoch: 1016 [45056/101520 (44%)] Loss: -1144.180054\n",
      "Train Epoch: 1016 [56320/101520 (55%)] Loss: -1156.910889\n",
      "Train Epoch: 1016 [67584/101520 (67%)] Loss: -1151.424072\n",
      "Train Epoch: 1016 [78848/101520 (78%)] Loss: -1145.747070\n",
      "Train Epoch: 1016 [90112/101520 (89%)] Loss: -1152.904541\n",
      "Train Epoch: 1016 [101376/101520 (100%)] Loss: -1144.577271\n",
      "    epoch          : 1016\n",
      "    loss           : -1150.7794398015467\n",
      "    ess            : 1.9677917022800924\n",
      "    log_marginal   : 1150.8082631173445\n",
      "    log_joint      : 1359.1862461722676\n",
      "    val_loss       : -1150.9223845108695\n",
      "    val_ess        : 1.9682245254516602\n",
      "    val_log_marginal: 1150.949123216712\n",
      "    val_log_joint  : 1359.327440344769\n",
      "Train Epoch: 1017 [0/101520 (0%)] Loss: -1150.124512\n",
      "Train Epoch: 1017 [11264/101520 (11%)] Loss: -1149.807129\n",
      "Train Epoch: 1017 [22528/101520 (22%)] Loss: -1148.368896\n",
      "Train Epoch: 1017 [33792/101520 (33%)] Loss: -1156.789551\n",
      "Train Epoch: 1017 [45056/101520 (44%)] Loss: -1157.581055\n",
      "Train Epoch: 1017 [56320/101520 (55%)] Loss: -1144.789795\n",
      "Train Epoch: 1017 [67584/101520 (67%)] Loss: -1146.976318\n",
      "Train Epoch: 1017 [78848/101520 (78%)] Loss: -1150.981934\n",
      "Train Epoch: 1017 [90112/101520 (89%)] Loss: -1156.957764\n",
      "Train Epoch: 1017 [101376/101520 (100%)] Loss: -1151.718018\n",
      "    epoch          : 1017\n",
      "    loss           : -1150.8064835898242\n",
      "    ess            : 1.967726524750791\n",
      "    log_marginal   : 1150.8356056405073\n",
      "    log_joint      : 1359.2094806306925\n",
      "    val_loss       : -1147.3169582201087\n",
      "    val_ess        : 1.9724384909090789\n",
      "    val_log_marginal: 1147.3411812160325\n",
      "    val_log_joint  : 1355.9407905910325\n",
      "Train Epoch: 1018 [0/101520 (0%)] Loss: -1155.742920\n",
      "Train Epoch: 1018 [11264/101520 (11%)] Loss: -1150.467407\n",
      "Train Epoch: 1018 [22528/101520 (22%)] Loss: -1155.231201\n",
      "Train Epoch: 1018 [33792/101520 (33%)] Loss: -1149.089600\n",
      "Train Epoch: 1018 [45056/101520 (44%)] Loss: -1142.122681\n",
      "Train Epoch: 1018 [56320/101520 (55%)] Loss: -1148.505859\n",
      "Train Epoch: 1018 [67584/101520 (67%)] Loss: -1145.337891\n",
      "Train Epoch: 1018 [78848/101520 (78%)] Loss: -1153.620972\n",
      "Train Epoch: 1018 [90112/101520 (89%)] Loss: -1148.165161\n",
      "Train Epoch: 1018 [101376/101520 (100%)] Loss: -1155.854248\n",
      "    epoch          : 1018\n",
      "    loss           : -1150.8590014280387\n",
      "    ess            : 1.9676413709793859\n",
      "    log_marginal   : 1150.8886301625314\n",
      "    log_joint      : 1359.2741944586212\n",
      "    val_loss       : -1148.7559230638587\n",
      "    val_ess        : 1.9678809798282126\n",
      "    val_log_marginal: 1148.7856233016305\n",
      "    val_log_joint  : 1357.0151791779892\n",
      "Train Epoch: 1019 [0/101520 (0%)] Loss: -1154.691406\n",
      "Train Epoch: 1019 [11264/101520 (11%)] Loss: -1155.147461\n",
      "Train Epoch: 1019 [22528/101520 (22%)] Loss: -1156.405640\n",
      "Train Epoch: 1019 [33792/101520 (33%)] Loss: -1151.189453\n",
      "Train Epoch: 1019 [45056/101520 (44%)] Loss: -1146.563232\n",
      "Train Epoch: 1019 [56320/101520 (55%)] Loss: -1149.101318\n",
      "Train Epoch: 1019 [67584/101520 (67%)] Loss: -1148.236328\n",
      "Train Epoch: 1019 [78848/101520 (78%)] Loss: -1149.718506\n",
      "Train Epoch: 1019 [90112/101520 (89%)] Loss: -1151.632080\n",
      "Train Epoch: 1019 [101376/101520 (100%)] Loss: -1149.214844\n",
      "    epoch          : 1019\n",
      "    loss           : -1151.0313555080088\n",
      "    ess            : 1.9678900008225562\n",
      "    log_marginal   : 1151.0604192839196\n",
      "    log_joint      : 1359.3916696519707\n",
      "    val_loss       : -1151.648580799932\n",
      "    val_ess        : 1.9693069924478945\n",
      "    val_log_marginal: 1151.6778405230978\n",
      "    val_log_joint  : 1359.8077817170517\n",
      "Train Epoch: 1020 [0/101520 (0%)] Loss: -1147.221680\n",
      "Train Epoch: 1020 [11264/101520 (11%)] Loss: -1155.523560\n",
      "Train Epoch: 1020 [22528/101520 (22%)] Loss: -1153.402832\n",
      "Train Epoch: 1020 [33792/101520 (33%)] Loss: -1155.815063\n",
      "Train Epoch: 1020 [45056/101520 (44%)] Loss: -1157.284058\n",
      "Train Epoch: 1020 [56320/101520 (55%)] Loss: -1148.993652\n",
      "Train Epoch: 1020 [67584/101520 (67%)] Loss: -1148.420166\n",
      "Train Epoch: 1020 [78848/101520 (78%)] Loss: -1150.240479\n",
      "Train Epoch: 1020 [90112/101520 (89%)] Loss: -1145.698730\n",
      "Train Epoch: 1020 [101376/101520 (100%)] Loss: -1151.178833\n",
      "    epoch          : 1020\n",
      "    loss           : -1150.9542506232333\n",
      "    ess            : 1.9667368072960245\n",
      "    log_marginal   : 1150.9857987447\n",
      "    log_joint      : 1359.3546657849795\n",
      "    val_loss       : -1149.2978515625\n",
      "    val_ess        : 1.9685023193774016\n",
      "    val_log_marginal: 1149.3253969938858\n",
      "    val_log_joint  : 1357.9707615064538\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1020.pth ...\n",
      "Train Epoch: 1021 [0/101520 (0%)] Loss: -1145.523071\n",
      "Train Epoch: 1021 [11264/101520 (11%)] Loss: -1154.176025\n",
      "Train Epoch: 1021 [22528/101520 (22%)] Loss: -1160.618530\n",
      "Train Epoch: 1021 [33792/101520 (33%)] Loss: -1152.744751\n",
      "Train Epoch: 1021 [45056/101520 (44%)] Loss: -1145.021240\n",
      "Train Epoch: 1021 [56320/101520 (55%)] Loss: -1151.224243\n",
      "Train Epoch: 1021 [67584/101520 (67%)] Loss: -1149.266724\n",
      "Train Epoch: 1021 [78848/101520 (78%)] Loss: -1156.584961\n",
      "Train Epoch: 1021 [90112/101520 (89%)] Loss: -1154.334839\n",
      "Train Epoch: 1021 [101376/101520 (100%)] Loss: -1149.393555\n",
      "    epoch          : 1021\n",
      "    loss           : -1151.1236836035646\n",
      "    ess            : 1.9675898713682165\n",
      "    log_marginal   : 1151.152922817211\n",
      "    log_joint      : 1359.4274019020886\n",
      "    val_loss       : -1148.804639733356\n",
      "    val_ess        : 1.9689826498860898\n",
      "    val_log_marginal: 1148.8300568953805\n",
      "    val_log_joint  : 1357.5788361922555\n",
      "Train Epoch: 1022 [0/101520 (0%)] Loss: -1154.488647\n",
      "Train Epoch: 1022 [11264/101520 (11%)] Loss: -1147.625122\n",
      "Train Epoch: 1022 [22528/101520 (22%)] Loss: -1152.967041\n",
      "Train Epoch: 1022 [33792/101520 (33%)] Loss: -1150.858887\n",
      "Train Epoch: 1022 [45056/101520 (44%)] Loss: -1154.158325\n",
      "Train Epoch: 1022 [56320/101520 (55%)] Loss: -1153.823975\n",
      "Train Epoch: 1022 [67584/101520 (67%)] Loss: -1154.693115\n",
      "Train Epoch: 1022 [78848/101520 (78%)] Loss: -1147.000244\n",
      "Train Epoch: 1022 [90112/101520 (89%)] Loss: -1149.246216\n",
      "Train Epoch: 1022 [101376/101520 (100%)] Loss: -1145.921143\n",
      "    epoch          : 1022\n",
      "    loss           : -1151.0417253503847\n",
      "    ess            : 1.9675960960100645\n",
      "    log_marginal   : 1151.070944934634\n",
      "    log_joint      : 1359.475461413513\n",
      "    val_loss       : -1148.2466085682745\n",
      "    val_ess        : 1.9680363717286482\n",
      "    val_log_marginal: 1148.2760912024457\n",
      "    val_log_joint  : 1356.756156589674\n",
      "Train Epoch: 1023 [0/101520 (0%)] Loss: -1160.885864\n",
      "Train Epoch: 1023 [11264/101520 (11%)] Loss: -1152.897461\n",
      "Train Epoch: 1023 [22528/101520 (22%)] Loss: -1152.914795\n",
      "Train Epoch: 1023 [33792/101520 (33%)] Loss: -1150.054199\n",
      "Train Epoch: 1023 [45056/101520 (44%)] Loss: -1159.105103\n",
      "Train Epoch: 1023 [56320/101520 (55%)] Loss: -1153.735596\n",
      "Train Epoch: 1023 [67584/101520 (67%)] Loss: -1151.607666\n",
      "Train Epoch: 1023 [78848/101520 (78%)] Loss: -1152.441650\n",
      "Train Epoch: 1023 [90112/101520 (89%)] Loss: -1152.225586\n",
      "Train Epoch: 1023 [101376/101520 (100%)] Loss: -1153.114136\n",
      "    epoch          : 1023\n",
      "    loss           : -1151.2238567103093\n",
      "    ess            : 1.9673498838990178\n",
      "    log_marginal   : 1151.2547944802136\n",
      "    log_joint      : 1359.6183131232333\n",
      "    val_loss       : -1150.6995478091033\n",
      "    val_ess        : 1.9685719894326252\n",
      "    val_log_marginal: 1150.7300494650135\n",
      "    val_log_joint  : 1358.7676524286685\n",
      "Train Epoch: 1024 [0/101520 (0%)] Loss: -1151.482056\n",
      "Train Epoch: 1024 [11264/101520 (11%)] Loss: -1154.850830\n",
      "Train Epoch: 1024 [22528/101520 (22%)] Loss: -1149.244629\n",
      "Train Epoch: 1024 [33792/101520 (33%)] Loss: -1147.007080\n",
      "Train Epoch: 1024 [45056/101520 (44%)] Loss: -1149.885132\n",
      "Train Epoch: 1024 [56320/101520 (55%)] Loss: -1140.701904\n",
      "Train Epoch: 1024 [67584/101520 (67%)] Loss: -1151.616455\n",
      "Train Epoch: 1024 [78848/101520 (78%)] Loss: -1149.338135\n",
      "Train Epoch: 1024 [90112/101520 (89%)] Loss: -1150.139648\n",
      "Train Epoch: 1024 [101376/101520 (100%)] Loss: -1157.832031\n",
      "    epoch          : 1024\n",
      "    loss           : -1151.1486184585035\n",
      "    ess            : 1.9672676103198947\n",
      "    log_marginal   : 1151.1782643687186\n",
      "    log_joint      : 1359.6229830794598\n",
      "    val_loss       : -1150.2346509850543\n",
      "    val_ess        : 1.9687474188597307\n",
      "    val_log_marginal: 1150.2636294157608\n",
      "    val_log_joint  : 1358.427739682405\n",
      "Train Epoch: 1025 [0/101520 (0%)] Loss: -1153.980713\n",
      "Train Epoch: 1025 [11264/101520 (11%)] Loss: -1151.684814\n",
      "Train Epoch: 1025 [22528/101520 (22%)] Loss: -1149.909668\n",
      "Train Epoch: 1025 [33792/101520 (33%)] Loss: -1146.897583\n",
      "Train Epoch: 1025 [45056/101520 (44%)] Loss: -1153.888916\n",
      "Train Epoch: 1025 [56320/101520 (55%)] Loss: -1150.025757\n",
      "Train Epoch: 1025 [67584/101520 (67%)] Loss: -1149.652588\n",
      "Train Epoch: 1025 [78848/101520 (78%)] Loss: -1155.814453\n",
      "Train Epoch: 1025 [90112/101520 (89%)] Loss: -1144.669678\n",
      "Train Epoch: 1025 [101376/101520 (100%)] Loss: -1153.721558\n",
      "    epoch          : 1025\n",
      "    loss           : -1151.3105155906485\n",
      "    ess            : 1.9679685585462867\n",
      "    log_marginal   : 1151.3388273152873\n",
      "    log_joint      : 1359.7393516655543\n",
      "    val_loss       : -1151.3058338994565\n",
      "    val_ess        : 1.9685455871664959\n",
      "    val_log_marginal: 1151.3345628821332\n",
      "    val_log_joint  : 1359.3186618970788\n",
      "Train Epoch: 1026 [0/101520 (0%)] Loss: -1151.060547\n",
      "Train Epoch: 1026 [11264/101520 (11%)] Loss: -1152.278198\n",
      "Train Epoch: 1026 [22528/101520 (22%)] Loss: -1149.100342\n",
      "Train Epoch: 1026 [33792/101520 (33%)] Loss: -1146.183350\n",
      "Train Epoch: 1026 [45056/101520 (44%)] Loss: -1146.852905\n",
      "Train Epoch: 1026 [56320/101520 (55%)] Loss: -1151.954834\n",
      "Train Epoch: 1026 [67584/101520 (67%)] Loss: -1145.493652\n",
      "Train Epoch: 1026 [78848/101520 (78%)] Loss: -1151.158203\n",
      "Train Epoch: 1026 [90112/101520 (89%)] Loss: -1150.617188\n",
      "Train Epoch: 1026 [101376/101520 (100%)] Loss: -1169.327026\n",
      "    epoch          : 1026\n",
      "    loss           : -1151.4284269246623\n",
      "    ess            : 1.9677860431335679\n",
      "    log_marginal   : 1151.4577587645258\n",
      "    log_joint      : 1359.8391554942682\n",
      "    val_loss       : -1151.111620032269\n",
      "    val_ess        : 1.9655707400778066\n",
      "    val_log_marginal: 1151.142429517663\n",
      "    val_log_joint  : 1359.1967614215353\n",
      "Train Epoch: 1027 [0/101520 (0%)] Loss: -1150.777832\n",
      "Train Epoch: 1027 [11264/101520 (11%)] Loss: -1150.130859\n",
      "Train Epoch: 1027 [22528/101520 (22%)] Loss: -1145.636963\n",
      "Train Epoch: 1027 [33792/101520 (33%)] Loss: -1150.564941\n",
      "Train Epoch: 1027 [45056/101520 (44%)] Loss: -1150.247314\n",
      "Train Epoch: 1027 [56320/101520 (55%)] Loss: -1153.801270\n",
      "Train Epoch: 1027 [67584/101520 (67%)] Loss: -1153.462891\n",
      "Train Epoch: 1027 [78848/101520 (78%)] Loss: -1149.740234\n",
      "Train Epoch: 1027 [90112/101520 (89%)] Loss: -1150.154297\n",
      "Train Epoch: 1027 [101376/101520 (100%)] Loss: -1145.011475\n",
      "    epoch          : 1027\n",
      "    loss           : -1151.384799363026\n",
      "    ess            : 1.9674567571237458\n",
      "    log_marginal   : 1151.414099918538\n",
      "    log_joint      : 1359.8133974314935\n",
      "    val_loss       : -1151.0963983950408\n",
      "    val_ess        : 1.9682023628898289\n",
      "    val_log_marginal: 1151.1215608016305\n",
      "    val_log_joint  : 1359.4194919752038\n",
      "Train Epoch: 1028 [0/101520 (0%)] Loss: -1151.108887\n",
      "Train Epoch: 1028 [11264/101520 (11%)] Loss: -1152.739868\n",
      "Train Epoch: 1028 [22528/101520 (22%)] Loss: -1142.252319\n",
      "Train Epoch: 1028 [33792/101520 (33%)] Loss: -1155.160889\n",
      "Train Epoch: 1028 [45056/101520 (44%)] Loss: -1159.255249\n",
      "Train Epoch: 1028 [56320/101520 (55%)] Loss: -1156.385010\n",
      "Train Epoch: 1028 [67584/101520 (67%)] Loss: -1152.434937\n",
      "Train Epoch: 1028 [78848/101520 (78%)] Loss: -1150.669434\n",
      "Train Epoch: 1028 [90112/101520 (89%)] Loss: -1157.589111\n",
      "Train Epoch: 1028 [101376/101520 (100%)] Loss: -1149.014282\n",
      "    epoch          : 1028\n",
      "    loss           : -1151.4652670088724\n",
      "    ess            : 1.9673306918024418\n",
      "    log_marginal   : 1151.494789008519\n",
      "    log_joint      : 1359.9025731685772\n",
      "    val_loss       : -1150.181539784307\n",
      "    val_ess        : 1.9659763470939968\n",
      "    val_log_marginal: 1150.2139786430027\n",
      "    val_log_joint  : 1358.5998322860055\n",
      "Train Epoch: 1029 [0/101520 (0%)] Loss: -1151.761963\n",
      "Train Epoch: 1029 [11264/101520 (11%)] Loss: -1147.455811\n",
      "Train Epoch: 1029 [22528/101520 (22%)] Loss: -1153.512085\n",
      "Train Epoch: 1029 [33792/101520 (33%)] Loss: -1151.757812\n",
      "Train Epoch: 1029 [45056/101520 (44%)] Loss: -1149.821045\n",
      "Train Epoch: 1029 [56320/101520 (55%)] Loss: -1154.078247\n",
      "Train Epoch: 1029 [67584/101520 (67%)] Loss: -1147.570679\n",
      "Train Epoch: 1029 [78848/101520 (78%)] Loss: -1143.824219\n",
      "Train Epoch: 1029 [90112/101520 (89%)] Loss: -1155.463257\n",
      "Train Epoch: 1029 [101376/101520 (100%)] Loss: -1143.126343\n",
      "    epoch          : 1029\n",
      "    loss           : -1151.5396973883087\n",
      "    ess            : 1.9678620393551773\n",
      "    log_marginal   : 1151.5685476945273\n",
      "    log_joint      : 1359.942785828557\n",
      "    val_loss       : -1149.2526643172555\n",
      "    val_ess        : 1.9686724413996157\n",
      "    val_log_marginal: 1149.280570652174\n",
      "    val_log_joint  : 1357.562744140625\n",
      "Train Epoch: 1030 [0/101520 (0%)] Loss: -1151.776123\n",
      "Train Epoch: 1030 [11264/101520 (11%)] Loss: -1145.088257\n",
      "Train Epoch: 1030 [22528/101520 (22%)] Loss: -1152.091064\n",
      "Train Epoch: 1030 [33792/101520 (33%)] Loss: -1141.551025\n",
      "Train Epoch: 1030 [45056/101520 (44%)] Loss: -1154.260254\n",
      "Train Epoch: 1030 [56320/101520 (55%)] Loss: -1152.707275\n",
      "Train Epoch: 1030 [67584/101520 (67%)] Loss: -1147.565186\n",
      "Train Epoch: 1030 [78848/101520 (78%)] Loss: -1152.711182\n",
      "Train Epoch: 1030 [90112/101520 (89%)] Loss: -1147.377441\n",
      "Train Epoch: 1030 [101376/101520 (100%)] Loss: -1130.262695\n",
      "    epoch          : 1030\n",
      "    loss           : -1151.5080468259266\n",
      "    ess            : 1.9679126967137783\n",
      "    log_marginal   : 1151.53684928549\n",
      "    log_joint      : 1359.943341585859\n",
      "    val_loss       : -1151.8569070567255\n",
      "    val_ess        : 1.969157592110012\n",
      "    val_log_marginal: 1151.884282651155\n",
      "    val_log_joint  : 1360.0429952870245\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1030.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1031 [0/101520 (0%)] Loss: -1154.225098\n",
      "Train Epoch: 1031 [11264/101520 (11%)] Loss: -1149.541138\n",
      "Train Epoch: 1031 [22528/101520 (22%)] Loss: -1146.382080\n",
      "Train Epoch: 1031 [33792/101520 (33%)] Loss: -1144.554932\n",
      "Train Epoch: 1031 [45056/101520 (44%)] Loss: -1149.875732\n",
      "Train Epoch: 1031 [56320/101520 (55%)] Loss: -1159.467041\n",
      "Train Epoch: 1031 [67584/101520 (67%)] Loss: -1157.225342\n",
      "Train Epoch: 1031 [78848/101520 (78%)] Loss: -1151.835449\n",
      "Train Epoch: 1031 [90112/101520 (89%)] Loss: -1146.596436\n",
      "Train Epoch: 1031 [101376/101520 (100%)] Loss: -1148.972900\n",
      "    epoch          : 1031\n",
      "    loss           : -1151.7187046070194\n",
      "    ess            : 1.967981975282257\n",
      "    log_marginal   : 1151.7477401656722\n",
      "    log_joint      : 1360.1109422846655\n",
      "    val_loss       : -1150.1121985394022\n",
      "    val_ess        : 1.9650477274604465\n",
      "    val_log_marginal: 1150.1487081776495\n",
      "    val_log_joint  : 1358.2782566236413\n",
      "Train Epoch: 1032 [0/101520 (0%)] Loss: -1151.975098\n",
      "Train Epoch: 1032 [11264/101520 (11%)] Loss: -1149.716309\n",
      "Train Epoch: 1032 [22528/101520 (22%)] Loss: -1152.380371\n",
      "Train Epoch: 1032 [33792/101520 (33%)] Loss: -1154.004395\n",
      "Train Epoch: 1032 [45056/101520 (44%)] Loss: -1144.923828\n",
      "Train Epoch: 1032 [56320/101520 (55%)] Loss: -1158.998779\n",
      "Train Epoch: 1032 [67584/101520 (67%)] Loss: -1149.294189\n",
      "Train Epoch: 1032 [78848/101520 (78%)] Loss: -1150.661377\n",
      "Train Epoch: 1032 [90112/101520 (89%)] Loss: -1158.137451\n",
      "Train Epoch: 1032 [101376/101520 (100%)] Loss: -1155.774902\n",
      "    epoch          : 1032\n",
      "    loss           : -1151.8229698296168\n",
      "    ess            : 1.9680986404418945\n",
      "    log_marginal   : 1151.851742231666\n",
      "    log_joint      : 1360.2488535205323\n",
      "    val_loss       : -1151.6779519786005\n",
      "    val_ess        : 1.9630650852037512\n",
      "    val_log_marginal: 1151.7120361328125\n",
      "    val_log_joint  : 1360.024605129076\n",
      "Train Epoch: 1033 [0/101520 (0%)] Loss: -1151.193115\n",
      "Train Epoch: 1033 [11264/101520 (11%)] Loss: -1152.572266\n",
      "Train Epoch: 1033 [22528/101520 (22%)] Loss: -1153.535645\n",
      "Train Epoch: 1033 [33792/101520 (33%)] Loss: -1148.899170\n",
      "Train Epoch: 1033 [45056/101520 (44%)] Loss: -1147.261719\n",
      "Train Epoch: 1033 [56320/101520 (55%)] Loss: -1153.589233\n",
      "Train Epoch: 1033 [67584/101520 (67%)] Loss: -1149.696777\n",
      "Train Epoch: 1033 [78848/101520 (78%)] Loss: -1161.213867\n",
      "Train Epoch: 1033 [90112/101520 (89%)] Loss: -1153.499268\n",
      "Train Epoch: 1033 [101376/101520 (100%)] Loss: -1144.997437\n",
      "    epoch          : 1033\n",
      "    loss           : -1151.8326814737752\n",
      "    ess            : 1.9682315797662016\n",
      "    log_marginal   : 1151.8611950131517\n",
      "    log_joint      : 1360.1995248459093\n",
      "    val_loss       : -1148.6915920091712\n",
      "    val_ess        : 1.9677640562472136\n",
      "    val_log_marginal: 1148.7215894616168\n",
      "    val_log_joint  : 1357.1734566066575\n",
      "Train Epoch: 1034 [0/101520 (0%)] Loss: -1153.935669\n",
      "Train Epoch: 1034 [11264/101520 (11%)] Loss: -1154.433594\n",
      "Train Epoch: 1034 [22528/101520 (22%)] Loss: -1149.245728\n",
      "Train Epoch: 1034 [33792/101520 (33%)] Loss: -1151.579224\n",
      "Train Epoch: 1034 [45056/101520 (44%)] Loss: -1148.794678\n",
      "Train Epoch: 1034 [56320/101520 (55%)] Loss: -1152.005371\n",
      "Train Epoch: 1034 [67584/101520 (67%)] Loss: -1158.843994\n",
      "Train Epoch: 1034 [78848/101520 (78%)] Loss: -1154.479492\n",
      "Train Epoch: 1034 [90112/101520 (89%)] Loss: -1153.780029\n",
      "Train Epoch: 1034 [101376/101520 (100%)] Loss: -1155.269775\n",
      "    epoch          : 1034\n",
      "    loss           : -1151.9699633421012\n",
      "    ess            : 1.9680932334919072\n",
      "    log_marginal   : 1151.998538836762\n",
      "    log_joint      : 1360.3052383499528\n",
      "    val_loss       : -1150.952201511549\n",
      "    val_ess        : 1.969553221826968\n",
      "    val_log_marginal: 1150.980463442595\n",
      "    val_log_joint  : 1359.4485712466033\n",
      "Train Epoch: 1035 [0/101520 (0%)] Loss: -1149.551025\n",
      "Train Epoch: 1035 [11264/101520 (11%)] Loss: -1151.076660\n",
      "Train Epoch: 1035 [22528/101520 (22%)] Loss: -1153.313843\n",
      "Train Epoch: 1035 [33792/101520 (33%)] Loss: -1148.127197\n",
      "Train Epoch: 1035 [45056/101520 (44%)] Loss: -1150.093750\n",
      "Train Epoch: 1035 [56320/101520 (55%)] Loss: -1156.247803\n",
      "Train Epoch: 1035 [67584/101520 (67%)] Loss: -1157.528198\n",
      "Train Epoch: 1035 [78848/101520 (78%)] Loss: -1149.443115\n",
      "Train Epoch: 1035 [90112/101520 (89%)] Loss: -1155.158447\n",
      "Train Epoch: 1035 [101376/101520 (100%)] Loss: -1160.371582\n",
      "    epoch          : 1035\n",
      "    loss           : -1151.9267823492462\n",
      "    ess            : 1.9679139714744223\n",
      "    log_marginal   : 1151.9559896651224\n",
      "    log_joint      : 1360.3568133636934\n",
      "    val_loss       : -1149.8150687839675\n",
      "    val_ess        : 1.9652051562848298\n",
      "    val_log_marginal: 1149.847512950068\n",
      "    val_log_joint  : 1358.1070609714675\n",
      "Train Epoch: 1036 [0/101520 (0%)] Loss: -1151.848877\n",
      "Train Epoch: 1036 [11264/101520 (11%)] Loss: -1149.843384\n",
      "Train Epoch: 1036 [22528/101520 (22%)] Loss: -1144.587402\n",
      "Train Epoch: 1036 [33792/101520 (33%)] Loss: -1160.180176\n",
      "Train Epoch: 1036 [45056/101520 (44%)] Loss: -1151.599731\n",
      "Train Epoch: 1036 [56320/101520 (55%)] Loss: -1153.630371\n",
      "Train Epoch: 1036 [67584/101520 (67%)] Loss: -1151.535278\n",
      "Train Epoch: 1036 [78848/101520 (78%)] Loss: -1148.088623\n",
      "Train Epoch: 1036 [90112/101520 (89%)] Loss: -1157.234741\n",
      "Train Epoch: 1036 [101376/101520 (100%)] Loss: -1163.789795\n",
      "    epoch          : 1036\n",
      "    loss           : -1152.0264138073178\n",
      "    ess            : 1.968066707927378\n",
      "    log_marginal   : 1152.055104624686\n",
      "    log_joint      : 1360.4347905297975\n",
      "    val_loss       : -1149.992038892663\n",
      "    val_ess        : 1.9664094137108845\n",
      "    val_log_marginal: 1150.0220841117527\n",
      "    val_log_joint  : 1358.381352963655\n",
      "Train Epoch: 1037 [0/101520 (0%)] Loss: -1148.007568\n",
      "Train Epoch: 1037 [11264/101520 (11%)] Loss: -1146.304688\n",
      "Train Epoch: 1037 [22528/101520 (22%)] Loss: -1155.835815\n",
      "Train Epoch: 1037 [33792/101520 (33%)] Loss: -1147.220703\n",
      "Train Epoch: 1037 [45056/101520 (44%)] Loss: -1153.965698\n",
      "Train Epoch: 1037 [56320/101520 (55%)] Loss: -1145.610840\n",
      "Train Epoch: 1037 [67584/101520 (67%)] Loss: -1149.523193\n",
      "Train Epoch: 1037 [78848/101520 (78%)] Loss: -1153.741455\n",
      "Train Epoch: 1037 [90112/101520 (89%)] Loss: -1151.213379\n",
      "Train Epoch: 1037 [101376/101520 (100%)] Loss: -1156.468750\n",
      "    epoch          : 1037\n",
      "    loss           : -1152.05096834269\n",
      "    ess            : 1.9668663452618087\n",
      "    log_marginal   : 1152.0811608089275\n",
      "    log_joint      : 1360.5091552734375\n",
      "    val_loss       : -1152.1268947435462\n",
      "    val_ess        : 1.9686427323714546\n",
      "    val_log_marginal: 1152.1534848420517\n",
      "    val_log_joint  : 1360.3351626188858\n",
      "Train Epoch: 1038 [0/101520 (0%)] Loss: -1153.928467\n",
      "Train Epoch: 1038 [11264/101520 (11%)] Loss: -1155.003052\n",
      "Train Epoch: 1038 [22528/101520 (22%)] Loss: -1153.716064\n",
      "Train Epoch: 1038 [33792/101520 (33%)] Loss: -1156.423340\n",
      "Train Epoch: 1038 [45056/101520 (44%)] Loss: -1157.812500\n",
      "Train Epoch: 1038 [56320/101520 (55%)] Loss: -1155.943481\n",
      "Train Epoch: 1038 [67584/101520 (67%)] Loss: -1152.293579\n",
      "Train Epoch: 1038 [78848/101520 (78%)] Loss: -1146.410889\n",
      "Train Epoch: 1038 [90112/101520 (89%)] Loss: -1149.241821\n",
      "Train Epoch: 1038 [101376/101520 (100%)] Loss: -1158.023071\n",
      "    epoch          : 1038\n",
      "    loss           : -1152.1096571725816\n",
      "    ess            : 1.96742093622984\n",
      "    log_marginal   : 1152.139047900636\n",
      "    log_joint      : 1360.5453807121546\n",
      "    val_loss       : -1152.5231243631115\n",
      "    val_ess        : 1.9679613890855208\n",
      "    val_log_marginal: 1152.5519913383152\n",
      "    val_log_joint  : 1360.836765455163\n",
      "Train Epoch: 1039 [0/101520 (0%)] Loss: -1158.774658\n",
      "Train Epoch: 1039 [11264/101520 (11%)] Loss: -1156.987915\n",
      "Train Epoch: 1039 [22528/101520 (22%)] Loss: -1155.794922\n",
      "Train Epoch: 1039 [33792/101520 (33%)] Loss: -1149.400757\n",
      "Train Epoch: 1039 [45056/101520 (44%)] Loss: -1156.808105\n",
      "Train Epoch: 1039 [56320/101520 (55%)] Loss: -1146.337646\n",
      "Train Epoch: 1039 [67584/101520 (67%)] Loss: -1152.639526\n",
      "Train Epoch: 1039 [78848/101520 (78%)] Loss: -1149.861572\n",
      "Train Epoch: 1039 [90112/101520 (89%)] Loss: -1154.865479\n",
      "Train Epoch: 1039 [101376/101520 (100%)] Loss: -1137.433228\n",
      "    epoch          : 1039\n",
      "    loss           : -1152.050570847401\n",
      "    ess            : 1.9677308863730887\n",
      "    log_marginal   : 1152.0795628533292\n",
      "    log_joint      : 1360.4884425791065\n",
      "    val_loss       : -1151.8449813179348\n",
      "    val_ess        : 1.9665102751358696\n",
      "    val_log_marginal: 1151.8742251188858\n",
      "    val_log_joint  : 1360.083400560462\n",
      "Train Epoch: 1040 [0/101520 (0%)] Loss: -1153.867920\n",
      "Train Epoch: 1040 [11264/101520 (11%)] Loss: -1154.473877\n",
      "Train Epoch: 1040 [22528/101520 (22%)] Loss: -1150.703857\n",
      "Train Epoch: 1040 [33792/101520 (33%)] Loss: -1147.875244\n",
      "Train Epoch: 1040 [45056/101520 (44%)] Loss: -1153.563721\n",
      "Train Epoch: 1040 [56320/101520 (55%)] Loss: -1149.375610\n",
      "Train Epoch: 1040 [67584/101520 (67%)] Loss: -1146.598633\n",
      "Train Epoch: 1040 [78848/101520 (78%)] Loss: -1152.352051\n",
      "Train Epoch: 1040 [90112/101520 (89%)] Loss: -1148.920410\n",
      "Train Epoch: 1040 [101376/101520 (100%)] Loss: -1161.008667\n",
      "    epoch          : 1040\n",
      "    loss           : -1152.2764499990185\n",
      "    ess            : 1.9680275228155317\n",
      "    log_marginal   : 1152.3050181326555\n",
      "    log_joint      : 1360.6542159037374\n",
      "    val_loss       : -1151.456001613451\n",
      "    val_ess        : 1.965892185335574\n",
      "    val_log_marginal: 1151.4865988026495\n",
      "    val_log_joint  : 1359.5624310037365\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1040.pth ...\n",
      "Train Epoch: 1041 [0/101520 (0%)] Loss: -1154.791382\n",
      "Train Epoch: 1041 [11264/101520 (11%)] Loss: -1146.526611\n",
      "Train Epoch: 1041 [22528/101520 (22%)] Loss: -1155.724609\n",
      "Train Epoch: 1041 [33792/101520 (33%)] Loss: -1165.095703\n",
      "Train Epoch: 1041 [45056/101520 (44%)] Loss: -1144.001343\n",
      "Train Epoch: 1041 [56320/101520 (55%)] Loss: -1157.574951\n",
      "Train Epoch: 1041 [67584/101520 (67%)] Loss: -1149.386719\n",
      "Train Epoch: 1041 [78848/101520 (78%)] Loss: -1148.313232\n",
      "Train Epoch: 1041 [90112/101520 (89%)] Loss: -1155.459229\n",
      "Train Epoch: 1041 [101376/101520 (100%)] Loss: -1155.503540\n",
      "    epoch          : 1041\n",
      "    loss           : -1152.3474249911667\n",
      "    ess            : 1.9674897625218684\n",
      "    log_marginal   : 1152.3769200003926\n",
      "    log_joint      : 1360.734195268334\n",
      "    val_loss       : -1150.3471838909647\n",
      "    val_ess        : 1.966948364091956\n",
      "    val_log_marginal: 1150.3768151324728\n",
      "    val_log_joint  : 1358.936231530231\n",
      "Train Epoch: 1042 [0/101520 (0%)] Loss: -1151.150146\n",
      "Train Epoch: 1042 [11264/101520 (11%)] Loss: -1141.776367\n",
      "Train Epoch: 1042 [22528/101520 (22%)] Loss: -1163.234985\n",
      "Train Epoch: 1042 [33792/101520 (33%)] Loss: -1150.354736\n",
      "Train Epoch: 1042 [45056/101520 (44%)] Loss: -1152.468750\n",
      "Train Epoch: 1042 [56320/101520 (55%)] Loss: -1150.034668\n",
      "Train Epoch: 1042 [67584/101520 (67%)] Loss: -1153.455322\n",
      "Train Epoch: 1042 [78848/101520 (78%)] Loss: -1160.092041\n",
      "Train Epoch: 1042 [90112/101520 (89%)] Loss: -1146.080811\n",
      "Train Epoch: 1042 [101376/101520 (100%)] Loss: -1156.299683\n",
      "    epoch          : 1042\n",
      "    loss           : -1152.4284036147535\n",
      "    ess            : 1.9671069683142044\n",
      "    log_marginal   : 1152.4582918253377\n",
      "    log_joint      : 1360.8041182474874\n",
      "    val_loss       : -1152.5424486243207\n",
      "    val_ess        : 1.970538414042929\n",
      "    val_log_marginal: 1152.5674889605978\n",
      "    val_log_joint  : 1360.6240393597147\n",
      "Train Epoch: 1043 [0/101520 (0%)] Loss: -1156.355225\n",
      "Train Epoch: 1043 [11264/101520 (11%)] Loss: -1153.151611\n",
      "Train Epoch: 1043 [22528/101520 (22%)] Loss: -1153.862549\n",
      "Train Epoch: 1043 [33792/101520 (33%)] Loss: -1146.989014\n",
      "Train Epoch: 1043 [45056/101520 (44%)] Loss: -1151.916260\n",
      "Train Epoch: 1043 [56320/101520 (55%)] Loss: -1149.706787\n",
      "Train Epoch: 1043 [67584/101520 (67%)] Loss: -1150.306274\n",
      "Train Epoch: 1043 [78848/101520 (78%)] Loss: -1145.440430\n",
      "Train Epoch: 1043 [90112/101520 (89%)] Loss: -1155.359131\n",
      "Train Epoch: 1043 [101376/101520 (100%)] Loss: -1160.340942\n",
      "    epoch          : 1043\n",
      "    loss           : -1152.555874465099\n",
      "    ess            : 1.9676134466525896\n",
      "    log_marginal   : 1152.5863423563128\n",
      "    log_joint      : 1360.9508461496937\n",
      "    val_loss       : -1149.9515009341033\n",
      "    val_ess        : 1.9677963567816692\n",
      "    val_log_marginal: 1149.978467858356\n",
      "    val_log_joint  : 1358.3173615828805\n",
      "Train Epoch: 1044 [0/101520 (0%)] Loss: -1153.715820\n",
      "Train Epoch: 1044 [11264/101520 (11%)] Loss: -1152.288452\n",
      "Train Epoch: 1044 [22528/101520 (22%)] Loss: -1149.754639\n",
      "Train Epoch: 1044 [33792/101520 (33%)] Loss: -1150.843994\n",
      "Train Epoch: 1044 [45056/101520 (44%)] Loss: -1154.136841\n",
      "Train Epoch: 1044 [56320/101520 (55%)] Loss: -1146.002930\n",
      "Train Epoch: 1044 [67584/101520 (67%)] Loss: -1148.857422\n",
      "Train Epoch: 1044 [78848/101520 (78%)] Loss: -1148.527100\n",
      "Train Epoch: 1044 [90112/101520 (89%)] Loss: -1157.656860\n",
      "Train Epoch: 1044 [101376/101520 (100%)] Loss: -1153.121338\n",
      "    epoch          : 1044\n",
      "    loss           : -1152.5139988271435\n",
      "    ess            : 1.9674597463416095\n",
      "    log_marginal   : 1152.543847165515\n",
      "    log_joint      : 1360.932349736966\n",
      "    val_loss       : -1152.1925632642663\n",
      "    val_ess        : 1.969987506451814\n",
      "    val_log_marginal: 1152.21923828125\n",
      "    val_log_joint  : 1360.4987952190897\n",
      "Train Epoch: 1045 [0/101520 (0%)] Loss: -1155.490723\n",
      "Train Epoch: 1045 [11264/101520 (11%)] Loss: -1151.906250\n",
      "Train Epoch: 1045 [22528/101520 (22%)] Loss: -1152.876831\n",
      "Train Epoch: 1045 [33792/101520 (33%)] Loss: -1150.848389\n",
      "Train Epoch: 1045 [45056/101520 (44%)] Loss: -1143.360352\n",
      "Train Epoch: 1045 [56320/101520 (55%)] Loss: -1158.935425\n",
      "Train Epoch: 1045 [67584/101520 (67%)] Loss: -1150.160889\n",
      "Train Epoch: 1045 [78848/101520 (78%)] Loss: -1154.213867\n",
      "Train Epoch: 1045 [90112/101520 (89%)] Loss: -1152.909912\n",
      "Train Epoch: 1045 [101376/101520 (100%)] Loss: -1136.908325\n",
      "    epoch          : 1045\n",
      "    loss           : -1152.514817741049\n",
      "    ess            : 1.9675317164042487\n",
      "    log_marginal   : 1152.5444918685223\n",
      "    log_joint      : 1360.931018618483\n",
      "    val_loss       : -1152.582620371943\n",
      "    val_ess        : 1.9683078631110813\n",
      "    val_log_marginal: 1152.6150963824728\n",
      "    val_log_joint  : 1360.97338336447\n",
      "Train Epoch: 1046 [0/101520 (0%)] Loss: -1158.972412\n",
      "Train Epoch: 1046 [11264/101520 (11%)] Loss: -1152.422607\n",
      "Train Epoch: 1046 [22528/101520 (22%)] Loss: -1151.450562\n",
      "Train Epoch: 1046 [33792/101520 (33%)] Loss: -1146.942139\n",
      "Train Epoch: 1046 [45056/101520 (44%)] Loss: -1152.174927\n",
      "Train Epoch: 1046 [56320/101520 (55%)] Loss: -1156.737305\n",
      "Train Epoch: 1046 [67584/101520 (67%)] Loss: -1154.364136\n",
      "Train Epoch: 1046 [78848/101520 (78%)] Loss: -1148.628296\n",
      "Train Epoch: 1046 [90112/101520 (89%)] Loss: -1155.331543\n",
      "Train Epoch: 1046 [101376/101520 (100%)] Loss: -1153.441406\n",
      "    epoch          : 1046\n",
      "    loss           : -1152.6460844260364\n",
      "    ess            : 1.9678496194245227\n",
      "    log_marginal   : 1152.6750929942682\n",
      "    log_joint      : 1361.036799598579\n",
      "    val_loss       : -1151.2265996518342\n",
      "    val_ess        : 1.9655532422273054\n",
      "    val_log_marginal: 1151.2552967900815\n",
      "    val_log_joint  : 1359.9194176715353\n",
      "Train Epoch: 1047 [0/101520 (0%)] Loss: -1154.511230\n",
      "Train Epoch: 1047 [11264/101520 (11%)] Loss: -1159.638916\n",
      "Train Epoch: 1047 [22528/101520 (22%)] Loss: -1146.027344\n",
      "Train Epoch: 1047 [33792/101520 (33%)] Loss: -1152.653198\n",
      "Train Epoch: 1047 [45056/101520 (44%)] Loss: -1148.417480\n",
      "Train Epoch: 1047 [56320/101520 (55%)] Loss: -1147.285400\n",
      "Train Epoch: 1047 [67584/101520 (67%)] Loss: -1154.379272\n",
      "Train Epoch: 1047 [78848/101520 (78%)] Loss: -1158.570068\n",
      "Train Epoch: 1047 [90112/101520 (89%)] Loss: -1153.379883\n",
      "Train Epoch: 1047 [101376/101520 (100%)] Loss: -1155.502197\n",
      "    epoch          : 1047\n",
      "    loss           : -1152.7576781613143\n",
      "    ess            : 1.967849730247229\n",
      "    log_marginal   : 1152.7874988958465\n",
      "    log_joint      : 1361.1192976601758\n",
      "    val_loss       : -1152.4240988026495\n",
      "    val_ess        : 1.9672592619191045\n",
      "    val_log_marginal: 1152.45483929178\n",
      "    val_log_joint  : 1361.042337168818\n",
      "Train Epoch: 1048 [0/101520 (0%)] Loss: -1157.678223\n",
      "Train Epoch: 1048 [11264/101520 (11%)] Loss: -1156.653931\n",
      "Train Epoch: 1048 [22528/101520 (22%)] Loss: -1149.888428\n",
      "Train Epoch: 1048 [33792/101520 (33%)] Loss: -1158.204102\n",
      "Train Epoch: 1048 [45056/101520 (44%)] Loss: -1150.444824\n",
      "Train Epoch: 1048 [56320/101520 (55%)] Loss: -1154.678711\n",
      "Train Epoch: 1048 [67584/101520 (67%)] Loss: -1152.083862\n",
      "Train Epoch: 1048 [78848/101520 (78%)] Loss: -1153.023560\n",
      "Train Epoch: 1048 [90112/101520 (89%)] Loss: -1150.099609\n",
      "Train Epoch: 1048 [101376/101520 (100%)] Loss: -1145.836182\n",
      "    epoch          : 1048\n",
      "    loss           : -1152.7731761836526\n",
      "    ess            : 1.9677432254331195\n",
      "    log_marginal   : 1152.8022270777717\n",
      "    log_joint      : 1361.131381394276\n",
      "    val_loss       : -1151.8302532693615\n",
      "    val_ess        : 1.9694769797117815\n",
      "    val_log_marginal: 1151.8578411599865\n",
      "    val_log_joint  : 1360.4334663722825\n",
      "Train Epoch: 1049 [0/101520 (0%)] Loss: -1151.317505\n",
      "Train Epoch: 1049 [11264/101520 (11%)] Loss: -1155.476074\n",
      "Train Epoch: 1049 [22528/101520 (22%)] Loss: -1148.483765\n",
      "Train Epoch: 1049 [33792/101520 (33%)] Loss: -1149.848145\n",
      "Train Epoch: 1049 [45056/101520 (44%)] Loss: -1150.191406\n",
      "Train Epoch: 1049 [56320/101520 (55%)] Loss: -1151.631470\n",
      "Train Epoch: 1049 [67584/101520 (67%)] Loss: -1157.557007\n",
      "Train Epoch: 1049 [78848/101520 (78%)] Loss: -1150.224121\n",
      "Train Epoch: 1049 [90112/101520 (89%)] Loss: -1146.660400\n",
      "Train Epoch: 1049 [101376/101520 (100%)] Loss: -1157.234985\n",
      "    epoch          : 1049\n",
      "    loss           : -1152.818804716944\n",
      "    ess            : 1.9678878574515108\n",
      "    log_marginal   : 1152.8489671256673\n",
      "    log_joint      : 1361.2695729624686\n",
      "    val_loss       : -1153.4174698539402\n",
      "    val_ess        : 1.9666562702344812\n",
      "    val_log_marginal: 1153.4460661514945\n",
      "    val_log_joint  : 1361.8691777768342\n",
      "Train Epoch: 1050 [0/101520 (0%)] Loss: -1159.734619\n",
      "Train Epoch: 1050 [11264/101520 (11%)] Loss: -1145.961060\n",
      "Train Epoch: 1050 [22528/101520 (22%)] Loss: -1142.236938\n",
      "Train Epoch: 1050 [33792/101520 (33%)] Loss: -1152.473633\n",
      "Train Epoch: 1050 [45056/101520 (44%)] Loss: -1156.160156\n",
      "Train Epoch: 1050 [56320/101520 (55%)] Loss: -1157.282227\n",
      "Train Epoch: 1050 [67584/101520 (67%)] Loss: -1159.724854\n",
      "Train Epoch: 1050 [78848/101520 (78%)] Loss: -1155.467896\n",
      "Train Epoch: 1050 [90112/101520 (89%)] Loss: -1149.057861\n",
      "Train Epoch: 1050 [101376/101520 (100%)] Loss: -1157.152954\n",
      "    epoch          : 1050\n",
      "    loss           : -1152.9332845869974\n",
      "    ess            : 1.966723793116047\n",
      "    log_marginal   : 1152.9641640821294\n",
      "    log_joint      : 1361.3625868600816\n",
      "    val_loss       : -1150.9502165421195\n",
      "    val_ess        : 1.9667529904324075\n",
      "    val_log_marginal: 1150.9791418987772\n",
      "    val_log_joint  : 1359.16870647928\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1050.pth ...\n",
      "Train Epoch: 1051 [0/101520 (0%)] Loss: -1155.074951\n",
      "Train Epoch: 1051 [11264/101520 (11%)] Loss: -1160.427246\n",
      "Train Epoch: 1051 [22528/101520 (22%)] Loss: -1155.415405\n",
      "Train Epoch: 1051 [33792/101520 (33%)] Loss: -1160.353394\n",
      "Train Epoch: 1051 [45056/101520 (44%)] Loss: -1150.703613\n",
      "Train Epoch: 1051 [56320/101520 (55%)] Loss: -1152.624390\n",
      "Train Epoch: 1051 [67584/101520 (67%)] Loss: -1156.691284\n",
      "Train Epoch: 1051 [78848/101520 (78%)] Loss: -1150.893066\n",
      "Train Epoch: 1051 [90112/101520 (89%)] Loss: -1142.593262\n",
      "Train Epoch: 1051 [101376/101520 (100%)] Loss: -1135.742432\n",
      "    epoch          : 1051\n",
      "    loss           : -1152.8789105439305\n",
      "    ess            : 1.9672865466256837\n",
      "    log_marginal   : 1152.9090913552136\n",
      "    log_joint      : 1361.3245254593278\n",
      "    val_loss       : -1152.0245945142663\n",
      "    val_ess        : 1.964539999547212\n",
      "    val_log_marginal: 1152.0551492442255\n",
      "    val_log_joint  : 1360.506246815557\n",
      "Train Epoch: 1052 [0/101520 (0%)] Loss: -1148.050171\n",
      "Train Epoch: 1052 [11264/101520 (11%)] Loss: -1153.866943\n",
      "Train Epoch: 1052 [22528/101520 (22%)] Loss: -1153.658691\n",
      "Train Epoch: 1052 [33792/101520 (33%)] Loss: -1150.213379\n",
      "Train Epoch: 1052 [45056/101520 (44%)] Loss: -1154.130127\n",
      "Train Epoch: 1052 [56320/101520 (55%)] Loss: -1152.800049\n",
      "Train Epoch: 1052 [67584/101520 (67%)] Loss: -1153.388916\n",
      "Train Epoch: 1052 [78848/101520 (78%)] Loss: -1158.036255\n",
      "Train Epoch: 1052 [90112/101520 (89%)] Loss: -1159.277832\n",
      "Train Epoch: 1052 [101376/101520 (100%)] Loss: -1153.993774\n",
      "    epoch          : 1052\n",
      "    loss           : -1153.02069490519\n",
      "    ess            : 1.9666694270905538\n",
      "    log_marginal   : 1153.051698924309\n",
      "    log_joint      : 1361.4420521798445\n",
      "    val_loss       : -1151.9593505859375\n",
      "    val_ess        : 1.9687717531038367\n",
      "    val_log_marginal: 1151.9887854534647\n",
      "    val_log_joint  : 1360.335496985394\n",
      "Train Epoch: 1053 [0/101520 (0%)] Loss: -1147.741211\n",
      "Train Epoch: 1053 [11264/101520 (11%)] Loss: -1153.239746\n",
      "Train Epoch: 1053 [22528/101520 (22%)] Loss: -1148.840698\n",
      "Train Epoch: 1053 [33792/101520 (33%)] Loss: -1157.435913\n",
      "Train Epoch: 1053 [45056/101520 (44%)] Loss: -1142.984253\n",
      "Train Epoch: 1053 [56320/101520 (55%)] Loss: -1159.177734\n",
      "Train Epoch: 1053 [67584/101520 (67%)] Loss: -1155.309937\n",
      "Train Epoch: 1053 [78848/101520 (78%)] Loss: -1152.317627\n",
      "Train Epoch: 1053 [90112/101520 (89%)] Loss: -1152.232910\n",
      "Train Epoch: 1053 [101376/101520 (100%)] Loss: -1152.732910\n",
      "    epoch          : 1053\n",
      "    loss           : -1153.1193013406878\n",
      "    ess            : 1.9672516524492196\n",
      "    log_marginal   : 1153.148882841944\n",
      "    log_joint      : 1361.49346525106\n",
      "    val_loss       : -1151.5552819293478\n",
      "    val_ess        : 1.9664033184880796\n",
      "    val_log_marginal: 1151.5858313519022\n",
      "    val_log_joint  : 1359.7321034307065\n",
      "Train Epoch: 1054 [0/101520 (0%)] Loss: -1156.236694\n",
      "Train Epoch: 1054 [11264/101520 (11%)] Loss: -1146.131104\n",
      "Train Epoch: 1054 [22528/101520 (22%)] Loss: -1154.847534\n",
      "Train Epoch: 1054 [33792/101520 (33%)] Loss: -1155.583252\n",
      "Train Epoch: 1054 [45056/101520 (44%)] Loss: -1150.814697\n",
      "Train Epoch: 1054 [56320/101520 (55%)] Loss: -1155.487183\n",
      "Train Epoch: 1054 [67584/101520 (67%)] Loss: -1144.855103\n",
      "Train Epoch: 1054 [78848/101520 (78%)] Loss: -1160.011719\n",
      "Train Epoch: 1054 [90112/101520 (89%)] Loss: -1147.675415\n",
      "Train Epoch: 1054 [101376/101520 (100%)] Loss: -1171.244507\n",
      "    epoch          : 1054\n",
      "    loss           : -1153.156027329028\n",
      "    ess            : 1.9674511986162195\n",
      "    log_marginal   : 1153.1852444596027\n",
      "    log_joint      : 1361.5894131301036\n",
      "    val_loss       : -1152.3153023097825\n",
      "    val_ess        : 1.9644476330798606\n",
      "    val_log_marginal: 1152.3479322350543\n",
      "    val_log_joint  : 1360.9355574898098\n",
      "Train Epoch: 1055 [0/101520 (0%)] Loss: -1148.235840\n",
      "Train Epoch: 1055 [11264/101520 (11%)] Loss: -1151.365479\n",
      "Train Epoch: 1055 [22528/101520 (22%)] Loss: -1144.090088\n",
      "Train Epoch: 1055 [33792/101520 (33%)] Loss: -1152.667847\n",
      "Train Epoch: 1055 [45056/101520 (44%)] Loss: -1157.246582\n",
      "Train Epoch: 1055 [56320/101520 (55%)] Loss: -1150.725342\n",
      "Train Epoch: 1055 [67584/101520 (67%)] Loss: -1154.118774\n",
      "Train Epoch: 1055 [78848/101520 (78%)] Loss: -1152.675659\n",
      "Train Epoch: 1055 [90112/101520 (89%)] Loss: -1148.410889\n",
      "Train Epoch: 1055 [101376/101520 (100%)] Loss: -1164.583130\n",
      "    epoch          : 1055\n",
      "    loss           : -1153.226022691583\n",
      "    ess            : 1.9677841435724766\n",
      "    log_marginal   : 1153.2547478603958\n",
      "    log_joint      : 1361.652359698885\n",
      "    val_loss       : -1152.103367017663\n",
      "    val_ess        : 1.9696979781855708\n",
      "    val_log_marginal: 1152.1288637907608\n",
      "    val_log_joint  : 1360.6164869225543\n",
      "Train Epoch: 1056 [0/101520 (0%)] Loss: -1153.385498\n",
      "Train Epoch: 1056 [11264/101520 (11%)] Loss: -1154.289551\n",
      "Train Epoch: 1056 [22528/101520 (22%)] Loss: -1157.102539\n",
      "Train Epoch: 1056 [33792/101520 (33%)] Loss: -1159.570190\n",
      "Train Epoch: 1056 [45056/101520 (44%)] Loss: -1150.404297\n",
      "Train Epoch: 1056 [56320/101520 (55%)] Loss: -1148.435791\n",
      "Train Epoch: 1056 [67584/101520 (67%)] Loss: -1154.348145\n",
      "Train Epoch: 1056 [78848/101520 (78%)] Loss: -1156.097778\n",
      "Train Epoch: 1056 [90112/101520 (89%)] Loss: -1156.781372\n",
      "Train Epoch: 1056 [101376/101520 (100%)] Loss: -1143.046021\n",
      "    epoch          : 1056\n",
      "    loss           : -1153.2624750952025\n",
      "    ess            : 1.9678586469822792\n",
      "    log_marginal   : 1153.2915989861417\n",
      "    log_joint      : 1361.6654181552292\n",
      "    val_loss       : -1154.3345575747283\n",
      "    val_ess        : 1.967227816581726\n",
      "    val_log_marginal: 1154.3648310122283\n",
      "    val_log_joint  : 1362.9564527428668\n",
      "Train Epoch: 1057 [0/101520 (0%)] Loss: -1157.223877\n",
      "Train Epoch: 1057 [11264/101520 (11%)] Loss: -1157.119629\n",
      "Train Epoch: 1057 [22528/101520 (22%)] Loss: -1152.812988\n",
      "Train Epoch: 1057 [33792/101520 (33%)] Loss: -1155.310303\n",
      "Train Epoch: 1057 [45056/101520 (44%)] Loss: -1153.724731\n",
      "Train Epoch: 1057 [56320/101520 (55%)] Loss: -1148.969482\n",
      "Train Epoch: 1057 [67584/101520 (67%)] Loss: -1155.616333\n",
      "Train Epoch: 1057 [78848/101520 (78%)] Loss: -1147.803345\n",
      "Train Epoch: 1057 [90112/101520 (89%)] Loss: -1142.140625\n",
      "Train Epoch: 1057 [101376/101520 (100%)] Loss: -1167.690796\n",
      "    epoch          : 1057\n",
      "    loss           : -1153.487794809006\n",
      "    ess            : 1.967591979395804\n",
      "    log_marginal   : 1153.517678112241\n",
      "    log_joint      : 1361.8693344652952\n",
      "    val_loss       : -1153.6937255859375\n",
      "    val_ess        : 1.9664245636566826\n",
      "    val_log_marginal: 1153.7225713315217\n",
      "    val_log_joint  : 1362.1132175611413\n",
      "Train Epoch: 1058 [0/101520 (0%)] Loss: -1155.637085\n",
      "Train Epoch: 1058 [11264/101520 (11%)] Loss: -1155.745361\n",
      "Train Epoch: 1058 [22528/101520 (22%)] Loss: -1153.488647\n",
      "Train Epoch: 1058 [33792/101520 (33%)] Loss: -1149.402710\n",
      "Train Epoch: 1058 [45056/101520 (44%)] Loss: -1153.925293\n",
      "Train Epoch: 1058 [56320/101520 (55%)] Loss: -1153.607300\n",
      "Train Epoch: 1058 [67584/101520 (67%)] Loss: -1154.939697\n",
      "Train Epoch: 1058 [78848/101520 (78%)] Loss: -1151.184326\n",
      "Train Epoch: 1058 [90112/101520 (89%)] Loss: -1157.473389\n",
      "Train Epoch: 1058 [101376/101520 (100%)] Loss: -1154.575562\n",
      "    epoch          : 1058\n",
      "    loss           : -1153.4758270110317\n",
      "    ess            : 1.9673876690505139\n",
      "    log_marginal   : 1153.5052827614636\n",
      "    log_joint      : 1361.948340334485\n",
      "    val_loss       : -1151.1780952785325\n",
      "    val_ess        : 1.9664819136909817\n",
      "    val_log_marginal: 1151.2071108610733\n",
      "    val_log_joint  : 1359.704542077106\n",
      "Train Epoch: 1059 [0/101520 (0%)] Loss: -1155.414673\n",
      "Train Epoch: 1059 [11264/101520 (11%)] Loss: -1155.873779\n",
      "Train Epoch: 1059 [22528/101520 (22%)] Loss: -1150.346191\n",
      "Train Epoch: 1059 [33792/101520 (33%)] Loss: -1154.208374\n",
      "Train Epoch: 1059 [45056/101520 (44%)] Loss: -1166.112793\n",
      "Train Epoch: 1059 [56320/101520 (55%)] Loss: -1153.652832\n",
      "Train Epoch: 1059 [67584/101520 (67%)] Loss: -1150.586792\n",
      "Train Epoch: 1059 [78848/101520 (78%)] Loss: -1161.696289\n",
      "Train Epoch: 1059 [90112/101520 (89%)] Loss: -1151.431763\n",
      "Train Epoch: 1059 [101376/101520 (100%)] Loss: -1159.440796\n",
      "    epoch          : 1059\n",
      "    loss           : -1153.5653376747016\n",
      "    ess            : 1.9681059793012226\n",
      "    log_marginal   : 1153.593949361063\n",
      "    log_joint      : 1361.9757951132617\n",
      "    val_loss       : -1152.485744310462\n",
      "    val_ess        : 1.9695056003072988\n",
      "    val_log_marginal: 1152.5113578464675\n",
      "    val_log_joint  : 1360.709812330163\n",
      "Train Epoch: 1060 [0/101520 (0%)] Loss: -1152.205078\n",
      "Train Epoch: 1060 [11264/101520 (11%)] Loss: -1147.586426\n",
      "Train Epoch: 1060 [22528/101520 (22%)] Loss: -1154.714844\n",
      "Train Epoch: 1060 [33792/101520 (33%)] Loss: -1148.871338\n",
      "Train Epoch: 1060 [45056/101520 (44%)] Loss: -1154.718506\n",
      "Train Epoch: 1060 [56320/101520 (55%)] Loss: -1145.729492\n",
      "Train Epoch: 1060 [67584/101520 (67%)] Loss: -1158.256226\n",
      "Train Epoch: 1060 [78848/101520 (78%)] Loss: -1152.683105\n",
      "Train Epoch: 1060 [90112/101520 (89%)] Loss: -1147.560059\n",
      "Train Epoch: 1060 [101376/101520 (100%)] Loss: -1146.437012\n",
      "    epoch          : 1060\n",
      "    loss           : -1153.6404385207286\n",
      "    ess            : 1.9676177891055544\n",
      "    log_marginal   : 1153.6703580156643\n",
      "    log_joint      : 1362.016357421875\n",
      "    val_loss       : -1153.1240128226902\n",
      "    val_ess        : 1.9656709743582683\n",
      "    val_log_marginal: 1153.1569664996603\n",
      "    val_log_joint  : 1361.7676152768342\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1060.pth ...\n",
      "Train Epoch: 1061 [0/101520 (0%)] Loss: -1148.525757\n",
      "Train Epoch: 1061 [11264/101520 (11%)] Loss: -1153.051758\n",
      "Train Epoch: 1061 [22528/101520 (22%)] Loss: -1158.782227\n",
      "Train Epoch: 1061 [33792/101520 (33%)] Loss: -1152.313599\n",
      "Train Epoch: 1061 [45056/101520 (44%)] Loss: -1157.007202\n",
      "Train Epoch: 1061 [56320/101520 (55%)] Loss: -1153.907227\n",
      "Train Epoch: 1061 [67584/101520 (67%)] Loss: -1154.915039\n",
      "Train Epoch: 1061 [78848/101520 (78%)] Loss: -1153.328369\n",
      "Train Epoch: 1061 [90112/101520 (89%)] Loss: -1149.115234\n",
      "Train Epoch: 1061 [101376/101520 (100%)] Loss: -1159.764282\n",
      "    epoch          : 1061\n",
      "    loss           : -1153.779330613026\n",
      "    ess            : 1.9681426424476969\n",
      "    log_marginal   : 1153.807661967062\n",
      "    log_joint      : 1362.1720878562735\n",
      "    val_loss       : -1153.211324940557\n",
      "    val_ess        : 1.966551511184029\n",
      "    val_log_marginal: 1153.2418372112772\n",
      "    val_log_joint  : 1361.4665633491848\n",
      "Train Epoch: 1062 [0/101520 (0%)] Loss: -1157.723022\n",
      "Train Epoch: 1062 [11264/101520 (11%)] Loss: -1152.316895\n",
      "Train Epoch: 1062 [22528/101520 (22%)] Loss: -1153.080444\n",
      "Train Epoch: 1062 [33792/101520 (33%)] Loss: -1155.895142\n",
      "Train Epoch: 1062 [45056/101520 (44%)] Loss: -1155.306396\n",
      "Train Epoch: 1062 [56320/101520 (55%)] Loss: -1160.282227\n",
      "Train Epoch: 1062 [67584/101520 (67%)] Loss: -1148.244629\n",
      "Train Epoch: 1062 [78848/101520 (78%)] Loss: -1158.681396\n",
      "Train Epoch: 1062 [90112/101520 (89%)] Loss: -1156.665771\n",
      "Train Epoch: 1062 [101376/101520 (100%)] Loss: -1163.923950\n",
      "    epoch          : 1062\n",
      "    loss           : -1153.865975998155\n",
      "    ess            : 1.9685902236095025\n",
      "    log_marginal   : 1153.8941582914572\n",
      "    log_joint      : 1362.23467618856\n",
      "    val_loss       : -1151.9349418308425\n",
      "    val_ess        : 1.9673734851505444\n",
      "    val_log_marginal: 1151.9635858950408\n",
      "    val_log_joint  : 1360.2881124745245\n",
      "Train Epoch: 1063 [0/101520 (0%)] Loss: -1149.272949\n",
      "Train Epoch: 1063 [11264/101520 (11%)] Loss: -1149.569702\n",
      "Train Epoch: 1063 [22528/101520 (22%)] Loss: -1165.904297\n",
      "Train Epoch: 1063 [33792/101520 (33%)] Loss: -1156.156128\n",
      "Train Epoch: 1063 [45056/101520 (44%)] Loss: -1154.825073\n",
      "Train Epoch: 1063 [56320/101520 (55%)] Loss: -1150.790771\n",
      "Train Epoch: 1063 [67584/101520 (67%)] Loss: -1151.033569\n",
      "Train Epoch: 1063 [78848/101520 (78%)] Loss: -1156.854248\n",
      "Train Epoch: 1063 [90112/101520 (89%)] Loss: -1153.568115\n",
      "Train Epoch: 1063 [101376/101520 (100%)] Loss: -1146.673218\n",
      "    epoch          : 1063\n",
      "    loss           : -1153.890334852976\n",
      "    ess            : 1.9680712804123386\n",
      "    log_marginal   : 1153.9193066160883\n",
      "    log_joint      : 1362.3159854448022\n",
      "    val_loss       : -1152.8550070057745\n",
      "    val_ess        : 1.9687727793403293\n",
      "    val_log_marginal: 1152.8835130774457\n",
      "    val_log_joint  : 1361.3325938349185\n",
      "Train Epoch: 1064 [0/101520 (0%)] Loss: -1158.108154\n",
      "Train Epoch: 1064 [11264/101520 (11%)] Loss: -1155.841431\n",
      "Train Epoch: 1064 [22528/101520 (22%)] Loss: -1161.952881\n",
      "Train Epoch: 1064 [33792/101520 (33%)] Loss: -1147.336426\n",
      "Train Epoch: 1064 [45056/101520 (44%)] Loss: -1148.330078\n",
      "Train Epoch: 1064 [56320/101520 (55%)] Loss: -1146.920898\n",
      "Train Epoch: 1064 [67584/101520 (67%)] Loss: -1154.664062\n",
      "Train Epoch: 1064 [78848/101520 (78%)] Loss: -1149.633301\n",
      "Train Epoch: 1064 [90112/101520 (89%)] Loss: -1154.226074\n",
      "Train Epoch: 1064 [101376/101520 (100%)] Loss: -1153.496582\n",
      "    epoch          : 1064\n",
      "    loss           : -1154.059844510639\n",
      "    ess            : 1.9678722362422465\n",
      "    log_marginal   : 1154.0890021396042\n",
      "    log_joint      : 1362.40249894492\n",
      "    val_loss       : -1151.7713676120925\n",
      "    val_ess        : 1.9678532869919487\n",
      "    val_log_marginal: 1151.79907757303\n",
      "    val_log_joint  : 1360.205131199049\n",
      "Train Epoch: 1065 [0/101520 (0%)] Loss: -1152.811523\n",
      "Train Epoch: 1065 [11264/101520 (11%)] Loss: -1158.886108\n",
      "Train Epoch: 1065 [22528/101520 (22%)] Loss: -1162.718262\n",
      "Train Epoch: 1065 [33792/101520 (33%)] Loss: -1146.642090\n",
      "Train Epoch: 1065 [45056/101520 (44%)] Loss: -1153.814453\n",
      "Train Epoch: 1065 [56320/101520 (55%)] Loss: -1154.691040\n",
      "Train Epoch: 1065 [67584/101520 (67%)] Loss: -1157.739746\n",
      "Train Epoch: 1065 [78848/101520 (78%)] Loss: -1151.512451\n",
      "Train Epoch: 1065 [90112/101520 (89%)] Loss: -1146.279297\n",
      "Train Epoch: 1065 [101376/101520 (100%)] Loss: -1153.682739\n",
      "    epoch          : 1065\n",
      "    loss           : -1154.0393575543735\n",
      "    ess            : 1.9673391963968325\n",
      "    log_marginal   : 1154.0688133048052\n",
      "    log_joint      : 1362.422459587979\n",
      "    val_loss       : -1153.0370032269022\n",
      "    val_ess        : 1.9693578118863313\n",
      "    val_log_marginal: 1153.0636941661005\n",
      "    val_log_joint  : 1361.1606126868207\n",
      "Train Epoch: 1066 [0/101520 (0%)] Loss: -1149.058105\n",
      "Train Epoch: 1066 [11264/101520 (11%)] Loss: -1157.504272\n",
      "Train Epoch: 1066 [22528/101520 (22%)] Loss: -1153.085327\n",
      "Train Epoch: 1066 [33792/101520 (33%)] Loss: -1151.602783\n",
      "Train Epoch: 1066 [45056/101520 (44%)] Loss: -1146.282227\n",
      "Train Epoch: 1066 [56320/101520 (55%)] Loss: -1154.094482\n",
      "Train Epoch: 1066 [67584/101520 (67%)] Loss: -1156.639160\n",
      "Train Epoch: 1066 [78848/101520 (78%)] Loss: -1159.144775\n",
      "Train Epoch: 1066 [90112/101520 (89%)] Loss: -1159.583374\n",
      "Train Epoch: 1066 [101376/101520 (100%)] Loss: -1147.828979\n",
      "    epoch          : 1066\n",
      "    loss           : -1154.0319548180355\n",
      "    ess            : 1.9678518748163578\n",
      "    log_marginal   : 1154.0614357186323\n",
      "    log_joint      : 1362.4363516802764\n",
      "    val_loss       : -1152.4713187839675\n",
      "    val_ess        : 1.9658838095872297\n",
      "    val_log_marginal: 1152.5008014181385\n",
      "    val_log_joint  : 1360.6415378736413\n",
      "Train Epoch: 1067 [0/101520 (0%)] Loss: -1154.930786\n",
      "Train Epoch: 1067 [11264/101520 (11%)] Loss: -1155.174316\n",
      "Train Epoch: 1067 [22528/101520 (22%)] Loss: -1153.453613\n",
      "Train Epoch: 1067 [33792/101520 (33%)] Loss: -1153.394775\n",
      "Train Epoch: 1067 [45056/101520 (44%)] Loss: -1156.344116\n",
      "Train Epoch: 1067 [56320/101520 (55%)] Loss: -1149.118042\n",
      "Train Epoch: 1067 [67584/101520 (67%)] Loss: -1152.067383\n",
      "Train Epoch: 1067 [78848/101520 (78%)] Loss: -1155.617065\n",
      "Train Epoch: 1067 [90112/101520 (89%)] Loss: -1153.281860\n",
      "Train Epoch: 1067 [101376/101520 (100%)] Loss: -1162.211670\n",
      "    epoch          : 1067\n",
      "    loss           : -1154.3116915142116\n",
      "    ess            : 1.9676793945494608\n",
      "    log_marginal   : 1154.3408515968515\n",
      "    log_joint      : 1362.653245475424\n",
      "    val_loss       : -1153.5630678923233\n",
      "    val_ess        : 1.9649375003317129\n",
      "    val_log_marginal: 1153.5971998131793\n",
      "    val_log_joint  : 1361.9194176715353\n",
      "Train Epoch: 1068 [0/101520 (0%)] Loss: -1154.292114\n",
      "Train Epoch: 1068 [11264/101520 (11%)] Loss: -1152.545898\n",
      "Train Epoch: 1068 [22528/101520 (22%)] Loss: -1152.453857\n",
      "Train Epoch: 1068 [33792/101520 (33%)] Loss: -1154.028931\n",
      "Train Epoch: 1068 [45056/101520 (44%)] Loss: -1145.207642\n",
      "Train Epoch: 1068 [56320/101520 (55%)] Loss: -1159.519409\n",
      "Train Epoch: 1068 [67584/101520 (67%)] Loss: -1150.912109\n",
      "Train Epoch: 1068 [78848/101520 (78%)] Loss: -1149.637451\n",
      "Train Epoch: 1068 [90112/101520 (89%)] Loss: -1156.454590\n",
      "Train Epoch: 1068 [101376/101520 (100%)] Loss: -1148.193359\n",
      "    epoch          : 1068\n",
      "    loss           : -1154.1743016842022\n",
      "    ess            : 1.9675875458885077\n",
      "    log_marginal   : 1154.2039936008166\n",
      "    log_joint      : 1362.554448880143\n",
      "    val_loss       : -1153.7917108950408\n",
      "    val_ess        : 1.9694227964981743\n",
      "    val_log_marginal: 1153.8201267408288\n",
      "    val_log_joint  : 1362.3221010954483\n",
      "Train Epoch: 1069 [0/101520 (0%)] Loss: -1156.922363\n",
      "Train Epoch: 1069 [11264/101520 (11%)] Loss: -1162.043945\n",
      "Train Epoch: 1069 [22528/101520 (22%)] Loss: -1149.947266\n",
      "Train Epoch: 1069 [33792/101520 (33%)] Loss: -1148.905762\n",
      "Train Epoch: 1069 [45056/101520 (44%)] Loss: -1154.252441\n",
      "Train Epoch: 1069 [56320/101520 (55%)] Loss: -1155.617188\n",
      "Train Epoch: 1069 [67584/101520 (67%)] Loss: -1157.792969\n",
      "Train Epoch: 1069 [78848/101520 (78%)] Loss: -1156.551025\n",
      "Train Epoch: 1069 [90112/101520 (89%)] Loss: -1155.851196\n",
      "Train Epoch: 1069 [101376/101520 (100%)] Loss: -1167.907104\n",
      "    epoch          : 1069\n",
      "    loss           : -1154.3141114508087\n",
      "    ess            : 1.967953199118226\n",
      "    log_marginal   : 1154.3430433417086\n",
      "    log_joint      : 1362.755606033095\n",
      "    val_loss       : -1154.3752865998642\n",
      "    val_ess        : 1.9678627615389617\n",
      "    val_log_marginal: 1154.4081235139267\n",
      "    val_log_joint  : 1362.4008576766305\n",
      "Train Epoch: 1070 [0/101520 (0%)] Loss: -1154.007568\n",
      "Train Epoch: 1070 [11264/101520 (11%)] Loss: -1154.217773\n",
      "Train Epoch: 1070 [22528/101520 (22%)] Loss: -1154.151001\n",
      "Train Epoch: 1070 [33792/101520 (33%)] Loss: -1161.158936\n",
      "Train Epoch: 1070 [45056/101520 (44%)] Loss: -1158.435303\n",
      "Train Epoch: 1070 [56320/101520 (55%)] Loss: -1153.192871\n",
      "Train Epoch: 1070 [67584/101520 (67%)] Loss: -1156.352539\n",
      "Train Epoch: 1070 [78848/101520 (78%)] Loss: -1151.577393\n",
      "Train Epoch: 1070 [90112/101520 (89%)] Loss: -1146.655029\n",
      "Train Epoch: 1070 [101376/101520 (100%)] Loss: -1152.648682\n",
      "    epoch          : 1070\n",
      "    loss           : -1154.3971499342415\n",
      "    ess            : 1.9673702561076563\n",
      "    log_marginal   : 1154.4267780553157\n",
      "    log_joint      : 1362.8125294440954\n",
      "    val_loss       : -1153.64233929178\n",
      "    val_ess        : 1.9697074838306592\n",
      "    val_log_marginal: 1153.6721987516983\n",
      "    val_log_joint  : 1361.9381103515625\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1070.pth ...\n",
      "Train Epoch: 1071 [0/101520 (0%)] Loss: -1156.573242\n",
      "Train Epoch: 1071 [11264/101520 (11%)] Loss: -1153.535156\n",
      "Train Epoch: 1071 [22528/101520 (22%)] Loss: -1149.860107\n",
      "Train Epoch: 1071 [33792/101520 (33%)] Loss: -1149.671387\n",
      "Train Epoch: 1071 [45056/101520 (44%)] Loss: -1156.533936\n",
      "Train Epoch: 1071 [56320/101520 (55%)] Loss: -1150.844604\n",
      "Train Epoch: 1071 [67584/101520 (67%)] Loss: -1161.773926\n",
      "Train Epoch: 1071 [78848/101520 (78%)] Loss: -1154.486694\n",
      "Train Epoch: 1071 [90112/101520 (89%)] Loss: -1154.205933\n",
      "Train Epoch: 1071 [101376/101520 (100%)] Loss: -1140.063354\n",
      "    epoch          : 1071\n",
      "    loss           : -1154.464794063089\n",
      "    ess            : 1.9680827586495098\n",
      "    log_marginal   : 1154.4941295834642\n",
      "    log_joint      : 1362.850808608472\n",
      "    val_loss       : -1154.723930027174\n",
      "    val_ess        : 1.9698020219802856\n",
      "    val_log_marginal: 1154.7498142408288\n",
      "    val_log_joint  : 1363.127096424932\n",
      "Train Epoch: 1072 [0/101520 (0%)] Loss: -1152.549561\n",
      "Train Epoch: 1072 [11264/101520 (11%)] Loss: -1154.879517\n",
      "Train Epoch: 1072 [22528/101520 (22%)] Loss: -1151.766235\n",
      "Train Epoch: 1072 [33792/101520 (33%)] Loss: -1156.455322\n",
      "Train Epoch: 1072 [45056/101520 (44%)] Loss: -1155.904053\n",
      "Train Epoch: 1072 [56320/101520 (55%)] Loss: -1152.208984\n",
      "Train Epoch: 1072 [67584/101520 (67%)] Loss: -1154.760498\n",
      "Train Epoch: 1072 [78848/101520 (78%)] Loss: -1162.291382\n",
      "Train Epoch: 1072 [90112/101520 (89%)] Loss: -1153.061523\n",
      "Train Epoch: 1072 [101376/101520 (100%)] Loss: -1144.240845\n",
      "    epoch          : 1072\n",
      "    loss           : -1154.5114083601602\n",
      "    ess            : 1.9669112781783444\n",
      "    log_marginal   : 1154.5423406092964\n",
      "    log_joint      : 1362.9308959347518\n",
      "    val_loss       : -1153.6000711192255\n",
      "    val_ess        : 1.9656567210736482\n",
      "    val_log_marginal: 1153.6311194378397\n",
      "    val_log_joint  : 1361.9324473505435\n",
      "Train Epoch: 1073 [0/101520 (0%)] Loss: -1160.394653\n",
      "Train Epoch: 1073 [11264/101520 (11%)] Loss: -1154.385620\n",
      "Train Epoch: 1073 [22528/101520 (22%)] Loss: -1147.516602\n",
      "Train Epoch: 1073 [33792/101520 (33%)] Loss: -1153.377686\n",
      "Train Epoch: 1073 [45056/101520 (44%)] Loss: -1159.304443\n",
      "Train Epoch: 1073 [56320/101520 (55%)] Loss: -1152.424438\n",
      "Train Epoch: 1073 [67584/101520 (67%)] Loss: -1158.094238\n",
      "Train Epoch: 1073 [78848/101520 (78%)] Loss: -1160.578857\n",
      "Train Epoch: 1073 [90112/101520 (89%)] Loss: -1149.898926\n",
      "Train Epoch: 1073 [101376/101520 (100%)] Loss: -1165.170166\n",
      "    epoch          : 1073\n",
      "    loss           : -1154.5564130466787\n",
      "    ess            : 1.967429825408974\n",
      "    log_marginal   : 1154.586707340413\n",
      "    log_joint      : 1363.0470375559437\n",
      "    val_loss       : -1153.8484629755435\n",
      "    val_ess        : 1.9662868354631506\n",
      "    val_log_marginal: 1153.8759924847147\n",
      "    val_log_joint  : 1362.2588155995245\n",
      "Train Epoch: 1074 [0/101520 (0%)] Loss: -1155.983154\n",
      "Train Epoch: 1074 [11264/101520 (11%)] Loss: -1156.814087\n",
      "Train Epoch: 1074 [22528/101520 (22%)] Loss: -1157.261230\n",
      "Train Epoch: 1074 [33792/101520 (33%)] Loss: -1158.987305\n",
      "Train Epoch: 1074 [45056/101520 (44%)] Loss: -1156.908691\n",
      "Train Epoch: 1074 [56320/101520 (55%)] Loss: -1155.463135\n",
      "Train Epoch: 1074 [67584/101520 (67%)] Loss: -1155.368896\n",
      "Train Epoch: 1074 [78848/101520 (78%)] Loss: -1153.471069\n",
      "Train Epoch: 1074 [90112/101520 (89%)] Loss: -1152.109375\n",
      "Train Epoch: 1074 [101376/101520 (100%)] Loss: -1151.534302\n",
      "    epoch          : 1074\n",
      "    loss           : -1154.693577138623\n",
      "    ess            : 1.967396348565068\n",
      "    log_marginal   : 1154.7236855665044\n",
      "    log_joint      : 1363.0493568918812\n",
      "    val_loss       : -1153.7795569378397\n",
      "    val_ess        : 1.9685796136441438\n",
      "    val_log_marginal: 1153.8073836616848\n",
      "    val_log_joint  : 1362.1966871178668\n",
      "Train Epoch: 1075 [0/101520 (0%)] Loss: -1159.182861\n",
      "Train Epoch: 1075 [11264/101520 (11%)] Loss: -1154.511963\n",
      "Train Epoch: 1075 [22528/101520 (22%)] Loss: -1155.660400\n",
      "Train Epoch: 1075 [33792/101520 (33%)] Loss: -1148.669189\n",
      "Train Epoch: 1075 [45056/101520 (44%)] Loss: -1146.125488\n",
      "Train Epoch: 1075 [56320/101520 (55%)] Loss: -1153.680420\n",
      "Train Epoch: 1075 [67584/101520 (67%)] Loss: -1154.721191\n",
      "Train Epoch: 1075 [78848/101520 (78%)] Loss: -1155.012695\n",
      "Train Epoch: 1075 [90112/101520 (89%)] Loss: -1156.600342\n",
      "Train Epoch: 1075 [101376/101520 (100%)] Loss: -1153.541626\n",
      "    epoch          : 1075\n",
      "    loss           : -1154.687929393059\n",
      "    ess            : 1.9675084388435786\n",
      "    log_marginal   : 1154.71796053019\n",
      "    log_joint      : 1363.09641039671\n",
      "    val_loss       : -1153.1523065981658\n",
      "    val_ess        : 1.9669811984767085\n",
      "    val_log_marginal: 1153.1833602241848\n",
      "    val_log_joint  : 1361.3267822265625\n",
      "Train Epoch: 1076 [0/101520 (0%)] Loss: -1153.517334\n",
      "Train Epoch: 1076 [11264/101520 (11%)] Loss: -1154.523315\n",
      "Train Epoch: 1076 [22528/101520 (22%)] Loss: -1155.018677\n",
      "Train Epoch: 1076 [33792/101520 (33%)] Loss: -1156.878052\n",
      "Train Epoch: 1076 [45056/101520 (44%)] Loss: -1156.305664\n",
      "Train Epoch: 1076 [56320/101520 (55%)] Loss: -1152.306519\n",
      "Train Epoch: 1076 [67584/101520 (67%)] Loss: -1154.621826\n",
      "Train Epoch: 1076 [78848/101520 (78%)] Loss: -1155.691284\n",
      "Train Epoch: 1076 [90112/101520 (89%)] Loss: -1155.289795\n",
      "Train Epoch: 1076 [101376/101520 (100%)] Loss: -1147.350220\n",
      "    epoch          : 1076\n",
      "    loss           : -1154.6969067750863\n",
      "    ess            : 1.9682045953357639\n",
      "    log_marginal   : 1154.726646538356\n",
      "    log_joint      : 1363.1376474658448\n",
      "    val_loss       : -1153.819341244905\n",
      "    val_ess        : 1.9659633170003477\n",
      "    val_log_marginal: 1153.853563391644\n",
      "    val_log_joint  : 1361.9680759595788\n",
      "Train Epoch: 1077 [0/101520 (0%)] Loss: -1150.335693\n",
      "Train Epoch: 1077 [11264/101520 (11%)] Loss: -1155.849121\n",
      "Train Epoch: 1077 [22528/101520 (22%)] Loss: -1151.734619\n",
      "Train Epoch: 1077 [33792/101520 (33%)] Loss: -1159.702148\n",
      "Train Epoch: 1077 [45056/101520 (44%)] Loss: -1151.883301\n",
      "Train Epoch: 1077 [56320/101520 (55%)] Loss: -1147.590210\n",
      "Train Epoch: 1077 [67584/101520 (67%)] Loss: -1153.219116\n",
      "Train Epoch: 1077 [78848/101520 (78%)] Loss: -1152.875244\n",
      "Train Epoch: 1077 [90112/101520 (89%)] Loss: -1155.129150\n",
      "Train Epoch: 1077 [101376/101520 (100%)] Loss: -1147.227783\n",
      "    epoch          : 1077\n",
      "    loss           : -1154.7169097440326\n",
      "    ess            : 1.9678970862872636\n",
      "    log_marginal   : 1154.7458428617697\n",
      "    log_joint      : 1363.1844967022614\n",
      "    val_loss       : -1154.7322201936142\n",
      "    val_ess        : 1.9663851986760679\n",
      "    val_log_marginal: 1154.7617877462635\n",
      "    val_log_joint  : 1363.056545091712\n",
      "Train Epoch: 1078 [0/101520 (0%)] Loss: -1160.199463\n",
      "Train Epoch: 1078 [11264/101520 (11%)] Loss: -1151.276367\n",
      "Train Epoch: 1078 [22528/101520 (22%)] Loss: -1150.136719\n",
      "Train Epoch: 1078 [33792/101520 (33%)] Loss: -1159.441406\n",
      "Train Epoch: 1078 [45056/101520 (44%)] Loss: -1158.109497\n",
      "Train Epoch: 1078 [56320/101520 (55%)] Loss: -1155.387573\n",
      "Train Epoch: 1078 [67584/101520 (67%)] Loss: -1160.368774\n",
      "Train Epoch: 1078 [78848/101520 (78%)] Loss: -1152.614990\n",
      "Train Epoch: 1078 [90112/101520 (89%)] Loss: -1156.180176\n",
      "Train Epoch: 1078 [101376/101520 (100%)] Loss: -1157.508911\n",
      "    epoch          : 1078\n",
      "    loss           : -1154.8770751953125\n",
      "    ess            : 1.9679565980805824\n",
      "    log_marginal   : 1154.9066524026382\n",
      "    log_joint      : 1363.2681792752826\n",
      "    val_loss       : -1153.3082699983017\n",
      "    val_ess        : 1.9681683156801306\n",
      "    val_log_marginal: 1153.3368822180707\n",
      "    val_log_joint  : 1361.59839397928\n",
      "Train Epoch: 1079 [0/101520 (0%)] Loss: -1154.660889\n",
      "Train Epoch: 1079 [11264/101520 (11%)] Loss: -1155.694580\n",
      "Train Epoch: 1079 [22528/101520 (22%)] Loss: -1152.575439\n",
      "Train Epoch: 1079 [33792/101520 (33%)] Loss: -1152.848145\n",
      "Train Epoch: 1079 [45056/101520 (44%)] Loss: -1152.784302\n",
      "Train Epoch: 1079 [56320/101520 (55%)] Loss: -1156.772949\n",
      "Train Epoch: 1079 [67584/101520 (67%)] Loss: -1156.192017\n",
      "Train Epoch: 1079 [78848/101520 (78%)] Loss: -1149.871826\n",
      "Train Epoch: 1079 [90112/101520 (89%)] Loss: -1152.121338\n",
      "Train Epoch: 1079 [101376/101520 (100%)] Loss: -1150.237915\n",
      "    epoch          : 1079\n",
      "    loss           : -1154.7946470634422\n",
      "    ess            : 1.967235439386799\n",
      "    log_marginal   : 1154.8257289867306\n",
      "    log_joint      : 1363.2774977180827\n",
      "    val_loss       : -1154.6977220618207\n",
      "    val_ess        : 1.968934028044991\n",
      "    val_log_marginal: 1154.7246465268342\n",
      "    val_log_joint  : 1363.362989342731\n",
      "Train Epoch: 1080 [0/101520 (0%)] Loss: -1157.269531\n",
      "Train Epoch: 1080 [11264/101520 (11%)] Loss: -1156.678589\n",
      "Train Epoch: 1080 [22528/101520 (22%)] Loss: -1156.821289\n",
      "Train Epoch: 1080 [33792/101520 (33%)] Loss: -1153.828613\n",
      "Train Epoch: 1080 [45056/101520 (44%)] Loss: -1160.785400\n",
      "Train Epoch: 1080 [56320/101520 (55%)] Loss: -1152.272949\n",
      "Train Epoch: 1080 [67584/101520 (67%)] Loss: -1154.095825\n",
      "Train Epoch: 1080 [78848/101520 (78%)] Loss: -1158.423828\n",
      "Train Epoch: 1080 [90112/101520 (89%)] Loss: -1152.427612\n",
      "Train Epoch: 1080 [101376/101520 (100%)] Loss: -1145.424927\n",
      "    epoch          : 1080\n",
      "    loss           : -1154.970417885325\n",
      "    ess            : 1.9675589955631811\n",
      "    log_marginal   : 1155.000354555983\n",
      "    log_joint      : 1363.3796803843436\n",
      "    val_loss       : -1154.385933254076\n",
      "    val_ess        : 1.9701524661934895\n",
      "    val_log_marginal: 1154.413383152174\n",
      "    val_log_joint  : 1362.763284434443\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1080.pth ...\n",
      "Train Epoch: 1081 [0/101520 (0%)] Loss: -1159.278076\n",
      "Train Epoch: 1081 [11264/101520 (11%)] Loss: -1159.328369\n",
      "Train Epoch: 1081 [22528/101520 (22%)] Loss: -1158.758545\n",
      "Train Epoch: 1081 [33792/101520 (33%)] Loss: -1158.098389\n",
      "Train Epoch: 1081 [45056/101520 (44%)] Loss: -1154.552246\n",
      "Train Epoch: 1081 [56320/101520 (55%)] Loss: -1159.143555\n",
      "Train Epoch: 1081 [67584/101520 (67%)] Loss: -1147.491089\n",
      "Train Epoch: 1081 [78848/101520 (78%)] Loss: -1157.459717\n",
      "Train Epoch: 1081 [90112/101520 (89%)] Loss: -1155.835449\n",
      "Train Epoch: 1081 [101376/101520 (100%)] Loss: -1137.374634\n",
      "    epoch          : 1081\n",
      "    loss           : -1154.9702810929648\n",
      "    ess            : 1.9676919103267805\n",
      "    log_marginal   : 1155.0004717189463\n",
      "    log_joint      : 1363.3917880417714\n",
      "    val_loss       : -1153.9345543902853\n",
      "    val_ess        : 1.9659433675848919\n",
      "    val_log_marginal: 1153.9641007133152\n",
      "    val_log_joint  : 1362.3212466032608\n",
      "Train Epoch: 1082 [0/101520 (0%)] Loss: -1150.770874\n",
      "Train Epoch: 1082 [11264/101520 (11%)] Loss: -1154.233398\n",
      "Train Epoch: 1082 [22528/101520 (22%)] Loss: -1152.265625\n",
      "Train Epoch: 1082 [33792/101520 (33%)] Loss: -1153.914062\n",
      "Train Epoch: 1082 [45056/101520 (44%)] Loss: -1160.883789\n",
      "Train Epoch: 1082 [56320/101520 (55%)] Loss: -1160.947388\n",
      "Train Epoch: 1082 [67584/101520 (67%)] Loss: -1154.589600\n",
      "Train Epoch: 1082 [78848/101520 (78%)] Loss: -1159.581299\n",
      "Train Epoch: 1082 [90112/101520 (89%)] Loss: -1151.442627\n",
      "Train Epoch: 1082 [101376/101520 (100%)] Loss: -1153.697144\n",
      "    epoch          : 1082\n",
      "    loss           : -1155.087833577065\n",
      "    ess            : 1.9674474138710367\n",
      "    log_marginal   : 1155.1175978770807\n",
      "    log_joint      : 1363.4855552174938\n",
      "    val_loss       : -1152.939400050951\n",
      "    val_ess        : 1.9682312581850134\n",
      "    val_log_marginal: 1152.9656663977582\n",
      "    val_log_joint  : 1361.5753810716712\n",
      "Train Epoch: 1083 [0/101520 (0%)] Loss: -1154.249390\n",
      "Train Epoch: 1083 [11264/101520 (11%)] Loss: -1156.147705\n",
      "Train Epoch: 1083 [22528/101520 (22%)] Loss: -1148.606445\n",
      "Train Epoch: 1083 [33792/101520 (33%)] Loss: -1155.764282\n",
      "Train Epoch: 1083 [45056/101520 (44%)] Loss: -1154.554443\n",
      "Train Epoch: 1083 [56320/101520 (55%)] Loss: -1154.996704\n",
      "Train Epoch: 1083 [67584/101520 (67%)] Loss: -1153.336060\n",
      "Train Epoch: 1083 [78848/101520 (78%)] Loss: -1158.561401\n",
      "Train Epoch: 1083 [90112/101520 (89%)] Loss: -1148.202026\n",
      "Train Epoch: 1083 [101376/101520 (100%)] Loss: -1162.978760\n",
      "    epoch          : 1083\n",
      "    loss           : -1155.1153595124058\n",
      "    ess            : 1.968844513198239\n",
      "    log_marginal   : 1155.1431743679334\n",
      "    log_joint      : 1363.610544175958\n",
      "    val_loss       : -1153.5886124320652\n",
      "    val_ess        : 1.965525544208029\n",
      "    val_log_marginal: 1153.6221870754075\n",
      "    val_log_joint  : 1362.2057203209918\n",
      "Train Epoch: 1084 [0/101520 (0%)] Loss: -1157.284424\n",
      "Train Epoch: 1084 [11264/101520 (11%)] Loss: -1156.521118\n",
      "Train Epoch: 1084 [22528/101520 (22%)] Loss: -1152.719971\n",
      "Train Epoch: 1084 [33792/101520 (33%)] Loss: -1155.768555\n",
      "Train Epoch: 1084 [45056/101520 (44%)] Loss: -1155.509521\n",
      "Train Epoch: 1084 [56320/101520 (55%)] Loss: -1155.609253\n",
      "Train Epoch: 1084 [67584/101520 (67%)] Loss: -1157.293457\n",
      "Train Epoch: 1084 [78848/101520 (78%)] Loss: -1157.384521\n",
      "Train Epoch: 1084 [90112/101520 (89%)] Loss: -1154.955322\n",
      "Train Epoch: 1084 [101376/101520 (100%)] Loss: -1154.957642\n",
      "    epoch          : 1084\n",
      "    loss           : -1155.256262391057\n",
      "    ess            : 1.967847633601433\n",
      "    log_marginal   : 1155.285207777167\n",
      "    log_joint      : 1363.660624901853\n",
      "    val_loss       : -1155.407523777174\n",
      "    val_ess        : 1.9670582024947456\n",
      "    val_log_marginal: 1155.4386304772418\n",
      "    val_log_joint  : 1363.8512069038723\n",
      "Train Epoch: 1085 [0/101520 (0%)] Loss: -1152.364746\n",
      "Train Epoch: 1085 [11264/101520 (11%)] Loss: -1154.312866\n",
      "Train Epoch: 1085 [22528/101520 (22%)] Loss: -1164.669678\n",
      "Train Epoch: 1085 [33792/101520 (33%)] Loss: -1153.908691\n",
      "Train Epoch: 1085 [45056/101520 (44%)] Loss: -1154.524414\n",
      "Train Epoch: 1085 [56320/101520 (55%)] Loss: -1150.454590\n",
      "Train Epoch: 1085 [67584/101520 (67%)] Loss: -1162.642212\n",
      "Train Epoch: 1085 [78848/101520 (78%)] Loss: -1159.356934\n",
      "Train Epoch: 1085 [90112/101520 (89%)] Loss: -1151.813477\n",
      "Train Epoch: 1085 [101376/101520 (100%)] Loss: -1147.330200\n",
      "    epoch          : 1085\n",
      "    loss           : -1155.292840545501\n",
      "    ess            : 1.968029016825422\n",
      "    log_marginal   : 1155.3216675418107\n",
      "    log_joint      : 1363.6765216463175\n",
      "    val_loss       : -1154.4026038128397\n",
      "    val_ess        : 1.9681856321251912\n",
      "    val_log_marginal: 1154.429878566576\n",
      "    val_log_joint  : 1362.9018713909647\n",
      "Train Epoch: 1086 [0/101520 (0%)] Loss: -1154.119141\n",
      "Train Epoch: 1086 [11264/101520 (11%)] Loss: -1156.800537\n",
      "Train Epoch: 1086 [22528/101520 (22%)] Loss: -1149.363037\n",
      "Train Epoch: 1086 [33792/101520 (33%)] Loss: -1148.089600\n",
      "Train Epoch: 1086 [45056/101520 (44%)] Loss: -1151.343262\n",
      "Train Epoch: 1086 [56320/101520 (55%)] Loss: -1157.798218\n",
      "Train Epoch: 1086 [67584/101520 (67%)] Loss: -1160.093262\n",
      "Train Epoch: 1086 [78848/101520 (78%)] Loss: -1162.924561\n",
      "Train Epoch: 1086 [90112/101520 (89%)] Loss: -1157.651611\n",
      "Train Epoch: 1086 [101376/101520 (100%)] Loss: -1153.944214\n",
      "    epoch          : 1086\n",
      "    loss           : -1155.3775260580244\n",
      "    ess            : 1.967334867721826\n",
      "    log_marginal   : 1155.408189016371\n",
      "    log_joint      : 1363.811990249097\n",
      "    val_loss       : -1151.673000169837\n",
      "    val_ess        : 1.9672580128130706\n",
      "    val_log_marginal: 1151.7048923658288\n",
      "    val_log_joint  : 1360.2985999065897\n",
      "Train Epoch: 1087 [0/101520 (0%)] Loss: -1151.261475\n",
      "Train Epoch: 1087 [11264/101520 (11%)] Loss: -1157.006836\n",
      "Train Epoch: 1087 [22528/101520 (22%)] Loss: -1166.177246\n",
      "Train Epoch: 1087 [33792/101520 (33%)] Loss: -1154.565674\n",
      "Train Epoch: 1087 [45056/101520 (44%)] Loss: -1154.942627\n",
      "Train Epoch: 1087 [56320/101520 (55%)] Loss: -1154.251099\n",
      "Train Epoch: 1087 [67584/101520 (67%)] Loss: -1159.191162\n",
      "Train Epoch: 1087 [78848/101520 (78%)] Loss: -1150.603149\n",
      "Train Epoch: 1087 [90112/101520 (89%)] Loss: -1154.349854\n",
      "Train Epoch: 1087 [101376/101520 (100%)] Loss: -1154.242920\n",
      "    epoch          : 1087\n",
      "    loss           : -1155.4681537570666\n",
      "    ess            : 1.9674058535590244\n",
      "    log_marginal   : 1155.497141469064\n",
      "    log_joint      : 1363.8244512356705\n",
      "    val_loss       : -1153.307569420856\n",
      "    val_ess        : 1.968397829843604\n",
      "    val_log_marginal: 1153.3350670855978\n",
      "    val_log_joint  : 1362.030416737432\n",
      "Train Epoch: 1088 [0/101520 (0%)] Loss: -1153.406738\n",
      "Train Epoch: 1088 [11264/101520 (11%)] Loss: -1162.061035\n",
      "Train Epoch: 1088 [22528/101520 (22%)] Loss: -1153.661255\n",
      "Train Epoch: 1088 [33792/101520 (33%)] Loss: -1152.717529\n",
      "Train Epoch: 1088 [45056/101520 (44%)] Loss: -1143.607178\n",
      "Train Epoch: 1088 [56320/101520 (55%)] Loss: -1154.134033\n",
      "Train Epoch: 1088 [67584/101520 (67%)] Loss: -1158.919556\n",
      "Train Epoch: 1088 [78848/101520 (78%)] Loss: -1162.312500\n",
      "Train Epoch: 1088 [90112/101520 (89%)] Loss: -1149.052979\n",
      "Train Epoch: 1088 [101376/101520 (100%)] Loss: -1153.975952\n",
      "    epoch          : 1088\n",
      "    loss           : -1155.5765092552606\n",
      "    ess            : 1.9675920728463023\n",
      "    log_marginal   : 1155.6062122134108\n",
      "    log_joint      : 1363.8718592964824\n",
      "    val_loss       : -1154.215629245924\n",
      "    val_ess        : 1.9671378498492034\n",
      "    val_log_marginal: 1154.2501273777175\n",
      "    val_log_joint  : 1362.6905623726223\n",
      "Train Epoch: 1089 [0/101520 (0%)] Loss: -1157.437744\n",
      "Train Epoch: 1089 [11264/101520 (11%)] Loss: -1153.724609\n",
      "Train Epoch: 1089 [22528/101520 (22%)] Loss: -1158.247192\n",
      "Train Epoch: 1089 [33792/101520 (33%)] Loss: -1155.646240\n",
      "Train Epoch: 1089 [45056/101520 (44%)] Loss: -1153.346802\n",
      "Train Epoch: 1089 [56320/101520 (55%)] Loss: -1151.450928\n",
      "Train Epoch: 1089 [67584/101520 (67%)] Loss: -1162.382202\n",
      "Train Epoch: 1089 [78848/101520 (78%)] Loss: -1147.230835\n",
      "Train Epoch: 1089 [90112/101520 (89%)] Loss: -1155.771729\n",
      "Train Epoch: 1089 [101376/101520 (100%)] Loss: -1143.891357\n",
      "    epoch          : 1089\n",
      "    loss           : -1155.497089941897\n",
      "    ess            : 1.9675643953246686\n",
      "    log_marginal   : 1155.5268971812186\n",
      "    log_joint      : 1363.921654169284\n",
      "    val_loss       : -1154.5953793733017\n",
      "    val_ess        : 1.966935598331949\n",
      "    val_log_marginal: 1154.6237103006115\n",
      "    val_log_joint  : 1363.262106190557\n",
      "Train Epoch: 1090 [0/101520 (0%)] Loss: -1146.524536\n",
      "Train Epoch: 1090 [11264/101520 (11%)] Loss: -1155.422119\n",
      "Train Epoch: 1090 [22528/101520 (22%)] Loss: -1157.735352\n",
      "Train Epoch: 1090 [33792/101520 (33%)] Loss: -1164.604614\n",
      "Train Epoch: 1090 [45056/101520 (44%)] Loss: -1142.913574\n",
      "Train Epoch: 1090 [56320/101520 (55%)] Loss: -1158.143433\n",
      "Train Epoch: 1090 [67584/101520 (67%)] Loss: -1154.724365\n",
      "Train Epoch: 1090 [78848/101520 (78%)] Loss: -1162.535034\n",
      "Train Epoch: 1090 [90112/101520 (89%)] Loss: -1154.318604\n",
      "Train Epoch: 1090 [101376/101520 (100%)] Loss: -1155.828735\n",
      "    epoch          : 1090\n",
      "    loss           : -1155.602225605567\n",
      "    ess            : 1.9673675544297875\n",
      "    log_marginal   : 1155.632279439188\n",
      "    log_joint      : 1363.967783865617\n",
      "    val_loss       : -1154.5009606402853\n",
      "    val_ess        : 1.9673636166945747\n",
      "    val_log_marginal: 1154.531934655231\n",
      "    val_log_joint  : 1362.7923053243885\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1090.pth ...\n",
      "Train Epoch: 1091 [0/101520 (0%)] Loss: -1151.679688\n",
      "Train Epoch: 1091 [11264/101520 (11%)] Loss: -1156.343140\n",
      "Train Epoch: 1091 [22528/101520 (22%)] Loss: -1152.066895\n",
      "Train Epoch: 1091 [33792/101520 (33%)] Loss: -1153.401733\n",
      "Train Epoch: 1091 [45056/101520 (44%)] Loss: -1150.414551\n",
      "Train Epoch: 1091 [56320/101520 (55%)] Loss: -1152.064697\n",
      "Train Epoch: 1091 [67584/101520 (67%)] Loss: -1154.219482\n",
      "Train Epoch: 1091 [78848/101520 (78%)] Loss: -1155.842163\n",
      "Train Epoch: 1091 [90112/101520 (89%)] Loss: -1163.204590\n",
      "Train Epoch: 1091 [101376/101520 (100%)] Loss: -1148.228760\n",
      "    epoch          : 1091\n",
      "    loss           : -1155.615806694606\n",
      "    ess            : 1.967917185332907\n",
      "    log_marginal   : 1155.644387096616\n",
      "    log_joint      : 1364.0809221890704\n",
      "    val_loss       : -1154.5026483950408\n",
      "    val_ess        : 1.9688929060231084\n",
      "    val_log_marginal: 1154.53052288553\n",
      "    val_log_joint  : 1362.940572987432\n",
      "Train Epoch: 1092 [0/101520 (0%)] Loss: -1158.670654\n",
      "Train Epoch: 1092 [11264/101520 (11%)] Loss: -1164.179932\n",
      "Train Epoch: 1092 [22528/101520 (22%)] Loss: -1155.957275\n",
      "Train Epoch: 1092 [33792/101520 (33%)] Loss: -1156.330811\n",
      "Train Epoch: 1092 [45056/101520 (44%)] Loss: -1161.267822\n",
      "Train Epoch: 1092 [56320/101520 (55%)] Loss: -1164.172119\n",
      "Train Epoch: 1092 [67584/101520 (67%)] Loss: -1160.271484\n",
      "Train Epoch: 1092 [78848/101520 (78%)] Loss: -1160.387695\n",
      "Train Epoch: 1092 [90112/101520 (89%)] Loss: -1157.350220\n",
      "Train Epoch: 1092 [101376/101520 (100%)] Loss: -1169.297607\n",
      "    epoch          : 1092\n",
      "    loss           : -1155.7894649026382\n",
      "    ess            : 1.9678313833984298\n",
      "    log_marginal   : 1155.8197248449278\n",
      "    log_joint      : 1364.1955321038788\n",
      "    val_loss       : -1154.5381496263587\n",
      "    val_ess        : 1.9710716993912407\n",
      "    val_log_marginal: 1154.5639966881793\n",
      "    val_log_joint  : 1362.97681194803\n",
      "Train Epoch: 1093 [0/101520 (0%)] Loss: -1154.409668\n",
      "Train Epoch: 1093 [11264/101520 (11%)] Loss: -1157.311646\n",
      "Train Epoch: 1093 [22528/101520 (22%)] Loss: -1155.816406\n",
      "Train Epoch: 1093 [33792/101520 (33%)] Loss: -1153.413818\n",
      "Train Epoch: 1093 [45056/101520 (44%)] Loss: -1156.960449\n",
      "Train Epoch: 1093 [56320/101520 (55%)] Loss: -1156.388184\n",
      "Train Epoch: 1093 [67584/101520 (67%)] Loss: -1149.318604\n",
      "Train Epoch: 1093 [78848/101520 (78%)] Loss: -1153.875977\n",
      "Train Epoch: 1093 [90112/101520 (89%)] Loss: -1157.729492\n",
      "Train Epoch: 1093 [101376/101520 (100%)] Loss: -1162.218628\n",
      "    epoch          : 1093\n",
      "    loss           : -1155.8362485032585\n",
      "    ess            : 1.9677998732082809\n",
      "    log_marginal   : 1155.8652460299545\n",
      "    log_joint      : 1364.2531462242855\n",
      "    val_loss       : -1153.2241423233695\n",
      "    val_ess        : 1.9672275677971218\n",
      "    val_log_marginal: 1153.2526059358017\n",
      "    val_log_joint  : 1361.3404647163723\n",
      "Train Epoch: 1094 [0/101520 (0%)] Loss: -1157.080688\n",
      "Train Epoch: 1094 [11264/101520 (11%)] Loss: -1159.681641\n",
      "Train Epoch: 1094 [22528/101520 (22%)] Loss: -1161.214111\n",
      "Train Epoch: 1094 [33792/101520 (33%)] Loss: -1163.325928\n",
      "Train Epoch: 1094 [45056/101520 (44%)] Loss: -1155.835083\n",
      "Train Epoch: 1094 [56320/101520 (55%)] Loss: -1150.445557\n",
      "Train Epoch: 1094 [67584/101520 (67%)] Loss: -1160.560181\n",
      "Train Epoch: 1094 [78848/101520 (78%)] Loss: -1153.449951\n",
      "Train Epoch: 1094 [90112/101520 (89%)] Loss: -1160.621826\n",
      "Train Epoch: 1094 [101376/101520 (100%)] Loss: -1165.447632\n",
      "    epoch          : 1094\n",
      "    loss           : -1155.9291660941426\n",
      "    ess            : 1.967414208393001\n",
      "    log_marginal   : 1155.9590469437028\n",
      "    log_joint      : 1364.3153806140076\n",
      "    val_loss       : -1152.642726732337\n",
      "    val_ess        : 1.9674232057903125\n",
      "    val_log_marginal: 1152.6716149371603\n",
      "    val_log_joint  : 1361.0538913892663\n",
      "Train Epoch: 1095 [0/101520 (0%)] Loss: -1154.898438\n",
      "Train Epoch: 1095 [11264/101520 (11%)] Loss: -1162.333984\n",
      "Train Epoch: 1095 [22528/101520 (22%)] Loss: -1154.035034\n",
      "Train Epoch: 1095 [33792/101520 (33%)] Loss: -1155.712158\n",
      "Train Epoch: 1095 [45056/101520 (44%)] Loss: -1147.627197\n",
      "Train Epoch: 1095 [56320/101520 (55%)] Loss: -1154.380615\n",
      "Train Epoch: 1095 [67584/101520 (67%)] Loss: -1153.488770\n",
      "Train Epoch: 1095 [78848/101520 (78%)] Loss: -1155.093262\n",
      "Train Epoch: 1095 [90112/101520 (89%)] Loss: -1161.772461\n",
      "Train Epoch: 1095 [101376/101520 (100%)] Loss: -1155.537598\n",
      "    epoch          : 1095\n",
      "    loss           : -1155.8975345477386\n",
      "    ess            : 1.9672278471328506\n",
      "    log_marginal   : 1155.9282183623195\n",
      "    log_joint      : 1364.2718113271435\n",
      "    val_loss       : -1153.3491317085598\n",
      "    val_ess        : 1.9704748340274976\n",
      "    val_log_marginal: 1153.3745223335598\n",
      "    val_log_joint  : 1361.8592635444973\n",
      "Train Epoch: 1096 [0/101520 (0%)] Loss: -1158.897461\n",
      "Train Epoch: 1096 [11264/101520 (11%)] Loss: -1153.833862\n",
      "Train Epoch: 1096 [22528/101520 (22%)] Loss: -1153.701050\n",
      "Train Epoch: 1096 [33792/101520 (33%)] Loss: -1160.637207\n",
      "Train Epoch: 1096 [45056/101520 (44%)] Loss: -1158.293091\n",
      "Train Epoch: 1096 [56320/101520 (55%)] Loss: -1160.525391\n",
      "Train Epoch: 1096 [67584/101520 (67%)] Loss: -1153.871582\n",
      "Train Epoch: 1096 [78848/101520 (78%)] Loss: -1147.878906\n",
      "Train Epoch: 1096 [90112/101520 (89%)] Loss: -1150.609375\n",
      "Train Epoch: 1096 [101376/101520 (100%)] Loss: -1149.999023\n",
      "    epoch          : 1096\n",
      "    loss           : -1155.853143279876\n",
      "    ess            : 1.9670442887886086\n",
      "    log_marginal   : 1155.8837148388427\n",
      "    log_joint      : 1364.327748974364\n",
      "    val_loss       : -1154.8958315641983\n",
      "    val_ess        : 1.9682624962018884\n",
      "    val_log_marginal: 1154.92553180197\n",
      "    val_log_joint  : 1363.0958570397418\n",
      "Train Epoch: 1097 [0/101520 (0%)] Loss: -1154.055420\n",
      "Train Epoch: 1097 [11264/101520 (11%)] Loss: -1160.830444\n",
      "Train Epoch: 1097 [22528/101520 (22%)] Loss: -1158.261841\n",
      "Train Epoch: 1097 [33792/101520 (33%)] Loss: -1152.011719\n",
      "Train Epoch: 1097 [45056/101520 (44%)] Loss: -1153.222656\n",
      "Train Epoch: 1097 [56320/101520 (55%)] Loss: -1158.050781\n",
      "Train Epoch: 1097 [67584/101520 (67%)] Loss: -1146.690063\n",
      "Train Epoch: 1097 [78848/101520 (78%)] Loss: -1153.198853\n",
      "Train Epoch: 1097 [90112/101520 (89%)] Loss: -1152.247192\n",
      "Train Epoch: 1097 [101376/101520 (100%)] Loss: -1158.902954\n",
      "    epoch          : 1097\n",
      "    loss           : -1156.0620166260992\n",
      "    ess            : 1.9669188931958759\n",
      "    log_marginal   : 1156.0926875588882\n",
      "    log_joint      : 1364.426645556886\n",
      "    val_loss       : -1155.3447424847147\n",
      "    val_ess        : 1.963426341181216\n",
      "    val_log_marginal: 1155.386670983356\n",
      "    val_log_joint  : 1363.6467975118885\n",
      "Train Epoch: 1098 [0/101520 (0%)] Loss: -1153.677612\n",
      "Train Epoch: 1098 [11264/101520 (11%)] Loss: -1155.697510\n",
      "Train Epoch: 1098 [22528/101520 (22%)] Loss: -1162.198242\n",
      "Train Epoch: 1098 [33792/101520 (33%)] Loss: -1155.381714\n",
      "Train Epoch: 1098 [45056/101520 (44%)] Loss: -1154.551758\n",
      "Train Epoch: 1098 [56320/101520 (55%)] Loss: -1157.837769\n",
      "Train Epoch: 1098 [67584/101520 (67%)] Loss: -1161.349854\n",
      "Train Epoch: 1098 [78848/101520 (78%)] Loss: -1155.675537\n",
      "Train Epoch: 1098 [90112/101520 (89%)] Loss: -1155.858398\n",
      "Train Epoch: 1098 [101376/101520 (100%)] Loss: -1160.714478\n",
      "    epoch          : 1098\n",
      "    loss           : -1156.0311745495053\n",
      "    ess            : 1.9681120889270725\n",
      "    log_marginal   : 1156.061316715413\n",
      "    log_joint      : 1364.4633776794126\n",
      "    val_loss       : -1154.572801672894\n",
      "    val_ess        : 1.9680516305177107\n",
      "    val_log_marginal: 1154.6022259256115\n",
      "    val_log_joint  : 1362.975049889606\n",
      "Train Epoch: 1099 [0/101520 (0%)] Loss: -1154.810425\n",
      "Train Epoch: 1099 [11264/101520 (11%)] Loss: -1158.335205\n",
      "Train Epoch: 1099 [22528/101520 (22%)] Loss: -1151.722168\n",
      "Train Epoch: 1099 [33792/101520 (33%)] Loss: -1148.719116\n",
      "Train Epoch: 1099 [45056/101520 (44%)] Loss: -1158.226929\n",
      "Train Epoch: 1099 [56320/101520 (55%)] Loss: -1151.997070\n",
      "Train Epoch: 1099 [67584/101520 (67%)] Loss: -1148.984009\n",
      "Train Epoch: 1099 [78848/101520 (78%)] Loss: -1149.102783\n",
      "Train Epoch: 1099 [90112/101520 (89%)] Loss: -1156.023438\n",
      "Train Epoch: 1099 [101376/101520 (100%)] Loss: -1155.690063\n",
      "    epoch          : 1099\n",
      "    loss           : -1156.1503415515076\n",
      "    ess            : 1.9675669628172066\n",
      "    log_marginal   : 1156.1795672699434\n",
      "    log_joint      : 1364.5874882223618\n",
      "    val_loss       : -1155.3604205587635\n",
      "    val_ess        : 1.9701860054679539\n",
      "    val_log_marginal: 1155.3876263162365\n",
      "    val_log_joint  : 1363.6608515200408\n",
      "Train Epoch: 1100 [0/101520 (0%)] Loss: -1155.853027\n",
      "Train Epoch: 1100 [11264/101520 (11%)] Loss: -1153.803711\n",
      "Train Epoch: 1100 [22528/101520 (22%)] Loss: -1150.604004\n",
      "Train Epoch: 1100 [33792/101520 (33%)] Loss: -1156.875244\n",
      "Train Epoch: 1100 [45056/101520 (44%)] Loss: -1151.979614\n",
      "Train Epoch: 1100 [56320/101520 (55%)] Loss: -1158.989746\n",
      "Train Epoch: 1100 [67584/101520 (67%)] Loss: -1149.964600\n",
      "Train Epoch: 1100 [78848/101520 (78%)] Loss: -1154.666748\n",
      "Train Epoch: 1100 [90112/101520 (89%)] Loss: -1152.712158\n",
      "Train Epoch: 1100 [101376/101520 (100%)] Loss: -1163.737305\n",
      "    epoch          : 1100\n",
      "    loss           : -1156.2871694900282\n",
      "    ess            : 1.9683608625402402\n",
      "    log_marginal   : 1156.316256575848\n",
      "    log_joint      : 1364.6391748782978\n",
      "    val_loss       : -1155.2478611158288\n",
      "    val_ess        : 1.9692429044972295\n",
      "    val_log_marginal: 1155.2744565217392\n",
      "    val_log_joint  : 1363.81494140625\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1100.pth ...\n",
      "Train Epoch: 1101 [0/101520 (0%)] Loss: -1158.743652\n",
      "Train Epoch: 1101 [11264/101520 (11%)] Loss: -1170.137451\n",
      "Train Epoch: 1101 [22528/101520 (22%)] Loss: -1166.885742\n",
      "Train Epoch: 1101 [33792/101520 (33%)] Loss: -1158.321289\n",
      "Train Epoch: 1101 [45056/101520 (44%)] Loss: -1157.804565\n",
      "Train Epoch: 1101 [56320/101520 (55%)] Loss: -1162.828369\n",
      "Train Epoch: 1101 [67584/101520 (67%)] Loss: -1152.073730\n",
      "Train Epoch: 1101 [78848/101520 (78%)] Loss: -1155.675293\n",
      "Train Epoch: 1101 [90112/101520 (89%)] Loss: -1161.437988\n",
      "Train Epoch: 1101 [101376/101520 (100%)] Loss: -1144.277832\n",
      "    epoch          : 1101\n",
      "    loss           : -1156.1494030209642\n",
      "    ess            : 1.9678472022914408\n",
      "    log_marginal   : 1156.1787121643374\n",
      "    log_joint      : 1364.598075214942\n",
      "    val_loss       : -1155.7476010529892\n",
      "    val_ess        : 1.9687789004781973\n",
      "    val_log_marginal: 1155.7766909391983\n",
      "    val_log_joint  : 1364.1467497452445\n",
      "Train Epoch: 1102 [0/101520 (0%)] Loss: -1149.980713\n",
      "Train Epoch: 1102 [11264/101520 (11%)] Loss: -1167.411865\n",
      "Train Epoch: 1102 [22528/101520 (22%)] Loss: -1148.462402\n",
      "Train Epoch: 1102 [33792/101520 (33%)] Loss: -1158.991821\n",
      "Train Epoch: 1102 [45056/101520 (44%)] Loss: -1158.617065\n",
      "Train Epoch: 1102 [56320/101520 (55%)] Loss: -1157.742188\n",
      "Train Epoch: 1102 [67584/101520 (67%)] Loss: -1155.247192\n",
      "Train Epoch: 1102 [78848/101520 (78%)] Loss: -1157.442139\n",
      "Train Epoch: 1102 [90112/101520 (89%)] Loss: -1150.099121\n",
      "Train Epoch: 1102 [101376/101520 (100%)] Loss: -1160.678345\n",
      "    epoch          : 1102\n",
      "    loss           : -1156.3489174387562\n",
      "    ess            : 1.967608863983921\n",
      "    log_marginal   : 1156.3787633234533\n",
      "    log_joint      : 1364.6999254082914\n",
      "    val_loss       : -1154.9259033203125\n",
      "    val_ess        : 1.9686273077259893\n",
      "    val_log_marginal: 1154.9548923658288\n",
      "    val_log_joint  : 1363.7526537024457\n",
      "Train Epoch: 1103 [0/101520 (0%)] Loss: -1158.730225\n",
      "Train Epoch: 1103 [11264/101520 (11%)] Loss: -1162.774292\n",
      "Train Epoch: 1103 [22528/101520 (22%)] Loss: -1156.536377\n",
      "Train Epoch: 1103 [33792/101520 (33%)] Loss: -1159.122559\n",
      "Train Epoch: 1103 [45056/101520 (44%)] Loss: -1165.009766\n",
      "Train Epoch: 1103 [56320/101520 (55%)] Loss: -1154.057129\n",
      "Train Epoch: 1103 [67584/101520 (67%)] Loss: -1157.501343\n",
      "Train Epoch: 1103 [78848/101520 (78%)] Loss: -1158.548096\n",
      "Train Epoch: 1103 [90112/101520 (89%)] Loss: -1153.593872\n",
      "Train Epoch: 1103 [101376/101520 (100%)] Loss: -1165.304443\n",
      "    epoch          : 1103\n",
      "    loss           : -1156.3057928804178\n",
      "    ess            : 1.9671617776305232\n",
      "    log_marginal   : 1156.3366226886385\n",
      "    log_joint      : 1364.7497981852623\n",
      "    val_loss       : -1155.73071819803\n",
      "    val_ess        : 1.967808495397153\n",
      "    val_log_marginal: 1155.7606519616168\n",
      "    val_log_joint  : 1364.1027088994565\n",
      "Train Epoch: 1104 [0/101520 (0%)] Loss: -1158.961304\n",
      "Train Epoch: 1104 [11264/101520 (11%)] Loss: -1155.905518\n",
      "Train Epoch: 1104 [22528/101520 (22%)] Loss: -1156.391113\n",
      "Train Epoch: 1104 [33792/101520 (33%)] Loss: -1153.428833\n",
      "Train Epoch: 1104 [45056/101520 (44%)] Loss: -1156.297119\n",
      "Train Epoch: 1104 [56320/101520 (55%)] Loss: -1153.531494\n",
      "Train Epoch: 1104 [67584/101520 (67%)] Loss: -1148.328369\n",
      "Train Epoch: 1104 [78848/101520 (78%)] Loss: -1154.442871\n",
      "Train Epoch: 1104 [90112/101520 (89%)] Loss: -1160.524414\n",
      "Train Epoch: 1104 [101376/101520 (100%)] Loss: -1158.341431\n",
      "    epoch          : 1104\n",
      "    loss           : -1156.3405841463175\n",
      "    ess            : 1.9681638485223205\n",
      "    log_marginal   : 1156.368935743169\n",
      "    log_joint      : 1364.7828559300408\n",
      "    val_loss       : -1153.5052967900815\n",
      "    val_ess        : 1.965328952540522\n",
      "    val_log_marginal: 1153.5367803158967\n",
      "    val_log_joint  : 1361.915872325068\n",
      "Train Epoch: 1105 [0/101520 (0%)] Loss: -1165.638428\n",
      "Train Epoch: 1105 [11264/101520 (11%)] Loss: -1162.316650\n",
      "Train Epoch: 1105 [22528/101520 (22%)] Loss: -1155.384033\n",
      "Train Epoch: 1105 [33792/101520 (33%)] Loss: -1159.740967\n",
      "Train Epoch: 1105 [45056/101520 (44%)] Loss: -1154.490845\n",
      "Train Epoch: 1105 [56320/101520 (55%)] Loss: -1154.913818\n",
      "Train Epoch: 1105 [67584/101520 (67%)] Loss: -1152.045410\n",
      "Train Epoch: 1105 [78848/101520 (78%)] Loss: -1158.762207\n",
      "Train Epoch: 1105 [90112/101520 (89%)] Loss: -1147.129028\n",
      "Train Epoch: 1105 [101376/101520 (100%)] Loss: -1154.663696\n",
      "    epoch          : 1105\n",
      "    loss           : -1156.485367511385\n",
      "    ess            : 1.9678013366670464\n",
      "    log_marginal   : 1156.5157912364557\n",
      "    log_joint      : 1364.8427176164023\n",
      "    val_loss       : -1153.3283319887908\n",
      "    val_ess        : 1.9665033402650252\n",
      "    val_log_marginal: 1153.3604364809783\n",
      "    val_log_joint  : 1361.7909997027853\n",
      "Train Epoch: 1106 [0/101520 (0%)] Loss: -1151.833496\n",
      "Train Epoch: 1106 [11264/101520 (11%)] Loss: -1151.366211\n",
      "Train Epoch: 1106 [22528/101520 (22%)] Loss: -1154.038574\n",
      "Train Epoch: 1106 [33792/101520 (33%)] Loss: -1158.971436\n",
      "Train Epoch: 1106 [45056/101520 (44%)] Loss: -1158.405029\n",
      "Train Epoch: 1106 [56320/101520 (55%)] Loss: -1162.683716\n",
      "Train Epoch: 1106 [67584/101520 (67%)] Loss: -1153.798462\n",
      "Train Epoch: 1106 [78848/101520 (78%)] Loss: -1155.523315\n",
      "Train Epoch: 1106 [90112/101520 (89%)] Loss: -1154.099854\n",
      "Train Epoch: 1106 [101376/101520 (100%)] Loss: -1161.031494\n",
      "    epoch          : 1106\n",
      "    loss           : -1156.51186474364\n",
      "    ess            : 1.9679642136971556\n",
      "    log_marginal   : 1156.5419572226367\n",
      "    log_joint      : 1364.848866525008\n",
      "    val_loss       : -1154.5876836362092\n",
      "    val_ess        : 1.9673239666482676\n",
      "    val_log_marginal: 1154.616746985394\n",
      "    val_log_joint  : 1363.334812330163\n",
      "Train Epoch: 1107 [0/101520 (0%)] Loss: -1156.968994\n",
      "Train Epoch: 1107 [11264/101520 (11%)] Loss: -1157.300415\n",
      "Train Epoch: 1107 [22528/101520 (22%)] Loss: -1161.755493\n",
      "Train Epoch: 1107 [33792/101520 (33%)] Loss: -1155.359497\n",
      "Train Epoch: 1107 [45056/101520 (44%)] Loss: -1154.604370\n",
      "Train Epoch: 1107 [56320/101520 (55%)] Loss: -1151.939941\n",
      "Train Epoch: 1107 [67584/101520 (67%)] Loss: -1157.142578\n",
      "Train Epoch: 1107 [78848/101520 (78%)] Loss: -1157.450684\n",
      "Train Epoch: 1107 [90112/101520 (89%)] Loss: -1158.311768\n",
      "Train Epoch: 1107 [101376/101520 (100%)] Loss: -1165.123047\n",
      "    epoch          : 1107\n",
      "    loss           : -1156.5428037403815\n",
      "    ess            : 1.9683908715319993\n",
      "    log_marginal   : 1156.5713240273633\n",
      "    log_joint      : 1364.9164284557553\n",
      "    val_loss       : -1154.518650220788\n",
      "    val_ess        : 1.9696936555530713\n",
      "    val_log_marginal: 1154.5449855638587\n",
      "    val_log_joint  : 1362.7881496263587\n",
      "Train Epoch: 1108 [0/101520 (0%)] Loss: -1155.037964\n",
      "Train Epoch: 1108 [11264/101520 (11%)] Loss: -1154.613159\n",
      "Train Epoch: 1108 [22528/101520 (22%)] Loss: -1154.123291\n",
      "Train Epoch: 1108 [33792/101520 (33%)] Loss: -1160.620850\n",
      "Train Epoch: 1108 [45056/101520 (44%)] Loss: -1158.860718\n",
      "Train Epoch: 1108 [56320/101520 (55%)] Loss: -1154.472168\n",
      "Train Epoch: 1108 [67584/101520 (67%)] Loss: -1161.546631\n",
      "Train Epoch: 1108 [78848/101520 (78%)] Loss: -1154.486084\n",
      "Train Epoch: 1108 [90112/101520 (89%)] Loss: -1157.188232\n",
      "Train Epoch: 1108 [101376/101520 (100%)] Loss: -1157.812744\n",
      "    epoch          : 1108\n",
      "    loss           : -1156.492181979232\n",
      "    ess            : 1.9678360667060968\n",
      "    log_marginal   : 1156.5221272377512\n",
      "    log_joint      : 1364.921404507891\n",
      "    val_loss       : -1156.3361497961957\n",
      "    val_ess        : 1.9645991377208545\n",
      "    val_log_marginal: 1156.36939007303\n",
      "    val_log_joint  : 1364.5103706691575\n",
      "Train Epoch: 1109 [0/101520 (0%)] Loss: -1153.863281\n",
      "Train Epoch: 1109 [11264/101520 (11%)] Loss: -1156.544434\n",
      "Train Epoch: 1109 [22528/101520 (22%)] Loss: -1152.897949\n",
      "Train Epoch: 1109 [33792/101520 (33%)] Loss: -1161.546265\n",
      "Train Epoch: 1109 [45056/101520 (44%)] Loss: -1155.284790\n",
      "Train Epoch: 1109 [56320/101520 (55%)] Loss: -1155.390137\n",
      "Train Epoch: 1109 [67584/101520 (67%)] Loss: -1152.154663\n",
      "Train Epoch: 1109 [78848/101520 (78%)] Loss: -1156.324585\n",
      "Train Epoch: 1109 [90112/101520 (89%)] Loss: -1158.508789\n",
      "Train Epoch: 1109 [101376/101520 (100%)] Loss: -1156.453369\n",
      "    epoch          : 1109\n",
      "    loss           : -1156.6001651323022\n",
      "    ess            : 1.96742365288375\n",
      "    log_marginal   : 1156.630741598618\n",
      "    log_joint      : 1364.9909030013348\n",
      "    val_loss       : -1155.088373598845\n",
      "    val_ess        : 1.9679039457569951\n",
      "    val_log_marginal: 1155.1166886039402\n",
      "    val_log_joint  : 1363.2331808338995\n",
      "Train Epoch: 1110 [0/101520 (0%)] Loss: -1159.886963\n",
      "Train Epoch: 1110 [11264/101520 (11%)] Loss: -1169.059326\n",
      "Train Epoch: 1110 [22528/101520 (22%)] Loss: -1157.459839\n",
      "Train Epoch: 1110 [33792/101520 (33%)] Loss: -1155.184814\n",
      "Train Epoch: 1110 [45056/101520 (44%)] Loss: -1151.641602\n",
      "Train Epoch: 1110 [56320/101520 (55%)] Loss: -1150.928955\n",
      "Train Epoch: 1110 [67584/101520 (67%)] Loss: -1152.885376\n",
      "Train Epoch: 1110 [78848/101520 (78%)] Loss: -1159.986206\n",
      "Train Epoch: 1110 [90112/101520 (89%)] Loss: -1158.549072\n",
      "Train Epoch: 1110 [101376/101520 (100%)] Loss: -1151.863037\n",
      "    epoch          : 1110\n",
      "    loss           : -1156.6477676468278\n",
      "    ess            : 1.9669464377302621\n",
      "    log_marginal   : 1156.6783747840766\n",
      "    log_joint      : 1365.001520051429\n",
      "    val_loss       : -1154.1361933169158\n",
      "    val_ess        : 1.96593787877456\n",
      "    val_log_marginal: 1154.1692106827445\n",
      "    val_log_joint  : 1362.7817064368207\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1110.pth ...\n",
      "Train Epoch: 1111 [0/101520 (0%)] Loss: -1150.955811\n",
      "Train Epoch: 1111 [11264/101520 (11%)] Loss: -1152.635254\n",
      "Train Epoch: 1111 [22528/101520 (22%)] Loss: -1163.817627\n",
      "Train Epoch: 1111 [33792/101520 (33%)] Loss: -1154.659546\n",
      "Train Epoch: 1111 [45056/101520 (44%)] Loss: -1152.803223\n",
      "Train Epoch: 1111 [56320/101520 (55%)] Loss: -1158.276855\n",
      "Train Epoch: 1111 [67584/101520 (67%)] Loss: -1157.782227\n",
      "Train Epoch: 1111 [78848/101520 (78%)] Loss: -1153.474365\n",
      "Train Epoch: 1111 [90112/101520 (89%)] Loss: -1156.278809\n",
      "Train Epoch: 1111 [101376/101520 (100%)] Loss: -1166.772095\n",
      "    epoch          : 1111\n",
      "    loss           : -1156.741098681886\n",
      "    ess            : 1.9675767128192\n",
      "    log_marginal   : 1156.770700425958\n",
      "    log_joint      : 1365.1170224903815\n",
      "    val_loss       : -1154.9385614809783\n",
      "    val_ess        : 1.9704635454260784\n",
      "    val_log_marginal: 1154.9660750679348\n",
      "    val_log_joint  : 1363.2152046535325\n",
      "Train Epoch: 1112 [0/101520 (0%)] Loss: -1156.503174\n",
      "Train Epoch: 1112 [11264/101520 (11%)] Loss: -1150.777710\n",
      "Train Epoch: 1112 [22528/101520 (22%)] Loss: -1160.692139\n",
      "Train Epoch: 1112 [33792/101520 (33%)] Loss: -1152.534790\n",
      "Train Epoch: 1112 [45056/101520 (44%)] Loss: -1162.563721\n",
      "Train Epoch: 1112 [56320/101520 (55%)] Loss: -1163.168457\n",
      "Train Epoch: 1112 [67584/101520 (67%)] Loss: -1158.351318\n",
      "Train Epoch: 1112 [78848/101520 (78%)] Loss: -1158.567261\n",
      "Train Epoch: 1112 [90112/101520 (89%)] Loss: -1157.777222\n",
      "Train Epoch: 1112 [101376/101520 (100%)] Loss: -1149.542847\n",
      "    epoch          : 1112\n",
      "    loss           : -1156.7200210034548\n",
      "    ess            : 1.9676213312388664\n",
      "    log_marginal   : 1156.7495344152403\n",
      "    log_joint      : 1365.110811013073\n",
      "    val_loss       : -1156.1838113536005\n",
      "    val_ess        : 1.9646340919577556\n",
      "    val_log_marginal: 1156.218558933424\n",
      "    val_log_joint  : 1364.6286939538043\n",
      "Train Epoch: 1113 [0/101520 (0%)] Loss: -1155.022095\n",
      "Train Epoch: 1113 [11264/101520 (11%)] Loss: -1156.926758\n",
      "Train Epoch: 1113 [22528/101520 (22%)] Loss: -1151.831543\n",
      "Train Epoch: 1113 [33792/101520 (33%)] Loss: -1159.269287\n",
      "Train Epoch: 1113 [45056/101520 (44%)] Loss: -1162.259155\n",
      "Train Epoch: 1113 [56320/101520 (55%)] Loss: -1157.962158\n",
      "Train Epoch: 1113 [67584/101520 (67%)] Loss: -1155.379150\n",
      "Train Epoch: 1113 [78848/101520 (78%)] Loss: -1160.525269\n",
      "Train Epoch: 1113 [90112/101520 (89%)] Loss: -1146.694092\n",
      "Train Epoch: 1113 [101376/101520 (100%)] Loss: -1153.493286\n",
      "    epoch          : 1113\n",
      "    loss           : -1156.815069610749\n",
      "    ess            : 1.9669822580251262\n",
      "    log_marginal   : 1156.8456393294598\n",
      "    log_joint      : 1365.161646243915\n",
      "    val_loss       : -1156.03052288553\n",
      "    val_ess        : 1.9692234992980957\n",
      "    val_log_marginal: 1156.0581320057745\n",
      "    val_log_joint  : 1364.1656918733017\n",
      "Train Epoch: 1114 [0/101520 (0%)] Loss: -1150.533936\n",
      "Train Epoch: 1114 [11264/101520 (11%)] Loss: -1154.353271\n",
      "Train Epoch: 1114 [22528/101520 (22%)] Loss: -1152.054565\n",
      "Train Epoch: 1114 [33792/101520 (33%)] Loss: -1159.353271\n",
      "Train Epoch: 1114 [45056/101520 (44%)] Loss: -1160.601929\n",
      "Train Epoch: 1114 [56320/101520 (55%)] Loss: -1159.385986\n",
      "Train Epoch: 1114 [67584/101520 (67%)] Loss: -1162.696167\n",
      "Train Epoch: 1114 [78848/101520 (78%)] Loss: -1154.063232\n",
      "Train Epoch: 1114 [90112/101520 (89%)] Loss: -1159.574219\n",
      "Train Epoch: 1114 [101376/101520 (100%)] Loss: -1152.634521\n",
      "    epoch          : 1114\n",
      "    loss           : -1156.7872124293342\n",
      "    ess            : 1.9663925158917603\n",
      "    log_marginal   : 1156.8185096625707\n",
      "    log_joint      : 1365.241474707522\n",
      "    val_loss       : -1156.1163170855978\n",
      "    val_ess        : 1.9696261986442234\n",
      "    val_log_marginal: 1156.1440801205842\n",
      "    val_log_joint  : 1364.394234035326\n",
      "Train Epoch: 1115 [0/101520 (0%)] Loss: -1159.635498\n",
      "Train Epoch: 1115 [11264/101520 (11%)] Loss: -1159.617065\n",
      "Train Epoch: 1115 [22528/101520 (22%)] Loss: -1151.110352\n",
      "Train Epoch: 1115 [33792/101520 (33%)] Loss: -1159.870605\n",
      "Train Epoch: 1115 [45056/101520 (44%)] Loss: -1152.151245\n",
      "Train Epoch: 1115 [56320/101520 (55%)] Loss: -1151.823486\n",
      "Train Epoch: 1115 [67584/101520 (67%)] Loss: -1156.694092\n",
      "Train Epoch: 1115 [78848/101520 (78%)] Loss: -1160.207642\n",
      "Train Epoch: 1115 [90112/101520 (89%)] Loss: -1158.673828\n",
      "Train Epoch: 1115 [101376/101520 (100%)] Loss: -1154.795776\n",
      "    epoch          : 1115\n",
      "    loss           : -1156.9443273496388\n",
      "    ess            : 1.9675279723938985\n",
      "    log_marginal   : 1156.9745928126963\n",
      "    log_joint      : 1365.3370091423917\n",
      "    val_loss       : -1156.2278893512228\n",
      "    val_ess        : 1.967627131420633\n",
      "    val_log_marginal: 1156.2568040930707\n",
      "    val_log_joint  : 1364.841064453125\n",
      "Train Epoch: 1116 [0/101520 (0%)] Loss: -1157.044189\n",
      "Train Epoch: 1116 [11264/101520 (11%)] Loss: -1159.440430\n",
      "Train Epoch: 1116 [22528/101520 (22%)] Loss: -1160.151123\n",
      "Train Epoch: 1116 [33792/101520 (33%)] Loss: -1154.142456\n",
      "Train Epoch: 1116 [45056/101520 (44%)] Loss: -1161.139404\n",
      "Train Epoch: 1116 [56320/101520 (55%)] Loss: -1160.884155\n",
      "Train Epoch: 1116 [67584/101520 (67%)] Loss: -1160.638428\n",
      "Train Epoch: 1116 [78848/101520 (78%)] Loss: -1156.005859\n",
      "Train Epoch: 1116 [90112/101520 (89%)] Loss: -1160.364990\n",
      "Train Epoch: 1116 [101376/101520 (100%)] Loss: -1143.784546\n",
      "    epoch          : 1116\n",
      "    loss           : -1156.9906196019158\n",
      "    ess            : 1.9669541485944586\n",
      "    log_marginal   : 1157.0215009373037\n",
      "    log_joint      : 1365.3331421511857\n",
      "    val_loss       : -1156.1605224609375\n",
      "    val_ess        : 1.9670758040054985\n",
      "    val_log_marginal: 1156.1932903787365\n",
      "    val_log_joint  : 1364.638820482337\n",
      "Train Epoch: 1117 [0/101520 (0%)] Loss: -1160.143311\n",
      "Train Epoch: 1117 [11264/101520 (11%)] Loss: -1157.978882\n",
      "Train Epoch: 1117 [22528/101520 (22%)] Loss: -1159.838257\n",
      "Train Epoch: 1117 [33792/101520 (33%)] Loss: -1154.666504\n",
      "Train Epoch: 1117 [45056/101520 (44%)] Loss: -1151.943848\n",
      "Train Epoch: 1117 [56320/101520 (55%)] Loss: -1159.687256\n",
      "Train Epoch: 1117 [67584/101520 (67%)] Loss: -1158.802856\n",
      "Train Epoch: 1117 [78848/101520 (78%)] Loss: -1156.370117\n",
      "Train Epoch: 1117 [90112/101520 (89%)] Loss: -1156.432617\n",
      "Train Epoch: 1117 [101376/101520 (100%)] Loss: -1153.364990\n",
      "    epoch          : 1117\n",
      "    loss           : -1156.9356014692603\n",
      "    ess            : 1.9672512552846018\n",
      "    log_marginal   : 1156.9661116863615\n",
      "    log_joint      : 1365.412307509226\n",
      "    val_loss       : -1156.3906621518342\n",
      "    val_ess        : 1.9674050030500994\n",
      "    val_log_marginal: 1156.420946204144\n",
      "    val_log_joint  : 1364.9072106402853\n",
      "Train Epoch: 1118 [0/101520 (0%)] Loss: -1157.313965\n",
      "Train Epoch: 1118 [11264/101520 (11%)] Loss: -1158.070801\n",
      "Train Epoch: 1118 [22528/101520 (22%)] Loss: -1153.337891\n",
      "Train Epoch: 1118 [33792/101520 (33%)] Loss: -1154.854004\n",
      "Train Epoch: 1118 [45056/101520 (44%)] Loss: -1151.718506\n",
      "Train Epoch: 1118 [56320/101520 (55%)] Loss: -1159.861694\n",
      "Train Epoch: 1118 [67584/101520 (67%)] Loss: -1154.061523\n",
      "Train Epoch: 1118 [78848/101520 (78%)] Loss: -1159.158325\n",
      "Train Epoch: 1118 [90112/101520 (89%)] Loss: -1156.872314\n",
      "Train Epoch: 1118 [101376/101520 (100%)] Loss: -1144.975220\n",
      "    epoch          : 1118\n",
      "    loss           : -1157.033425795972\n",
      "    ess            : 1.9673245887660502\n",
      "    log_marginal   : 1157.0633041918577\n",
      "    log_joint      : 1365.440380000589\n",
      "    val_loss       : -1155.3495669157608\n",
      "    val_ess        : 1.9696708191996035\n",
      "    val_log_marginal: 1155.3756156589675\n",
      "    val_log_joint  : 1363.7507218070652\n",
      "Train Epoch: 1119 [0/101520 (0%)] Loss: -1154.221191\n",
      "Train Epoch: 1119 [11264/101520 (11%)] Loss: -1160.323486\n",
      "Train Epoch: 1119 [22528/101520 (22%)] Loss: -1155.737915\n",
      "Train Epoch: 1119 [33792/101520 (33%)] Loss: -1158.757080\n",
      "Train Epoch: 1119 [45056/101520 (44%)] Loss: -1154.833008\n",
      "Train Epoch: 1119 [56320/101520 (55%)] Loss: -1159.404053\n",
      "Train Epoch: 1119 [67584/101520 (67%)] Loss: -1154.675781\n",
      "Train Epoch: 1119 [78848/101520 (78%)] Loss: -1160.503662\n",
      "Train Epoch: 1119 [90112/101520 (89%)] Loss: -1159.173096\n",
      "Train Epoch: 1119 [101376/101520 (100%)] Loss: -1161.353149\n",
      "    epoch          : 1119\n",
      "    loss           : -1157.037324684948\n",
      "    ess            : 1.9681603980423816\n",
      "    log_marginal   : 1157.0667129593278\n",
      "    log_joint      : 1365.5021218151303\n",
      "    val_loss       : -1155.1692531419837\n",
      "    val_ess        : 1.9667733544888704\n",
      "    val_log_marginal: 1155.201023267663\n",
      "    val_log_joint  : 1363.4747792119565\n",
      "Train Epoch: 1120 [0/101520 (0%)] Loss: -1152.537598\n",
      "Train Epoch: 1120 [11264/101520 (11%)] Loss: -1158.022949\n",
      "Train Epoch: 1120 [22528/101520 (22%)] Loss: -1157.270752\n",
      "Train Epoch: 1120 [33792/101520 (33%)] Loss: -1156.734375\n",
      "Train Epoch: 1120 [45056/101520 (44%)] Loss: -1155.164307\n",
      "Train Epoch: 1120 [56320/101520 (55%)] Loss: -1156.346069\n",
      "Train Epoch: 1120 [67584/101520 (67%)] Loss: -1154.892212\n",
      "Train Epoch: 1120 [78848/101520 (78%)] Loss: -1159.516235\n",
      "Train Epoch: 1120 [90112/101520 (89%)] Loss: -1155.876465\n",
      "Train Epoch: 1120 [101376/101520 (100%)] Loss: -1153.923218\n",
      "    epoch          : 1120\n",
      "    loss           : -1157.2101897426587\n",
      "    ess            : 1.967130022432337\n",
      "    log_marginal   : 1157.240807921443\n",
      "    log_joint      : 1365.563838479507\n",
      "    val_loss       : -1155.181056810462\n",
      "    val_ess        : 1.9650184175242549\n",
      "    val_log_marginal: 1155.2157353940217\n",
      "    val_log_joint  : 1363.832328464674\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1120.pth ...\n",
      "Train Epoch: 1121 [0/101520 (0%)] Loss: -1153.600220\n",
      "Train Epoch: 1121 [11264/101520 (11%)] Loss: -1159.552490\n",
      "Train Epoch: 1121 [22528/101520 (22%)] Loss: -1159.324341\n",
      "Train Epoch: 1121 [33792/101520 (33%)] Loss: -1158.715576\n",
      "Train Epoch: 1121 [45056/101520 (44%)] Loss: -1166.033203\n",
      "Train Epoch: 1121 [56320/101520 (55%)] Loss: -1157.628174\n",
      "Train Epoch: 1121 [67584/101520 (67%)] Loss: -1161.941040\n",
      "Train Epoch: 1121 [78848/101520 (78%)] Loss: -1152.663330\n",
      "Train Epoch: 1121 [90112/101520 (89%)] Loss: -1154.722900\n",
      "Train Epoch: 1121 [101376/101520 (100%)] Loss: -1150.777832\n",
      "    epoch          : 1121\n",
      "    loss           : -1157.2400405347048\n",
      "    ess            : 1.967696803299027\n",
      "    log_marginal   : 1157.2700833267902\n",
      "    log_joint      : 1365.6401502139604\n",
      "    val_loss       : -1156.6533893087635\n",
      "    val_ess        : 1.9695893733397773\n",
      "    val_log_marginal: 1156.682033372962\n",
      "    val_log_joint  : 1365.0449855638587\n",
      "Train Epoch: 1122 [0/101520 (0%)] Loss: -1154.404541\n",
      "Train Epoch: 1122 [11264/101520 (11%)] Loss: -1154.870605\n",
      "Train Epoch: 1122 [22528/101520 (22%)] Loss: -1157.634033\n",
      "Train Epoch: 1122 [33792/101520 (33%)] Loss: -1159.711426\n",
      "Train Epoch: 1122 [45056/101520 (44%)] Loss: -1160.415527\n",
      "Train Epoch: 1122 [56320/101520 (55%)] Loss: -1163.088501\n",
      "Train Epoch: 1122 [67584/101520 (67%)] Loss: -1148.009277\n",
      "Train Epoch: 1122 [78848/101520 (78%)] Loss: -1150.390747\n",
      "Train Epoch: 1122 [90112/101520 (89%)] Loss: -1161.681030\n",
      "Train Epoch: 1122 [101376/101520 (100%)] Loss: -1153.822876\n",
      "    epoch          : 1122\n",
      "    loss           : -1157.2424095575534\n",
      "    ess            : 1.9678477318442646\n",
      "    log_marginal   : 1157.2715862024968\n",
      "    log_joint      : 1365.6726933004868\n",
      "    val_loss       : -1157.6309708305027\n",
      "    val_ess        : 1.9697355913079304\n",
      "    val_log_marginal: 1157.6572424847147\n",
      "    val_log_joint  : 1365.8578040081522\n",
      "Train Epoch: 1123 [0/101520 (0%)] Loss: -1157.875000\n",
      "Train Epoch: 1123 [11264/101520 (11%)] Loss: -1150.370850\n",
      "Train Epoch: 1123 [22528/101520 (22%)] Loss: -1148.730225\n",
      "Train Epoch: 1123 [33792/101520 (33%)] Loss: -1151.771362\n",
      "Train Epoch: 1123 [45056/101520 (44%)] Loss: -1156.044556\n",
      "Train Epoch: 1123 [56320/101520 (55%)] Loss: -1163.409790\n",
      "Train Epoch: 1123 [67584/101520 (67%)] Loss: -1150.624756\n",
      "Train Epoch: 1123 [78848/101520 (78%)] Loss: -1150.799683\n",
      "Train Epoch: 1123 [90112/101520 (89%)] Loss: -1158.390137\n",
      "Train Epoch: 1123 [101376/101520 (100%)] Loss: -1163.880493\n",
      "    epoch          : 1123\n",
      "    loss           : -1157.425271499097\n",
      "    ess            : 1.9678180259675835\n",
      "    log_marginal   : 1157.4549315179413\n",
      "    log_joint      : 1365.815540102858\n",
      "    val_loss       : -1154.7957657523777\n",
      "    val_ess        : 1.96699117059293\n",
      "    val_log_marginal: 1154.826665463655\n",
      "    val_log_joint  : 1363.1622155230978\n",
      "Train Epoch: 1124 [0/101520 (0%)] Loss: -1160.972656\n",
      "Train Epoch: 1124 [11264/101520 (11%)] Loss: -1164.541138\n",
      "Train Epoch: 1124 [22528/101520 (22%)] Loss: -1152.323975\n",
      "Train Epoch: 1124 [33792/101520 (33%)] Loss: -1155.808838\n",
      "Train Epoch: 1124 [45056/101520 (44%)] Loss: -1158.827881\n",
      "Train Epoch: 1124 [56320/101520 (55%)] Loss: -1154.866577\n",
      "Train Epoch: 1124 [67584/101520 (67%)] Loss: -1155.587158\n",
      "Train Epoch: 1124 [78848/101520 (78%)] Loss: -1150.996094\n",
      "Train Epoch: 1124 [90112/101520 (89%)] Loss: -1154.806152\n",
      "Train Epoch: 1124 [101376/101520 (100%)] Loss: -1155.025146\n",
      "    epoch          : 1124\n",
      "    loss           : -1157.371186376217\n",
      "    ess            : 1.9685698957299467\n",
      "    log_marginal   : 1157.4003581138113\n",
      "    log_joint      : 1365.8352682602465\n",
      "    val_loss       : -1157.423387610394\n",
      "    val_ess        : 1.9661831544793171\n",
      "    val_log_marginal: 1157.4561396059783\n",
      "    val_log_joint  : 1365.674953294837\n",
      "Train Epoch: 1125 [0/101520 (0%)] Loss: -1158.149170\n",
      "Train Epoch: 1125 [11264/101520 (11%)] Loss: -1162.062744\n",
      "Train Epoch: 1125 [22528/101520 (22%)] Loss: -1154.746338\n",
      "Train Epoch: 1125 [33792/101520 (33%)] Loss: -1161.719360\n",
      "Train Epoch: 1125 [45056/101520 (44%)] Loss: -1146.795410\n",
      "Train Epoch: 1125 [56320/101520 (55%)] Loss: -1156.139404\n",
      "Train Epoch: 1125 [67584/101520 (67%)] Loss: -1160.593262\n",
      "Train Epoch: 1125 [78848/101520 (78%)] Loss: -1161.546021\n",
      "Train Epoch: 1125 [90112/101520 (89%)] Loss: -1152.370117\n",
      "Train Epoch: 1125 [101376/101520 (100%)] Loss: -1165.032593\n",
      "    epoch          : 1125\n",
      "    loss           : -1157.503941828282\n",
      "    ess            : 1.968261962559954\n",
      "    log_marginal   : 1157.5326117894158\n",
      "    log_joint      : 1365.9245378503847\n",
      "    val_loss       : -1157.5144149116848\n",
      "    val_ess        : 1.965296807496444\n",
      "    val_log_marginal: 1157.5504203464675\n",
      "    val_log_joint  : 1366.2242325492527\n",
      "Train Epoch: 1126 [0/101520 (0%)] Loss: -1163.177002\n",
      "Train Epoch: 1126 [11264/101520 (11%)] Loss: -1159.149414\n",
      "Train Epoch: 1126 [22528/101520 (22%)] Loss: -1157.134521\n",
      "Train Epoch: 1126 [33792/101520 (33%)] Loss: -1150.651367\n",
      "Train Epoch: 1126 [45056/101520 (44%)] Loss: -1169.231812\n",
      "Train Epoch: 1126 [56320/101520 (55%)] Loss: -1160.032837\n",
      "Train Epoch: 1126 [67584/101520 (67%)] Loss: -1158.606445\n",
      "Train Epoch: 1126 [78848/101520 (78%)] Loss: -1155.865356\n",
      "Train Epoch: 1126 [90112/101520 (89%)] Loss: -1160.645752\n",
      "Train Epoch: 1126 [101376/101520 (100%)] Loss: -1169.705078\n",
      "    epoch          : 1126\n",
      "    loss           : -1157.583364208739\n",
      "    ess            : 1.9669998273178562\n",
      "    log_marginal   : 1157.613616176586\n",
      "    log_joint      : 1366.031039597401\n",
      "    val_loss       : -1156.07593304178\n",
      "    val_ess        : 1.9694550296534663\n",
      "    val_log_marginal: 1156.1051821501358\n",
      "    val_log_joint  : 1364.5303583559783\n",
      "Train Epoch: 1127 [0/101520 (0%)] Loss: -1162.498413\n",
      "Train Epoch: 1127 [11264/101520 (11%)] Loss: -1159.075806\n",
      "Train Epoch: 1127 [22528/101520 (22%)] Loss: -1163.045166\n",
      "Train Epoch: 1127 [33792/101520 (33%)] Loss: -1157.598999\n",
      "Train Epoch: 1127 [45056/101520 (44%)] Loss: -1157.217285\n",
      "Train Epoch: 1127 [56320/101520 (55%)] Loss: -1156.133789\n",
      "Train Epoch: 1127 [67584/101520 (67%)] Loss: -1157.264526\n",
      "Train Epoch: 1127 [78848/101520 (78%)] Loss: -1167.677490\n",
      "Train Epoch: 1127 [90112/101520 (89%)] Loss: -1158.183716\n",
      "Train Epoch: 1127 [101376/101520 (100%)] Loss: -1166.251953\n",
      "    epoch          : 1127\n",
      "    loss           : -1157.605242398516\n",
      "    ess            : 1.9679749209677155\n",
      "    log_marginal   : 1157.63465152913\n",
      "    log_joint      : 1366.0028658919598\n",
      "    val_loss       : -1156.1675494650135\n",
      "    val_ess        : 1.9685924623323523\n",
      "    val_log_marginal: 1156.1933169157608\n",
      "    val_log_joint  : 1364.4690365998642\n",
      "Train Epoch: 1128 [0/101520 (0%)] Loss: -1163.569824\n",
      "Train Epoch: 1128 [11264/101520 (11%)] Loss: -1159.205200\n",
      "Train Epoch: 1128 [22528/101520 (22%)] Loss: -1153.183350\n",
      "Train Epoch: 1128 [33792/101520 (33%)] Loss: -1152.307373\n",
      "Train Epoch: 1128 [45056/101520 (44%)] Loss: -1152.717285\n",
      "Train Epoch: 1128 [56320/101520 (55%)] Loss: -1148.513306\n",
      "Train Epoch: 1128 [67584/101520 (67%)] Loss: -1160.437622\n",
      "Train Epoch: 1128 [78848/101520 (78%)] Loss: -1156.424805\n",
      "Train Epoch: 1128 [90112/101520 (89%)] Loss: -1156.286865\n",
      "Train Epoch: 1128 [101376/101520 (100%)] Loss: -1160.398193\n",
      "    epoch          : 1128\n",
      "    loss           : -1157.626805291104\n",
      "    ess            : 1.9674817024163864\n",
      "    log_marginal   : 1157.6574523005654\n",
      "    log_joint      : 1366.035902780504\n",
      "    val_loss       : -1157.0598303753397\n",
      "    val_ess        : 1.9705159871474556\n",
      "    val_log_marginal: 1157.0875774881115\n",
      "    val_log_joint  : 1365.437155018682\n",
      "Train Epoch: 1129 [0/101520 (0%)] Loss: -1161.739014\n",
      "Train Epoch: 1129 [11264/101520 (11%)] Loss: -1163.517090\n",
      "Train Epoch: 1129 [22528/101520 (22%)] Loss: -1151.567383\n",
      "Train Epoch: 1129 [33792/101520 (33%)] Loss: -1159.168091\n",
      "Train Epoch: 1129 [45056/101520 (44%)] Loss: -1158.435913\n",
      "Train Epoch: 1129 [56320/101520 (55%)] Loss: -1162.255615\n",
      "Train Epoch: 1129 [67584/101520 (67%)] Loss: -1162.815063\n",
      "Train Epoch: 1129 [78848/101520 (78%)] Loss: -1162.223633\n",
      "Train Epoch: 1129 [90112/101520 (89%)] Loss: -1159.317139\n",
      "Train Epoch: 1129 [101376/101520 (100%)] Loss: -1155.741089\n",
      "    epoch          : 1129\n",
      "    loss           : -1157.7083899723225\n",
      "    ess            : 1.9675896437323872\n",
      "    log_marginal   : 1157.7381272819173\n",
      "    log_joint      : 1366.090704376374\n",
      "    val_loss       : -1156.9218006963315\n",
      "    val_ess        : 1.963088517603667\n",
      "    val_log_marginal: 1156.9560918393342\n",
      "    val_log_joint  : 1365.1656547214675\n",
      "Train Epoch: 1130 [0/101520 (0%)] Loss: -1155.105957\n",
      "Train Epoch: 1130 [11264/101520 (11%)] Loss: -1159.895874\n",
      "Train Epoch: 1130 [22528/101520 (22%)] Loss: -1156.609619\n",
      "Train Epoch: 1130 [33792/101520 (33%)] Loss: -1159.108643\n",
      "Train Epoch: 1130 [45056/101520 (44%)] Loss: -1154.797974\n",
      "Train Epoch: 1130 [56320/101520 (55%)] Loss: -1157.273682\n",
      "Train Epoch: 1130 [67584/101520 (67%)] Loss: -1160.980103\n",
      "Train Epoch: 1130 [78848/101520 (78%)] Loss: -1156.018555\n",
      "Train Epoch: 1130 [90112/101520 (89%)] Loss: -1155.457520\n",
      "Train Epoch: 1130 [101376/101520 (100%)] Loss: -1150.346313\n",
      "    epoch          : 1130\n",
      "    loss           : -1157.7358919843357\n",
      "    ess            : 1.9680707556518477\n",
      "    log_marginal   : 1157.7657771278266\n",
      "    log_joint      : 1366.122838312657\n",
      "    val_loss       : -1156.0346467391305\n",
      "    val_ess        : 1.9685320439546004\n",
      "    val_log_marginal: 1156.0642195991848\n",
      "    val_log_joint  : 1364.5341903023098\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1130.pth ...\n",
      "Train Epoch: 1131 [0/101520 (0%)] Loss: -1155.673096\n",
      "Train Epoch: 1131 [11264/101520 (11%)] Loss: -1157.495361\n",
      "Train Epoch: 1131 [22528/101520 (22%)] Loss: -1157.872559\n",
      "Train Epoch: 1131 [33792/101520 (33%)] Loss: -1164.410889\n",
      "Train Epoch: 1131 [45056/101520 (44%)] Loss: -1156.725952\n",
      "Train Epoch: 1131 [56320/101520 (55%)] Loss: -1157.355591\n",
      "Train Epoch: 1131 [67584/101520 (67%)] Loss: -1157.888062\n",
      "Train Epoch: 1131 [78848/101520 (78%)] Loss: -1161.932495\n",
      "Train Epoch: 1131 [90112/101520 (89%)] Loss: -1158.763672\n",
      "Train Epoch: 1131 [101376/101520 (100%)] Loss: -1166.656616\n",
      "    epoch          : 1131\n",
      "    loss           : -1157.8442787668812\n",
      "    ess            : 1.9668703594399457\n",
      "    log_marginal   : 1157.8761415721185\n",
      "    log_joint      : 1366.212038586487\n",
      "    val_loss       : -1156.9564102836277\n",
      "    val_ess        : 1.971142748127813\n",
      "    val_log_marginal: 1156.9808031165082\n",
      "    val_log_joint  : 1365.2160273012908\n",
      "Train Epoch: 1132 [0/101520 (0%)] Loss: -1156.253174\n",
      "Train Epoch: 1132 [11264/101520 (11%)] Loss: -1154.640381\n",
      "Train Epoch: 1132 [22528/101520 (22%)] Loss: -1161.491211\n",
      "Train Epoch: 1132 [33792/101520 (33%)] Loss: -1151.434814\n",
      "Train Epoch: 1132 [45056/101520 (44%)] Loss: -1152.522949\n",
      "Train Epoch: 1132 [56320/101520 (55%)] Loss: -1159.036499\n",
      "Train Epoch: 1132 [67584/101520 (67%)] Loss: -1152.729126\n",
      "Train Epoch: 1132 [78848/101520 (78%)] Loss: -1156.733643\n",
      "Train Epoch: 1132 [90112/101520 (89%)] Loss: -1154.506714\n",
      "Train Epoch: 1132 [101376/101520 (100%)] Loss: -1156.368896\n",
      "    epoch          : 1132\n",
      "    loss           : -1157.829575735121\n",
      "    ess            : 1.9681580252383821\n",
      "    log_marginal   : 1157.8590413002512\n",
      "    log_joint      : 1366.270931684791\n",
      "    val_loss       : -1157.5031579059103\n",
      "    val_ess        : 1.9672516791716865\n",
      "    val_log_marginal: 1157.5311226222825\n",
      "    val_log_joint  : 1366.039938221807\n",
      "Train Epoch: 1133 [0/101520 (0%)] Loss: -1156.488525\n",
      "Train Epoch: 1133 [11264/101520 (11%)] Loss: -1155.040161\n",
      "Train Epoch: 1133 [22528/101520 (22%)] Loss: -1157.449951\n",
      "Train Epoch: 1133 [33792/101520 (33%)] Loss: -1150.899658\n",
      "Train Epoch: 1133 [45056/101520 (44%)] Loss: -1158.305054\n",
      "Train Epoch: 1133 [56320/101520 (55%)] Loss: -1157.527832\n",
      "Train Epoch: 1133 [67584/101520 (67%)] Loss: -1160.420410\n",
      "Train Epoch: 1133 [78848/101520 (78%)] Loss: -1159.386353\n",
      "Train Epoch: 1133 [90112/101520 (89%)] Loss: -1157.372314\n",
      "Train Epoch: 1133 [101376/101520 (100%)] Loss: -1168.632446\n",
      "    epoch          : 1133\n",
      "    loss           : -1157.965301973736\n",
      "    ess            : 1.9681018069760883\n",
      "    log_marginal   : 1157.9950196539337\n",
      "    log_joint      : 1366.3764691376805\n",
      "    val_loss       : -1155.8035941745925\n",
      "    val_ess        : 1.9665850763735564\n",
      "    val_log_marginal: 1155.8357039741848\n",
      "    val_log_joint  : 1364.351127292799\n",
      "Train Epoch: 1134 [0/101520 (0%)] Loss: -1151.574707\n",
      "Train Epoch: 1134 [11264/101520 (11%)] Loss: -1160.476318\n",
      "Train Epoch: 1134 [22528/101520 (22%)] Loss: -1157.459473\n",
      "Train Epoch: 1134 [33792/101520 (33%)] Loss: -1160.772827\n",
      "Train Epoch: 1134 [45056/101520 (44%)] Loss: -1157.389648\n",
      "Train Epoch: 1134 [56320/101520 (55%)] Loss: -1164.351318\n",
      "Train Epoch: 1134 [67584/101520 (67%)] Loss: -1158.210205\n",
      "Train Epoch: 1134 [78848/101520 (78%)] Loss: -1155.134277\n",
      "Train Epoch: 1134 [90112/101520 (89%)] Loss: -1159.270752\n",
      "Train Epoch: 1134 [101376/101520 (100%)] Loss: -1151.878052\n",
      "    epoch          : 1134\n",
      "    loss           : -1157.913631266685\n",
      "    ess            : 1.9676123120676932\n",
      "    log_marginal   : 1157.9424098029208\n",
      "    log_joint      : 1366.3899747762248\n",
      "    val_loss       : -1155.2094301970108\n",
      "    val_ess        : 1.964208131251128\n",
      "    val_log_marginal: 1155.2463166610055\n",
      "    val_log_joint  : 1363.741216244905\n",
      "Train Epoch: 1135 [0/101520 (0%)] Loss: -1156.031982\n",
      "Train Epoch: 1135 [11264/101520 (11%)] Loss: -1151.317383\n",
      "Train Epoch: 1135 [22528/101520 (22%)] Loss: -1152.991821\n",
      "Train Epoch: 1135 [33792/101520 (33%)] Loss: -1157.703491\n",
      "Train Epoch: 1135 [45056/101520 (44%)] Loss: -1157.727295\n",
      "Train Epoch: 1135 [56320/101520 (55%)] Loss: -1162.313232\n",
      "Train Epoch: 1135 [67584/101520 (67%)] Loss: -1157.143677\n",
      "Train Epoch: 1135 [78848/101520 (78%)] Loss: -1158.591553\n",
      "Train Epoch: 1135 [90112/101520 (89%)] Loss: -1157.164795\n",
      "Train Epoch: 1135 [101376/101520 (100%)] Loss: -1161.639648\n",
      "    epoch          : 1135\n",
      "    loss           : -1158.0601009196373\n",
      "    ess            : 1.9676409720176429\n",
      "    log_marginal   : 1158.0903038139918\n",
      "    log_joint      : 1366.4460167046168\n",
      "    val_loss       : -1155.4251549762228\n",
      "    val_ess        : 1.9678000781847083\n",
      "    val_log_marginal: 1155.4560387652853\n",
      "    val_log_joint  : 1364.2272524626358\n",
      "Train Epoch: 1136 [0/101520 (0%)] Loss: -1161.075928\n",
      "Train Epoch: 1136 [11264/101520 (11%)] Loss: -1162.438599\n",
      "Train Epoch: 1136 [22528/101520 (22%)] Loss: -1162.947876\n",
      "Train Epoch: 1136 [33792/101520 (33%)] Loss: -1155.774902\n",
      "Train Epoch: 1136 [45056/101520 (44%)] Loss: -1160.824585\n",
      "Train Epoch: 1136 [56320/101520 (55%)] Loss: -1158.793945\n",
      "Train Epoch: 1136 [67584/101520 (67%)] Loss: -1151.509888\n",
      "Train Epoch: 1136 [78848/101520 (78%)] Loss: -1159.954102\n",
      "Train Epoch: 1136 [90112/101520 (89%)] Loss: -1155.225098\n",
      "Train Epoch: 1136 [101376/101520 (100%)] Loss: -1160.293213\n",
      "    epoch          : 1136\n",
      "    loss           : -1158.0297845428313\n",
      "    ess            : 1.9672630761736003\n",
      "    log_marginal   : 1158.0598622997802\n",
      "    log_joint      : 1366.501589367737\n",
      "    val_loss       : -1156.4245499320652\n",
      "    val_ess        : 1.967271607855092\n",
      "    val_log_marginal: 1156.454881751019\n",
      "    val_log_joint  : 1364.754983653193\n",
      "Train Epoch: 1137 [0/101520 (0%)] Loss: -1158.626099\n",
      "Train Epoch: 1137 [11264/101520 (11%)] Loss: -1161.450195\n",
      "Train Epoch: 1137 [22528/101520 (22%)] Loss: -1153.947021\n",
      "Train Epoch: 1137 [33792/101520 (33%)] Loss: -1156.473511\n",
      "Train Epoch: 1137 [45056/101520 (44%)] Loss: -1158.310547\n",
      "Train Epoch: 1137 [56320/101520 (55%)] Loss: -1162.731201\n",
      "Train Epoch: 1137 [67584/101520 (67%)] Loss: -1164.647705\n",
      "Train Epoch: 1137 [78848/101520 (78%)] Loss: -1158.333496\n",
      "Train Epoch: 1137 [90112/101520 (89%)] Loss: -1154.716553\n",
      "Train Epoch: 1137 [101376/101520 (100%)] Loss: -1156.566162\n",
      "    epoch          : 1137\n",
      "    loss           : -1158.2103731548366\n",
      "    ess            : 1.9673704573856525\n",
      "    log_marginal   : 1158.2410833464196\n",
      "    log_joint      : 1366.582548361927\n",
      "    val_loss       : -1155.731450619905\n",
      "    val_ess        : 1.9653786990953528\n",
      "    val_log_marginal: 1155.7618089758832\n",
      "    val_log_joint  : 1364.1702987007473\n",
      "Train Epoch: 1138 [0/101520 (0%)] Loss: -1155.788452\n",
      "Train Epoch: 1138 [11264/101520 (11%)] Loss: -1163.756348\n",
      "Train Epoch: 1138 [22528/101520 (22%)] Loss: -1153.936035\n",
      "Train Epoch: 1138 [33792/101520 (33%)] Loss: -1158.487549\n",
      "Train Epoch: 1138 [45056/101520 (44%)] Loss: -1155.215332\n",
      "Train Epoch: 1138 [56320/101520 (55%)] Loss: -1164.078613\n",
      "Train Epoch: 1138 [67584/101520 (67%)] Loss: -1155.761841\n",
      "Train Epoch: 1138 [78848/101520 (78%)] Loss: -1159.718750\n",
      "Train Epoch: 1138 [90112/101520 (89%)] Loss: -1158.526489\n",
      "Train Epoch: 1138 [101376/101520 (100%)] Loss: -1162.922363\n",
      "    epoch          : 1138\n",
      "    loss           : -1158.2391891096106\n",
      "    ess            : 1.9671073463094892\n",
      "    log_marginal   : 1158.269347837822\n",
      "    log_joint      : 1366.6493557877277\n",
      "    val_loss       : -1157.1143321161685\n",
      "    val_ess        : 1.9690924157267031\n",
      "    val_log_marginal: 1157.1422225288723\n",
      "    val_log_joint  : 1365.2852305536685\n",
      "Train Epoch: 1139 [0/101520 (0%)] Loss: -1157.954834\n",
      "Train Epoch: 1139 [11264/101520 (11%)] Loss: -1156.267334\n",
      "Train Epoch: 1139 [22528/101520 (22%)] Loss: -1155.378296\n",
      "Train Epoch: 1139 [33792/101520 (33%)] Loss: -1159.052246\n",
      "Train Epoch: 1139 [45056/101520 (44%)] Loss: -1156.343994\n",
      "Train Epoch: 1139 [56320/101520 (55%)] Loss: -1157.423462\n",
      "Train Epoch: 1139 [67584/101520 (67%)] Loss: -1152.680176\n",
      "Train Epoch: 1139 [78848/101520 (78%)] Loss: -1154.740112\n",
      "Train Epoch: 1139 [90112/101520 (89%)] Loss: -1157.455322\n",
      "Train Epoch: 1139 [101376/101520 (100%)] Loss: -1168.640259\n",
      "    epoch          : 1139\n",
      "    loss           : -1158.343915623037\n",
      "    ess            : 1.9682680068902634\n",
      "    log_marginal   : 1158.3726162551036\n",
      "    log_joint      : 1366.770737844496\n",
      "    val_loss       : -1157.7711712381115\n",
      "    val_ess        : 1.965403587921806\n",
      "    val_log_marginal: 1157.8072615913723\n",
      "    val_log_joint  : 1365.9859937584918\n",
      "Train Epoch: 1140 [0/101520 (0%)] Loss: -1156.959961\n",
      "Train Epoch: 1140 [11264/101520 (11%)] Loss: -1160.151855\n",
      "Train Epoch: 1140 [22528/101520 (22%)] Loss: -1159.416992\n",
      "Train Epoch: 1140 [33792/101520 (33%)] Loss: -1159.943848\n",
      "Train Epoch: 1140 [45056/101520 (44%)] Loss: -1160.321167\n",
      "Train Epoch: 1140 [56320/101520 (55%)] Loss: -1157.053833\n",
      "Train Epoch: 1140 [67584/101520 (67%)] Loss: -1161.069336\n",
      "Train Epoch: 1140 [78848/101520 (78%)] Loss: -1162.516846\n",
      "Train Epoch: 1140 [90112/101520 (89%)] Loss: -1163.474731\n",
      "Train Epoch: 1140 [101376/101520 (100%)] Loss: -1176.047363\n",
      "    epoch          : 1140\n",
      "    loss           : -1158.383183004868\n",
      "    ess            : 1.9667643889709934\n",
      "    log_marginal   : 1158.4142293498744\n",
      "    log_joint      : 1366.8784026332837\n",
      "    val_loss       : -1156.7399796195652\n",
      "    val_ess        : 1.9657140348268591\n",
      "    val_log_marginal: 1156.7749660326087\n",
      "    val_log_joint  : 1364.7943115234375\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1140.pth ...\n",
      "Train Epoch: 1141 [0/101520 (0%)] Loss: -1163.916260\n",
      "Train Epoch: 1141 [11264/101520 (11%)] Loss: -1154.744385\n",
      "Train Epoch: 1141 [22528/101520 (22%)] Loss: -1158.300781\n",
      "Train Epoch: 1141 [33792/101520 (33%)] Loss: -1152.919922\n",
      "Train Epoch: 1141 [45056/101520 (44%)] Loss: -1163.280518\n",
      "Train Epoch: 1141 [56320/101520 (55%)] Loss: -1162.959717\n",
      "Train Epoch: 1141 [67584/101520 (67%)] Loss: -1153.138184\n",
      "Train Epoch: 1141 [78848/101520 (78%)] Loss: -1158.516113\n",
      "Train Epoch: 1141 [90112/101520 (89%)] Loss: -1160.979126\n",
      "Train Epoch: 1141 [101376/101520 (100%)] Loss: -1155.697266\n",
      "    epoch          : 1141\n",
      "    loss           : -1158.3865292036353\n",
      "    ess            : 1.9679158620498887\n",
      "    log_marginal   : 1158.4159837272298\n",
      "    log_joint      : 1366.7986244700062\n",
      "    val_loss       : -1156.755954908288\n",
      "    val_ess        : 1.9651822007220725\n",
      "    val_log_marginal: 1156.7929422129755\n",
      "    val_log_joint  : 1365.0217975118885\n",
      "Train Epoch: 1142 [0/101520 (0%)] Loss: -1157.361694\n",
      "Train Epoch: 1142 [11264/101520 (11%)] Loss: -1154.998047\n",
      "Train Epoch: 1142 [22528/101520 (22%)] Loss: -1155.338379\n",
      "Train Epoch: 1142 [33792/101520 (33%)] Loss: -1149.803101\n",
      "Train Epoch: 1142 [45056/101520 (44%)] Loss: -1168.379272\n",
      "Train Epoch: 1142 [56320/101520 (55%)] Loss: -1158.725830\n",
      "Train Epoch: 1142 [67584/101520 (67%)] Loss: -1158.552734\n",
      "Train Epoch: 1142 [78848/101520 (78%)] Loss: -1158.087280\n",
      "Train Epoch: 1142 [90112/101520 (89%)] Loss: -1163.833740\n",
      "Train Epoch: 1142 [101376/101520 (100%)] Loss: -1146.173462\n",
      "    epoch          : 1142\n",
      "    loss           : -1158.5094742511385\n",
      "    ess            : 1.968073297984636\n",
      "    log_marginal   : 1158.538950244386\n",
      "    log_joint      : 1366.9132521739557\n",
      "    val_loss       : -1156.728860606318\n",
      "    val_ess        : 1.9656800809113875\n",
      "    val_log_marginal: 1156.761962890625\n",
      "    val_log_joint  : 1365.2593622622283\n",
      "Train Epoch: 1143 [0/101520 (0%)] Loss: -1156.209961\n",
      "Train Epoch: 1143 [11264/101520 (11%)] Loss: -1154.677368\n",
      "Train Epoch: 1143 [22528/101520 (22%)] Loss: -1165.929199\n",
      "Train Epoch: 1143 [33792/101520 (33%)] Loss: -1158.102539\n",
      "Train Epoch: 1143 [45056/101520 (44%)] Loss: -1157.108154\n",
      "Train Epoch: 1143 [56320/101520 (55%)] Loss: -1156.224487\n",
      "Train Epoch: 1143 [67584/101520 (67%)] Loss: -1157.385986\n",
      "Train Epoch: 1143 [78848/101520 (78%)] Loss: -1162.859619\n",
      "Train Epoch: 1143 [90112/101520 (89%)] Loss: -1151.849976\n",
      "Train Epoch: 1143 [101376/101520 (100%)] Loss: -1156.745972\n",
      "    epoch          : 1143\n",
      "    loss           : -1158.5602978270258\n",
      "    ess            : 1.9669580339786394\n",
      "    log_marginal   : 1158.5905485680355\n",
      "    log_joint      : 1366.960009397574\n",
      "    val_loss       : -1158.5845947265625\n",
      "    val_ess        : 1.9663821251496025\n",
      "    val_log_marginal: 1158.6130477241848\n",
      "    val_log_joint  : 1367.229056980299\n",
      "Train Epoch: 1144 [0/101520 (0%)] Loss: -1153.066406\n",
      "Train Epoch: 1144 [11264/101520 (11%)] Loss: -1156.304688\n",
      "Train Epoch: 1144 [22528/101520 (22%)] Loss: -1161.092529\n",
      "Train Epoch: 1144 [33792/101520 (33%)] Loss: -1159.183838\n",
      "Train Epoch: 1144 [45056/101520 (44%)] Loss: -1159.783569\n",
      "Train Epoch: 1144 [56320/101520 (55%)] Loss: -1155.840820\n",
      "Train Epoch: 1144 [67584/101520 (67%)] Loss: -1155.860352\n",
      "Train Epoch: 1144 [78848/101520 (78%)] Loss: -1167.368408\n",
      "Train Epoch: 1144 [90112/101520 (89%)] Loss: -1155.621338\n",
      "Train Epoch: 1144 [101376/101520 (100%)] Loss: -1148.561157\n",
      "    epoch          : 1144\n",
      "    loss           : -1158.6336123979272\n",
      "    ess            : 1.967724810892613\n",
      "    log_marginal   : 1158.6638723402168\n",
      "    log_joint      : 1367.0183921315563\n",
      "    val_loss       : -1157.5242972995925\n",
      "    val_ess        : 1.9673936729845793\n",
      "    val_log_marginal: 1157.5529095193615\n",
      "    val_log_joint  : 1365.8088007387908\n",
      "Train Epoch: 1145 [0/101520 (0%)] Loss: -1164.025879\n",
      "Train Epoch: 1145 [11264/101520 (11%)] Loss: -1159.075928\n",
      "Train Epoch: 1145 [22528/101520 (22%)] Loss: -1167.888184\n",
      "Train Epoch: 1145 [33792/101520 (33%)] Loss: -1156.730591\n",
      "Train Epoch: 1145 [45056/101520 (44%)] Loss: -1162.310547\n",
      "Train Epoch: 1145 [56320/101520 (55%)] Loss: -1151.866455\n",
      "Train Epoch: 1145 [67584/101520 (67%)] Loss: -1155.951782\n",
      "Train Epoch: 1145 [78848/101520 (78%)] Loss: -1154.974854\n",
      "Train Epoch: 1145 [90112/101520 (89%)] Loss: -1161.315186\n",
      "Train Epoch: 1145 [101376/101520 (100%)] Loss: -1161.730713\n",
      "    epoch          : 1145\n",
      "    loss           : -1158.690433368012\n",
      "    ess            : 1.9668135846679533\n",
      "    log_marginal   : 1158.7208699748744\n",
      "    log_joint      : 1367.0854357235396\n",
      "    val_loss       : -1158.0041610054348\n",
      "    val_ess        : 1.9709799082382866\n",
      "    val_log_marginal: 1158.0314835258152\n",
      "    val_log_joint  : 1366.4040421195652\n",
      "Train Epoch: 1146 [0/101520 (0%)] Loss: -1155.416260\n",
      "Train Epoch: 1146 [11264/101520 (11%)] Loss: -1159.231934\n",
      "Train Epoch: 1146 [22528/101520 (22%)] Loss: -1154.510254\n",
      "Train Epoch: 1146 [33792/101520 (33%)] Loss: -1159.453247\n",
      "Train Epoch: 1146 [45056/101520 (44%)] Loss: -1158.340332\n",
      "Train Epoch: 1146 [56320/101520 (55%)] Loss: -1160.575806\n",
      "Train Epoch: 1146 [67584/101520 (67%)] Loss: -1162.870850\n",
      "Train Epoch: 1146 [78848/101520 (78%)] Loss: -1154.399536\n",
      "Train Epoch: 1146 [90112/101520 (89%)] Loss: -1164.234131\n",
      "Train Epoch: 1146 [101376/101520 (100%)] Loss: -1165.697632\n",
      "    epoch          : 1146\n",
      "    loss           : -1158.692790735906\n",
      "    ess            : 1.9670503942211668\n",
      "    log_marginal   : 1158.7238266527952\n",
      "    log_joint      : 1367.11655322511\n",
      "    val_loss       : -1157.4410931131115\n",
      "    val_ess        : 1.9650418913882712\n",
      "    val_log_marginal: 1157.4707349694293\n",
      "    val_log_joint  : 1365.7640805451767\n",
      "Train Epoch: 1147 [0/101520 (0%)] Loss: -1162.416626\n",
      "Train Epoch: 1147 [11264/101520 (11%)] Loss: -1163.322266\n",
      "Train Epoch: 1147 [22528/101520 (22%)] Loss: -1160.802490\n",
      "Train Epoch: 1147 [33792/101520 (33%)] Loss: -1157.689453\n",
      "Train Epoch: 1147 [45056/101520 (44%)] Loss: -1157.001465\n",
      "Train Epoch: 1147 [56320/101520 (55%)] Loss: -1152.330200\n",
      "Train Epoch: 1147 [67584/101520 (67%)] Loss: -1160.187500\n",
      "Train Epoch: 1147 [78848/101520 (78%)] Loss: -1156.263428\n",
      "Train Epoch: 1147 [90112/101520 (89%)] Loss: -1152.973145\n",
      "Train Epoch: 1147 [101376/101520 (100%)] Loss: -1160.819214\n",
      "    epoch          : 1147\n",
      "    loss           : -1158.7233322373586\n",
      "    ess            : 1.9680197167037121\n",
      "    log_marginal   : 1158.7529517705716\n",
      "    log_joint      : 1367.170702756949\n",
      "    val_loss       : -1158.7277035920517\n",
      "    val_ess        : 1.9665387920711352\n",
      "    val_log_marginal: 1158.7566289487092\n",
      "    val_log_joint  : 1366.9533426036005\n",
      "Train Epoch: 1148 [0/101520 (0%)] Loss: -1161.387817\n",
      "Train Epoch: 1148 [11264/101520 (11%)] Loss: -1158.809570\n",
      "Train Epoch: 1148 [22528/101520 (22%)] Loss: -1159.142090\n",
      "Train Epoch: 1148 [33792/101520 (33%)] Loss: -1158.746582\n",
      "Train Epoch: 1148 [45056/101520 (44%)] Loss: -1156.479248\n",
      "Train Epoch: 1148 [56320/101520 (55%)] Loss: -1158.786865\n",
      "Train Epoch: 1148 [67584/101520 (67%)] Loss: -1159.579346\n",
      "Train Epoch: 1148 [78848/101520 (78%)] Loss: -1166.064697\n",
      "Train Epoch: 1148 [90112/101520 (89%)] Loss: -1150.812622\n",
      "Train Epoch: 1148 [101376/101520 (100%)] Loss: -1162.485718\n",
      "    epoch          : 1148\n",
      "    loss           : -1158.8341757616206\n",
      "    ess            : 1.9678430095988901\n",
      "    log_marginal   : 1158.864202604821\n",
      "    log_joint      : 1367.2006277726523\n",
      "    val_loss       : -1156.8568752122962\n",
      "    val_ess        : 1.9697741581046062\n",
      "    val_log_marginal: 1156.8853388247283\n",
      "    val_log_joint  : 1365.1635158372962\n",
      "Train Epoch: 1149 [0/101520 (0%)] Loss: -1152.976318\n",
      "Train Epoch: 1149 [11264/101520 (11%)] Loss: -1161.795044\n",
      "Train Epoch: 1149 [22528/101520 (22%)] Loss: -1161.524414\n",
      "Train Epoch: 1149 [33792/101520 (33%)] Loss: -1154.129517\n",
      "Train Epoch: 1149 [45056/101520 (44%)] Loss: -1163.673706\n",
      "Train Epoch: 1149 [56320/101520 (55%)] Loss: -1161.187500\n",
      "Train Epoch: 1149 [67584/101520 (67%)] Loss: -1156.326416\n",
      "Train Epoch: 1149 [78848/101520 (78%)] Loss: -1161.438721\n",
      "Train Epoch: 1149 [90112/101520 (89%)] Loss: -1158.639282\n",
      "Train Epoch: 1149 [101376/101520 (100%)] Loss: -1146.360962\n",
      "    epoch          : 1149\n",
      "    loss           : -1158.8136924858668\n",
      "    ess            : 1.9671910719655865\n",
      "    log_marginal   : 1158.8432193928627\n",
      "    log_joint      : 1367.2511900321922\n",
      "    val_loss       : -1156.7650995669158\n",
      "    val_ess        : 1.9709498312162317\n",
      "    val_log_marginal: 1156.7934835682745\n",
      "    val_log_joint  : 1365.127245032269\n",
      "Train Epoch: 1150 [0/101520 (0%)] Loss: -1161.481567\n",
      "Train Epoch: 1150 [11264/101520 (11%)] Loss: -1157.107422\n",
      "Train Epoch: 1150 [22528/101520 (22%)] Loss: -1151.610596\n",
      "Train Epoch: 1150 [33792/101520 (33%)] Loss: -1160.993896\n",
      "Train Epoch: 1150 [45056/101520 (44%)] Loss: -1165.153564\n",
      "Train Epoch: 1150 [56320/101520 (55%)] Loss: -1168.017334\n",
      "Train Epoch: 1150 [67584/101520 (67%)] Loss: -1158.588379\n",
      "Train Epoch: 1150 [78848/101520 (78%)] Loss: -1155.882324\n",
      "Train Epoch: 1150 [90112/101520 (89%)] Loss: -1156.615601\n",
      "Train Epoch: 1150 [101376/101520 (100%)] Loss: -1162.194214\n",
      "    epoch          : 1150\n",
      "    loss           : -1158.8989196470634\n",
      "    ess            : 1.9673416572599556\n",
      "    log_marginal   : 1158.9292372507066\n",
      "    log_joint      : 1367.335493998312\n",
      "    val_loss       : -1157.5823125424592\n",
      "    val_ess        : 1.9700900575389033\n",
      "    val_log_marginal: 1157.6097783627717\n",
      "    val_log_joint  : 1365.8861455502717\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1150.pth ...\n",
      "Train Epoch: 1151 [0/101520 (0%)] Loss: -1160.340332\n",
      "Train Epoch: 1151 [11264/101520 (11%)] Loss: -1158.075562\n",
      "Train Epoch: 1151 [22528/101520 (22%)] Loss: -1155.544434\n",
      "Train Epoch: 1151 [33792/101520 (33%)] Loss: -1151.792969\n",
      "Train Epoch: 1151 [45056/101520 (44%)] Loss: -1152.074951\n",
      "Train Epoch: 1151 [56320/101520 (55%)] Loss: -1155.945435\n",
      "Train Epoch: 1151 [67584/101520 (67%)] Loss: -1154.877563\n",
      "Train Epoch: 1151 [78848/101520 (78%)] Loss: -1155.716187\n",
      "Train Epoch: 1151 [90112/101520 (89%)] Loss: -1155.125000\n",
      "Train Epoch: 1151 [101376/101520 (100%)] Loss: -1159.815063\n",
      "    epoch          : 1151\n",
      "    loss           : -1159.0159918243562\n",
      "    ess            : 1.9679650565487656\n",
      "    log_marginal   : 1159.0457162521593\n",
      "    log_joint      : 1367.4250991284548\n",
      "    val_loss       : -1157.7339981742527\n",
      "    val_ess        : 1.9699753471042798\n",
      "    val_log_marginal: 1157.7596117102582\n",
      "    val_log_joint  : 1366.0346945057745\n",
      "Train Epoch: 1152 [0/101520 (0%)] Loss: -1152.236572\n",
      "Train Epoch: 1152 [11264/101520 (11%)] Loss: -1158.658691\n",
      "Train Epoch: 1152 [22528/101520 (22%)] Loss: -1163.995117\n",
      "Train Epoch: 1152 [33792/101520 (33%)] Loss: -1156.390747\n",
      "Train Epoch: 1152 [45056/101520 (44%)] Loss: -1161.433960\n",
      "Train Epoch: 1152 [56320/101520 (55%)] Loss: -1155.681519\n",
      "Train Epoch: 1152 [67584/101520 (67%)] Loss: -1157.831909\n",
      "Train Epoch: 1152 [78848/101520 (78%)] Loss: -1160.927490\n",
      "Train Epoch: 1152 [90112/101520 (89%)] Loss: -1163.342407\n",
      "Train Epoch: 1152 [101376/101520 (100%)] Loss: -1165.487915\n",
      "    epoch          : 1152\n",
      "    loss           : -1159.0071855861338\n",
      "    ess            : 1.9674794014973855\n",
      "    log_marginal   : 1159.0369922120367\n",
      "    log_joint      : 1367.4090827673524\n",
      "    val_loss       : -1156.1064718495245\n",
      "    val_ess        : 1.966290375460749\n",
      "    val_log_marginal: 1156.137015964674\n",
      "    val_log_joint  : 1364.8316809612772\n",
      "Train Epoch: 1153 [0/101520 (0%)] Loss: -1154.834961\n",
      "Train Epoch: 1153 [11264/101520 (11%)] Loss: -1165.109985\n",
      "Train Epoch: 1153 [22528/101520 (22%)] Loss: -1154.761719\n",
      "Train Epoch: 1153 [33792/101520 (33%)] Loss: -1158.603027\n",
      "Train Epoch: 1153 [45056/101520 (44%)] Loss: -1156.008057\n",
      "Train Epoch: 1153 [56320/101520 (55%)] Loss: -1156.217285\n",
      "Train Epoch: 1153 [67584/101520 (67%)] Loss: -1155.186523\n",
      "Train Epoch: 1153 [78848/101520 (78%)] Loss: -1157.327881\n",
      "Train Epoch: 1153 [90112/101520 (89%)] Loss: -1159.062256\n",
      "Train Epoch: 1153 [101376/101520 (100%)] Loss: -1161.419434\n",
      "    epoch          : 1153\n",
      "    loss           : -1159.0064108383715\n",
      "    ess            : 1.9670386128689177\n",
      "    log_marginal   : 1159.0368676880496\n",
      "    log_joint      : 1367.4743247487438\n",
      "    val_loss       : -1157.0241911514945\n",
      "    val_ess        : 1.9654840230941772\n",
      "    val_log_marginal: 1157.056831691576\n",
      "    val_log_joint  : 1365.1066788383152\n",
      "Train Epoch: 1154 [0/101520 (0%)] Loss: -1162.868286\n",
      "Train Epoch: 1154 [11264/101520 (11%)] Loss: -1157.230347\n",
      "Train Epoch: 1154 [22528/101520 (22%)] Loss: -1151.224121\n",
      "Train Epoch: 1154 [33792/101520 (33%)] Loss: -1157.596924\n",
      "Train Epoch: 1154 [45056/101520 (44%)] Loss: -1160.610352\n",
      "Train Epoch: 1154 [56320/101520 (55%)] Loss: -1155.732788\n",
      "Train Epoch: 1154 [67584/101520 (67%)] Loss: -1163.094482\n",
      "Train Epoch: 1154 [78848/101520 (78%)] Loss: -1162.468140\n",
      "Train Epoch: 1154 [90112/101520 (89%)] Loss: -1156.490112\n",
      "Train Epoch: 1154 [101376/101520 (100%)] Loss: -1153.333618\n",
      "    epoch          : 1154\n",
      "    loss           : -1159.0710596439228\n",
      "    ess            : 1.9681286805838196\n",
      "    log_marginal   : 1159.1006227426194\n",
      "    log_joint      : 1367.5259831874214\n",
      "    val_loss       : -1157.565424380095\n",
      "    val_ess        : 1.9674327269844387\n",
      "    val_log_marginal: 1157.5959101137908\n",
      "    val_log_joint  : 1365.8002611243207\n",
      "Train Epoch: 1155 [0/101520 (0%)] Loss: -1162.110352\n",
      "Train Epoch: 1155 [11264/101520 (11%)] Loss: -1156.685303\n",
      "Train Epoch: 1155 [22528/101520 (22%)] Loss: -1166.404053\n",
      "Train Epoch: 1155 [33792/101520 (33%)] Loss: -1156.463623\n",
      "Train Epoch: 1155 [45056/101520 (44%)] Loss: -1167.312012\n",
      "Train Epoch: 1155 [56320/101520 (55%)] Loss: -1155.547607\n",
      "Train Epoch: 1155 [67584/101520 (67%)] Loss: -1167.057129\n",
      "Train Epoch: 1155 [78848/101520 (78%)] Loss: -1155.895874\n",
      "Train Epoch: 1155 [90112/101520 (89%)] Loss: -1160.253296\n",
      "Train Epoch: 1155 [101376/101520 (100%)] Loss: -1153.735962\n",
      "    epoch          : 1155\n",
      "    loss           : -1159.1655439060537\n",
      "    ess            : 1.9681261526280311\n",
      "    log_marginal   : 1159.1945647426587\n",
      "    log_joint      : 1367.5445519835505\n",
      "    val_loss       : -1159.103908372962\n",
      "    val_ess        : 1.9690432963163957\n",
      "    val_log_marginal: 1159.133199940557\n",
      "    val_log_joint  : 1367.3174411939538\n",
      "Train Epoch: 1156 [0/101520 (0%)] Loss: -1159.444092\n",
      "Train Epoch: 1156 [11264/101520 (11%)] Loss: -1156.568481\n",
      "Train Epoch: 1156 [22528/101520 (22%)] Loss: -1156.833984\n",
      "Train Epoch: 1156 [33792/101520 (33%)] Loss: -1160.103760\n",
      "Train Epoch: 1156 [45056/101520 (44%)] Loss: -1157.329956\n",
      "Train Epoch: 1156 [56320/101520 (55%)] Loss: -1156.635742\n",
      "Train Epoch: 1156 [67584/101520 (67%)] Loss: -1161.360474\n",
      "Train Epoch: 1156 [78848/101520 (78%)] Loss: -1160.253296\n",
      "Train Epoch: 1156 [90112/101520 (89%)] Loss: -1161.591309\n",
      "Train Epoch: 1156 [101376/101520 (100%)] Loss: -1160.013306\n",
      "    epoch          : 1156\n",
      "    loss           : -1159.2598478230998\n",
      "    ess            : 1.9677919023600057\n",
      "    log_marginal   : 1159.289666717376\n",
      "    log_joint      : 1367.663659483943\n",
      "    val_loss       : -1159.3628354279892\n",
      "    val_ess        : 1.9649123782696931\n",
      "    val_log_marginal: 1159.394632090693\n",
      "    val_log_joint  : 1367.7562680451767\n",
      "Train Epoch: 1157 [0/101520 (0%)] Loss: -1161.944946\n",
      "Train Epoch: 1157 [11264/101520 (11%)] Loss: -1150.157715\n",
      "Train Epoch: 1157 [22528/101520 (22%)] Loss: -1156.158936\n",
      "Train Epoch: 1157 [33792/101520 (33%)] Loss: -1161.860474\n",
      "Train Epoch: 1157 [45056/101520 (44%)] Loss: -1159.499390\n",
      "Train Epoch: 1157 [56320/101520 (55%)] Loss: -1155.987549\n",
      "Train Epoch: 1157 [67584/101520 (67%)] Loss: -1160.581665\n",
      "Train Epoch: 1157 [78848/101520 (78%)] Loss: -1159.541504\n",
      "Train Epoch: 1157 [90112/101520 (89%)] Loss: -1156.470093\n",
      "Train Epoch: 1157 [101376/101520 (100%)] Loss: -1158.692505\n",
      "    epoch          : 1157\n",
      "    loss           : -1159.243618605724\n",
      "    ess            : 1.967677408726371\n",
      "    log_marginal   : 1159.2733031613143\n",
      "    log_joint      : 1367.65640519492\n",
      "    val_loss       : -1157.6936300526495\n",
      "    val_ess        : 1.9677103395047395\n",
      "    val_log_marginal: 1157.7222581946332\n",
      "    val_log_joint  : 1366.0610245414402\n",
      "Train Epoch: 1158 [0/101520 (0%)] Loss: -1159.141846\n",
      "Train Epoch: 1158 [11264/101520 (11%)] Loss: -1160.109619\n",
      "Train Epoch: 1158 [22528/101520 (22%)] Loss: -1162.022705\n",
      "Train Epoch: 1158 [33792/101520 (33%)] Loss: -1157.017578\n",
      "Train Epoch: 1158 [45056/101520 (44%)] Loss: -1156.966919\n",
      "Train Epoch: 1158 [56320/101520 (55%)] Loss: -1156.116333\n",
      "Train Epoch: 1158 [67584/101520 (67%)] Loss: -1161.995483\n",
      "Train Epoch: 1158 [78848/101520 (78%)] Loss: -1151.381836\n",
      "Train Epoch: 1158 [90112/101520 (89%)] Loss: -1161.166138\n",
      "Train Epoch: 1158 [101376/101520 (100%)] Loss: -1163.860229\n",
      "    epoch          : 1158\n",
      "    loss           : -1159.3889601817682\n",
      "    ess            : 1.9670897129193023\n",
      "    log_marginal   : 1159.4192569291772\n",
      "    log_joint      : 1367.7940244435065\n",
      "    val_loss       : -1159.83080523947\n",
      "    val_ess        : 1.9662834146748418\n",
      "    val_log_marginal: 1159.8594811480978\n",
      "    val_log_joint  : 1368.2443104619565\n",
      "Train Epoch: 1159 [0/101520 (0%)] Loss: -1161.134277\n",
      "Train Epoch: 1159 [11264/101520 (11%)] Loss: -1157.604492\n",
      "Train Epoch: 1159 [22528/101520 (22%)] Loss: -1156.240601\n",
      "Train Epoch: 1159 [33792/101520 (33%)] Loss: -1160.221191\n",
      "Train Epoch: 1159 [45056/101520 (44%)] Loss: -1160.813232\n",
      "Train Epoch: 1159 [56320/101520 (55%)] Loss: -1157.446533\n",
      "Train Epoch: 1159 [67584/101520 (67%)] Loss: -1154.256592\n",
      "Train Epoch: 1159 [78848/101520 (78%)] Loss: -1159.639648\n",
      "Train Epoch: 1159 [90112/101520 (89%)] Loss: -1151.671143\n",
      "Train Epoch: 1159 [101376/101520 (100%)] Loss: -1172.702881\n",
      "    epoch          : 1159\n",
      "    loss           : -1159.43175472087\n",
      "    ess            : 1.9665018128390288\n",
      "    log_marginal   : 1159.4635414621937\n",
      "    log_joint      : 1367.9107794833542\n",
      "    val_loss       : -1156.630174719769\n",
      "    val_ess        : 1.9673381059066108\n",
      "    val_log_marginal: 1156.6571416440217\n",
      "    val_log_joint  : 1365.0080354110055\n",
      "Train Epoch: 1160 [0/101520 (0%)] Loss: -1158.784180\n",
      "Train Epoch: 1160 [11264/101520 (11%)] Loss: -1154.650879\n",
      "Train Epoch: 1160 [22528/101520 (22%)] Loss: -1160.779785\n",
      "Train Epoch: 1160 [33792/101520 (33%)] Loss: -1155.536621\n",
      "Train Epoch: 1160 [45056/101520 (44%)] Loss: -1160.789307\n",
      "Train Epoch: 1160 [56320/101520 (55%)] Loss: -1166.724609\n",
      "Train Epoch: 1160 [67584/101520 (67%)] Loss: -1164.244629\n",
      "Train Epoch: 1160 [78848/101520 (78%)] Loss: -1165.382568\n",
      "Train Epoch: 1160 [90112/101520 (89%)] Loss: -1158.433105\n",
      "Train Epoch: 1160 [101376/101520 (100%)] Loss: -1151.238525\n",
      "    epoch          : 1160\n",
      "    loss           : -1159.4816189099795\n",
      "    ess            : 1.9674827357632432\n",
      "    log_marginal   : 1159.511752488026\n",
      "    log_joint      : 1367.8071816602544\n",
      "    val_loss       : -1159.0341372282608\n",
      "    val_ess        : 1.9652445886446082\n",
      "    val_log_marginal: 1159.0663637907608\n",
      "    val_log_joint  : 1367.477151621943\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1160.pth ...\n",
      "Train Epoch: 1161 [0/101520 (0%)] Loss: -1162.626221\n",
      "Train Epoch: 1161 [11264/101520 (11%)] Loss: -1158.093994\n",
      "Train Epoch: 1161 [22528/101520 (22%)] Loss: -1160.551514\n",
      "Train Epoch: 1161 [33792/101520 (33%)] Loss: -1155.099854\n",
      "Train Epoch: 1161 [45056/101520 (44%)] Loss: -1159.340088\n",
      "Train Epoch: 1161 [56320/101520 (55%)] Loss: -1158.227295\n",
      "Train Epoch: 1161 [67584/101520 (67%)] Loss: -1155.186035\n",
      "Train Epoch: 1161 [78848/101520 (78%)] Loss: -1160.022217\n",
      "Train Epoch: 1161 [90112/101520 (89%)] Loss: -1160.056152\n",
      "Train Epoch: 1161 [101376/101520 (100%)] Loss: -1155.203979\n",
      "    epoch          : 1161\n",
      "    loss           : -1159.4907355380417\n",
      "    ess            : 1.967385101557976\n",
      "    log_marginal   : 1159.5208942662532\n",
      "    log_joint      : 1367.8900802842336\n",
      "    val_loss       : -1157.4522598930027\n",
      "    val_ess        : 1.9669815405555393\n",
      "    val_log_marginal: 1157.4809464164402\n",
      "    val_log_joint  : 1365.9597486413043\n",
      "Train Epoch: 1162 [0/101520 (0%)] Loss: -1161.210449\n",
      "Train Epoch: 1162 [11264/101520 (11%)] Loss: -1159.668457\n",
      "Train Epoch: 1162 [22528/101520 (22%)] Loss: -1161.104004\n",
      "Train Epoch: 1162 [33792/101520 (33%)] Loss: -1152.557129\n",
      "Train Epoch: 1162 [45056/101520 (44%)] Loss: -1158.796387\n",
      "Train Epoch: 1162 [56320/101520 (55%)] Loss: -1157.368652\n",
      "Train Epoch: 1162 [67584/101520 (67%)] Loss: -1161.062988\n",
      "Train Epoch: 1162 [78848/101520 (78%)] Loss: -1156.231934\n",
      "Train Epoch: 1162 [90112/101520 (89%)] Loss: -1154.060791\n",
      "Train Epoch: 1162 [101376/101520 (100%)] Loss: -1152.277832\n",
      "    epoch          : 1162\n",
      "    loss           : -1159.596697476641\n",
      "    ess            : 1.9679424870553328\n",
      "    log_marginal   : 1159.625686415476\n",
      "    log_joint      : 1367.9849264633715\n",
      "    val_loss       : -1157.7383077870245\n",
      "    val_ess        : 1.969016458677209\n",
      "    val_log_marginal: 1157.7673658288043\n",
      "    val_log_joint  : 1366.099604067595\n",
      "Train Epoch: 1163 [0/101520 (0%)] Loss: -1169.588623\n",
      "Train Epoch: 1163 [11264/101520 (11%)] Loss: -1160.041260\n",
      "Train Epoch: 1163 [22528/101520 (22%)] Loss: -1158.051270\n",
      "Train Epoch: 1163 [33792/101520 (33%)] Loss: -1158.114014\n",
      "Train Epoch: 1163 [45056/101520 (44%)] Loss: -1163.987549\n",
      "Train Epoch: 1163 [56320/101520 (55%)] Loss: -1164.638672\n",
      "Train Epoch: 1163 [67584/101520 (67%)] Loss: -1163.852661\n",
      "Train Epoch: 1163 [78848/101520 (78%)] Loss: -1154.768799\n",
      "Train Epoch: 1163 [90112/101520 (89%)] Loss: -1161.875610\n",
      "Train Epoch: 1163 [101376/101520 (100%)] Loss: -1153.089966\n",
      "    epoch          : 1163\n",
      "    loss           : -1159.6021366598618\n",
      "    ess            : 1.9674208835141742\n",
      "    log_marginal   : 1159.6327671070194\n",
      "    log_joint      : 1368.0735562578518\n",
      "    val_loss       : -1158.0168881623642\n",
      "    val_ess        : 1.9664519610612288\n",
      "    val_log_marginal: 1158.045415463655\n",
      "    val_log_joint  : 1366.6138491423233\n",
      "Train Epoch: 1164 [0/101520 (0%)] Loss: -1165.844238\n",
      "Train Epoch: 1164 [11264/101520 (11%)] Loss: -1164.072510\n",
      "Train Epoch: 1164 [22528/101520 (22%)] Loss: -1162.494385\n",
      "Train Epoch: 1164 [33792/101520 (33%)] Loss: -1152.931030\n",
      "Train Epoch: 1164 [45056/101520 (44%)] Loss: -1155.977051\n",
      "Train Epoch: 1164 [56320/101520 (55%)] Loss: -1162.716309\n",
      "Train Epoch: 1164 [67584/101520 (67%)] Loss: -1160.531006\n",
      "Train Epoch: 1164 [78848/101520 (78%)] Loss: -1162.604980\n",
      "Train Epoch: 1164 [90112/101520 (89%)] Loss: -1163.115112\n",
      "Train Epoch: 1164 [101376/101520 (100%)] Loss: -1155.252686\n",
      "    epoch          : 1164\n",
      "    loss           : -1159.716452747134\n",
      "    ess            : 1.9668830471422205\n",
      "    log_marginal   : 1159.74695621663\n",
      "    log_joint      : 1368.1105981567996\n",
      "    val_loss       : -1157.6669125764267\n",
      "    val_ess        : 1.9692908991938052\n",
      "    val_log_marginal: 1157.6938582710598\n",
      "    val_log_joint  : 1366.1010636039402\n",
      "Train Epoch: 1165 [0/101520 (0%)] Loss: -1167.891846\n",
      "Train Epoch: 1165 [11264/101520 (11%)] Loss: -1159.582520\n",
      "Train Epoch: 1165 [22528/101520 (22%)] Loss: -1160.664795\n",
      "Train Epoch: 1165 [33792/101520 (33%)] Loss: -1157.945312\n",
      "Train Epoch: 1165 [45056/101520 (44%)] Loss: -1159.571289\n",
      "Train Epoch: 1165 [56320/101520 (55%)] Loss: -1161.886719\n",
      "Train Epoch: 1165 [67584/101520 (67%)] Loss: -1154.947021\n",
      "Train Epoch: 1165 [78848/101520 (78%)] Loss: -1154.777832\n",
      "Train Epoch: 1165 [90112/101520 (89%)] Loss: -1162.257324\n",
      "Train Epoch: 1165 [101376/101520 (100%)] Loss: -1172.952881\n",
      "    epoch          : 1165\n",
      "    loss           : -1159.8307896906408\n",
      "    ess            : 1.9667591114140035\n",
      "    log_marginal   : 1159.8612275243404\n",
      "    log_joint      : 1368.236304815091\n",
      "    val_loss       : -1158.3546779466712\n",
      "    val_ess        : 1.9677791595458984\n",
      "    val_log_marginal: 1158.386235776155\n",
      "    val_log_joint  : 1366.8307415506115\n",
      "Train Epoch: 1166 [0/101520 (0%)] Loss: -1158.847656\n",
      "Train Epoch: 1166 [11264/101520 (11%)] Loss: -1173.534424\n",
      "Train Epoch: 1166 [22528/101520 (22%)] Loss: -1155.277344\n",
      "Train Epoch: 1166 [33792/101520 (33%)] Loss: -1156.761353\n",
      "Train Epoch: 1166 [45056/101520 (44%)] Loss: -1155.161255\n",
      "Train Epoch: 1166 [56320/101520 (55%)] Loss: -1162.970215\n",
      "Train Epoch: 1166 [67584/101520 (67%)] Loss: -1156.053711\n",
      "Train Epoch: 1166 [78848/101520 (78%)] Loss: -1156.835815\n",
      "Train Epoch: 1166 [90112/101520 (89%)] Loss: -1160.674561\n",
      "Train Epoch: 1166 [101376/101520 (100%)] Loss: -1160.500610\n",
      "    epoch          : 1166\n",
      "    loss           : -1159.848921732687\n",
      "    ess            : 1.9677255261483504\n",
      "    log_marginal   : 1159.8793816494583\n",
      "    log_joint      : 1368.290557401264\n",
      "    val_loss       : -1158.8540410580842\n",
      "    val_ess        : 1.962965446969737\n",
      "    val_log_marginal: 1158.8892344599185\n",
      "    val_log_joint  : 1367.5283468495245\n",
      "Train Epoch: 1167 [0/101520 (0%)] Loss: -1156.132568\n",
      "Train Epoch: 1167 [11264/101520 (11%)] Loss: -1156.522827\n",
      "Train Epoch: 1167 [22528/101520 (22%)] Loss: -1164.810913\n",
      "Train Epoch: 1167 [33792/101520 (33%)] Loss: -1156.914307\n",
      "Train Epoch: 1167 [45056/101520 (44%)] Loss: -1156.298584\n",
      "Train Epoch: 1167 [56320/101520 (55%)] Loss: -1158.392090\n",
      "Train Epoch: 1167 [67584/101520 (67%)] Loss: -1157.124512\n",
      "Train Epoch: 1167 [78848/101520 (78%)] Loss: -1159.794067\n",
      "Train Epoch: 1167 [90112/101520 (89%)] Loss: -1159.003174\n",
      "Train Epoch: 1167 [101376/101520 (100%)] Loss: -1149.081787\n",
      "    epoch          : 1167\n",
      "    loss           : -1159.793122104664\n",
      "    ess            : 1.966647462030152\n",
      "    log_marginal   : 1159.8234262130968\n",
      "    log_joint      : 1368.2832859365185\n",
      "    val_loss       : -1158.3652449898098\n",
      "    val_ess        : 1.9698847739592842\n",
      "    val_log_marginal: 1158.3928381878397\n",
      "    val_log_joint  : 1366.727931810462\n",
      "Train Epoch: 1168 [0/101520 (0%)] Loss: -1162.931396\n",
      "Train Epoch: 1168 [11264/101520 (11%)] Loss: -1157.949585\n",
      "Train Epoch: 1168 [22528/101520 (22%)] Loss: -1156.304077\n",
      "Train Epoch: 1168 [33792/101520 (33%)] Loss: -1159.117188\n",
      "Train Epoch: 1168 [45056/101520 (44%)] Loss: -1156.013916\n",
      "Train Epoch: 1168 [56320/101520 (55%)] Loss: -1156.820190\n",
      "Train Epoch: 1168 [67584/101520 (67%)] Loss: -1158.086304\n",
      "Train Epoch: 1168 [78848/101520 (78%)] Loss: -1153.719971\n",
      "Train Epoch: 1168 [90112/101520 (89%)] Loss: -1159.020264\n",
      "Train Epoch: 1168 [101376/101520 (100%)] Loss: -1159.118530\n",
      "    epoch          : 1168\n",
      "    loss           : -1159.9728476366206\n",
      "    ess            : 1.967284529053386\n",
      "    log_marginal   : 1160.0035406524812\n",
      "    log_joint      : 1368.4106739753454\n",
      "    val_loss       : -1158.4144180961277\n",
      "    val_ess        : 1.9670000853745833\n",
      "    val_log_marginal: 1158.4472072435462\n",
      "    val_log_joint  : 1366.7516452955163\n",
      "Train Epoch: 1169 [0/101520 (0%)] Loss: -1166.998047\n",
      "Train Epoch: 1169 [11264/101520 (11%)] Loss: -1160.239746\n",
      "Train Epoch: 1169 [22528/101520 (22%)] Loss: -1162.519409\n",
      "Train Epoch: 1169 [33792/101520 (33%)] Loss: -1155.565552\n",
      "Train Epoch: 1169 [45056/101520 (44%)] Loss: -1160.153076\n",
      "Train Epoch: 1169 [56320/101520 (55%)] Loss: -1155.942993\n",
      "Train Epoch: 1169 [67584/101520 (67%)] Loss: -1165.454834\n",
      "Train Epoch: 1169 [78848/101520 (78%)] Loss: -1153.437012\n",
      "Train Epoch: 1169 [90112/101520 (89%)] Loss: -1157.423218\n",
      "Train Epoch: 1169 [101376/101520 (100%)] Loss: -1166.721436\n",
      "    epoch          : 1169\n",
      "    loss           : -1160.0641924220713\n",
      "    ess            : 1.9677065748665201\n",
      "    log_marginal   : 1160.0932801213096\n",
      "    log_joint      : 1368.4696259618404\n",
      "    val_loss       : -1157.620111880095\n",
      "    val_ess        : 1.9676794694817585\n",
      "    val_log_marginal: 1157.6501252547555\n",
      "    val_log_joint  : 1366.3135880180027\n",
      "Train Epoch: 1170 [0/101520 (0%)] Loss: -1161.174561\n",
      "Train Epoch: 1170 [11264/101520 (11%)] Loss: -1162.994385\n",
      "Train Epoch: 1170 [22528/101520 (22%)] Loss: -1158.564209\n",
      "Train Epoch: 1170 [33792/101520 (33%)] Loss: -1167.539551\n",
      "Train Epoch: 1170 [45056/101520 (44%)] Loss: -1161.483276\n",
      "Train Epoch: 1170 [56320/101520 (55%)] Loss: -1157.410156\n",
      "Train Epoch: 1170 [67584/101520 (67%)] Loss: -1163.147705\n",
      "Train Epoch: 1170 [78848/101520 (78%)] Loss: -1161.604248\n",
      "Train Epoch: 1170 [90112/101520 (89%)] Loss: -1161.156494\n",
      "Train Epoch: 1170 [101376/101520 (100%)] Loss: -1168.152100\n",
      "    epoch          : 1170\n",
      "    loss           : -1160.0906767725346\n",
      "    ess            : 1.9663374016632387\n",
      "    log_marginal   : 1160.1223567790123\n",
      "    log_joint      : 1368.5639212910255\n",
      "    val_loss       : -1158.0093038807745\n",
      "    val_ess        : 1.9692963620890742\n",
      "    val_log_marginal: 1158.0343601392663\n",
      "    val_log_joint  : 1366.4133990743885\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1170.pth ...\n",
      "Train Epoch: 1171 [0/101520 (0%)] Loss: -1157.115967\n",
      "Train Epoch: 1171 [11264/101520 (11%)] Loss: -1158.248901\n",
      "Train Epoch: 1171 [22528/101520 (22%)] Loss: -1167.383545\n",
      "Train Epoch: 1171 [33792/101520 (33%)] Loss: -1158.972656\n",
      "Train Epoch: 1171 [45056/101520 (44%)] Loss: -1159.160645\n",
      "Train Epoch: 1171 [56320/101520 (55%)] Loss: -1157.013184\n",
      "Train Epoch: 1171 [67584/101520 (67%)] Loss: -1156.185059\n",
      "Train Epoch: 1171 [78848/101520 (78%)] Loss: -1163.617188\n",
      "Train Epoch: 1171 [90112/101520 (89%)] Loss: -1155.347656\n",
      "Train Epoch: 1171 [101376/101520 (100%)] Loss: -1154.304565\n",
      "    epoch          : 1171\n",
      "    loss           : -1160.195736372291\n",
      "    ess            : 1.9674678154327163\n",
      "    log_marginal   : 1160.225292109964\n",
      "    log_joint      : 1368.6008429599167\n",
      "    val_loss       : -1158.144340183424\n",
      "    val_ess        : 1.9688176745953767\n",
      "    val_log_marginal: 1158.1724694293478\n",
      "    val_log_joint  : 1366.558737049932\n",
      "Train Epoch: 1172 [0/101520 (0%)] Loss: -1169.909424\n",
      "Train Epoch: 1172 [11264/101520 (11%)] Loss: -1155.843872\n",
      "Train Epoch: 1172 [22528/101520 (22%)] Loss: -1157.634766\n",
      "Train Epoch: 1172 [33792/101520 (33%)] Loss: -1162.826294\n",
      "Train Epoch: 1172 [45056/101520 (44%)] Loss: -1152.916138\n",
      "Train Epoch: 1172 [56320/101520 (55%)] Loss: -1159.189453\n",
      "Train Epoch: 1172 [67584/101520 (67%)] Loss: -1155.687256\n",
      "Train Epoch: 1172 [78848/101520 (78%)] Loss: -1157.292603\n",
      "Train Epoch: 1172 [90112/101520 (89%)] Loss: -1156.511841\n",
      "Train Epoch: 1172 [101376/101520 (100%)] Loss: -1149.455566\n",
      "    epoch          : 1172\n",
      "    loss           : -1160.182330107569\n",
      "    ess            : 1.9677908276792746\n",
      "    log_marginal   : 1160.2124998773163\n",
      "    log_joint      : 1368.614606847715\n",
      "    val_loss       : -1158.6534529976223\n",
      "    val_ess        : 1.9650298978971399\n",
      "    val_log_marginal: 1158.6841138756793\n",
      "    val_log_joint  : 1367.4559326171875\n",
      "Train Epoch: 1173 [0/101520 (0%)] Loss: -1164.397949\n",
      "Train Epoch: 1173 [11264/101520 (11%)] Loss: -1155.941528\n",
      "Train Epoch: 1173 [22528/101520 (22%)] Loss: -1164.899414\n",
      "Train Epoch: 1173 [33792/101520 (33%)] Loss: -1161.452271\n",
      "Train Epoch: 1173 [45056/101520 (44%)] Loss: -1159.593994\n",
      "Train Epoch: 1173 [56320/101520 (55%)] Loss: -1154.674194\n",
      "Train Epoch: 1173 [67584/101520 (67%)] Loss: -1162.697388\n",
      "Train Epoch: 1173 [78848/101520 (78%)] Loss: -1163.508057\n",
      "Train Epoch: 1173 [90112/101520 (89%)] Loss: -1159.556152\n",
      "Train Epoch: 1173 [101376/101520 (100%)] Loss: -1160.201660\n",
      "    epoch          : 1173\n",
      "    loss           : -1160.2959916771356\n",
      "    ess            : 1.9680682714261002\n",
      "    log_marginal   : 1160.3249622134108\n",
      "    log_joint      : 1368.6708009039337\n",
      "    val_loss       : -1159.5978367017663\n",
      "    val_ess        : 1.967370375342991\n",
      "    val_log_marginal: 1159.6269106657608\n",
      "    val_log_joint  : 1368.0009075662365\n",
      "Train Epoch: 1174 [0/101520 (0%)] Loss: -1167.060303\n",
      "Train Epoch: 1174 [11264/101520 (11%)] Loss: -1167.142090\n",
      "Train Epoch: 1174 [22528/101520 (22%)] Loss: -1162.664551\n",
      "Train Epoch: 1174 [33792/101520 (33%)] Loss: -1165.640747\n",
      "Train Epoch: 1174 [45056/101520 (44%)] Loss: -1162.762451\n",
      "Train Epoch: 1174 [56320/101520 (55%)] Loss: -1161.636230\n",
      "Train Epoch: 1174 [67584/101520 (67%)] Loss: -1159.896729\n",
      "Train Epoch: 1174 [78848/101520 (78%)] Loss: -1164.496582\n",
      "Train Epoch: 1174 [90112/101520 (89%)] Loss: -1162.095947\n",
      "Train Epoch: 1174 [101376/101520 (100%)] Loss: -1157.840088\n",
      "    epoch          : 1174\n",
      "    loss           : -1160.23798006144\n",
      "    ess            : 1.967296708169295\n",
      "    log_marginal   : 1160.267941882263\n",
      "    log_joint      : 1368.654950779287\n",
      "    val_loss       : -1158.8186725118885\n",
      "    val_ess        : 1.9676374818967737\n",
      "    val_log_marginal: 1158.849269701087\n",
      "    val_log_joint  : 1367.4627314028533\n",
      "Train Epoch: 1175 [0/101520 (0%)] Loss: -1160.074707\n",
      "Train Epoch: 1175 [11264/101520 (11%)] Loss: -1159.819092\n",
      "Train Epoch: 1175 [22528/101520 (22%)] Loss: -1155.805908\n",
      "Train Epoch: 1175 [33792/101520 (33%)] Loss: -1156.119141\n",
      "Train Epoch: 1175 [45056/101520 (44%)] Loss: -1152.749268\n",
      "Train Epoch: 1175 [56320/101520 (55%)] Loss: -1153.906250\n",
      "Train Epoch: 1175 [67584/101520 (67%)] Loss: -1167.987305\n",
      "Train Epoch: 1175 [78848/101520 (78%)] Loss: -1155.682007\n",
      "Train Epoch: 1175 [90112/101520 (89%)] Loss: -1154.462646\n",
      "Train Epoch: 1175 [101376/101520 (100%)] Loss: -1168.221924\n",
      "    epoch          : 1175\n",
      "    loss           : -1160.3215822766174\n",
      "    ess            : 1.966693643349499\n",
      "    log_marginal   : 1160.3530481999842\n",
      "    log_joint      : 1368.8096341080402\n",
      "    val_loss       : -1160.268363620924\n",
      "    val_ess        : 1.96800202390422\n",
      "    val_log_marginal: 1160.294428286345\n",
      "    val_log_joint  : 1368.499708092731\n",
      "Train Epoch: 1176 [0/101520 (0%)] Loss: -1161.992188\n",
      "Train Epoch: 1176 [11264/101520 (11%)] Loss: -1158.947754\n",
      "Train Epoch: 1176 [22528/101520 (22%)] Loss: -1158.862671\n",
      "Train Epoch: 1176 [33792/101520 (33%)] Loss: -1162.529175\n",
      "Train Epoch: 1176 [45056/101520 (44%)] Loss: -1158.955322\n",
      "Train Epoch: 1176 [56320/101520 (55%)] Loss: -1162.765991\n",
      "Train Epoch: 1176 [67584/101520 (67%)] Loss: -1164.281738\n",
      "Train Epoch: 1176 [78848/101520 (78%)] Loss: -1165.767456\n",
      "Train Epoch: 1176 [90112/101520 (89%)] Loss: -1162.192993\n",
      "Train Epoch: 1176 [101376/101520 (100%)] Loss: -1171.371704\n",
      "    epoch          : 1176\n",
      "    loss           : -1160.4896393589038\n",
      "    ess            : 1.9676902282178101\n",
      "    log_marginal   : 1160.5196422787767\n",
      "    log_joint      : 1368.847157540633\n",
      "    val_loss       : -1159.3061364215353\n",
      "    val_ess        : 1.964869779089223\n",
      "    val_log_marginal: 1159.343065344769\n",
      "    val_log_joint  : 1367.7425059442935\n",
      "Train Epoch: 1177 [0/101520 (0%)] Loss: -1168.373291\n",
      "Train Epoch: 1177 [11264/101520 (11%)] Loss: -1161.549927\n",
      "Train Epoch: 1177 [22528/101520 (22%)] Loss: -1161.465088\n",
      "Train Epoch: 1177 [33792/101520 (33%)] Loss: -1155.977783\n",
      "Train Epoch: 1177 [45056/101520 (44%)] Loss: -1161.311646\n",
      "Train Epoch: 1177 [56320/101520 (55%)] Loss: -1165.409668\n",
      "Train Epoch: 1177 [67584/101520 (67%)] Loss: -1159.835938\n",
      "Train Epoch: 1177 [78848/101520 (78%)] Loss: -1158.911133\n",
      "Train Epoch: 1177 [90112/101520 (89%)] Loss: -1162.986328\n",
      "Train Epoch: 1177 [101376/101520 (100%)] Loss: -1154.343384\n",
      "    epoch          : 1177\n",
      "    loss           : -1160.3826763210584\n",
      "    ess            : 1.9673336882088053\n",
      "    log_marginal   : 1160.4123504485317\n",
      "    log_joint      : 1368.80938321981\n",
      "    val_loss       : -1158.6886835512908\n",
      "    val_ess        : 1.969479861466781\n",
      "    val_log_marginal: 1158.7163775900135\n",
      "    val_log_joint  : 1366.8444505774457\n",
      "Train Epoch: 1178 [0/101520 (0%)] Loss: -1156.519775\n",
      "Train Epoch: 1178 [11264/101520 (11%)] Loss: -1156.378662\n",
      "Train Epoch: 1178 [22528/101520 (22%)] Loss: -1160.593994\n",
      "Train Epoch: 1178 [33792/101520 (33%)] Loss: -1159.585693\n",
      "Train Epoch: 1178 [45056/101520 (44%)] Loss: -1165.818115\n",
      "Train Epoch: 1178 [56320/101520 (55%)] Loss: -1164.201050\n",
      "Train Epoch: 1178 [67584/101520 (67%)] Loss: -1163.833862\n",
      "Train Epoch: 1178 [78848/101520 (78%)] Loss: -1163.370117\n",
      "Train Epoch: 1178 [90112/101520 (89%)] Loss: -1165.332397\n",
      "Train Epoch: 1178 [101376/101520 (100%)] Loss: -1148.259521\n",
      "    epoch          : 1178\n",
      "    loss           : -1160.3304645787532\n",
      "    ess            : 1.9667170317328755\n",
      "    log_marginal   : 1160.3619219142588\n",
      "    log_joint      : 1368.8251272230293\n",
      "    val_loss       : -1159.0709865404212\n",
      "    val_ess        : 1.9624435953471973\n",
      "    val_log_marginal: 1159.1030432659647\n",
      "    val_log_joint  : 1367.7231551460598\n",
      "Train Epoch: 1179 [0/101520 (0%)] Loss: -1160.004395\n",
      "Train Epoch: 1179 [11264/101520 (11%)] Loss: -1161.296753\n",
      "Train Epoch: 1179 [22528/101520 (22%)] Loss: -1157.838135\n",
      "Train Epoch: 1179 [33792/101520 (33%)] Loss: -1158.697632\n",
      "Train Epoch: 1179 [45056/101520 (44%)] Loss: -1160.674805\n",
      "Train Epoch: 1179 [56320/101520 (55%)] Loss: -1162.243652\n",
      "Train Epoch: 1179 [67584/101520 (67%)] Loss: -1160.959595\n",
      "Train Epoch: 1179 [78848/101520 (78%)] Loss: -1166.194092\n",
      "Train Epoch: 1179 [90112/101520 (89%)] Loss: -1157.011475\n",
      "Train Epoch: 1179 [101376/101520 (100%)] Loss: -1166.151245\n",
      "    epoch          : 1179\n",
      "    loss           : -1160.582261281996\n",
      "    ess            : 1.9675885726459061\n",
      "    log_marginal   : 1160.6117851218985\n",
      "    log_joint      : 1368.9914060046326\n",
      "    val_loss       : -1157.6158075747283\n",
      "    val_ess        : 1.9664743050285007\n",
      "    val_log_marginal: 1157.6438572095788\n",
      "    val_log_joint  : 1366.0572668987772\n",
      "Train Epoch: 1180 [0/101520 (0%)] Loss: -1163.147339\n",
      "Train Epoch: 1180 [11264/101520 (11%)] Loss: -1165.500122\n",
      "Train Epoch: 1180 [22528/101520 (22%)] Loss: -1158.900024\n",
      "Train Epoch: 1180 [33792/101520 (33%)] Loss: -1161.514648\n",
      "Train Epoch: 1180 [45056/101520 (44%)] Loss: -1160.868164\n",
      "Train Epoch: 1180 [56320/101520 (55%)] Loss: -1165.293213\n",
      "Train Epoch: 1180 [67584/101520 (67%)] Loss: -1159.049072\n",
      "Train Epoch: 1180 [78848/101520 (78%)] Loss: -1161.889526\n",
      "Train Epoch: 1180 [90112/101520 (89%)] Loss: -1155.683594\n",
      "Train Epoch: 1180 [101376/101520 (100%)] Loss: -1166.241577\n",
      "    epoch          : 1180\n",
      "    loss           : -1160.6297251639055\n",
      "    ess            : 1.9672802489007537\n",
      "    log_marginal   : 1160.6603691062735\n",
      "    log_joint      : 1369.0558260050252\n",
      "    val_loss       : -1159.2787661345108\n",
      "    val_ess        : 1.9702111275299736\n",
      "    val_log_marginal: 1159.3029997452445\n",
      "    val_log_joint  : 1367.56810992697\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1180.pth ...\n",
      "Train Epoch: 1181 [0/101520 (0%)] Loss: -1156.956543\n",
      "Train Epoch: 1181 [11264/101520 (11%)] Loss: -1159.721069\n",
      "Train Epoch: 1181 [22528/101520 (22%)] Loss: -1158.026123\n",
      "Train Epoch: 1181 [33792/101520 (33%)] Loss: -1157.514404\n",
      "Train Epoch: 1181 [45056/101520 (44%)] Loss: -1161.636597\n",
      "Train Epoch: 1181 [56320/101520 (55%)] Loss: -1162.263184\n",
      "Train Epoch: 1181 [67584/101520 (67%)] Loss: -1167.587280\n",
      "Train Epoch: 1181 [78848/101520 (78%)] Loss: -1158.012207\n",
      "Train Epoch: 1181 [90112/101520 (89%)] Loss: -1159.022705\n",
      "Train Epoch: 1181 [101376/101520 (100%)] Loss: -1153.555786\n",
      "    epoch          : 1181\n",
      "    loss           : -1160.6618284292556\n",
      "    ess            : 1.9664772838803393\n",
      "    log_marginal   : 1160.692821406839\n",
      "    log_joint      : 1368.9981836673603\n",
      "    val_loss       : -1158.7631623641305\n",
      "    val_ess        : 1.9687551415484885\n",
      "    val_log_marginal: 1158.7907874065897\n",
      "    val_log_joint  : 1367.050149668818\n",
      "Train Epoch: 1182 [0/101520 (0%)] Loss: -1164.010864\n",
      "Train Epoch: 1182 [11264/101520 (11%)] Loss: -1161.968994\n",
      "Train Epoch: 1182 [22528/101520 (22%)] Loss: -1157.127319\n",
      "Train Epoch: 1182 [33792/101520 (33%)] Loss: -1159.099365\n",
      "Train Epoch: 1182 [45056/101520 (44%)] Loss: -1162.487305\n",
      "Train Epoch: 1182 [56320/101520 (55%)] Loss: -1160.931396\n",
      "Train Epoch: 1182 [67584/101520 (67%)] Loss: -1165.171143\n",
      "Train Epoch: 1182 [78848/101520 (78%)] Loss: -1165.628174\n",
      "Train Epoch: 1182 [90112/101520 (89%)] Loss: -1161.562744\n",
      "Train Epoch: 1182 [101376/101520 (100%)] Loss: -1154.940796\n",
      "    epoch          : 1182\n",
      "    loss           : -1160.7479137631517\n",
      "    ess            : 1.9669911759582597\n",
      "    log_marginal   : 1160.778826382891\n",
      "    log_joint      : 1369.0744702516488\n",
      "    val_loss       : -1159.6762483016305\n",
      "    val_ess        : 1.9683817625045776\n",
      "    val_log_marginal: 1159.7051842730978\n",
      "    val_log_joint  : 1368.1628205672555\n",
      "Train Epoch: 1183 [0/101520 (0%)] Loss: -1165.585449\n",
      "Train Epoch: 1183 [11264/101520 (11%)] Loss: -1163.741577\n",
      "Train Epoch: 1183 [22528/101520 (22%)] Loss: -1158.820801\n",
      "Train Epoch: 1183 [33792/101520 (33%)] Loss: -1155.665527\n",
      "Train Epoch: 1183 [45056/101520 (44%)] Loss: -1164.934937\n",
      "Train Epoch: 1183 [56320/101520 (55%)] Loss: -1156.779053\n",
      "Train Epoch: 1183 [67584/101520 (67%)] Loss: -1160.343872\n",
      "Train Epoch: 1183 [78848/101520 (78%)] Loss: -1163.079102\n",
      "Train Epoch: 1183 [90112/101520 (89%)] Loss: -1155.768799\n",
      "Train Epoch: 1183 [101376/101520 (100%)] Loss: -1145.163086\n",
      "    epoch          : 1183\n",
      "    loss           : -1160.6856695587312\n",
      "    ess            : 1.96720082915608\n",
      "    log_marginal   : 1160.7154774114715\n",
      "    log_joint      : 1369.0898161461605\n",
      "    val_loss       : -1159.4586765455163\n",
      "    val_ess        : 1.9680654795273491\n",
      "    val_log_marginal: 1159.4883130944293\n",
      "    val_log_joint  : 1367.5387122112772\n",
      "Train Epoch: 1184 [0/101520 (0%)] Loss: -1162.583008\n",
      "Train Epoch: 1184 [11264/101520 (11%)] Loss: -1163.117432\n",
      "Train Epoch: 1184 [22528/101520 (22%)] Loss: -1155.318481\n",
      "Train Epoch: 1184 [33792/101520 (33%)] Loss: -1158.252197\n",
      "Train Epoch: 1184 [45056/101520 (44%)] Loss: -1159.827881\n",
      "Train Epoch: 1184 [56320/101520 (55%)] Loss: -1162.147339\n",
      "Train Epoch: 1184 [67584/101520 (67%)] Loss: -1157.578125\n",
      "Train Epoch: 1184 [78848/101520 (78%)] Loss: -1161.157715\n",
      "Train Epoch: 1184 [90112/101520 (89%)] Loss: -1160.390137\n",
      "Train Epoch: 1184 [101376/101520 (100%)] Loss: -1158.607422\n",
      "    epoch          : 1184\n",
      "    loss           : -1160.7401926625314\n",
      "    ess            : 1.9666010380989343\n",
      "    log_marginal   : 1160.771076451594\n",
      "    log_joint      : 1369.1489392764604\n",
      "    val_loss       : -1160.5760073454483\n",
      "    val_ess        : 1.9696415351784748\n",
      "    val_log_marginal: 1160.6036005434783\n",
      "    val_log_joint  : 1368.8250201681385\n",
      "Train Epoch: 1185 [0/101520 (0%)] Loss: -1163.346680\n",
      "Train Epoch: 1185 [11264/101520 (11%)] Loss: -1163.606689\n",
      "Train Epoch: 1185 [22528/101520 (22%)] Loss: -1154.867676\n",
      "Train Epoch: 1185 [33792/101520 (33%)] Loss: -1159.378174\n",
      "Train Epoch: 1185 [45056/101520 (44%)] Loss: -1157.240967\n",
      "Train Epoch: 1185 [56320/101520 (55%)] Loss: -1158.709229\n",
      "Train Epoch: 1185 [67584/101520 (67%)] Loss: -1158.312256\n",
      "Train Epoch: 1185 [78848/101520 (78%)] Loss: -1162.822021\n",
      "Train Epoch: 1185 [90112/101520 (89%)] Loss: -1160.997559\n",
      "Train Epoch: 1185 [101376/101520 (100%)] Loss: -1158.901978\n",
      "    epoch          : 1185\n",
      "    loss           : -1160.704078252591\n",
      "    ess            : 1.9676022793180379\n",
      "    log_marginal   : 1160.7340002012013\n",
      "    log_joint      : 1369.263035759854\n",
      "    val_loss       : -1160.4390497622283\n",
      "    val_ess        : 1.9662910751674487\n",
      "    val_log_marginal: 1160.470405910326\n",
      "    val_log_joint  : 1368.6786313264267\n",
      "Train Epoch: 1186 [0/101520 (0%)] Loss: -1163.618896\n",
      "Train Epoch: 1186 [11264/101520 (11%)] Loss: -1153.547607\n",
      "Train Epoch: 1186 [22528/101520 (22%)] Loss: -1164.488892\n",
      "Train Epoch: 1186 [33792/101520 (33%)] Loss: -1164.857300\n",
      "Train Epoch: 1186 [45056/101520 (44%)] Loss: -1152.973633\n",
      "Train Epoch: 1186 [56320/101520 (55%)] Loss: -1163.613647\n",
      "Train Epoch: 1186 [67584/101520 (67%)] Loss: -1160.899902\n",
      "Train Epoch: 1186 [78848/101520 (78%)] Loss: -1157.398315\n",
      "Train Epoch: 1186 [90112/101520 (89%)] Loss: -1156.439209\n",
      "Train Epoch: 1186 [101376/101520 (100%)] Loss: -1167.805786\n",
      "    epoch          : 1186\n",
      "    loss           : -1160.8993281838882\n",
      "    ess            : 1.9666775848398257\n",
      "    log_marginal   : 1160.9307241775282\n",
      "    log_joint      : 1369.323263657153\n",
      "    val_loss       : -1159.8177065641983\n",
      "    val_ess        : 1.9674420564070991\n",
      "    val_log_marginal: 1159.8498322860055\n",
      "    val_log_joint  : 1368.3356774371603\n",
      "Train Epoch: 1187 [0/101520 (0%)] Loss: -1169.196289\n",
      "Train Epoch: 1187 [11264/101520 (11%)] Loss: -1158.048584\n",
      "Train Epoch: 1187 [22528/101520 (22%)] Loss: -1166.701904\n",
      "Train Epoch: 1187 [33792/101520 (33%)] Loss: -1161.688232\n",
      "Train Epoch: 1187 [45056/101520 (44%)] Loss: -1166.478638\n",
      "Train Epoch: 1187 [56320/101520 (55%)] Loss: -1160.455811\n",
      "Train Epoch: 1187 [67584/101520 (67%)] Loss: -1152.530396\n",
      "Train Epoch: 1187 [78848/101520 (78%)] Loss: -1165.151367\n",
      "Train Epoch: 1187 [90112/101520 (89%)] Loss: -1157.328613\n",
      "Train Epoch: 1187 [101376/101520 (100%)] Loss: -1170.540649\n",
      "    epoch          : 1187\n",
      "    loss           : -1160.990947167478\n",
      "    ess            : 1.9667444474733056\n",
      "    log_marginal   : 1161.0224364007538\n",
      "    log_joint      : 1369.40575251747\n",
      "    val_loss       : -1160.891797936481\n",
      "    val_ess        : 1.968100905418396\n",
      "    val_log_marginal: 1160.9212964928668\n",
      "    val_log_joint  : 1368.9895444123642\n",
      "Train Epoch: 1188 [0/101520 (0%)] Loss: -1157.349121\n",
      "Train Epoch: 1188 [11264/101520 (11%)] Loss: -1156.373413\n",
      "Train Epoch: 1188 [22528/101520 (22%)] Loss: -1160.762573\n",
      "Train Epoch: 1188 [33792/101520 (33%)] Loss: -1159.178955\n",
      "Train Epoch: 1188 [45056/101520 (44%)] Loss: -1159.159912\n",
      "Train Epoch: 1188 [56320/101520 (55%)] Loss: -1157.855469\n",
      "Train Epoch: 1188 [67584/101520 (67%)] Loss: -1153.854736\n",
      "Train Epoch: 1188 [78848/101520 (78%)] Loss: -1161.267578\n",
      "Train Epoch: 1188 [90112/101520 (89%)] Loss: -1159.003052\n",
      "Train Epoch: 1188 [101376/101520 (100%)] Loss: -1147.259521\n",
      "    epoch          : 1188\n",
      "    loss           : -1160.9825273830088\n",
      "    ess            : 1.9666568597956517\n",
      "    log_marginal   : 1161.0137786098462\n",
      "    log_joint      : 1369.388436935655\n",
      "    val_loss       : -1157.963575280231\n",
      "    val_ess        : 1.9659684948299243\n",
      "    val_log_marginal: 1157.9949845023777\n",
      "    val_log_joint  : 1366.3657969599185\n",
      "Train Epoch: 1189 [0/101520 (0%)] Loss: -1160.051270\n",
      "Train Epoch: 1189 [11264/101520 (11%)] Loss: -1162.727783\n",
      "Train Epoch: 1189 [22528/101520 (22%)] Loss: -1171.314941\n",
      "Train Epoch: 1189 [33792/101520 (33%)] Loss: -1158.771729\n",
      "Train Epoch: 1189 [45056/101520 (44%)] Loss: -1159.617432\n",
      "Train Epoch: 1189 [56320/101520 (55%)] Loss: -1157.258789\n",
      "Train Epoch: 1189 [67584/101520 (67%)] Loss: -1168.187012\n",
      "Train Epoch: 1189 [78848/101520 (78%)] Loss: -1164.365723\n",
      "Train Epoch: 1189 [90112/101520 (89%)] Loss: -1166.459717\n",
      "Train Epoch: 1189 [101376/101520 (100%)] Loss: -1170.291260\n",
      "    epoch          : 1189\n",
      "    loss           : -1161.092885693114\n",
      "    ess            : 1.96660305866644\n",
      "    log_marginal   : 1161.124435041418\n",
      "    log_joint      : 1369.408240543538\n",
      "    val_loss       : -1158.3525602921195\n",
      "    val_ess        : 1.9695351538450823\n",
      "    val_log_marginal: 1158.379346764606\n",
      "    val_log_joint  : 1366.5680250084918\n",
      "Train Epoch: 1190 [0/101520 (0%)] Loss: -1158.823242\n",
      "Train Epoch: 1190 [11264/101520 (11%)] Loss: -1161.909546\n",
      "Train Epoch: 1190 [22528/101520 (22%)] Loss: -1159.606201\n",
      "Train Epoch: 1190 [33792/101520 (33%)] Loss: -1164.080078\n",
      "Train Epoch: 1190 [45056/101520 (44%)] Loss: -1164.035400\n",
      "Train Epoch: 1190 [56320/101520 (55%)] Loss: -1160.379517\n",
      "Train Epoch: 1190 [67584/101520 (67%)] Loss: -1163.842041\n",
      "Train Epoch: 1190 [78848/101520 (78%)] Loss: -1161.265747\n",
      "Train Epoch: 1190 [90112/101520 (89%)] Loss: -1165.548096\n",
      "Train Epoch: 1190 [101376/101520 (100%)] Loss: -1171.318970\n",
      "    epoch          : 1190\n",
      "    loss           : -1161.090941155975\n",
      "    ess            : 1.9671778067871555\n",
      "    log_marginal   : 1161.1211802420305\n",
      "    log_joint      : 1369.486664891842\n",
      "    val_loss       : -1159.898878014606\n",
      "    val_ess        : 1.9650706519251284\n",
      "    val_log_marginal: 1159.9406632133152\n",
      "    val_log_joint  : 1368.056200110394\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1190.pth ...\n",
      "Train Epoch: 1191 [0/101520 (0%)] Loss: -1164.435059\n",
      "Train Epoch: 1191 [11264/101520 (11%)] Loss: -1157.060791\n",
      "Train Epoch: 1191 [22528/101520 (22%)] Loss: -1162.674072\n",
      "Train Epoch: 1191 [33792/101520 (33%)] Loss: -1164.325928\n",
      "Train Epoch: 1191 [45056/101520 (44%)] Loss: -1158.350098\n",
      "Train Epoch: 1191 [56320/101520 (55%)] Loss: -1153.572754\n",
      "Train Epoch: 1191 [67584/101520 (67%)] Loss: -1171.548584\n",
      "Train Epoch: 1191 [78848/101520 (78%)] Loss: -1161.570312\n",
      "Train Epoch: 1191 [90112/101520 (89%)] Loss: -1162.786133\n",
      "Train Epoch: 1191 [101376/101520 (100%)] Loss: -1173.284546\n",
      "    epoch          : 1191\n",
      "    loss           : -1161.091670510757\n",
      "    ess            : 1.9675529308654556\n",
      "    log_marginal   : 1161.1213900312107\n",
      "    log_joint      : 1369.5290772711212\n",
      "    val_loss       : -1159.6933912194293\n",
      "    val_ess        : 1.9677241781483525\n",
      "    val_log_marginal: 1159.7256973930027\n",
      "    val_log_joint  : 1368.2114045516305\n",
      "Train Epoch: 1192 [0/101520 (0%)] Loss: -1159.785645\n",
      "Train Epoch: 1192 [11264/101520 (11%)] Loss: -1160.921631\n",
      "Train Epoch: 1192 [22528/101520 (22%)] Loss: -1162.559937\n",
      "Train Epoch: 1192 [33792/101520 (33%)] Loss: -1162.238525\n",
      "Train Epoch: 1192 [45056/101520 (44%)] Loss: -1159.770508\n",
      "Train Epoch: 1192 [56320/101520 (55%)] Loss: -1162.033813\n",
      "Train Epoch: 1192 [67584/101520 (67%)] Loss: -1153.656250\n",
      "Train Epoch: 1192 [78848/101520 (78%)] Loss: -1155.417114\n",
      "Train Epoch: 1192 [90112/101520 (89%)] Loss: -1152.260010\n",
      "Train Epoch: 1192 [101376/101520 (100%)] Loss: -1162.256470\n",
      "    epoch          : 1192\n",
      "    loss           : -1161.139334980567\n",
      "    ess            : 1.9677360399284554\n",
      "    log_marginal   : 1161.1697801752905\n",
      "    log_joint      : 1369.5468069105293\n",
      "    val_loss       : -1160.0340523097825\n",
      "    val_ess        : 1.9646416384240855\n",
      "    val_log_marginal: 1160.0678445567255\n",
      "    val_log_joint  : 1368.4649180536685\n",
      "Train Epoch: 1193 [0/101520 (0%)] Loss: -1159.911133\n",
      "Train Epoch: 1193 [11264/101520 (11%)] Loss: -1164.871582\n",
      "Train Epoch: 1193 [22528/101520 (22%)] Loss: -1160.184814\n",
      "Train Epoch: 1193 [33792/101520 (33%)] Loss: -1165.125244\n",
      "Train Epoch: 1193 [45056/101520 (44%)] Loss: -1157.214600\n",
      "Train Epoch: 1193 [56320/101520 (55%)] Loss: -1164.168213\n",
      "Train Epoch: 1193 [67584/101520 (67%)] Loss: -1162.501831\n",
      "Train Epoch: 1193 [78848/101520 (78%)] Loss: -1157.145020\n",
      "Train Epoch: 1193 [90112/101520 (89%)] Loss: -1155.666870\n",
      "Train Epoch: 1193 [101376/101520 (100%)] Loss: -1155.682251\n",
      "    epoch          : 1193\n",
      "    loss           : -1161.1813547719064\n",
      "    ess            : 1.9670642416680877\n",
      "    log_marginal   : 1161.2118110081658\n",
      "    log_joint      : 1369.6186228996546\n",
      "    val_loss       : -1161.7212179432745\n",
      "    val_ess        : 1.967909854391347\n",
      "    val_log_marginal: 1161.7496019446332\n",
      "    val_log_joint  : 1369.8572891898777\n",
      "Train Epoch: 1194 [0/101520 (0%)] Loss: -1164.156006\n",
      "Train Epoch: 1194 [11264/101520 (11%)] Loss: -1162.153320\n",
      "Train Epoch: 1194 [22528/101520 (22%)] Loss: -1162.560547\n",
      "Train Epoch: 1194 [33792/101520 (33%)] Loss: -1154.618164\n",
      "Train Epoch: 1194 [45056/101520 (44%)] Loss: -1161.734619\n",
      "Train Epoch: 1194 [56320/101520 (55%)] Loss: -1163.114014\n",
      "Train Epoch: 1194 [67584/101520 (67%)] Loss: -1160.411865\n",
      "Train Epoch: 1194 [78848/101520 (78%)] Loss: -1162.488281\n",
      "Train Epoch: 1194 [90112/101520 (89%)] Loss: -1161.004028\n",
      "Train Epoch: 1194 [101376/101520 (100%)] Loss: -1156.575928\n",
      "    epoch          : 1194\n",
      "    loss           : -1161.3003328409627\n",
      "    ess            : 1.967623158914959\n",
      "    log_marginal   : 1161.3304553774733\n",
      "    log_joint      : 1369.7182427027717\n",
      "    val_loss       : -1160.3480596127717\n",
      "    val_ess        : 1.9679817583249963\n",
      "    val_log_marginal: 1160.3761145550272\n",
      "    val_log_joint  : 1369.0067085597825\n",
      "Train Epoch: 1195 [0/101520 (0%)] Loss: -1163.548706\n",
      "Train Epoch: 1195 [11264/101520 (11%)] Loss: -1158.757568\n",
      "Train Epoch: 1195 [22528/101520 (22%)] Loss: -1165.344482\n",
      "Train Epoch: 1195 [33792/101520 (33%)] Loss: -1167.076050\n",
      "Train Epoch: 1195 [45056/101520 (44%)] Loss: -1160.939941\n",
      "Train Epoch: 1195 [56320/101520 (55%)] Loss: -1160.980713\n",
      "Train Epoch: 1195 [67584/101520 (67%)] Loss: -1159.402344\n",
      "Train Epoch: 1195 [78848/101520 (78%)] Loss: -1166.438965\n",
      "Train Epoch: 1195 [90112/101520 (89%)] Loss: -1166.586060\n",
      "Train Epoch: 1195 [101376/101520 (100%)] Loss: -1169.150635\n",
      "    epoch          : 1195\n",
      "    loss           : -1161.387378175055\n",
      "    ess            : 1.9661995495983104\n",
      "    log_marginal   : 1161.4194808269865\n",
      "    log_joint      : 1369.7928141684988\n",
      "    val_loss       : -1158.635450280231\n",
      "    val_ess        : 1.9643909257391226\n",
      "    val_log_marginal: 1158.6730797809103\n",
      "    val_log_joint  : 1367.3270794412365\n",
      "Train Epoch: 1196 [0/101520 (0%)] Loss: -1162.670410\n",
      "Train Epoch: 1196 [11264/101520 (11%)] Loss: -1156.613403\n",
      "Train Epoch: 1196 [22528/101520 (22%)] Loss: -1174.535400\n",
      "Train Epoch: 1196 [33792/101520 (33%)] Loss: -1163.750488\n",
      "Train Epoch: 1196 [45056/101520 (44%)] Loss: -1155.200317\n",
      "Train Epoch: 1196 [56320/101520 (55%)] Loss: -1158.283936\n",
      "Train Epoch: 1196 [67584/101520 (67%)] Loss: -1168.035400\n",
      "Train Epoch: 1196 [78848/101520 (78%)] Loss: -1162.045044\n",
      "Train Epoch: 1196 [90112/101520 (89%)] Loss: -1162.493408\n",
      "Train Epoch: 1196 [101376/101520 (100%)] Loss: -1153.249634\n",
      "    epoch          : 1196\n",
      "    loss           : -1161.409992467219\n",
      "    ess            : 1.9674768867205137\n",
      "    log_marginal   : 1161.4406032849795\n",
      "    log_joint      : 1369.8489935026696\n",
      "    val_loss       : -1160.0026271654212\n",
      "    val_ess        : 1.9711718144624129\n",
      "    val_log_marginal: 1160.029296875\n",
      "    val_log_joint  : 1368.0582806131115\n",
      "Train Epoch: 1197 [0/101520 (0%)] Loss: -1170.145996\n",
      "Train Epoch: 1197 [11264/101520 (11%)] Loss: -1165.151367\n",
      "Train Epoch: 1197 [22528/101520 (22%)] Loss: -1162.737549\n",
      "Train Epoch: 1197 [33792/101520 (33%)] Loss: -1162.524048\n",
      "Train Epoch: 1197 [45056/101520 (44%)] Loss: -1169.905029\n",
      "Train Epoch: 1197 [56320/101520 (55%)] Loss: -1162.977783\n",
      "Train Epoch: 1197 [67584/101520 (67%)] Loss: -1152.203857\n",
      "Train Epoch: 1197 [78848/101520 (78%)] Loss: -1158.682617\n",
      "Train Epoch: 1197 [90112/101520 (89%)] Loss: -1167.822632\n",
      "Train Epoch: 1197 [101376/101520 (100%)] Loss: -1172.013184\n",
      "    epoch          : 1197\n",
      "    loss           : -1161.5709210113066\n",
      "    ess            : 1.9664847778914563\n",
      "    log_marginal   : 1161.6024918292635\n",
      "    log_joint      : 1369.955117383794\n",
      "    val_loss       : -1160.2452923318615\n",
      "    val_ess        : 1.9713698832885078\n",
      "    val_log_marginal: 1160.2697170091712\n",
      "    val_log_joint  : 1368.7806502632473\n",
      "Train Epoch: 1198 [0/101520 (0%)] Loss: -1161.396851\n",
      "Train Epoch: 1198 [11264/101520 (11%)] Loss: -1161.612305\n",
      "Train Epoch: 1198 [22528/101520 (22%)] Loss: -1157.992554\n",
      "Train Epoch: 1198 [33792/101520 (33%)] Loss: -1159.971924\n",
      "Train Epoch: 1198 [45056/101520 (44%)] Loss: -1160.608276\n",
      "Train Epoch: 1198 [56320/101520 (55%)] Loss: -1171.359131\n",
      "Train Epoch: 1198 [67584/101520 (67%)] Loss: -1156.334473\n",
      "Train Epoch: 1198 [78848/101520 (78%)] Loss: -1166.395020\n",
      "Train Epoch: 1198 [90112/101520 (89%)] Loss: -1159.343140\n",
      "Train Epoch: 1198 [101376/101520 (100%)] Loss: -1160.541382\n",
      "    epoch          : 1198\n",
      "    loss           : -1161.4863759716552\n",
      "    ess            : 1.9677890587092643\n",
      "    log_marginal   : 1161.5167782270728\n",
      "    log_joint      : 1369.942901151264\n",
      "    val_loss       : -1160.4062075407608\n",
      "    val_ess        : 1.968633998995242\n",
      "    val_log_marginal: 1160.4336309018342\n",
      "    val_log_joint  : 1368.8887143342392\n",
      "Train Epoch: 1199 [0/101520 (0%)] Loss: -1166.514648\n",
      "Train Epoch: 1199 [11264/101520 (11%)] Loss: -1158.384033\n",
      "Train Epoch: 1199 [22528/101520 (22%)] Loss: -1162.882812\n",
      "Train Epoch: 1199 [33792/101520 (33%)] Loss: -1155.764282\n",
      "Train Epoch: 1199 [45056/101520 (44%)] Loss: -1162.244629\n",
      "Train Epoch: 1199 [56320/101520 (55%)] Loss: -1163.121582\n",
      "Train Epoch: 1199 [67584/101520 (67%)] Loss: -1169.432983\n",
      "Train Epoch: 1199 [78848/101520 (78%)] Loss: -1157.518799\n",
      "Train Epoch: 1199 [90112/101520 (89%)] Loss: -1173.564575\n",
      "Train Epoch: 1199 [101376/101520 (100%)] Loss: -1169.753418\n",
      "    epoch          : 1199\n",
      "    loss           : -1161.5690292281722\n",
      "    ess            : 1.9667248564149866\n",
      "    log_marginal   : 1161.600368173877\n",
      "    log_joint      : 1370.042658973579\n",
      "    val_loss       : -1160.9890826681385\n",
      "    val_ess        : 1.9686307751614114\n",
      "    val_log_marginal: 1161.0180398692255\n",
      "    val_log_joint  : 1369.3237729279892\n",
      "Train Epoch: 1200 [0/101520 (0%)] Loss: -1161.135132\n",
      "Train Epoch: 1200 [11264/101520 (11%)] Loss: -1158.194580\n",
      "Train Epoch: 1200 [22528/101520 (22%)] Loss: -1160.726807\n",
      "Train Epoch: 1200 [33792/101520 (33%)] Loss: -1166.666504\n",
      "Train Epoch: 1200 [45056/101520 (44%)] Loss: -1161.529053\n",
      "Train Epoch: 1200 [56320/101520 (55%)] Loss: -1161.994019\n",
      "Train Epoch: 1200 [67584/101520 (67%)] Loss: -1167.490601\n",
      "Train Epoch: 1200 [78848/101520 (78%)] Loss: -1167.358398\n",
      "Train Epoch: 1200 [90112/101520 (89%)] Loss: -1166.288818\n",
      "Train Epoch: 1200 [101376/101520 (100%)] Loss: -1169.694214\n",
      "    epoch          : 1200\n",
      "    loss           : -1161.7112558642823\n",
      "    ess            : 1.9677455365358285\n",
      "    log_marginal   : 1161.7407852249528\n",
      "    log_joint      : 1370.128860243601\n",
      "    val_loss       : -1161.2081935716712\n",
      "    val_ess        : 1.964636180711829\n",
      "    val_log_marginal: 1161.2388279127038\n",
      "    val_log_joint  : 1369.74146038553\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1200.pth ...\n",
      "Train Epoch: 1201 [0/101520 (0%)] Loss: -1164.296265\n",
      "Train Epoch: 1201 [11264/101520 (11%)] Loss: -1164.005615\n",
      "Train Epoch: 1201 [22528/101520 (22%)] Loss: -1170.020874\n",
      "Train Epoch: 1201 [33792/101520 (33%)] Loss: -1158.847534\n",
      "Train Epoch: 1201 [45056/101520 (44%)] Loss: -1163.543701\n",
      "Train Epoch: 1201 [56320/101520 (55%)] Loss: -1169.137939\n",
      "Train Epoch: 1201 [67584/101520 (67%)] Loss: -1163.084351\n",
      "Train Epoch: 1201 [78848/101520 (78%)] Loss: -1163.038818\n",
      "Train Epoch: 1201 [90112/101520 (89%)] Loss: -1167.434082\n",
      "Train Epoch: 1201 [101376/101520 (100%)] Loss: -1163.295288\n",
      "    epoch          : 1201\n",
      "    loss           : -1161.6193019541065\n",
      "    ess            : 1.967368927433263\n",
      "    log_marginal   : 1161.6498048101837\n",
      "    log_joint      : 1370.049989939934\n",
      "    val_loss       : -1161.1208071501358\n",
      "    val_ess        : 1.9655630329380864\n",
      "    val_log_marginal: 1161.1508046025815\n",
      "    val_log_joint  : 1369.6491858440897\n",
      "Train Epoch: 1202 [0/101520 (0%)] Loss: -1164.919678\n",
      "Train Epoch: 1202 [11264/101520 (11%)] Loss: -1158.890747\n",
      "Train Epoch: 1202 [22528/101520 (22%)] Loss: -1166.265381\n",
      "Train Epoch: 1202 [33792/101520 (33%)] Loss: -1161.586670\n",
      "Train Epoch: 1202 [45056/101520 (44%)] Loss: -1149.622070\n",
      "Train Epoch: 1202 [56320/101520 (55%)] Loss: -1163.112427\n",
      "Train Epoch: 1202 [67584/101520 (67%)] Loss: -1164.418945\n",
      "Train Epoch: 1202 [78848/101520 (78%)] Loss: -1157.172119\n",
      "Train Epoch: 1202 [90112/101520 (89%)] Loss: -1166.855957\n",
      "Train Epoch: 1202 [101376/101520 (100%)] Loss: -1151.979004\n",
      "    epoch          : 1202\n",
      "    loss           : -1161.6430547512955\n",
      "    ess            : 1.96722018539007\n",
      "    log_marginal   : 1161.6733484316112\n",
      "    log_joint      : 1370.1535742678234\n",
      "    val_loss       : -1160.5060345193615\n",
      "    val_ess        : 1.9679779177126677\n",
      "    val_log_marginal: 1160.534715735394\n",
      "    val_log_joint  : 1369.2775454313858\n",
      "Train Epoch: 1203 [0/101520 (0%)] Loss: -1167.532349\n",
      "Train Epoch: 1203 [11264/101520 (11%)] Loss: -1164.879395\n",
      "Train Epoch: 1203 [22528/101520 (22%)] Loss: -1162.655762\n",
      "Train Epoch: 1203 [33792/101520 (33%)] Loss: -1154.147339\n",
      "Train Epoch: 1203 [45056/101520 (44%)] Loss: -1161.348389\n",
      "Train Epoch: 1203 [56320/101520 (55%)] Loss: -1166.085693\n",
      "Train Epoch: 1203 [67584/101520 (67%)] Loss: -1162.343994\n",
      "Train Epoch: 1203 [78848/101520 (78%)] Loss: -1155.112061\n",
      "Train Epoch: 1203 [90112/101520 (89%)] Loss: -1158.565674\n",
      "Train Epoch: 1203 [101376/101520 (100%)] Loss: -1164.522095\n",
      "    epoch          : 1203\n",
      "    loss           : -1161.7726793145414\n",
      "    ess            : 1.9674533419872648\n",
      "    log_marginal   : 1161.8024969819803\n",
      "    log_joint      : 1370.239323448296\n",
      "    val_loss       : -1160.5936916185462\n",
      "    val_ess        : 1.9674959079079006\n",
      "    val_log_marginal: 1160.6232326341712\n",
      "    val_log_joint  : 1369.179103685462\n",
      "Train Epoch: 1204 [0/101520 (0%)] Loss: -1161.686646\n",
      "Train Epoch: 1204 [11264/101520 (11%)] Loss: -1164.038574\n",
      "Train Epoch: 1204 [22528/101520 (22%)] Loss: -1165.183838\n",
      "Train Epoch: 1204 [33792/101520 (33%)] Loss: -1159.504395\n",
      "Train Epoch: 1204 [45056/101520 (44%)] Loss: -1163.958252\n",
      "Train Epoch: 1204 [56320/101520 (55%)] Loss: -1169.390015\n",
      "Train Epoch: 1204 [67584/101520 (67%)] Loss: -1161.788208\n",
      "Train Epoch: 1204 [78848/101520 (78%)] Loss: -1161.647827\n",
      "Train Epoch: 1204 [90112/101520 (89%)] Loss: -1158.740234\n",
      "Train Epoch: 1204 [101376/101520 (100%)] Loss: -1148.571899\n",
      "    epoch          : 1204\n",
      "    loss           : -1161.814545751217\n",
      "    ess            : 1.967791151161769\n",
      "    log_marginal   : 1161.844460952222\n",
      "    log_joint      : 1370.2686368855998\n",
      "    val_loss       : -1159.8764383067255\n",
      "    val_ess        : 1.9637035390605098\n",
      "    val_log_marginal: 1159.9137759001358\n",
      "    val_log_joint  : 1368.297702955163\n",
      "Train Epoch: 1205 [0/101520 (0%)] Loss: -1160.455200\n",
      "Train Epoch: 1205 [11264/101520 (11%)] Loss: -1160.990234\n",
      "Train Epoch: 1205 [22528/101520 (22%)] Loss: -1158.737305\n",
      "Train Epoch: 1205 [33792/101520 (33%)] Loss: -1161.150024\n",
      "Train Epoch: 1205 [45056/101520 (44%)] Loss: -1160.971313\n",
      "Train Epoch: 1205 [56320/101520 (55%)] Loss: -1163.754883\n",
      "Train Epoch: 1205 [67584/101520 (67%)] Loss: -1150.237305\n",
      "Train Epoch: 1205 [78848/101520 (78%)] Loss: -1168.409546\n",
      "Train Epoch: 1205 [90112/101520 (89%)] Loss: -1168.512207\n",
      "Train Epoch: 1205 [101376/101520 (100%)] Loss: -1165.047119\n",
      "    epoch          : 1205\n",
      "    loss           : -1161.9536157349246\n",
      "    ess            : 1.9668684784491457\n",
      "    log_marginal   : 1161.9846430639525\n",
      "    log_joint      : 1370.3661422346106\n",
      "    val_loss       : -1159.6079844599185\n",
      "    val_ess        : 1.9690580834513125\n",
      "    val_log_marginal: 1159.6372654127038\n",
      "    val_log_joint  : 1367.8209706182065\n",
      "Train Epoch: 1206 [0/101520 (0%)] Loss: -1160.375366\n",
      "Train Epoch: 1206 [11264/101520 (11%)] Loss: -1158.830811\n",
      "Train Epoch: 1206 [22528/101520 (22%)] Loss: -1161.652832\n",
      "Train Epoch: 1206 [33792/101520 (33%)] Loss: -1158.605469\n",
      "Train Epoch: 1206 [45056/101520 (44%)] Loss: -1163.545654\n",
      "Train Epoch: 1206 [56320/101520 (55%)] Loss: -1163.175903\n",
      "Train Epoch: 1206 [67584/101520 (67%)] Loss: -1158.191528\n",
      "Train Epoch: 1206 [78848/101520 (78%)] Loss: -1163.830688\n",
      "Train Epoch: 1206 [90112/101520 (89%)] Loss: -1161.813721\n",
      "Train Epoch: 1206 [101376/101520 (100%)] Loss: -1169.843384\n",
      "    epoch          : 1206\n",
      "    loss           : -1162.0445575043184\n",
      "    ess            : 1.9669933666535957\n",
      "    log_marginal   : 1162.0755216512248\n",
      "    log_joint      : 1370.4288415956737\n",
      "    val_loss       : -1162.2175558338995\n",
      "    val_ess        : 1.9695030347160671\n",
      "    val_log_marginal: 1162.2455895465353\n",
      "    val_log_joint  : 1370.6270433508832\n",
      "Train Epoch: 1207 [0/101520 (0%)] Loss: -1165.418701\n",
      "Train Epoch: 1207 [11264/101520 (11%)] Loss: -1164.155640\n",
      "Train Epoch: 1207 [22528/101520 (22%)] Loss: -1165.775391\n",
      "Train Epoch: 1207 [33792/101520 (33%)] Loss: -1161.614868\n",
      "Train Epoch: 1207 [45056/101520 (44%)] Loss: -1164.938721\n",
      "Train Epoch: 1207 [56320/101520 (55%)] Loss: -1155.410156\n",
      "Train Epoch: 1207 [67584/101520 (67%)] Loss: -1156.026245\n",
      "Train Epoch: 1207 [78848/101520 (78%)] Loss: -1162.630493\n",
      "Train Epoch: 1207 [90112/101520 (89%)] Loss: -1162.630859\n",
      "Train Epoch: 1207 [101376/101520 (100%)] Loss: -1162.899048\n",
      "    epoch          : 1207\n",
      "    loss           : -1162.0279277245604\n",
      "    ess            : 1.9673184563766173\n",
      "    log_marginal   : 1162.0581471812186\n",
      "    log_joint      : 1370.4584255506045\n",
      "    val_loss       : -1160.6829940132473\n",
      "    val_ess        : 1.9660898913507876\n",
      "    val_log_marginal: 1160.7140529466712\n",
      "    val_log_joint  : 1369.2973048997962\n",
      "Train Epoch: 1208 [0/101520 (0%)] Loss: -1152.743896\n",
      "Train Epoch: 1208 [11264/101520 (11%)] Loss: -1158.660034\n",
      "Train Epoch: 1208 [22528/101520 (22%)] Loss: -1158.697021\n",
      "Train Epoch: 1208 [33792/101520 (33%)] Loss: -1154.759521\n",
      "Train Epoch: 1208 [45056/101520 (44%)] Loss: -1164.569092\n",
      "Train Epoch: 1208 [56320/101520 (55%)] Loss: -1163.857666\n",
      "Train Epoch: 1208 [67584/101520 (67%)] Loss: -1163.970215\n",
      "Train Epoch: 1208 [78848/101520 (78%)] Loss: -1158.879883\n",
      "Train Epoch: 1208 [90112/101520 (89%)] Loss: -1159.284058\n",
      "Train Epoch: 1208 [101376/101520 (100%)] Loss: -1161.223145\n",
      "    epoch          : 1208\n",
      "    loss           : -1162.0693641547582\n",
      "    ess            : 1.9675070023416874\n",
      "    log_marginal   : 1162.0991290681925\n",
      "    log_joint      : 1370.4849976199357\n",
      "    val_loss       : -1161.5848707116168\n",
      "    val_ess        : 1.9692608791848887\n",
      "    val_log_marginal: 1161.610494862432\n",
      "    val_log_joint  : 1370.1007239300272\n",
      "Train Epoch: 1209 [0/101520 (0%)] Loss: -1158.954102\n",
      "Train Epoch: 1209 [11264/101520 (11%)] Loss: -1154.627686\n",
      "Train Epoch: 1209 [22528/101520 (22%)] Loss: -1167.056152\n",
      "Train Epoch: 1209 [33792/101520 (33%)] Loss: -1157.790649\n",
      "Train Epoch: 1209 [45056/101520 (44%)] Loss: -1157.996582\n",
      "Train Epoch: 1209 [56320/101520 (55%)] Loss: -1159.171631\n",
      "Train Epoch: 1209 [67584/101520 (67%)] Loss: -1161.880371\n",
      "Train Epoch: 1209 [78848/101520 (78%)] Loss: -1162.362793\n",
      "Train Epoch: 1209 [90112/101520 (89%)] Loss: -1162.721558\n",
      "Train Epoch: 1209 [101376/101520 (100%)] Loss: -1172.244751\n",
      "    epoch          : 1209\n",
      "    loss           : -1162.1002718671482\n",
      "    ess            : 1.9670379233719715\n",
      "    log_marginal   : 1162.130337355724\n",
      "    log_joint      : 1370.5300569007145\n",
      "    val_loss       : -1161.6897238026495\n",
      "    val_ess        : 1.9676231301349143\n",
      "    val_log_marginal: 1161.7194134256115\n",
      "    val_log_joint  : 1369.9615903108017\n",
      "Train Epoch: 1210 [0/101520 (0%)] Loss: -1159.736816\n",
      "Train Epoch: 1210 [11264/101520 (11%)] Loss: -1166.409668\n",
      "Train Epoch: 1210 [22528/101520 (22%)] Loss: -1164.576294\n",
      "Train Epoch: 1210 [33792/101520 (33%)] Loss: -1161.402588\n",
      "Train Epoch: 1210 [45056/101520 (44%)] Loss: -1162.345703\n",
      "Train Epoch: 1210 [56320/101520 (55%)] Loss: -1157.125244\n",
      "Train Epoch: 1210 [67584/101520 (67%)] Loss: -1159.078857\n",
      "Train Epoch: 1210 [78848/101520 (78%)] Loss: -1163.791016\n",
      "Train Epoch: 1210 [90112/101520 (89%)] Loss: -1154.088501\n",
      "Train Epoch: 1210 [101376/101520 (100%)] Loss: -1165.794067\n",
      "    epoch          : 1210\n",
      "    loss           : -1162.1965466983354\n",
      "    ess            : 1.9672775975423842\n",
      "    log_marginal   : 1162.2259515350188\n",
      "    log_joint      : 1370.5752351847127\n",
      "    val_loss       : -1161.247367527174\n",
      "    val_ess        : 1.9643489692522131\n",
      "    val_log_marginal: 1161.282375169837\n",
      "    val_log_joint  : 1369.3564346976902\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1210.pth ...\n",
      "Train Epoch: 1211 [0/101520 (0%)] Loss: -1164.625122\n",
      "Train Epoch: 1211 [11264/101520 (11%)] Loss: -1163.108887\n",
      "Train Epoch: 1211 [22528/101520 (22%)] Loss: -1165.562744\n",
      "Train Epoch: 1211 [33792/101520 (33%)] Loss: -1166.892578\n",
      "Train Epoch: 1211 [45056/101520 (44%)] Loss: -1156.747803\n",
      "Train Epoch: 1211 [56320/101520 (55%)] Loss: -1161.009033\n",
      "Train Epoch: 1211 [67584/101520 (67%)] Loss: -1164.823975\n",
      "Train Epoch: 1211 [78848/101520 (78%)] Loss: -1161.737549\n",
      "Train Epoch: 1211 [90112/101520 (89%)] Loss: -1160.963623\n",
      "Train Epoch: 1211 [101376/101520 (100%)] Loss: -1170.865356\n",
      "    epoch          : 1211\n",
      "    loss           : -1162.2193247732805\n",
      "    ess            : 1.9680152268865\n",
      "    log_marginal   : 1162.2488983000942\n",
      "    log_joint      : 1370.664166167753\n",
      "    val_loss       : -1159.8498747452445\n",
      "    val_ess        : 1.9678762892018193\n",
      "    val_log_marginal: 1159.8780464504075\n",
      "    val_log_joint  : 1368.0943125849185\n",
      "Train Epoch: 1212 [0/101520 (0%)] Loss: -1162.952271\n",
      "Train Epoch: 1212 [11264/101520 (11%)] Loss: -1159.541016\n",
      "Train Epoch: 1212 [22528/101520 (22%)] Loss: -1167.095703\n",
      "Train Epoch: 1212 [33792/101520 (33%)] Loss: -1159.677612\n",
      "Train Epoch: 1212 [45056/101520 (44%)] Loss: -1154.056152\n",
      "Train Epoch: 1212 [56320/101520 (55%)] Loss: -1160.737061\n",
      "Train Epoch: 1212 [67584/101520 (67%)] Loss: -1154.583496\n",
      "Train Epoch: 1212 [78848/101520 (78%)] Loss: -1161.463013\n",
      "Train Epoch: 1212 [90112/101520 (89%)] Loss: -1154.584351\n",
      "Train Epoch: 1212 [101376/101520 (100%)] Loss: -1164.111084\n",
      "    epoch          : 1212\n",
      "    loss           : -1162.2548619562658\n",
      "    ess            : 1.966766635976245\n",
      "    log_marginal   : 1162.28614201378\n",
      "    log_joint      : 1370.7175814374607\n",
      "    val_loss       : -1159.9618503736413\n",
      "    val_ess        : 1.9663080339846404\n",
      "    val_log_marginal: 1159.994634213655\n",
      "    val_log_joint  : 1368.6337678328805\n",
      "Train Epoch: 1213 [0/101520 (0%)] Loss: -1159.535156\n",
      "Train Epoch: 1213 [11264/101520 (11%)] Loss: -1159.776733\n",
      "Train Epoch: 1213 [22528/101520 (22%)] Loss: -1163.716431\n",
      "Train Epoch: 1213 [33792/101520 (33%)] Loss: -1163.203491\n",
      "Train Epoch: 1213 [45056/101520 (44%)] Loss: -1150.895508\n",
      "Train Epoch: 1213 [56320/101520 (55%)] Loss: -1164.575684\n",
      "Train Epoch: 1213 [67584/101520 (67%)] Loss: -1161.224121\n",
      "Train Epoch: 1213 [78848/101520 (78%)] Loss: -1160.227417\n",
      "Train Epoch: 1213 [90112/101520 (89%)] Loss: -1161.584473\n",
      "Train Epoch: 1213 [101376/101520 (100%)] Loss: -1171.762085\n",
      "    epoch          : 1213\n",
      "    loss           : -1162.325460922778\n",
      "    ess            : 1.9672703048092637\n",
      "    log_marginal   : 1162.354502615617\n",
      "    log_joint      : 1370.7270587556925\n",
      "    val_loss       : -1161.1509479025135\n",
      "    val_ess        : 1.9695351642111074\n",
      "    val_log_marginal: 1161.1770231827445\n",
      "    val_log_joint  : 1369.3722454568615\n",
      "Train Epoch: 1214 [0/101520 (0%)] Loss: -1161.932861\n",
      "Train Epoch: 1214 [11264/101520 (11%)] Loss: -1166.838867\n",
      "Train Epoch: 1214 [22528/101520 (22%)] Loss: -1166.930908\n",
      "Train Epoch: 1214 [33792/101520 (33%)] Loss: -1158.137939\n",
      "Train Epoch: 1214 [45056/101520 (44%)] Loss: -1160.236084\n",
      "Train Epoch: 1214 [56320/101520 (55%)] Loss: -1160.640259\n",
      "Train Epoch: 1214 [67584/101520 (67%)] Loss: -1161.829346\n",
      "Train Epoch: 1214 [78848/101520 (78%)] Loss: -1160.985229\n",
      "Train Epoch: 1214 [90112/101520 (89%)] Loss: -1157.949463\n",
      "Train Epoch: 1214 [101376/101520 (100%)] Loss: -1165.679688\n",
      "    epoch          : 1214\n",
      "    loss           : -1162.272405729821\n",
      "    ess            : 1.9673015849674167\n",
      "    log_marginal   : 1162.3022847391253\n",
      "    log_joint      : 1370.728107701594\n",
      "    val_loss       : -1162.9589737601902\n",
      "    val_ess        : 1.9672429095143857\n",
      "    val_log_marginal: 1162.9897089419158\n",
      "    val_log_joint  : 1371.471876061481\n",
      "Train Epoch: 1215 [0/101520 (0%)] Loss: -1153.157471\n",
      "Train Epoch: 1215 [11264/101520 (11%)] Loss: -1163.782715\n",
      "Train Epoch: 1215 [22528/101520 (22%)] Loss: -1159.134155\n",
      "Train Epoch: 1215 [33792/101520 (33%)] Loss: -1167.296387\n",
      "Train Epoch: 1215 [45056/101520 (44%)] Loss: -1161.462280\n",
      "Train Epoch: 1215 [56320/101520 (55%)] Loss: -1161.212646\n",
      "Train Epoch: 1215 [67584/101520 (67%)] Loss: -1163.964111\n",
      "Train Epoch: 1215 [78848/101520 (78%)] Loss: -1159.355225\n",
      "Train Epoch: 1215 [90112/101520 (89%)] Loss: -1161.848877\n",
      "Train Epoch: 1215 [101376/101520 (100%)] Loss: -1168.827637\n",
      "    epoch          : 1215\n",
      "    loss           : -1162.336026445705\n",
      "    ess            : 1.9671535845377937\n",
      "    log_marginal   : 1162.366551384854\n",
      "    log_joint      : 1370.7409692505496\n",
      "    val_loss       : -1161.5470395295517\n",
      "    val_ess        : 1.9685427572416223\n",
      "    val_log_marginal: 1161.5765433933425\n",
      "    val_log_joint  : 1369.999018130095\n",
      "Train Epoch: 1216 [0/101520 (0%)] Loss: -1162.604492\n",
      "Train Epoch: 1216 [11264/101520 (11%)] Loss: -1159.697266\n",
      "Train Epoch: 1216 [22528/101520 (22%)] Loss: -1159.844360\n",
      "Train Epoch: 1216 [33792/101520 (33%)] Loss: -1164.382935\n",
      "Train Epoch: 1216 [45056/101520 (44%)] Loss: -1157.756348\n",
      "Train Epoch: 1216 [56320/101520 (55%)] Loss: -1157.176392\n",
      "Train Epoch: 1216 [67584/101520 (67%)] Loss: -1157.588135\n",
      "Train Epoch: 1216 [78848/101520 (78%)] Loss: -1160.724487\n",
      "Train Epoch: 1216 [90112/101520 (89%)] Loss: -1160.539062\n",
      "Train Epoch: 1216 [101376/101520 (100%)] Loss: -1167.892944\n",
      "    epoch          : 1216\n",
      "    loss           : -1162.3468673936086\n",
      "    ess            : 1.9665605650475277\n",
      "    log_marginal   : 1162.3787001413316\n",
      "    log_joint      : 1370.8038323943938\n",
      "    val_loss       : -1162.4060430112092\n",
      "    val_ess        : 1.968504962713822\n",
      "    val_log_marginal: 1162.436183763587\n",
      "    val_log_joint  : 1370.5151632557745\n",
      "Train Epoch: 1217 [0/101520 (0%)] Loss: -1166.021729\n",
      "Train Epoch: 1217 [11264/101520 (11%)] Loss: -1158.188965\n",
      "Train Epoch: 1217 [22528/101520 (22%)] Loss: -1162.753052\n",
      "Train Epoch: 1217 [33792/101520 (33%)] Loss: -1159.637207\n",
      "Train Epoch: 1217 [45056/101520 (44%)] Loss: -1164.612305\n",
      "Train Epoch: 1217 [56320/101520 (55%)] Loss: -1163.363281\n",
      "Train Epoch: 1217 [67584/101520 (67%)] Loss: -1163.144043\n",
      "Train Epoch: 1217 [78848/101520 (78%)] Loss: -1159.157959\n",
      "Train Epoch: 1217 [90112/101520 (89%)] Loss: -1161.692871\n",
      "Train Epoch: 1217 [101376/101520 (100%)] Loss: -1158.375610\n",
      "    epoch          : 1217\n",
      "    loss           : -1162.455471939777\n",
      "    ess            : 1.9675006423164252\n",
      "    log_marginal   : 1162.4863072687658\n",
      "    log_joint      : 1370.8893730125235\n",
      "    val_loss       : -1161.8451033882473\n",
      "    val_ess        : 1.9709786902303281\n",
      "    val_log_marginal: 1161.870260487432\n",
      "    val_log_joint  : 1370.1616051715353\n",
      "Train Epoch: 1218 [0/101520 (0%)] Loss: -1159.187988\n",
      "Train Epoch: 1218 [11264/101520 (11%)] Loss: -1159.208252\n",
      "Train Epoch: 1218 [22528/101520 (22%)] Loss: -1158.098389\n",
      "Train Epoch: 1218 [33792/101520 (33%)] Loss: -1162.215576\n",
      "Train Epoch: 1218 [45056/101520 (44%)] Loss: -1174.503296\n",
      "Train Epoch: 1218 [56320/101520 (55%)] Loss: -1169.843018\n",
      "Train Epoch: 1218 [67584/101520 (67%)] Loss: -1166.497192\n",
      "Train Epoch: 1218 [78848/101520 (78%)] Loss: -1158.008301\n",
      "Train Epoch: 1218 [90112/101520 (89%)] Loss: -1161.269409\n",
      "Train Epoch: 1218 [101376/101520 (100%)] Loss: -1168.641479\n",
      "    epoch          : 1218\n",
      "    loss           : -1162.4741897966394\n",
      "    ess            : 1.9662718455396107\n",
      "    log_marginal   : 1162.5059550683104\n",
      "    log_joint      : 1370.912881055669\n",
      "    val_loss       : -1160.0576490319293\n",
      "    val_ess        : 1.9651974802431853\n",
      "    val_log_marginal: 1160.0899976647418\n",
      "    val_log_joint  : 1368.402391516644\n",
      "Train Epoch: 1219 [0/101520 (0%)] Loss: -1156.309814\n",
      "Train Epoch: 1219 [11264/101520 (11%)] Loss: -1165.581299\n",
      "Train Epoch: 1219 [22528/101520 (22%)] Loss: -1159.765747\n",
      "Train Epoch: 1219 [33792/101520 (33%)] Loss: -1162.942993\n",
      "Train Epoch: 1219 [45056/101520 (44%)] Loss: -1154.374878\n",
      "Train Epoch: 1219 [56320/101520 (55%)] Loss: -1158.412598\n",
      "Train Epoch: 1219 [67584/101520 (67%)] Loss: -1160.742798\n",
      "Train Epoch: 1219 [78848/101520 (78%)] Loss: -1157.145020\n",
      "Train Epoch: 1219 [90112/101520 (89%)] Loss: -1169.612549\n",
      "Train Epoch: 1219 [101376/101520 (100%)] Loss: -1159.956543\n",
      "    epoch          : 1219\n",
      "    loss           : -1162.465599481784\n",
      "    ess            : 1.9670548738546707\n",
      "    log_marginal   : 1162.495520203557\n",
      "    log_joint      : 1370.9197789484533\n",
      "    val_loss       : -1161.7845777428668\n",
      "    val_ess        : 1.9700560517933057\n",
      "    val_log_marginal: 1161.8135614809783\n",
      "    val_log_joint  : 1370.270900560462\n",
      "Train Epoch: 1220 [0/101520 (0%)] Loss: -1165.993896\n",
      "Train Epoch: 1220 [11264/101520 (11%)] Loss: -1163.248291\n",
      "Train Epoch: 1220 [22528/101520 (22%)] Loss: -1163.042480\n",
      "Train Epoch: 1220 [33792/101520 (33%)] Loss: -1165.942383\n",
      "Train Epoch: 1220 [45056/101520 (44%)] Loss: -1159.958374\n",
      "Train Epoch: 1220 [56320/101520 (55%)] Loss: -1164.361572\n",
      "Train Epoch: 1220 [67584/101520 (67%)] Loss: -1163.694702\n",
      "Train Epoch: 1220 [78848/101520 (78%)] Loss: -1164.325195\n",
      "Train Epoch: 1220 [90112/101520 (89%)] Loss: -1163.728882\n",
      "Train Epoch: 1220 [101376/101520 (100%)] Loss: -1153.792725\n",
      "    epoch          : 1220\n",
      "    loss           : -1162.570897087979\n",
      "    ess            : 1.9665480277047085\n",
      "    log_marginal   : 1162.6025458101053\n",
      "    log_joint      : 1371.0169696136934\n",
      "    val_loss       : -1159.5122919497283\n",
      "    val_ess        : 1.9631564098855723\n",
      "    val_log_marginal: 1159.552835215693\n",
      "    val_log_joint  : 1367.9726244055707\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1220.pth ...\n",
      "Train Epoch: 1221 [0/101520 (0%)] Loss: -1158.545898\n",
      "Train Epoch: 1221 [11264/101520 (11%)] Loss: -1166.688721\n",
      "Train Epoch: 1221 [22528/101520 (22%)] Loss: -1161.430420\n",
      "Train Epoch: 1221 [33792/101520 (33%)] Loss: -1154.134277\n",
      "Train Epoch: 1221 [45056/101520 (44%)] Loss: -1163.909180\n",
      "Train Epoch: 1221 [56320/101520 (55%)] Loss: -1152.318726\n",
      "Train Epoch: 1221 [67584/101520 (67%)] Loss: -1157.727661\n",
      "Train Epoch: 1221 [78848/101520 (78%)] Loss: -1165.309570\n",
      "Train Epoch: 1221 [90112/101520 (89%)] Loss: -1158.970459\n",
      "Train Epoch: 1221 [101376/101520 (100%)] Loss: -1154.623657\n",
      "    epoch          : 1221\n",
      "    loss           : -1162.621578350738\n",
      "    ess            : 1.966774842247891\n",
      "    log_marginal   : 1162.6527682357098\n",
      "    log_joint      : 1371.023685321137\n",
      "    val_loss       : -1161.860155188519\n",
      "    val_ess        : 1.9647795946701714\n",
      "    val_log_marginal: 1161.8906090777853\n",
      "    val_log_joint  : 1370.376809825068\n",
      "Train Epoch: 1222 [0/101520 (0%)] Loss: -1164.266479\n",
      "Train Epoch: 1222 [11264/101520 (11%)] Loss: -1161.635498\n",
      "Train Epoch: 1222 [22528/101520 (22%)] Loss: -1168.381348\n",
      "Train Epoch: 1222 [33792/101520 (33%)] Loss: -1166.306030\n",
      "Train Epoch: 1222 [45056/101520 (44%)] Loss: -1160.661133\n",
      "Train Epoch: 1222 [56320/101520 (55%)] Loss: -1156.442261\n",
      "Train Epoch: 1222 [67584/101520 (67%)] Loss: -1163.142578\n",
      "Train Epoch: 1222 [78848/101520 (78%)] Loss: -1160.539551\n",
      "Train Epoch: 1222 [90112/101520 (89%)] Loss: -1157.858643\n",
      "Train Epoch: 1222 [101376/101520 (100%)] Loss: -1163.877441\n",
      "    epoch          : 1222\n",
      "    loss           : -1162.6625792536904\n",
      "    ess            : 1.967376856947664\n",
      "    log_marginal   : 1162.6919252021828\n",
      "    log_joint      : 1371.1082978368404\n",
      "    val_loss       : -1163.2993588654892\n",
      "    val_ess        : 1.9676326461460278\n",
      "    val_log_marginal: 1163.3280029296875\n",
      "    val_log_joint  : 1371.9320386803668\n",
      "Train Epoch: 1223 [0/101520 (0%)] Loss: -1158.867554\n",
      "Train Epoch: 1223 [11264/101520 (11%)] Loss: -1166.835938\n",
      "Train Epoch: 1223 [22528/101520 (22%)] Loss: -1160.846191\n",
      "Train Epoch: 1223 [33792/101520 (33%)] Loss: -1161.659424\n",
      "Train Epoch: 1223 [45056/101520 (44%)] Loss: -1161.335205\n",
      "Train Epoch: 1223 [56320/101520 (55%)] Loss: -1171.355469\n",
      "Train Epoch: 1223 [67584/101520 (67%)] Loss: -1163.779785\n",
      "Train Epoch: 1223 [78848/101520 (78%)] Loss: -1167.593140\n",
      "Train Epoch: 1223 [90112/101520 (89%)] Loss: -1164.226807\n",
      "Train Epoch: 1223 [101376/101520 (100%)] Loss: -1159.840698\n",
      "    epoch          : 1223\n",
      "    loss           : -1162.7279138612987\n",
      "    ess            : 1.9668005033953107\n",
      "    log_marginal   : 1162.7585124106863\n",
      "    log_joint      : 1371.1604114321608\n",
      "    val_loss       : -1161.3151112432065\n",
      "    val_ess        : 1.9696448315744814\n",
      "    val_log_marginal: 1161.3424496858017\n",
      "    val_log_joint  : 1369.5449431046195\n",
      "Train Epoch: 1224 [0/101520 (0%)] Loss: -1163.216919\n",
      "Train Epoch: 1224 [11264/101520 (11%)] Loss: -1163.189087\n",
      "Train Epoch: 1224 [22528/101520 (22%)] Loss: -1155.811035\n",
      "Train Epoch: 1224 [33792/101520 (33%)] Loss: -1164.246704\n",
      "Train Epoch: 1224 [45056/101520 (44%)] Loss: -1161.478027\n",
      "Train Epoch: 1224 [56320/101520 (55%)] Loss: -1160.509033\n",
      "Train Epoch: 1224 [67584/101520 (67%)] Loss: -1160.690430\n",
      "Train Epoch: 1224 [78848/101520 (78%)] Loss: -1158.923950\n",
      "Train Epoch: 1224 [90112/101520 (89%)] Loss: -1158.500366\n",
      "Train Epoch: 1224 [101376/101520 (100%)] Loss: -1154.523071\n",
      "    epoch          : 1224\n",
      "    loss           : -1162.711347263662\n",
      "    ess            : 1.9675973396205424\n",
      "    log_marginal   : 1162.7412778001335\n",
      "    log_joint      : 1371.222071662021\n",
      "    val_loss       : -1160.6043223505435\n",
      "    val_ess        : 1.971114308937736\n",
      "    val_log_marginal: 1160.6319739300272\n",
      "    val_log_joint  : 1369.14794921875\n",
      "Train Epoch: 1225 [0/101520 (0%)] Loss: -1154.563232\n",
      "Train Epoch: 1225 [11264/101520 (11%)] Loss: -1160.709106\n",
      "Train Epoch: 1225 [22528/101520 (22%)] Loss: -1172.837402\n",
      "Train Epoch: 1225 [33792/101520 (33%)] Loss: -1161.264648\n",
      "Train Epoch: 1225 [45056/101520 (44%)] Loss: -1159.225220\n",
      "Train Epoch: 1225 [56320/101520 (55%)] Loss: -1168.515625\n",
      "Train Epoch: 1225 [67584/101520 (67%)] Loss: -1168.736572\n",
      "Train Epoch: 1225 [78848/101520 (78%)] Loss: -1157.060303\n",
      "Train Epoch: 1225 [90112/101520 (89%)] Loss: -1168.353271\n",
      "Train Epoch: 1225 [101376/101520 (100%)] Loss: -1155.847290\n",
      "    epoch          : 1225\n",
      "    loss           : -1162.9202985140546\n",
      "    ess            : 1.9674526141516526\n",
      "    log_marginal   : 1162.950344986652\n",
      "    log_joint      : 1371.3295818693075\n",
      "    val_loss       : -1161.6302012567935\n",
      "    val_ess        : 1.9672652275665947\n",
      "    val_log_marginal: 1161.6617750084918\n",
      "    val_log_joint  : 1370.3363939368207\n",
      "Train Epoch: 1226 [0/101520 (0%)] Loss: -1158.518799\n",
      "Train Epoch: 1226 [11264/101520 (11%)] Loss: -1162.912842\n",
      "Train Epoch: 1226 [22528/101520 (22%)] Loss: -1160.420166\n",
      "Train Epoch: 1226 [33792/101520 (33%)] Loss: -1165.843994\n",
      "Train Epoch: 1226 [45056/101520 (44%)] Loss: -1171.553467\n",
      "Train Epoch: 1226 [56320/101520 (55%)] Loss: -1151.821289\n",
      "Train Epoch: 1226 [67584/101520 (67%)] Loss: -1164.318848\n",
      "Train Epoch: 1226 [78848/101520 (78%)] Loss: -1159.678711\n",
      "Train Epoch: 1226 [90112/101520 (89%)] Loss: -1168.808228\n",
      "Train Epoch: 1226 [101376/101520 (100%)] Loss: -1165.024536\n",
      "    epoch          : 1226\n",
      "    loss           : -1163.0102171011306\n",
      "    ess            : 1.9679668752392332\n",
      "    log_marginal   : 1163.039415215727\n",
      "    log_joint      : 1371.3822855733747\n",
      "    val_loss       : -1160.5870679772418\n",
      "    val_ess        : 1.9679959234984026\n",
      "    val_log_marginal: 1160.6163860818615\n",
      "    val_log_joint  : 1369.1156642747962\n",
      "Train Epoch: 1227 [0/101520 (0%)] Loss: -1162.215088\n",
      "Train Epoch: 1227 [11264/101520 (11%)] Loss: -1160.829346\n",
      "Train Epoch: 1227 [22528/101520 (22%)] Loss: -1159.996826\n",
      "Train Epoch: 1227 [33792/101520 (33%)] Loss: -1165.318237\n",
      "Train Epoch: 1227 [45056/101520 (44%)] Loss: -1167.517578\n",
      "Train Epoch: 1227 [56320/101520 (55%)] Loss: -1164.863770\n",
      "Train Epoch: 1227 [67584/101520 (67%)] Loss: -1161.091797\n",
      "Train Epoch: 1227 [78848/101520 (78%)] Loss: -1160.416260\n",
      "Train Epoch: 1227 [90112/101520 (89%)] Loss: -1166.094727\n",
      "Train Epoch: 1227 [101376/101520 (100%)] Loss: -1158.756104\n",
      "    epoch          : 1227\n",
      "    loss           : -1162.9961103123037\n",
      "    ess            : 1.96672475996928\n",
      "    log_marginal   : 1163.0271910087547\n",
      "    log_joint      : 1371.4356100571215\n",
      "    val_loss       : -1160.9490807574728\n",
      "    val_ess        : 1.9694304621737937\n",
      "    val_log_marginal: 1160.9809358016305\n",
      "    val_log_joint  : 1369.3593006963315\n",
      "Train Epoch: 1228 [0/101520 (0%)] Loss: -1162.391602\n",
      "Train Epoch: 1228 [11264/101520 (11%)] Loss: -1170.569458\n",
      "Train Epoch: 1228 [22528/101520 (22%)] Loss: -1164.010498\n",
      "Train Epoch: 1228 [33792/101520 (33%)] Loss: -1163.446167\n",
      "Train Epoch: 1228 [45056/101520 (44%)] Loss: -1160.457642\n",
      "Train Epoch: 1228 [56320/101520 (55%)] Loss: -1157.919678\n",
      "Train Epoch: 1228 [67584/101520 (67%)] Loss: -1169.389893\n",
      "Train Epoch: 1228 [78848/101520 (78%)] Loss: -1161.854736\n",
      "Train Epoch: 1228 [90112/101520 (89%)] Loss: -1165.032349\n",
      "Train Epoch: 1228 [101376/101520 (100%)] Loss: -1162.645142\n",
      "    epoch          : 1228\n",
      "    loss           : -1162.9915569056218\n",
      "    ess            : 1.9666655518900809\n",
      "    log_marginal   : 1163.022915480724\n",
      "    log_joint      : 1371.5112102259343\n",
      "    val_loss       : -1162.8420622452445\n",
      "    val_ess        : 1.9673405367395151\n",
      "    val_log_marginal: 1162.8715448794158\n",
      "    val_log_joint  : 1371.3446204144022\n",
      "Train Epoch: 1229 [0/101520 (0%)] Loss: -1157.284180\n",
      "Train Epoch: 1229 [11264/101520 (11%)] Loss: -1168.157959\n",
      "Train Epoch: 1229 [22528/101520 (22%)] Loss: -1170.736816\n",
      "Train Epoch: 1229 [33792/101520 (33%)] Loss: -1156.508667\n",
      "Train Epoch: 1229 [45056/101520 (44%)] Loss: -1169.601318\n",
      "Train Epoch: 1229 [56320/101520 (55%)] Loss: -1168.257568\n",
      "Train Epoch: 1229 [67584/101520 (67%)] Loss: -1163.895508\n",
      "Train Epoch: 1229 [78848/101520 (78%)] Loss: -1157.795044\n",
      "Train Epoch: 1229 [90112/101520 (89%)] Loss: -1160.962769\n",
      "Train Epoch: 1229 [101376/101520 (100%)] Loss: -1175.942017\n",
      "    epoch          : 1229\n",
      "    loss           : -1163.2114466374842\n",
      "    ess            : 1.9672429992924982\n",
      "    log_marginal   : 1163.241758106941\n",
      "    log_joint      : 1371.575110047307\n",
      "    val_loss       : -1162.46898883322\n",
      "    val_ess        : 1.9652695344842\n",
      "    val_log_marginal: 1162.501809825068\n",
      "    val_log_joint  : 1370.7644679857337\n",
      "Train Epoch: 1230 [0/101520 (0%)] Loss: -1164.056152\n",
      "Train Epoch: 1230 [11264/101520 (11%)] Loss: -1167.487793\n",
      "Train Epoch: 1230 [22528/101520 (22%)] Loss: -1162.440918\n",
      "Train Epoch: 1230 [33792/101520 (33%)] Loss: -1157.729004\n",
      "Train Epoch: 1230 [45056/101520 (44%)] Loss: -1171.582764\n",
      "Train Epoch: 1230 [56320/101520 (55%)] Loss: -1163.588501\n",
      "Train Epoch: 1230 [67584/101520 (67%)] Loss: -1157.869873\n",
      "Train Epoch: 1230 [78848/101520 (78%)] Loss: -1159.454834\n",
      "Train Epoch: 1230 [90112/101520 (89%)] Loss: -1157.117432\n",
      "Train Epoch: 1230 [101376/101520 (100%)] Loss: -1151.340698\n",
      "    epoch          : 1230\n",
      "    loss           : -1163.111607843907\n",
      "    ess            : 1.9668812619980856\n",
      "    log_marginal   : 1163.1426517352386\n",
      "    log_joint      : 1371.5701106852623\n",
      "    val_loss       : -1160.2581309442935\n",
      "    val_ess        : 1.9653412673784338\n",
      "    val_log_marginal: 1160.2886379076087\n",
      "    val_log_joint  : 1368.5814155910325\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1230.pth ...\n",
      "Train Epoch: 1231 [0/101520 (0%)] Loss: -1164.458984\n",
      "Train Epoch: 1231 [11264/101520 (11%)] Loss: -1163.074829\n",
      "Train Epoch: 1231 [22528/101520 (22%)] Loss: -1169.879639\n",
      "Train Epoch: 1231 [33792/101520 (33%)] Loss: -1157.455566\n",
      "Train Epoch: 1231 [45056/101520 (44%)] Loss: -1164.520996\n",
      "Train Epoch: 1231 [56320/101520 (55%)] Loss: -1163.106445\n",
      "Train Epoch: 1231 [67584/101520 (67%)] Loss: -1165.000854\n",
      "Train Epoch: 1231 [78848/101520 (78%)] Loss: -1160.969971\n",
      "Train Epoch: 1231 [90112/101520 (89%)] Loss: -1158.861572\n",
      "Train Epoch: 1231 [101376/101520 (100%)] Loss: -1161.147705\n",
      "    epoch          : 1231\n",
      "    loss           : -1163.1664376570352\n",
      "    ess            : 1.9667517228342182\n",
      "    log_marginal   : 1163.1971453949434\n",
      "    log_joint      : 1371.6375450249293\n",
      "    val_loss       : -1161.917873216712\n",
      "    val_ess        : 1.9687778172285662\n",
      "    val_log_marginal: 1161.9445906929348\n",
      "    val_log_joint  : 1370.092720363451\n",
      "Train Epoch: 1232 [0/101520 (0%)] Loss: -1156.100098\n",
      "Train Epoch: 1232 [11264/101520 (11%)] Loss: -1161.789551\n",
      "Train Epoch: 1232 [22528/101520 (22%)] Loss: -1157.603394\n",
      "Train Epoch: 1232 [33792/101520 (33%)] Loss: -1162.163818\n",
      "Train Epoch: 1232 [45056/101520 (44%)] Loss: -1163.744629\n",
      "Train Epoch: 1232 [56320/101520 (55%)] Loss: -1160.620605\n",
      "Train Epoch: 1232 [67584/101520 (67%)] Loss: -1163.742920\n",
      "Train Epoch: 1232 [78848/101520 (78%)] Loss: -1160.903687\n",
      "Train Epoch: 1232 [90112/101520 (89%)] Loss: -1169.250244\n",
      "Train Epoch: 1232 [101376/101520 (100%)] Loss: -1165.327271\n",
      "    epoch          : 1232\n",
      "    loss           : -1163.2652851660646\n",
      "    ess            : 1.966669428288637\n",
      "    log_marginal   : 1163.2965357794833\n",
      "    log_joint      : 1371.7682081059595\n",
      "    val_loss       : -1163.070116126019\n",
      "    val_ess        : 1.9632541770520417\n",
      "    val_log_marginal: 1163.1039349099865\n",
      "    val_log_joint  : 1371.2790898862092\n",
      "Train Epoch: 1233 [0/101520 (0%)] Loss: -1173.220459\n",
      "Train Epoch: 1233 [11264/101520 (11%)] Loss: -1163.910034\n",
      "Train Epoch: 1233 [22528/101520 (22%)] Loss: -1169.465942\n",
      "Train Epoch: 1233 [33792/101520 (33%)] Loss: -1166.806885\n",
      "Train Epoch: 1233 [45056/101520 (44%)] Loss: -1159.761353\n",
      "Train Epoch: 1233 [56320/101520 (55%)] Loss: -1156.364136\n",
      "Train Epoch: 1233 [67584/101520 (67%)] Loss: -1158.483032\n",
      "Train Epoch: 1233 [78848/101520 (78%)] Loss: -1163.817383\n",
      "Train Epoch: 1233 [90112/101520 (89%)] Loss: -1166.948486\n",
      "Train Epoch: 1233 [101376/101520 (100%)] Loss: -1172.458130\n",
      "    epoch          : 1233\n",
      "    loss           : -1163.386752488026\n",
      "    ess            : 1.967109792196571\n",
      "    log_marginal   : 1163.4179221301822\n",
      "    log_joint      : 1371.8594437028894\n",
      "    val_loss       : -1161.843213952106\n",
      "    val_ess        : 1.967707483664803\n",
      "    val_log_marginal: 1161.8726488196332\n",
      "    val_log_joint  : 1370.54224163553\n",
      "Train Epoch: 1234 [0/101520 (0%)] Loss: -1167.319214\n",
      "Train Epoch: 1234 [11264/101520 (11%)] Loss: -1161.398438\n",
      "Train Epoch: 1234 [22528/101520 (22%)] Loss: -1159.191650\n",
      "Train Epoch: 1234 [33792/101520 (33%)] Loss: -1159.095459\n",
      "Train Epoch: 1234 [45056/101520 (44%)] Loss: -1166.582520\n",
      "Train Epoch: 1234 [56320/101520 (55%)] Loss: -1161.664917\n",
      "Train Epoch: 1234 [67584/101520 (67%)] Loss: -1162.022949\n",
      "Train Epoch: 1234 [78848/101520 (78%)] Loss: -1163.705566\n",
      "Train Epoch: 1234 [90112/101520 (89%)] Loss: -1169.915527\n",
      "Train Epoch: 1234 [101376/101520 (100%)] Loss: -1154.944824\n",
      "    epoch          : 1234\n",
      "    loss           : -1163.34025903443\n",
      "    ess            : 1.9675269815789995\n",
      "    log_marginal   : 1163.3703515134266\n",
      "    log_joint      : 1371.7677744189698\n",
      "    val_loss       : -1162.923881199049\n",
      "    val_ess        : 1.9682929723159126\n",
      "    val_log_marginal: 1162.953756581182\n",
      "    val_log_joint  : 1371.6103940217392\n",
      "Train Epoch: 1235 [0/101520 (0%)] Loss: -1165.952637\n",
      "Train Epoch: 1235 [11264/101520 (11%)] Loss: -1159.976074\n",
      "Train Epoch: 1235 [22528/101520 (22%)] Loss: -1166.773926\n",
      "Train Epoch: 1235 [33792/101520 (33%)] Loss: -1161.334473\n",
      "Train Epoch: 1235 [45056/101520 (44%)] Loss: -1167.774902\n",
      "Train Epoch: 1235 [56320/101520 (55%)] Loss: -1154.471924\n",
      "Train Epoch: 1235 [67584/101520 (67%)] Loss: -1167.421631\n",
      "Train Epoch: 1235 [78848/101520 (78%)] Loss: -1167.461670\n",
      "Train Epoch: 1235 [90112/101520 (89%)] Loss: -1168.130737\n",
      "Train Epoch: 1235 [101376/101520 (100%)] Loss: -1163.703369\n",
      "    epoch          : 1235\n",
      "    loss           : -1163.4623127846262\n",
      "    ess            : 1.967066730087127\n",
      "    log_marginal   : 1163.4927285352544\n",
      "    log_joint      : 1371.9017278776696\n",
      "    val_loss       : -1161.7916843580163\n",
      "    val_ess        : 1.9650294262429941\n",
      "    val_log_marginal: 1161.825386379076\n",
      "    val_log_joint  : 1369.7634383491848\n",
      "Train Epoch: 1236 [0/101520 (0%)] Loss: -1159.480469\n",
      "Train Epoch: 1236 [11264/101520 (11%)] Loss: -1162.253174\n",
      "Train Epoch: 1236 [22528/101520 (22%)] Loss: -1162.897095\n",
      "Train Epoch: 1236 [33792/101520 (33%)] Loss: -1159.693848\n",
      "Train Epoch: 1236 [45056/101520 (44%)] Loss: -1157.165771\n",
      "Train Epoch: 1236 [56320/101520 (55%)] Loss: -1166.361328\n",
      "Train Epoch: 1236 [67584/101520 (67%)] Loss: -1168.565186\n",
      "Train Epoch: 1236 [78848/101520 (78%)] Loss: -1163.166626\n",
      "Train Epoch: 1236 [90112/101520 (89%)] Loss: -1163.804688\n",
      "Train Epoch: 1236 [101376/101520 (100%)] Loss: -1156.722900\n",
      "    epoch          : 1236\n",
      "    loss           : -1163.4154794970948\n",
      "    ess            : 1.9669184265424258\n",
      "    log_marginal   : 1163.446260231823\n",
      "    log_joint      : 1371.8820383656564\n",
      "    val_loss       : -1161.8985754925272\n",
      "    val_ess        : 1.9660359880198603\n",
      "    val_log_marginal: 1161.9309506623642\n",
      "    val_log_joint  : 1370.149997877038\n",
      "Train Epoch: 1237 [0/101520 (0%)] Loss: -1170.037598\n",
      "Train Epoch: 1237 [11264/101520 (11%)] Loss: -1161.021729\n",
      "Train Epoch: 1237 [22528/101520 (22%)] Loss: -1165.301758\n",
      "Train Epoch: 1237 [33792/101520 (33%)] Loss: -1160.438232\n",
      "Train Epoch: 1237 [45056/101520 (44%)] Loss: -1168.798218\n",
      "Train Epoch: 1237 [56320/101520 (55%)] Loss: -1161.612671\n",
      "Train Epoch: 1237 [67584/101520 (67%)] Loss: -1165.307129\n",
      "Train Epoch: 1237 [78848/101520 (78%)] Loss: -1166.022461\n",
      "Train Epoch: 1237 [90112/101520 (89%)] Loss: -1165.854004\n",
      "Train Epoch: 1237 [101376/101520 (100%)] Loss: -1168.283691\n",
      "    epoch          : 1237\n",
      "    loss           : -1163.4442402441896\n",
      "    ess            : 1.966722650743609\n",
      "    log_marginal   : 1163.4749504357726\n",
      "    log_joint      : 1371.9150507174545\n",
      "    val_loss       : -1163.3588495669158\n",
      "    val_ess        : 1.9674508727115134\n",
      "    val_log_marginal: 1163.388671875\n",
      "    val_log_joint  : 1372.1697944972825\n",
      "Train Epoch: 1238 [0/101520 (0%)] Loss: -1168.127075\n",
      "Train Epoch: 1238 [11264/101520 (11%)] Loss: -1161.826172\n",
      "Train Epoch: 1238 [22528/101520 (22%)] Loss: -1159.376221\n",
      "Train Epoch: 1238 [33792/101520 (33%)] Loss: -1166.209473\n",
      "Train Epoch: 1238 [45056/101520 (44%)] Loss: -1170.115601\n",
      "Train Epoch: 1238 [56320/101520 (55%)] Loss: -1158.465942\n",
      "Train Epoch: 1238 [67584/101520 (67%)] Loss: -1162.750732\n",
      "Train Epoch: 1238 [78848/101520 (78%)] Loss: -1171.936279\n",
      "Train Epoch: 1238 [90112/101520 (89%)] Loss: -1155.017578\n",
      "Train Epoch: 1238 [101376/101520 (100%)] Loss: -1172.722290\n",
      "    epoch          : 1238\n",
      "    loss           : -1163.6380222646435\n",
      "    ess            : 1.9673362808610926\n",
      "    log_marginal   : 1163.6684772740657\n",
      "    log_joint      : 1371.9932793852072\n",
      "    val_loss       : -1161.817335045856\n",
      "    val_ess        : 1.968228236488674\n",
      "    val_log_marginal: 1161.844673488451\n",
      "    val_log_joint  : 1370.3293191661005\n",
      "Train Epoch: 1239 [0/101520 (0%)] Loss: -1168.376465\n",
      "Train Epoch: 1239 [11264/101520 (11%)] Loss: -1159.880005\n",
      "Train Epoch: 1239 [22528/101520 (22%)] Loss: -1163.306152\n",
      "Train Epoch: 1239 [33792/101520 (33%)] Loss: -1162.460693\n",
      "Train Epoch: 1239 [45056/101520 (44%)] Loss: -1159.220947\n",
      "Train Epoch: 1239 [56320/101520 (55%)] Loss: -1155.584595\n",
      "Train Epoch: 1239 [67584/101520 (67%)] Loss: -1159.825806\n",
      "Train Epoch: 1239 [78848/101520 (78%)] Loss: -1161.814697\n",
      "Train Epoch: 1239 [90112/101520 (89%)] Loss: -1162.668701\n",
      "Train Epoch: 1239 [101376/101520 (100%)] Loss: -1163.591797\n",
      "    epoch          : 1239\n",
      "    loss           : -1163.5363143844222\n",
      "    ess            : 1.9671014763003019\n",
      "    log_marginal   : 1163.5672263907427\n",
      "    log_joint      : 1371.9810649929334\n",
      "    val_loss       : -1162.1699643342392\n",
      "    val_ess        : 1.971509689870088\n",
      "    val_log_marginal: 1162.1963155995245\n",
      "    val_log_joint  : 1370.5997367527175\n",
      "Train Epoch: 1240 [0/101520 (0%)] Loss: -1167.729736\n",
      "Train Epoch: 1240 [11264/101520 (11%)] Loss: -1165.326904\n",
      "Train Epoch: 1240 [22528/101520 (22%)] Loss: -1164.653931\n",
      "Train Epoch: 1240 [33792/101520 (33%)] Loss: -1166.292114\n",
      "Train Epoch: 1240 [45056/101520 (44%)] Loss: -1163.236450\n",
      "Train Epoch: 1240 [56320/101520 (55%)] Loss: -1161.198242\n",
      "Train Epoch: 1240 [67584/101520 (67%)] Loss: -1171.643555\n",
      "Train Epoch: 1240 [78848/101520 (78%)] Loss: -1167.189819\n",
      "Train Epoch: 1240 [90112/101520 (89%)] Loss: -1158.622314\n",
      "Train Epoch: 1240 [101376/101520 (100%)] Loss: -1164.910767\n",
      "    epoch          : 1240\n",
      "    loss           : -1163.6171942476053\n",
      "    ess            : 1.9667705171671346\n",
      "    log_marginal   : 1163.648742369072\n",
      "    log_joint      : 1372.0960724030308\n",
      "    val_loss       : -1162.3148671025815\n",
      "    val_ess        : 1.9675842834555584\n",
      "    val_log_marginal: 1162.3478738536005\n",
      "    val_log_joint  : 1370.8644594938858\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1240.pth ...\n",
      "Train Epoch: 1241 [0/101520 (0%)] Loss: -1170.171631\n",
      "Train Epoch: 1241 [11264/101520 (11%)] Loss: -1164.012695\n",
      "Train Epoch: 1241 [22528/101520 (22%)] Loss: -1164.801514\n",
      "Train Epoch: 1241 [33792/101520 (33%)] Loss: -1164.946533\n",
      "Train Epoch: 1241 [45056/101520 (44%)] Loss: -1161.439209\n",
      "Train Epoch: 1241 [56320/101520 (55%)] Loss: -1164.767334\n",
      "Train Epoch: 1241 [67584/101520 (67%)] Loss: -1161.597412\n",
      "Train Epoch: 1241 [78848/101520 (78%)] Loss: -1161.072998\n",
      "Train Epoch: 1241 [90112/101520 (89%)] Loss: -1165.010498\n",
      "Train Epoch: 1241 [101376/101520 (100%)] Loss: -1157.450562\n",
      "    epoch          : 1241\n",
      "    loss           : -1163.7125747143923\n",
      "    ess            : 1.9668502945396769\n",
      "    log_marginal   : 1163.7438296217415\n",
      "    log_joint      : 1372.1801745544126\n",
      "    val_loss       : -1163.8016198199728\n",
      "    val_ess        : 1.9691423333209495\n",
      "    val_log_marginal: 1163.827928626019\n",
      "    val_log_joint  : 1372.2756029211957\n",
      "Train Epoch: 1242 [0/101520 (0%)] Loss: -1160.228760\n",
      "Train Epoch: 1242 [11264/101520 (11%)] Loss: -1167.007935\n",
      "Train Epoch: 1242 [22528/101520 (22%)] Loss: -1158.385010\n",
      "Train Epoch: 1242 [33792/101520 (33%)] Loss: -1157.739990\n",
      "Train Epoch: 1242 [45056/101520 (44%)] Loss: -1165.307861\n",
      "Train Epoch: 1242 [56320/101520 (55%)] Loss: -1164.487671\n",
      "Train Epoch: 1242 [67584/101520 (67%)] Loss: -1164.690674\n",
      "Train Epoch: 1242 [78848/101520 (78%)] Loss: -1163.209229\n",
      "Train Epoch: 1242 [90112/101520 (89%)] Loss: -1166.046265\n",
      "Train Epoch: 1242 [101376/101520 (100%)] Loss: -1165.702881\n",
      "    epoch          : 1242\n",
      "    loss           : -1163.8191003847362\n",
      "    ess            : 1.966928261009293\n",
      "    log_marginal   : 1163.8499786530308\n",
      "    log_joint      : 1372.2502165367855\n",
      "    val_loss       : -1162.1883651069973\n",
      "    val_ess        : 1.9663693956706836\n",
      "    val_log_marginal: 1162.2200874660325\n",
      "    val_log_joint  : 1370.438179347826\n",
      "Train Epoch: 1243 [0/101520 (0%)] Loss: -1163.327759\n",
      "Train Epoch: 1243 [11264/101520 (11%)] Loss: -1171.369873\n",
      "Train Epoch: 1243 [22528/101520 (22%)] Loss: -1159.337891\n",
      "Train Epoch: 1243 [33792/101520 (33%)] Loss: -1163.951904\n",
      "Train Epoch: 1243 [45056/101520 (44%)] Loss: -1165.306152\n",
      "Train Epoch: 1243 [56320/101520 (55%)] Loss: -1166.100952\n",
      "Train Epoch: 1243 [67584/101520 (67%)] Loss: -1167.403442\n",
      "Train Epoch: 1243 [78848/101520 (78%)] Loss: -1157.909668\n",
      "Train Epoch: 1243 [90112/101520 (89%)] Loss: -1171.525635\n",
      "Train Epoch: 1243 [101376/101520 (100%)] Loss: -1170.011475\n",
      "    epoch          : 1243\n",
      "    loss           : -1163.7895974010678\n",
      "    ess            : 1.9672550580010342\n",
      "    log_marginal   : 1163.8200192858826\n",
      "    log_joint      : 1372.2179261787453\n",
      "    val_loss       : -1160.9577318274457\n",
      "    val_ess        : 1.967793697896211\n",
      "    val_log_marginal: 1160.9871666949728\n",
      "    val_log_joint  : 1369.657720151155\n",
      "Train Epoch: 1244 [0/101520 (0%)] Loss: -1156.658203\n",
      "Train Epoch: 1244 [11264/101520 (11%)] Loss: -1162.239624\n",
      "Train Epoch: 1244 [22528/101520 (22%)] Loss: -1161.525391\n",
      "Train Epoch: 1244 [33792/101520 (33%)] Loss: -1173.129028\n",
      "Train Epoch: 1244 [45056/101520 (44%)] Loss: -1170.792603\n",
      "Train Epoch: 1244 [56320/101520 (55%)] Loss: -1163.964355\n",
      "Train Epoch: 1244 [67584/101520 (67%)] Loss: -1165.177856\n",
      "Train Epoch: 1244 [78848/101520 (78%)] Loss: -1161.294922\n",
      "Train Epoch: 1244 [90112/101520 (89%)] Loss: -1160.512695\n",
      "Train Epoch: 1244 [101376/101520 (100%)] Loss: -1157.818115\n",
      "    epoch          : 1244\n",
      "    loss           : -1163.8048028227072\n",
      "    ess            : 1.9672327946178878\n",
      "    log_marginal   : 1163.8358694105293\n",
      "    log_joint      : 1372.245074248194\n",
      "    val_loss       : -1161.56005859375\n",
      "    val_ess        : 1.9702898004780645\n",
      "    val_log_marginal: 1161.5883895210598\n",
      "    val_log_joint  : 1370.0929963485055\n",
      "Train Epoch: 1245 [0/101520 (0%)] Loss: -1166.720215\n",
      "Train Epoch: 1245 [11264/101520 (11%)] Loss: -1162.072754\n",
      "Train Epoch: 1245 [22528/101520 (22%)] Loss: -1164.861572\n",
      "Train Epoch: 1245 [33792/101520 (33%)] Loss: -1172.024536\n",
      "Train Epoch: 1245 [45056/101520 (44%)] Loss: -1168.529053\n",
      "Train Epoch: 1245 [56320/101520 (55%)] Loss: -1163.196899\n",
      "Train Epoch: 1245 [67584/101520 (67%)] Loss: -1160.559814\n",
      "Train Epoch: 1245 [78848/101520 (78%)] Loss: -1164.474365\n",
      "Train Epoch: 1245 [90112/101520 (89%)] Loss: -1163.447754\n",
      "Train Epoch: 1245 [101376/101520 (100%)] Loss: -1164.833130\n",
      "    epoch          : 1245\n",
      "    loss           : -1163.8461570548052\n",
      "    ess            : 1.9673948982852187\n",
      "    log_marginal   : 1163.8767954764055\n",
      "    log_joint      : 1372.275898535647\n",
      "    val_loss       : -1163.3911026664402\n",
      "    val_ess        : 1.967915768208711\n",
      "    val_log_marginal: 1163.4191469938858\n",
      "    val_log_joint  : 1371.8108706266983\n",
      "Train Epoch: 1246 [0/101520 (0%)] Loss: -1172.879395\n",
      "Train Epoch: 1246 [11264/101520 (11%)] Loss: -1161.241699\n",
      "Train Epoch: 1246 [22528/101520 (22%)] Loss: -1167.551025\n",
      "Train Epoch: 1246 [33792/101520 (33%)] Loss: -1167.539185\n",
      "Train Epoch: 1246 [45056/101520 (44%)] Loss: -1165.303833\n",
      "Train Epoch: 1246 [56320/101520 (55%)] Loss: -1163.122559\n",
      "Train Epoch: 1246 [67584/101520 (67%)] Loss: -1160.553833\n",
      "Train Epoch: 1246 [78848/101520 (78%)] Loss: -1162.564209\n",
      "Train Epoch: 1246 [90112/101520 (89%)] Loss: -1161.328003\n",
      "Train Epoch: 1246 [101376/101520 (100%)] Loss: -1152.579468\n",
      "    epoch          : 1246\n",
      "    loss           : -1163.8460601346576\n",
      "    ess            : 1.9678005022020195\n",
      "    log_marginal   : 1163.8758612397928\n",
      "    log_joint      : 1372.333193064934\n",
      "    val_loss       : -1161.4108992866848\n",
      "    val_ess        : 1.9631751516590947\n",
      "    val_log_marginal: 1161.448534094769\n",
      "    val_log_joint  : 1369.920850670856\n",
      "Train Epoch: 1247 [0/101520 (0%)] Loss: -1165.304688\n",
      "Train Epoch: 1247 [11264/101520 (11%)] Loss: -1169.061890\n",
      "Train Epoch: 1247 [22528/101520 (22%)] Loss: -1168.939453\n",
      "Train Epoch: 1247 [33792/101520 (33%)] Loss: -1169.083374\n",
      "Train Epoch: 1247 [45056/101520 (44%)] Loss: -1165.635498\n",
      "Train Epoch: 1247 [56320/101520 (55%)] Loss: -1162.401733\n",
      "Train Epoch: 1247 [67584/101520 (67%)] Loss: -1161.319824\n",
      "Train Epoch: 1247 [78848/101520 (78%)] Loss: -1165.042236\n",
      "Train Epoch: 1247 [90112/101520 (89%)] Loss: -1155.203491\n",
      "Train Epoch: 1247 [101376/101520 (100%)] Loss: -1165.902100\n",
      "    epoch          : 1247\n",
      "    loss           : -1163.9217854408762\n",
      "    ess            : 1.9669441937202186\n",
      "    log_marginal   : 1163.9528851533055\n",
      "    log_joint      : 1372.4262449945038\n",
      "    val_loss       : -1160.868705417799\n",
      "    val_ess        : 1.9691958582919578\n",
      "    val_log_marginal: 1160.8977316151495\n",
      "    val_log_joint  : 1369.5481593919837\n",
      "Train Epoch: 1248 [0/101520 (0%)] Loss: -1167.687988\n",
      "Train Epoch: 1248 [11264/101520 (11%)] Loss: -1160.977783\n",
      "Train Epoch: 1248 [22528/101520 (22%)] Loss: -1162.894531\n",
      "Train Epoch: 1248 [33792/101520 (33%)] Loss: -1158.940430\n",
      "Train Epoch: 1248 [45056/101520 (44%)] Loss: -1165.304810\n",
      "Train Epoch: 1248 [56320/101520 (55%)] Loss: -1160.704346\n",
      "Train Epoch: 1248 [67584/101520 (67%)] Loss: -1162.435303\n",
      "Train Epoch: 1248 [78848/101520 (78%)] Loss: -1170.789795\n",
      "Train Epoch: 1248 [90112/101520 (89%)] Loss: -1162.583862\n",
      "Train Epoch: 1248 [101376/101520 (100%)] Loss: -1165.373779\n",
      "    epoch          : 1248\n",
      "    loss           : -1164.079201549741\n",
      "    ess            : 1.9677575005957828\n",
      "    log_marginal   : 1164.1092259392667\n",
      "    log_joint      : 1372.4824457983275\n",
      "    val_loss       : -1162.5400709069293\n",
      "    val_ess        : 1.9655281616293865\n",
      "    val_log_marginal: 1162.57568359375\n",
      "    val_log_joint  : 1370.9574770720108\n",
      "Train Epoch: 1249 [0/101520 (0%)] Loss: -1168.084229\n",
      "Train Epoch: 1249 [11264/101520 (11%)] Loss: -1159.667236\n",
      "Train Epoch: 1249 [22528/101520 (22%)] Loss: -1167.020996\n",
      "Train Epoch: 1249 [33792/101520 (33%)] Loss: -1167.936890\n",
      "Train Epoch: 1249 [45056/101520 (44%)] Loss: -1157.737305\n",
      "Train Epoch: 1249 [56320/101520 (55%)] Loss: -1166.239990\n",
      "Train Epoch: 1249 [67584/101520 (67%)] Loss: -1166.487671\n",
      "Train Epoch: 1249 [78848/101520 (78%)] Loss: -1161.390015\n",
      "Train Epoch: 1249 [90112/101520 (89%)] Loss: -1160.634521\n",
      "Train Epoch: 1249 [101376/101520 (100%)] Loss: -1168.866699\n",
      "    epoch          : 1249\n",
      "    loss           : -1164.0651235915907\n",
      "    ess            : 1.9670304605110207\n",
      "    log_marginal   : 1164.0957706010522\n",
      "    log_joint      : 1372.4942375451476\n",
      "    val_loss       : -1162.3272545855978\n",
      "    val_ess        : 1.9637118215146272\n",
      "    val_log_marginal: 1162.3622728430707\n",
      "    val_log_joint  : 1371.360346255095\n",
      "Train Epoch: 1250 [0/101520 (0%)] Loss: -1163.524902\n",
      "Train Epoch: 1250 [11264/101520 (11%)] Loss: -1169.432617\n",
      "Train Epoch: 1250 [22528/101520 (22%)] Loss: -1164.695923\n",
      "Train Epoch: 1250 [33792/101520 (33%)] Loss: -1161.292847\n",
      "Train Epoch: 1250 [45056/101520 (44%)] Loss: -1159.005005\n",
      "Train Epoch: 1250 [56320/101520 (55%)] Loss: -1160.763184\n",
      "Train Epoch: 1250 [67584/101520 (67%)] Loss: -1166.737915\n",
      "Train Epoch: 1250 [78848/101520 (78%)] Loss: -1172.172852\n",
      "Train Epoch: 1250 [90112/101520 (89%)] Loss: -1171.582153\n",
      "Train Epoch: 1250 [101376/101520 (100%)] Loss: -1169.599243\n",
      "    epoch          : 1250\n",
      "    loss           : -1164.1542496417635\n",
      "    ess            : 1.966644287109375\n",
      "    log_marginal   : 1164.185074542635\n",
      "    log_joint      : 1372.590547341198\n",
      "    val_loss       : -1164.0419550356658\n",
      "    val_ess        : 1.9664637368658315\n",
      "    val_log_marginal: 1164.0732262652853\n",
      "    val_log_joint  : 1372.4246348505435\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1250.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1251 [0/101520 (0%)] Loss: -1169.326660\n",
      "Train Epoch: 1251 [11264/101520 (11%)] Loss: -1160.591309\n",
      "Train Epoch: 1251 [22528/101520 (22%)] Loss: -1159.835938\n",
      "Train Epoch: 1251 [33792/101520 (33%)] Loss: -1167.036621\n",
      "Train Epoch: 1251 [45056/101520 (44%)] Loss: -1167.586670\n",
      "Train Epoch: 1251 [56320/101520 (55%)] Loss: -1160.906250\n",
      "Train Epoch: 1251 [67584/101520 (67%)] Loss: -1168.512451\n",
      "Train Epoch: 1251 [78848/101520 (78%)] Loss: -1165.710815\n",
      "Train Epoch: 1251 [90112/101520 (89%)] Loss: -1165.667480\n",
      "Train Epoch: 1251 [101376/101520 (100%)] Loss: -1173.120850\n",
      "    epoch          : 1251\n",
      "    loss           : -1164.2864432023398\n",
      "    ess            : 1.9666301059962517\n",
      "    log_marginal   : 1164.3180453046482\n",
      "    log_joint      : 1372.6251760511543\n",
      "    val_loss       : -1161.9840937075408\n",
      "    val_ess        : 1.9643015706020852\n",
      "    val_log_marginal: 1162.0191650390625\n",
      "    val_log_joint  : 1370.4796620244565\n",
      "Train Epoch: 1252 [0/101520 (0%)] Loss: -1157.426270\n",
      "Train Epoch: 1252 [11264/101520 (11%)] Loss: -1172.760498\n",
      "Train Epoch: 1252 [22528/101520 (22%)] Loss: -1161.984375\n",
      "Train Epoch: 1252 [33792/101520 (33%)] Loss: -1165.546875\n",
      "Train Epoch: 1252 [45056/101520 (44%)] Loss: -1161.320801\n",
      "Train Epoch: 1252 [56320/101520 (55%)] Loss: -1169.585327\n",
      "Train Epoch: 1252 [67584/101520 (67%)] Loss: -1163.063477\n",
      "Train Epoch: 1252 [78848/101520 (78%)] Loss: -1171.042969\n",
      "Train Epoch: 1252 [90112/101520 (89%)] Loss: -1154.849121\n",
      "Train Epoch: 1252 [101376/101520 (100%)] Loss: -1164.597168\n",
      "    epoch          : 1252\n",
      "    loss           : -1164.1527369513583\n",
      "    ess            : 1.9675276315391963\n",
      "    log_marginal   : 1164.1830171364636\n",
      "    log_joint      : 1372.6303385825613\n",
      "    val_loss       : -1163.7039688773777\n",
      "    val_ess        : 1.9645899430565212\n",
      "    val_log_marginal: 1163.7364183508832\n",
      "    val_log_joint  : 1371.8866922129755\n",
      "Train Epoch: 1253 [0/101520 (0%)] Loss: -1166.359619\n",
      "Train Epoch: 1253 [11264/101520 (11%)] Loss: -1165.595459\n",
      "Train Epoch: 1253 [22528/101520 (22%)] Loss: -1168.832031\n",
      "Train Epoch: 1253 [33792/101520 (33%)] Loss: -1174.605225\n",
      "Train Epoch: 1253 [45056/101520 (44%)] Loss: -1160.964111\n",
      "Train Epoch: 1253 [56320/101520 (55%)] Loss: -1165.766602\n",
      "Train Epoch: 1253 [67584/101520 (67%)] Loss: -1163.538086\n",
      "Train Epoch: 1253 [78848/101520 (78%)] Loss: -1160.857910\n",
      "Train Epoch: 1253 [90112/101520 (89%)] Loss: -1164.872559\n",
      "Train Epoch: 1253 [101376/101520 (100%)] Loss: -1165.293457\n",
      "    epoch          : 1253\n",
      "    loss           : -1164.331165102858\n",
      "    ess            : 1.9677541028315098\n",
      "    log_marginal   : 1164.3602810193545\n",
      "    log_joint      : 1372.7553115921403\n",
      "    val_loss       : -1160.991014563519\n",
      "    val_ess        : 1.9663687633431477\n",
      "    val_log_marginal: 1161.0225777004075\n",
      "    val_log_joint  : 1369.245111880095\n",
      "Train Epoch: 1254 [0/101520 (0%)] Loss: -1171.837891\n",
      "Train Epoch: 1254 [11264/101520 (11%)] Loss: -1164.651733\n",
      "Train Epoch: 1254 [22528/101520 (22%)] Loss: -1170.877197\n",
      "Train Epoch: 1254 [33792/101520 (33%)] Loss: -1159.953369\n",
      "Train Epoch: 1254 [45056/101520 (44%)] Loss: -1167.262451\n",
      "Train Epoch: 1254 [56320/101520 (55%)] Loss: -1158.703125\n",
      "Train Epoch: 1254 [67584/101520 (67%)] Loss: -1164.446045\n",
      "Train Epoch: 1254 [78848/101520 (78%)] Loss: -1161.249023\n",
      "Train Epoch: 1254 [90112/101520 (89%)] Loss: -1159.912231\n",
      "Train Epoch: 1254 [101376/101520 (100%)] Loss: -1171.720215\n",
      "    epoch          : 1254\n",
      "    loss           : -1164.3573801625314\n",
      "    ess            : 1.9673878559515106\n",
      "    log_marginal   : 1164.3870493826555\n",
      "    log_joint      : 1372.8167927037532\n",
      "    val_loss       : -1163.2166217306385\n",
      "    val_ess        : 1.9690723160038823\n",
      "    val_log_marginal: 1163.2462105129075\n",
      "    val_log_joint  : 1371.4795664911685\n",
      "Train Epoch: 1255 [0/101520 (0%)] Loss: -1168.516846\n",
      "Train Epoch: 1255 [11264/101520 (11%)] Loss: -1166.164307\n",
      "Train Epoch: 1255 [22528/101520 (22%)] Loss: -1164.052856\n",
      "Train Epoch: 1255 [33792/101520 (33%)] Loss: -1165.557129\n",
      "Train Epoch: 1255 [45056/101520 (44%)] Loss: -1163.058716\n",
      "Train Epoch: 1255 [56320/101520 (55%)] Loss: -1163.505493\n",
      "Train Epoch: 1255 [67584/101520 (67%)] Loss: -1167.708740\n",
      "Train Epoch: 1255 [78848/101520 (78%)] Loss: -1161.993896\n",
      "Train Epoch: 1255 [90112/101520 (89%)] Loss: -1161.662109\n",
      "Train Epoch: 1255 [101376/101520 (100%)] Loss: -1156.994995\n",
      "    epoch          : 1255\n",
      "    loss           : -1164.365189595438\n",
      "    ess            : 1.9667208661985158\n",
      "    log_marginal   : 1164.3957985729428\n",
      "    log_joint      : 1372.8379035067917\n",
      "    val_loss       : -1162.5072870669158\n",
      "    val_ess        : 1.9698157984277476\n",
      "    val_log_marginal: 1162.535543690557\n",
      "    val_log_joint  : 1371.092964504076\n",
      "Train Epoch: 1256 [0/101520 (0%)] Loss: -1171.536133\n",
      "Train Epoch: 1256 [11264/101520 (11%)] Loss: -1159.281494\n",
      "Train Epoch: 1256 [22528/101520 (22%)] Loss: -1165.968872\n",
      "Train Epoch: 1256 [33792/101520 (33%)] Loss: -1164.198730\n",
      "Train Epoch: 1256 [45056/101520 (44%)] Loss: -1163.135254\n",
      "Train Epoch: 1256 [56320/101520 (55%)] Loss: -1164.273438\n",
      "Train Epoch: 1256 [67584/101520 (67%)] Loss: -1170.306641\n",
      "Train Epoch: 1256 [78848/101520 (78%)] Loss: -1163.309814\n",
      "Train Epoch: 1256 [90112/101520 (89%)] Loss: -1177.705811\n",
      "Train Epoch: 1256 [101376/101520 (100%)] Loss: -1168.161499\n",
      "    epoch          : 1256\n",
      "    loss           : -1164.498745558849\n",
      "    ess            : 1.9675327359731474\n",
      "    log_marginal   : 1164.528739277442\n",
      "    log_joint      : 1372.9103568379005\n",
      "    val_loss       : -1164.3600437330163\n",
      "    val_ess        : 1.966055507245271\n",
      "    val_log_marginal: 1164.3901738705842\n",
      "    val_log_joint  : 1373.0306024966033\n",
      "Train Epoch: 1257 [0/101520 (0%)] Loss: -1158.919678\n",
      "Train Epoch: 1257 [11264/101520 (11%)] Loss: -1168.549194\n",
      "Train Epoch: 1257 [22528/101520 (22%)] Loss: -1167.278320\n",
      "Train Epoch: 1257 [33792/101520 (33%)] Loss: -1166.827148\n",
      "Train Epoch: 1257 [45056/101520 (44%)] Loss: -1165.399170\n",
      "Train Epoch: 1257 [56320/101520 (55%)] Loss: -1161.662109\n",
      "Train Epoch: 1257 [67584/101520 (67%)] Loss: -1170.027832\n",
      "Train Epoch: 1257 [78848/101520 (78%)] Loss: -1165.459961\n",
      "Train Epoch: 1257 [90112/101520 (89%)] Loss: -1163.736084\n",
      "Train Epoch: 1257 [101376/101520 (100%)] Loss: -1163.708984\n",
      "    epoch          : 1257\n",
      "    loss           : -1164.4907913591394\n",
      "    ess            : 1.9678187987313198\n",
      "    log_marginal   : 1164.5206047326476\n",
      "    log_joint      : 1373.0102766027403\n",
      "    val_loss       : -1162.8982570482337\n",
      "    val_ess        : 1.9678729824397876\n",
      "    val_log_marginal: 1162.9271134086277\n",
      "    val_log_joint  : 1371.2318062160325\n",
      "Train Epoch: 1258 [0/101520 (0%)] Loss: -1166.993164\n",
      "Train Epoch: 1258 [11264/101520 (11%)] Loss: -1163.538574\n",
      "Train Epoch: 1258 [22528/101520 (22%)] Loss: -1165.300049\n",
      "Train Epoch: 1258 [33792/101520 (33%)] Loss: -1166.879395\n",
      "Train Epoch: 1258 [45056/101520 (44%)] Loss: -1157.953857\n",
      "Train Epoch: 1258 [56320/101520 (55%)] Loss: -1167.398926\n",
      "Train Epoch: 1258 [67584/101520 (67%)] Loss: -1162.621094\n",
      "Train Epoch: 1258 [78848/101520 (78%)] Loss: -1164.849121\n",
      "Train Epoch: 1258 [90112/101520 (89%)] Loss: -1160.663818\n",
      "Train Epoch: 1258 [101376/101520 (100%)] Loss: -1179.042725\n",
      "    epoch          : 1258\n",
      "    loss           : -1164.7001217022614\n",
      "    ess            : 1.9671126586108951\n",
      "    log_marginal   : 1164.730618424152\n",
      "    log_joint      : 1373.034767955991\n",
      "    val_loss       : -1162.5543319038723\n",
      "    val_ess        : 1.969636284786722\n",
      "    val_log_marginal: 1162.5839472231658\n",
      "    val_log_joint  : 1370.949314283288\n",
      "Train Epoch: 1259 [0/101520 (0%)] Loss: -1161.266846\n",
      "Train Epoch: 1259 [11264/101520 (11%)] Loss: -1164.948975\n",
      "Train Epoch: 1259 [22528/101520 (22%)] Loss: -1166.739990\n",
      "Train Epoch: 1259 [33792/101520 (33%)] Loss: -1159.781494\n",
      "Train Epoch: 1259 [45056/101520 (44%)] Loss: -1158.660278\n",
      "Train Epoch: 1259 [56320/101520 (55%)] Loss: -1167.887695\n",
      "Train Epoch: 1259 [67584/101520 (67%)] Loss: -1167.103271\n",
      "Train Epoch: 1259 [78848/101520 (78%)] Loss: -1169.269287\n",
      "Train Epoch: 1259 [90112/101520 (89%)] Loss: -1164.413818\n",
      "Train Epoch: 1259 [101376/101520 (100%)] Loss: -1167.842651\n",
      "    epoch          : 1259\n",
      "    loss           : -1164.674794259383\n",
      "    ess            : 1.9670708844410114\n",
      "    log_marginal   : 1164.7051143167007\n",
      "    log_joint      : 1373.0864730144865\n",
      "    val_loss       : -1164.582222316576\n",
      "    val_ess        : 1.9681152934613435\n",
      "    val_log_marginal: 1164.6140242866848\n",
      "    val_log_joint  : 1373.1339695142663\n",
      "Train Epoch: 1260 [0/101520 (0%)] Loss: -1167.986206\n",
      "Train Epoch: 1260 [11264/101520 (11%)] Loss: -1161.204102\n",
      "Train Epoch: 1260 [22528/101520 (22%)] Loss: -1172.765381\n",
      "Train Epoch: 1260 [33792/101520 (33%)] Loss: -1166.360596\n",
      "Train Epoch: 1260 [45056/101520 (44%)] Loss: -1166.416260\n",
      "Train Epoch: 1260 [56320/101520 (55%)] Loss: -1174.698242\n",
      "Train Epoch: 1260 [67584/101520 (67%)] Loss: -1168.594238\n",
      "Train Epoch: 1260 [78848/101520 (78%)] Loss: -1166.470703\n",
      "Train Epoch: 1260 [90112/101520 (89%)] Loss: -1164.183960\n",
      "Train Epoch: 1260 [101376/101520 (100%)] Loss: -1170.845703\n",
      "    epoch          : 1260\n",
      "    loss           : -1164.7275862957365\n",
      "    ess            : 1.967335656659687\n",
      "    log_marginal   : 1164.757910646985\n",
      "    log_joint      : 1373.1203748233354\n",
      "    val_loss       : -1162.5778171705163\n",
      "    val_ess        : 1.969537766083427\n",
      "    val_log_marginal: 1162.6057022758152\n",
      "    val_log_joint  : 1371.2141431725543\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1260.pth ...\n",
      "Train Epoch: 1261 [0/101520 (0%)] Loss: -1162.643066\n",
      "Train Epoch: 1261 [11264/101520 (11%)] Loss: -1162.934204\n",
      "Train Epoch: 1261 [22528/101520 (22%)] Loss: -1173.527100\n",
      "Train Epoch: 1261 [33792/101520 (33%)] Loss: -1163.611450\n",
      "Train Epoch: 1261 [45056/101520 (44%)] Loss: -1171.947510\n",
      "Train Epoch: 1261 [56320/101520 (55%)] Loss: -1160.917725\n",
      "Train Epoch: 1261 [67584/101520 (67%)] Loss: -1167.338257\n",
      "Train Epoch: 1261 [78848/101520 (78%)] Loss: -1165.640381\n",
      "Train Epoch: 1261 [90112/101520 (89%)] Loss: -1166.649780\n",
      "Train Epoch: 1261 [101376/101520 (100%)] Loss: -1147.461060\n",
      "    epoch          : 1261\n",
      "    loss           : -1164.571822123312\n",
      "    ess            : 1.9671453830584809\n",
      "    log_marginal   : 1164.6023022828988\n",
      "    log_joint      : 1372.9800215678\n",
      "    val_loss       : -1165.47681194803\n",
      "    val_ess        : 1.966784321743509\n",
      "    val_log_marginal: 1165.5094843325408\n",
      "    val_log_joint  : 1373.9734576681385\n",
      "Train Epoch: 1262 [0/101520 (0%)] Loss: -1165.586670\n",
      "Train Epoch: 1262 [11264/101520 (11%)] Loss: -1166.058350\n",
      "Train Epoch: 1262 [22528/101520 (22%)] Loss: -1173.115234\n",
      "Train Epoch: 1262 [33792/101520 (33%)] Loss: -1159.934570\n",
      "Train Epoch: 1262 [45056/101520 (44%)] Loss: -1164.635498\n",
      "Train Epoch: 1262 [56320/101520 (55%)] Loss: -1163.354736\n",
      "Train Epoch: 1262 [67584/101520 (67%)] Loss: -1170.168945\n",
      "Train Epoch: 1262 [78848/101520 (78%)] Loss: -1168.602661\n",
      "Train Epoch: 1262 [90112/101520 (89%)] Loss: -1166.565186\n",
      "Train Epoch: 1262 [101376/101520 (100%)] Loss: -1167.956055\n",
      "    epoch          : 1262\n",
      "    loss           : -1164.7690748665202\n",
      "    ess            : 1.9675468985159792\n",
      "    log_marginal   : 1164.799366093161\n",
      "    log_joint      : 1373.2099511228016\n",
      "    val_loss       : -1164.1882748811142\n",
      "    val_ess        : 1.9654299124427463\n",
      "    val_log_marginal: 1164.2255753226902\n",
      "    val_log_joint  : 1372.798923658288\n",
      "Train Epoch: 1263 [0/101520 (0%)] Loss: -1169.543579\n",
      "Train Epoch: 1263 [11264/101520 (11%)] Loss: -1163.189331\n",
      "Train Epoch: 1263 [22528/101520 (22%)] Loss: -1168.512207\n",
      "Train Epoch: 1263 [33792/101520 (33%)] Loss: -1163.708374\n",
      "Train Epoch: 1263 [45056/101520 (44%)] Loss: -1163.226196\n",
      "Train Epoch: 1263 [56320/101520 (55%)] Loss: -1161.123901\n",
      "Train Epoch: 1263 [67584/101520 (67%)] Loss: -1162.952393\n",
      "Train Epoch: 1263 [78848/101520 (78%)] Loss: -1171.589111\n",
      "Train Epoch: 1263 [90112/101520 (89%)] Loss: -1160.095337\n",
      "Train Epoch: 1263 [101376/101520 (100%)] Loss: -1161.182739\n",
      "    epoch          : 1263\n",
      "    loss           : -1164.744746682632\n",
      "    ess            : 1.9664826650715352\n",
      "    log_marginal   : 1164.776003430237\n",
      "    log_joint      : 1373.1788563177215\n",
      "    val_loss       : -1163.8863949983017\n",
      "    val_ess        : 1.9682597699372664\n",
      "    val_log_marginal: 1163.9139881963315\n",
      "    val_log_joint  : 1372.0649573284647\n",
      "Train Epoch: 1264 [0/101520 (0%)] Loss: -1162.589355\n",
      "Train Epoch: 1264 [11264/101520 (11%)] Loss: -1164.093384\n",
      "Train Epoch: 1264 [22528/101520 (22%)] Loss: -1167.699341\n",
      "Train Epoch: 1264 [33792/101520 (33%)] Loss: -1156.506592\n",
      "Train Epoch: 1264 [45056/101520 (44%)] Loss: -1169.145996\n",
      "Train Epoch: 1264 [56320/101520 (55%)] Loss: -1161.604736\n",
      "Train Epoch: 1264 [67584/101520 (67%)] Loss: -1165.924316\n",
      "Train Epoch: 1264 [78848/101520 (78%)] Loss: -1156.099609\n",
      "Train Epoch: 1264 [90112/101520 (89%)] Loss: -1165.474609\n",
      "Train Epoch: 1264 [101376/101520 (100%)] Loss: -1179.832886\n",
      "    epoch          : 1264\n",
      "    loss           : -1164.8992312637406\n",
      "    ess            : 1.9667733680063755\n",
      "    log_marginal   : 1164.9313578389997\n",
      "    log_joint      : 1373.3807158350346\n",
      "    val_loss       : -1163.659471594769\n",
      "    val_ess        : 1.9658614189728447\n",
      "    val_log_marginal: 1163.6905517578125\n",
      "    val_log_joint  : 1372.2169614045517\n",
      "Train Epoch: 1265 [0/101520 (0%)] Loss: -1166.143188\n",
      "Train Epoch: 1265 [11264/101520 (11%)] Loss: -1162.909668\n",
      "Train Epoch: 1265 [22528/101520 (22%)] Loss: -1168.920166\n",
      "Train Epoch: 1265 [33792/101520 (33%)] Loss: -1167.529053\n",
      "Train Epoch: 1265 [45056/101520 (44%)] Loss: -1162.174683\n",
      "Train Epoch: 1265 [56320/101520 (55%)] Loss: -1160.784424\n",
      "Train Epoch: 1265 [67584/101520 (67%)] Loss: -1158.468506\n",
      "Train Epoch: 1265 [78848/101520 (78%)] Loss: -1162.118164\n",
      "Train Epoch: 1265 [90112/101520 (89%)] Loss: -1169.044067\n",
      "Train Epoch: 1265 [101376/101520 (100%)] Loss: -1176.502563\n",
      "    epoch          : 1265\n",
      "    loss           : -1164.896878189777\n",
      "    ess            : 1.9671083904390958\n",
      "    log_marginal   : 1164.926369518491\n",
      "    log_joint      : 1373.348429770925\n",
      "    val_loss       : -1163.8623153023098\n",
      "    val_ess        : 1.9646607378254766\n",
      "    val_log_marginal: 1163.8963304602582\n",
      "    val_log_joint  : 1372.343267026155\n",
      "Train Epoch: 1266 [0/101520 (0%)] Loss: -1164.470215\n",
      "Train Epoch: 1266 [11264/101520 (11%)] Loss: -1163.863525\n",
      "Train Epoch: 1266 [22528/101520 (22%)] Loss: -1164.535645\n",
      "Train Epoch: 1266 [33792/101520 (33%)] Loss: -1160.699707\n",
      "Train Epoch: 1266 [45056/101520 (44%)] Loss: -1165.610107\n",
      "Train Epoch: 1266 [56320/101520 (55%)] Loss: -1164.908447\n",
      "Train Epoch: 1266 [67584/101520 (67%)] Loss: -1166.601807\n",
      "Train Epoch: 1266 [78848/101520 (78%)] Loss: -1161.433228\n",
      "Train Epoch: 1266 [90112/101520 (89%)] Loss: -1162.202393\n",
      "Train Epoch: 1266 [101376/101520 (100%)] Loss: -1171.700195\n",
      "    epoch          : 1266\n",
      "    loss           : -1164.971750844064\n",
      "    ess            : 1.9663805721992225\n",
      "    log_marginal   : 1165.0031965246153\n",
      "    log_joint      : 1373.4192041751728\n",
      "    val_loss       : -1163.6367399796195\n",
      "    val_ess        : 1.9687042029007622\n",
      "    val_log_marginal: 1163.666456139606\n",
      "    val_log_joint  : 1372.0388024371603\n",
      "Train Epoch: 1267 [0/101520 (0%)] Loss: -1165.192139\n",
      "Train Epoch: 1267 [11264/101520 (11%)] Loss: -1170.026245\n",
      "Train Epoch: 1267 [22528/101520 (22%)] Loss: -1167.677246\n",
      "Train Epoch: 1267 [33792/101520 (33%)] Loss: -1166.082520\n",
      "Train Epoch: 1267 [45056/101520 (44%)] Loss: -1159.830322\n",
      "Train Epoch: 1267 [56320/101520 (55%)] Loss: -1164.987305\n",
      "Train Epoch: 1267 [67584/101520 (67%)] Loss: -1163.916748\n",
      "Train Epoch: 1267 [78848/101520 (78%)] Loss: -1162.908936\n",
      "Train Epoch: 1267 [90112/101520 (89%)] Loss: -1170.624146\n",
      "Train Epoch: 1267 [101376/101520 (100%)] Loss: -1171.489746\n",
      "    epoch          : 1267\n",
      "    loss           : -1165.0460413640467\n",
      "    ess            : 1.9667379886061702\n",
      "    log_marginal   : 1165.0780728594143\n",
      "    log_joint      : 1373.4495095104428\n",
      "    val_loss       : -1164.6914274796195\n",
      "    val_ess        : 1.9670781363611636\n",
      "    val_log_marginal: 1164.722268809443\n",
      "    val_log_joint  : 1373.269971764606\n",
      "Train Epoch: 1268 [0/101520 (0%)] Loss: -1164.619629\n",
      "Train Epoch: 1268 [11264/101520 (11%)] Loss: -1161.344238\n",
      "Train Epoch: 1268 [22528/101520 (22%)] Loss: -1159.166626\n",
      "Train Epoch: 1268 [33792/101520 (33%)] Loss: -1162.820068\n",
      "Train Epoch: 1268 [45056/101520 (44%)] Loss: -1171.309570\n",
      "Train Epoch: 1268 [56320/101520 (55%)] Loss: -1174.567993\n",
      "Train Epoch: 1268 [67584/101520 (67%)] Loss: -1166.625244\n",
      "Train Epoch: 1268 [78848/101520 (78%)] Loss: -1155.414062\n",
      "Train Epoch: 1268 [90112/101520 (89%)] Loss: -1157.125000\n",
      "Train Epoch: 1268 [101376/101520 (100%)] Loss: -1176.162842\n",
      "    epoch          : 1268\n",
      "    loss           : -1165.1512776283762\n",
      "    ess            : 1.9682128968550332\n",
      "    log_marginal   : 1165.1805370848383\n",
      "    log_joint      : 1373.505720128965\n",
      "    val_loss       : -1162.7694463315217\n",
      "    val_ess        : 1.9697152687155681\n",
      "    val_log_marginal: 1162.7976498811142\n",
      "    val_log_joint  : 1371.2842221467392\n",
      "Train Epoch: 1269 [0/101520 (0%)] Loss: -1168.133301\n",
      "Train Epoch: 1269 [11264/101520 (11%)] Loss: -1168.027588\n",
      "Train Epoch: 1269 [22528/101520 (22%)] Loss: -1174.269287\n",
      "Train Epoch: 1269 [33792/101520 (33%)] Loss: -1161.707520\n",
      "Train Epoch: 1269 [45056/101520 (44%)] Loss: -1162.668213\n",
      "Train Epoch: 1269 [56320/101520 (55%)] Loss: -1163.892822\n",
      "Train Epoch: 1269 [67584/101520 (67%)] Loss: -1168.204102\n",
      "Train Epoch: 1269 [78848/101520 (78%)] Loss: -1163.540771\n",
      "Train Epoch: 1269 [90112/101520 (89%)] Loss: -1172.141846\n",
      "Train Epoch: 1269 [101376/101520 (100%)] Loss: -1182.699951\n",
      "    epoch          : 1269\n",
      "    loss           : -1165.202520782624\n",
      "    ess            : 1.9679329053840446\n",
      "    log_marginal   : 1165.232587498037\n",
      "    log_joint      : 1373.647172017313\n",
      "    val_loss       : -1165.5124087126358\n",
      "    val_ess        : 1.967897891998291\n",
      "    val_log_marginal: 1165.5439771569293\n",
      "    val_log_joint  : 1373.9176078464675\n",
      "Train Epoch: 1270 [0/101520 (0%)] Loss: -1160.598877\n",
      "Train Epoch: 1270 [11264/101520 (11%)] Loss: -1162.147461\n",
      "Train Epoch: 1270 [22528/101520 (22%)] Loss: -1167.802002\n",
      "Train Epoch: 1270 [33792/101520 (33%)] Loss: -1163.101318\n",
      "Train Epoch: 1270 [45056/101520 (44%)] Loss: -1164.044067\n",
      "Train Epoch: 1270 [56320/101520 (55%)] Loss: -1161.259644\n",
      "Train Epoch: 1270 [67584/101520 (67%)] Loss: -1168.599731\n",
      "Train Epoch: 1270 [78848/101520 (78%)] Loss: -1157.496582\n",
      "Train Epoch: 1270 [90112/101520 (89%)] Loss: -1165.717896\n",
      "Train Epoch: 1270 [101376/101520 (100%)] Loss: -1181.177002\n",
      "    epoch          : 1270\n",
      "    loss           : -1165.1593318153266\n",
      "    ess            : 1.9668173328715952\n",
      "    log_marginal   : 1165.1899831187186\n",
      "    log_joint      : 1373.6711505525675\n",
      "    val_loss       : -1162.82788616678\n",
      "    val_ess        : 1.9668954869975215\n",
      "    val_log_marginal: 1162.8565249235733\n",
      "    val_log_joint  : 1371.2153426460598\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1270.pth ...\n",
      "Train Epoch: 1271 [0/101520 (0%)] Loss: -1162.147705\n",
      "Train Epoch: 1271 [11264/101520 (11%)] Loss: -1167.984863\n",
      "Train Epoch: 1271 [22528/101520 (22%)] Loss: -1169.846802\n",
      "Train Epoch: 1271 [33792/101520 (33%)] Loss: -1166.053467\n",
      "Train Epoch: 1271 [45056/101520 (44%)] Loss: -1164.082520\n",
      "Train Epoch: 1271 [56320/101520 (55%)] Loss: -1157.402344\n",
      "Train Epoch: 1271 [67584/101520 (67%)] Loss: -1159.182983\n",
      "Train Epoch: 1271 [78848/101520 (78%)] Loss: -1172.556030\n",
      "Train Epoch: 1271 [90112/101520 (89%)] Loss: -1164.504272\n",
      "Train Epoch: 1271 [101376/101520 (100%)] Loss: -1171.970093\n",
      "    epoch          : 1271\n",
      "    loss           : -1165.3062670530387\n",
      "    ess            : 1.966646307077839\n",
      "    log_marginal   : 1165.3375354555983\n",
      "    log_joint      : 1373.7413201260208\n",
      "    val_loss       : -1164.1914911684783\n",
      "    val_ess        : 1.967068024303602\n",
      "    val_log_marginal: 1164.2239671790082\n",
      "    val_log_joint  : 1372.620260487432\n",
      "Train Epoch: 1272 [0/101520 (0%)] Loss: -1171.935059\n",
      "Train Epoch: 1272 [11264/101520 (11%)] Loss: -1164.047119\n",
      "Train Epoch: 1272 [22528/101520 (22%)] Loss: -1166.040161\n",
      "Train Epoch: 1272 [33792/101520 (33%)] Loss: -1168.479492\n",
      "Train Epoch: 1272 [45056/101520 (44%)] Loss: -1157.514648\n",
      "Train Epoch: 1272 [56320/101520 (55%)] Loss: -1164.304688\n",
      "Train Epoch: 1272 [67584/101520 (67%)] Loss: -1160.020020\n",
      "Train Epoch: 1272 [78848/101520 (78%)] Loss: -1168.799561\n",
      "Train Epoch: 1272 [90112/101520 (89%)] Loss: -1161.999268\n",
      "Train Epoch: 1272 [101376/101520 (100%)] Loss: -1158.197876\n",
      "    epoch          : 1272\n",
      "    loss           : -1165.269915863497\n",
      "    ess            : 1.9674206960141358\n",
      "    log_marginal   : 1165.3005223873272\n",
      "    log_joint      : 1373.7107865990106\n",
      "    val_loss       : -1164.0855341372283\n",
      "    val_ess        : 1.968701544015304\n",
      "    val_log_marginal: 1164.1153458305027\n",
      "    val_log_joint  : 1372.4144180961277\n",
      "Train Epoch: 1273 [0/101520 (0%)] Loss: -1163.680420\n",
      "Train Epoch: 1273 [11264/101520 (11%)] Loss: -1157.229004\n",
      "Train Epoch: 1273 [22528/101520 (22%)] Loss: -1165.594482\n",
      "Train Epoch: 1273 [33792/101520 (33%)] Loss: -1167.073242\n",
      "Train Epoch: 1273 [45056/101520 (44%)] Loss: -1163.819824\n",
      "Train Epoch: 1273 [56320/101520 (55%)] Loss: -1159.105957\n",
      "Train Epoch: 1273 [67584/101520 (67%)] Loss: -1173.320679\n",
      "Train Epoch: 1273 [78848/101520 (78%)] Loss: -1162.590454\n",
      "Train Epoch: 1273 [90112/101520 (89%)] Loss: -1165.244141\n",
      "Train Epoch: 1273 [101376/101520 (100%)] Loss: -1176.449829\n",
      "    epoch          : 1273\n",
      "    loss           : -1165.3720801271984\n",
      "    ess            : 1.9672205879460627\n",
      "    log_marginal   : 1165.4024443506596\n",
      "    log_joint      : 1373.8147782368876\n",
      "    val_loss       : -1163.5818136463995\n",
      "    val_ess        : 1.9672597439392754\n",
      "    val_log_marginal: 1163.6126125169837\n",
      "    val_log_joint  : 1372.1834292204483\n",
      "Train Epoch: 1274 [0/101520 (0%)] Loss: -1167.905518\n",
      "Train Epoch: 1274 [11264/101520 (11%)] Loss: -1161.632080\n",
      "Train Epoch: 1274 [22528/101520 (22%)] Loss: -1164.141113\n",
      "Train Epoch: 1274 [33792/101520 (33%)] Loss: -1168.589966\n",
      "Train Epoch: 1274 [45056/101520 (44%)] Loss: -1165.887939\n",
      "Train Epoch: 1274 [56320/101520 (55%)] Loss: -1163.263794\n",
      "Train Epoch: 1274 [67584/101520 (67%)] Loss: -1169.351074\n",
      "Train Epoch: 1274 [78848/101520 (78%)] Loss: -1165.261475\n",
      "Train Epoch: 1274 [90112/101520 (89%)] Loss: -1166.315674\n",
      "Train Epoch: 1274 [101376/101520 (100%)] Loss: -1170.351929\n",
      "    epoch          : 1274\n",
      "    loss           : -1165.4623195322315\n",
      "    ess            : 1.9660504277627073\n",
      "    log_marginal   : 1165.4945105164495\n",
      "    log_joint      : 1373.8193690621074\n",
      "    val_loss       : -1165.487501061481\n",
      "    val_ess        : 1.9680389321368674\n",
      "    val_log_marginal: 1165.5170155400815\n",
      "    val_log_joint  : 1373.6930568529212\n",
      "Train Epoch: 1275 [0/101520 (0%)] Loss: -1162.358887\n",
      "Train Epoch: 1275 [11264/101520 (11%)] Loss: -1174.776367\n",
      "Train Epoch: 1275 [22528/101520 (22%)] Loss: -1167.949585\n",
      "Train Epoch: 1275 [33792/101520 (33%)] Loss: -1171.156982\n",
      "Train Epoch: 1275 [45056/101520 (44%)] Loss: -1167.034912\n",
      "Train Epoch: 1275 [56320/101520 (55%)] Loss: -1163.779297\n",
      "Train Epoch: 1275 [67584/101520 (67%)] Loss: -1167.593628\n",
      "Train Epoch: 1275 [78848/101520 (78%)] Loss: -1172.206787\n",
      "Train Epoch: 1275 [90112/101520 (89%)] Loss: -1156.403931\n",
      "Train Epoch: 1275 [101376/101520 (100%)] Loss: -1170.482178\n",
      "    epoch          : 1275\n",
      "    loss           : -1165.4309959219927\n",
      "    ess            : 1.9665343054574937\n",
      "    log_marginal   : 1165.4612515703518\n",
      "    log_joint      : 1373.8557177979742\n",
      "    val_loss       : -1164.8938731317935\n",
      "    val_ess        : 1.9708060544470083\n",
      "    val_log_marginal: 1164.9200067934783\n",
      "    val_log_joint  : 1373.326707922894\n",
      "Train Epoch: 1276 [0/101520 (0%)] Loss: -1161.723389\n",
      "Train Epoch: 1276 [11264/101520 (11%)] Loss: -1166.394531\n",
      "Train Epoch: 1276 [22528/101520 (22%)] Loss: -1166.645508\n",
      "Train Epoch: 1276 [33792/101520 (33%)] Loss: -1169.478394\n",
      "Train Epoch: 1276 [45056/101520 (44%)] Loss: -1164.849609\n",
      "Train Epoch: 1276 [56320/101520 (55%)] Loss: -1166.729492\n",
      "Train Epoch: 1276 [67584/101520 (67%)] Loss: -1163.282715\n",
      "Train Epoch: 1276 [78848/101520 (78%)] Loss: -1166.077637\n",
      "Train Epoch: 1276 [90112/101520 (89%)] Loss: -1168.292236\n",
      "Train Epoch: 1276 [101376/101520 (100%)] Loss: -1181.019287\n",
      "    epoch          : 1276\n",
      "    loss           : -1165.5079548131282\n",
      "    ess            : 1.9660621558002491\n",
      "    log_marginal   : 1165.5396605831893\n",
      "    log_joint      : 1373.9024547787767\n",
      "    val_loss       : -1164.1188115658967\n",
      "    val_ess        : 1.9697946828344595\n",
      "    val_log_marginal: 1164.144287109375\n",
      "    val_log_joint  : 1372.5954749065897\n",
      "Train Epoch: 1277 [0/101520 (0%)] Loss: -1168.111084\n",
      "Train Epoch: 1277 [11264/101520 (11%)] Loss: -1164.650635\n",
      "Train Epoch: 1277 [22528/101520 (22%)] Loss: -1164.039551\n",
      "Train Epoch: 1277 [33792/101520 (33%)] Loss: -1162.873779\n",
      "Train Epoch: 1277 [45056/101520 (44%)] Loss: -1164.269409\n",
      "Train Epoch: 1277 [56320/101520 (55%)] Loss: -1166.542603\n",
      "Train Epoch: 1277 [67584/101520 (67%)] Loss: -1169.601807\n",
      "Train Epoch: 1277 [78848/101520 (78%)] Loss: -1166.646729\n",
      "Train Epoch: 1277 [90112/101520 (89%)] Loss: -1164.423828\n",
      "Train Epoch: 1277 [101376/101520 (100%)] Loss: -1157.522095\n",
      "    epoch          : 1277\n",
      "    loss           : -1165.485515345281\n",
      "    ess            : 1.966892633605842\n",
      "    log_marginal   : 1165.5159863035883\n",
      "    log_joint      : 1373.9070278148556\n",
      "    val_loss       : -1163.8497208305027\n",
      "    val_ess        : 1.9676820765370908\n",
      "    val_log_marginal: 1163.8760667883832\n",
      "    val_log_joint  : 1372.1765826681385\n",
      "Train Epoch: 1278 [0/101520 (0%)] Loss: -1168.260254\n",
      "Train Epoch: 1278 [11264/101520 (11%)] Loss: -1167.208984\n",
      "Train Epoch: 1278 [22528/101520 (22%)] Loss: -1168.464722\n",
      "Train Epoch: 1278 [33792/101520 (33%)] Loss: -1166.491943\n",
      "Train Epoch: 1278 [45056/101520 (44%)] Loss: -1156.860474\n",
      "Train Epoch: 1278 [56320/101520 (55%)] Loss: -1169.361328\n",
      "Train Epoch: 1278 [67584/101520 (67%)] Loss: -1168.783325\n",
      "Train Epoch: 1278 [78848/101520 (78%)] Loss: -1161.225098\n",
      "Train Epoch: 1278 [90112/101520 (89%)] Loss: -1166.333862\n",
      "Train Epoch: 1278 [101376/101520 (100%)] Loss: -1173.612427\n",
      "    epoch          : 1278\n",
      "    loss           : -1165.6331823914495\n",
      "    ess            : 1.967573389935134\n",
      "    log_marginal   : 1165.6632810046326\n",
      "    log_joint      : 1374.0687967425015\n",
      "    val_loss       : -1163.2146102241848\n",
      "    val_ess        : 1.9696387985478276\n",
      "    val_log_marginal: 1163.240770422894\n",
      "    val_log_joint  : 1371.5501390540082\n",
      "Train Epoch: 1279 [0/101520 (0%)] Loss: -1164.738770\n",
      "Train Epoch: 1279 [11264/101520 (11%)] Loss: -1167.030396\n",
      "Train Epoch: 1279 [22528/101520 (22%)] Loss: -1162.887451\n",
      "Train Epoch: 1279 [33792/101520 (33%)] Loss: -1164.976562\n",
      "Train Epoch: 1279 [45056/101520 (44%)] Loss: -1169.541626\n",
      "Train Epoch: 1279 [56320/101520 (55%)] Loss: -1169.730469\n",
      "Train Epoch: 1279 [67584/101520 (67%)] Loss: -1164.094482\n",
      "Train Epoch: 1279 [78848/101520 (78%)] Loss: -1167.466309\n",
      "Train Epoch: 1279 [90112/101520 (89%)] Loss: -1165.613770\n",
      "Train Epoch: 1279 [101376/101520 (100%)] Loss: -1158.111084\n",
      "    epoch          : 1279\n",
      "    loss           : -1165.5980095791458\n",
      "    ess            : 1.9670923732632968\n",
      "    log_marginal   : 1165.6281038983982\n",
      "    log_joint      : 1374.0640654444096\n",
      "    val_loss       : -1163.4506942085598\n",
      "    val_ess        : 1.9666400577711023\n",
      "    val_log_marginal: 1163.4827084748642\n",
      "    val_log_joint  : 1371.9935674252717\n",
      "Train Epoch: 1280 [0/101520 (0%)] Loss: -1160.833252\n",
      "Train Epoch: 1280 [11264/101520 (11%)] Loss: -1165.447998\n",
      "Train Epoch: 1280 [22528/101520 (22%)] Loss: -1166.562500\n",
      "Train Epoch: 1280 [33792/101520 (33%)] Loss: -1162.372559\n",
      "Train Epoch: 1280 [45056/101520 (44%)] Loss: -1162.718506\n",
      "Train Epoch: 1280 [56320/101520 (55%)] Loss: -1165.953369\n",
      "Train Epoch: 1280 [67584/101520 (67%)] Loss: -1161.257935\n",
      "Train Epoch: 1280 [78848/101520 (78%)] Loss: -1176.749756\n",
      "Train Epoch: 1280 [90112/101520 (89%)] Loss: -1164.240967\n",
      "Train Epoch: 1280 [101376/101520 (100%)] Loss: -1175.327759\n",
      "    epoch          : 1280\n",
      "    loss           : -1165.765083351327\n",
      "    ess            : 1.9672992954302073\n",
      "    log_marginal   : 1165.796191038199\n",
      "    log_joint      : 1374.1886452526303\n",
      "    val_loss       : -1164.194771144701\n",
      "    val_ess        : 1.9687337719875833\n",
      "    val_log_marginal: 1164.221483313519\n",
      "    val_log_joint  : 1372.649759043818\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1280.pth ...\n",
      "Train Epoch: 1281 [0/101520 (0%)] Loss: -1160.504517\n",
      "Train Epoch: 1281 [11264/101520 (11%)] Loss: -1159.792725\n",
      "Train Epoch: 1281 [22528/101520 (22%)] Loss: -1166.728271\n",
      "Train Epoch: 1281 [33792/101520 (33%)] Loss: -1161.731079\n",
      "Train Epoch: 1281 [45056/101520 (44%)] Loss: -1164.454102\n",
      "Train Epoch: 1281 [56320/101520 (55%)] Loss: -1167.593750\n",
      "Train Epoch: 1281 [67584/101520 (67%)] Loss: -1164.289307\n",
      "Train Epoch: 1281 [78848/101520 (78%)] Loss: -1169.021973\n",
      "Train Epoch: 1281 [90112/101520 (89%)] Loss: -1160.290771\n",
      "Train Epoch: 1281 [101376/101520 (100%)] Loss: -1167.811035\n",
      "    epoch          : 1281\n",
      "    loss           : -1165.7805151244504\n",
      "    ess            : 1.9669100573314495\n",
      "    log_marginal   : 1165.811432651539\n",
      "    log_joint      : 1374.244796369543\n",
      "    val_loss       : -1163.7137557319973\n",
      "    val_ess        : 1.9659427093422932\n",
      "    val_log_marginal: 1163.743944251019\n",
      "    val_log_joint  : 1372.0134171195652\n",
      "Train Epoch: 1282 [0/101520 (0%)] Loss: -1160.621704\n",
      "Train Epoch: 1282 [11264/101520 (11%)] Loss: -1167.734619\n",
      "Train Epoch: 1282 [22528/101520 (22%)] Loss: -1166.605103\n",
      "Train Epoch: 1282 [33792/101520 (33%)] Loss: -1161.797485\n",
      "Train Epoch: 1282 [45056/101520 (44%)] Loss: -1166.315674\n",
      "Train Epoch: 1282 [56320/101520 (55%)] Loss: -1165.624878\n",
      "Train Epoch: 1282 [67584/101520 (67%)] Loss: -1162.863525\n",
      "Train Epoch: 1282 [78848/101520 (78%)] Loss: -1165.052124\n",
      "Train Epoch: 1282 [90112/101520 (89%)] Loss: -1169.207031\n",
      "Train Epoch: 1282 [101376/101520 (100%)] Loss: -1162.131592\n",
      "    epoch          : 1282\n",
      "    loss           : -1165.711382841944\n",
      "    ess            : 1.96672287897848\n",
      "    log_marginal   : 1165.742829135914\n",
      "    log_joint      : 1374.174082693742\n",
      "    val_loss       : -1164.7943433678668\n",
      "    val_ess        : 1.967812470767809\n",
      "    val_log_marginal: 1164.8236190132473\n",
      "    val_log_joint  : 1373.4105065387228\n",
      "Train Epoch: 1283 [0/101520 (0%)] Loss: -1166.610474\n",
      "Train Epoch: 1283 [11264/101520 (11%)] Loss: -1169.449219\n",
      "Train Epoch: 1283 [22528/101520 (22%)] Loss: -1172.808228\n",
      "Train Epoch: 1283 [33792/101520 (33%)] Loss: -1165.956543\n",
      "Train Epoch: 1283 [45056/101520 (44%)] Loss: -1172.614136\n",
      "Train Epoch: 1283 [56320/101520 (55%)] Loss: -1162.201416\n",
      "Train Epoch: 1283 [67584/101520 (67%)] Loss: -1170.594971\n",
      "Train Epoch: 1283 [78848/101520 (78%)] Loss: -1160.552002\n",
      "Train Epoch: 1283 [90112/101520 (89%)] Loss: -1165.394287\n",
      "Train Epoch: 1283 [101376/101520 (100%)] Loss: -1169.506470\n",
      "    epoch          : 1283\n",
      "    loss           : -1165.8184207168656\n",
      "    ess            : 1.9668166673363154\n",
      "    log_marginal   : 1165.8486610297582\n",
      "    log_joint      : 1374.2219140134266\n",
      "    val_loss       : -1165.638135827106\n",
      "    val_ess        : 1.9675584720528645\n",
      "    val_log_marginal: 1165.665575110394\n",
      "    val_log_joint  : 1374.4224216627038\n",
      "Train Epoch: 1284 [0/101520 (0%)] Loss: -1168.743164\n",
      "Train Epoch: 1284 [11264/101520 (11%)] Loss: -1168.663452\n",
      "Train Epoch: 1284 [22528/101520 (22%)] Loss: -1167.998291\n",
      "Train Epoch: 1284 [33792/101520 (33%)] Loss: -1163.541138\n",
      "Train Epoch: 1284 [45056/101520 (44%)] Loss: -1155.395264\n",
      "Train Epoch: 1284 [56320/101520 (55%)] Loss: -1171.137451\n",
      "Train Epoch: 1284 [67584/101520 (67%)] Loss: -1164.661133\n",
      "Train Epoch: 1284 [78848/101520 (78%)] Loss: -1177.163574\n",
      "Train Epoch: 1284 [90112/101520 (89%)] Loss: -1166.684570\n",
      "Train Epoch: 1284 [101376/101520 (100%)] Loss: -1159.578369\n",
      "    epoch          : 1284\n",
      "    loss           : -1165.8459871378377\n",
      "    ess            : 1.9663810502344639\n",
      "    log_marginal   : 1165.877257994072\n",
      "    log_joint      : 1374.285382601484\n",
      "    val_loss       : -1165.168504797894\n",
      "    val_ess        : 1.968627753465072\n",
      "    val_log_marginal: 1165.1986880095108\n",
      "    val_log_joint  : 1373.8344355044158\n",
      "Train Epoch: 1285 [0/101520 (0%)] Loss: -1173.465698\n",
      "Train Epoch: 1285 [11264/101520 (11%)] Loss: -1164.964966\n",
      "Train Epoch: 1285 [22528/101520 (22%)] Loss: -1166.918945\n",
      "Train Epoch: 1285 [33792/101520 (33%)] Loss: -1167.513428\n",
      "Train Epoch: 1285 [45056/101520 (44%)] Loss: -1168.142578\n",
      "Train Epoch: 1285 [56320/101520 (55%)] Loss: -1165.656982\n",
      "Train Epoch: 1285 [67584/101520 (67%)] Loss: -1164.023193\n",
      "Train Epoch: 1285 [78848/101520 (78%)] Loss: -1162.840820\n",
      "Train Epoch: 1285 [90112/101520 (89%)] Loss: -1165.617920\n",
      "Train Epoch: 1285 [101376/101520 (100%)] Loss: -1174.374390\n",
      "    epoch          : 1285\n",
      "    loss           : -1165.9003348039023\n",
      "    ess            : 1.966827493816165\n",
      "    log_marginal   : 1165.9310370210428\n",
      "    log_joint      : 1374.3670758578046\n",
      "    val_loss       : -1164.351854407269\n",
      "    val_ess        : 1.970204524371935\n",
      "    val_log_marginal: 1164.38037109375\n",
      "    val_log_joint  : 1373.0637366253397\n",
      "Train Epoch: 1286 [0/101520 (0%)] Loss: -1170.367188\n",
      "Train Epoch: 1286 [11264/101520 (11%)] Loss: -1160.242920\n",
      "Train Epoch: 1286 [22528/101520 (22%)] Loss: -1170.479736\n",
      "Train Epoch: 1286 [33792/101520 (33%)] Loss: -1162.008057\n",
      "Train Epoch: 1286 [45056/101520 (44%)] Loss: -1169.326172\n",
      "Train Epoch: 1286 [56320/101520 (55%)] Loss: -1166.907104\n",
      "Train Epoch: 1286 [67584/101520 (67%)] Loss: -1158.807739\n",
      "Train Epoch: 1286 [78848/101520 (78%)] Loss: -1166.436768\n",
      "Train Epoch: 1286 [90112/101520 (89%)] Loss: -1160.814453\n",
      "Train Epoch: 1286 [101376/101520 (100%)] Loss: -1161.193115\n",
      "    epoch          : 1286\n",
      "    loss           : -1165.955954700259\n",
      "    ess            : 1.9669798001572116\n",
      "    log_marginal   : 1165.986397441308\n",
      "    log_joint      : 1374.430535858001\n",
      "    val_loss       : -1162.954292629076\n",
      "    val_ess        : 1.9678695875665415\n",
      "    val_log_marginal: 1162.983738111413\n",
      "    val_log_joint  : 1371.3716881793478\n",
      "Train Epoch: 1287 [0/101520 (0%)] Loss: -1170.141724\n",
      "Train Epoch: 1287 [11264/101520 (11%)] Loss: -1164.474121\n",
      "Train Epoch: 1287 [22528/101520 (22%)] Loss: -1173.796387\n",
      "Train Epoch: 1287 [33792/101520 (33%)] Loss: -1164.871094\n",
      "Train Epoch: 1287 [45056/101520 (44%)] Loss: -1166.348755\n",
      "Train Epoch: 1287 [56320/101520 (55%)] Loss: -1169.119141\n",
      "Train Epoch: 1287 [67584/101520 (67%)] Loss: -1167.660889\n",
      "Train Epoch: 1287 [78848/101520 (78%)] Loss: -1164.885010\n",
      "Train Epoch: 1287 [90112/101520 (89%)] Loss: -1165.853882\n",
      "Train Epoch: 1287 [101376/101520 (100%)] Loss: -1159.488770\n",
      "    epoch          : 1287\n",
      "    loss           : -1166.0431061557788\n",
      "    ess            : 1.967026212107596\n",
      "    log_marginal   : 1166.0737924240343\n",
      "    log_joint      : 1374.4847013387248\n",
      "    val_loss       : -1164.5475490404212\n",
      "    val_ess        : 1.970170700031778\n",
      "    val_log_marginal: 1164.5752643087635\n",
      "    val_log_joint  : 1373.1093909222147\n",
      "Train Epoch: 1288 [0/101520 (0%)] Loss: -1159.684448\n",
      "Train Epoch: 1288 [11264/101520 (11%)] Loss: -1159.524414\n",
      "Train Epoch: 1288 [22528/101520 (22%)] Loss: -1162.477051\n",
      "Train Epoch: 1288 [33792/101520 (33%)] Loss: -1160.205566\n",
      "Train Epoch: 1288 [45056/101520 (44%)] Loss: -1168.244873\n",
      "Train Epoch: 1288 [56320/101520 (55%)] Loss: -1167.966309\n",
      "Train Epoch: 1288 [67584/101520 (67%)] Loss: -1173.136353\n",
      "Train Epoch: 1288 [78848/101520 (78%)] Loss: -1162.952759\n",
      "Train Epoch: 1288 [90112/101520 (89%)] Loss: -1167.628418\n",
      "Train Epoch: 1288 [101376/101520 (100%)] Loss: -1160.908691\n",
      "    epoch          : 1288\n",
      "    loss           : -1166.0850989566975\n",
      "    ess            : 1.9666738162687676\n",
      "    log_marginal   : 1166.1160661706972\n",
      "    log_joint      : 1374.5389796884815\n",
      "    val_loss       : -1166.6164816151495\n",
      "    val_ess        : 1.9671091618745222\n",
      "    val_log_marginal: 1166.6458368716033\n",
      "    val_log_joint  : 1375.0633969514267\n",
      "Train Epoch: 1289 [0/101520 (0%)] Loss: -1162.409058\n",
      "Train Epoch: 1289 [11264/101520 (11%)] Loss: -1172.535767\n",
      "Train Epoch: 1289 [22528/101520 (22%)] Loss: -1171.948120\n",
      "Train Epoch: 1289 [33792/101520 (33%)] Loss: -1165.842773\n",
      "Train Epoch: 1289 [45056/101520 (44%)] Loss: -1164.398926\n",
      "Train Epoch: 1289 [56320/101520 (55%)] Loss: -1166.369873\n",
      "Train Epoch: 1289 [67584/101520 (67%)] Loss: -1168.098511\n",
      "Train Epoch: 1289 [78848/101520 (78%)] Loss: -1168.243164\n",
      "Train Epoch: 1289 [90112/101520 (89%)] Loss: -1168.837402\n",
      "Train Epoch: 1289 [101376/101520 (100%)] Loss: -1153.774902\n",
      "    epoch          : 1289\n",
      "    loss           : -1166.170852431101\n",
      "    ess            : 1.967226651445705\n",
      "    log_marginal   : 1166.2015313383322\n",
      "    log_joint      : 1374.5158077987594\n",
      "    val_loss       : -1164.846000339674\n",
      "    val_ess        : 1.9677073489064756\n",
      "    val_log_marginal: 1164.8756421959918\n",
      "    val_log_joint  : 1373.2585555366848\n",
      "Train Epoch: 1290 [0/101520 (0%)] Loss: -1162.993652\n",
      "Train Epoch: 1290 [11264/101520 (11%)] Loss: -1164.786865\n",
      "Train Epoch: 1290 [22528/101520 (22%)] Loss: -1159.935059\n",
      "Train Epoch: 1290 [33792/101520 (33%)] Loss: -1169.806396\n",
      "Train Epoch: 1290 [45056/101520 (44%)] Loss: -1163.966309\n",
      "Train Epoch: 1290 [56320/101520 (55%)] Loss: -1167.536011\n",
      "Train Epoch: 1290 [67584/101520 (67%)] Loss: -1165.420654\n",
      "Train Epoch: 1290 [78848/101520 (78%)] Loss: -1167.587158\n",
      "Train Epoch: 1290 [90112/101520 (89%)] Loss: -1166.606323\n",
      "Train Epoch: 1290 [101376/101520 (100%)] Loss: -1162.759399\n",
      "    epoch          : 1290\n",
      "    loss           : -1166.1966503660883\n",
      "    ess            : 1.9665933188481546\n",
      "    log_marginal   : 1166.2283039955637\n",
      "    log_joint      : 1374.5949792909862\n",
      "    val_loss       : -1165.2628810716712\n",
      "    val_ess        : 1.9678041261175405\n",
      "    val_log_marginal: 1165.2915729025135\n",
      "    val_log_joint  : 1373.7247367527175\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1290.pth ...\n",
      "Train Epoch: 1291 [0/101520 (0%)] Loss: -1158.553467\n",
      "Train Epoch: 1291 [11264/101520 (11%)] Loss: -1162.987061\n",
      "Train Epoch: 1291 [22528/101520 (22%)] Loss: -1167.834473\n",
      "Train Epoch: 1291 [33792/101520 (33%)] Loss: -1163.735962\n",
      "Train Epoch: 1291 [45056/101520 (44%)] Loss: -1165.599854\n",
      "Train Epoch: 1291 [56320/101520 (55%)] Loss: -1167.450439\n",
      "Train Epoch: 1291 [67584/101520 (67%)] Loss: -1164.179565\n",
      "Train Epoch: 1291 [78848/101520 (78%)] Loss: -1163.790039\n",
      "Train Epoch: 1291 [90112/101520 (89%)] Loss: -1171.792236\n",
      "Train Epoch: 1291 [101376/101520 (100%)] Loss: -1160.320801\n",
      "    epoch          : 1291\n",
      "    loss           : -1166.2282046217415\n",
      "    ess            : 1.9665985784338946\n",
      "    log_marginal   : 1166.2594656632773\n",
      "    log_joint      : 1374.6802125863694\n",
      "    val_loss       : -1166.3664975373642\n",
      "    val_ess        : 1.9669737556706304\n",
      "    val_log_marginal: 1166.4020306131115\n",
      "    val_log_joint  : 1375.0249660326087\n",
      "Train Epoch: 1292 [0/101520 (0%)] Loss: -1164.110474\n",
      "Train Epoch: 1292 [11264/101520 (11%)] Loss: -1170.188232\n",
      "Train Epoch: 1292 [22528/101520 (22%)] Loss: -1164.928467\n",
      "Train Epoch: 1292 [33792/101520 (33%)] Loss: -1169.639160\n",
      "Train Epoch: 1292 [45056/101520 (44%)] Loss: -1165.094727\n",
      "Train Epoch: 1292 [56320/101520 (55%)] Loss: -1169.860840\n",
      "Train Epoch: 1292 [67584/101520 (67%)] Loss: -1169.007324\n",
      "Train Epoch: 1292 [78848/101520 (78%)] Loss: -1171.708008\n",
      "Train Epoch: 1292 [90112/101520 (89%)] Loss: -1170.451782\n",
      "Train Epoch: 1292 [101376/101520 (100%)] Loss: -1160.119751\n",
      "    epoch          : 1292\n",
      "    loss           : -1166.339089858472\n",
      "    ess            : 1.9671637113369889\n",
      "    log_marginal   : 1166.3690756026224\n",
      "    log_joint      : 1374.7086979084877\n",
      "    val_loss       : -1164.058540675951\n",
      "    val_ess        : 1.968828823255456\n",
      "    val_log_marginal: 1164.087598717731\n",
      "    val_log_joint  : 1372.4923095703125\n",
      "Train Epoch: 1293 [0/101520 (0%)] Loss: -1166.495605\n",
      "Train Epoch: 1293 [11264/101520 (11%)] Loss: -1168.644287\n",
      "Train Epoch: 1293 [22528/101520 (22%)] Loss: -1165.824707\n",
      "Train Epoch: 1293 [33792/101520 (33%)] Loss: -1161.601318\n",
      "Train Epoch: 1293 [45056/101520 (44%)] Loss: -1166.708740\n",
      "Train Epoch: 1293 [56320/101520 (55%)] Loss: -1161.573242\n",
      "Train Epoch: 1293 [67584/101520 (67%)] Loss: -1169.449463\n",
      "Train Epoch: 1293 [78848/101520 (78%)] Loss: -1167.008301\n",
      "Train Epoch: 1293 [90112/101520 (89%)] Loss: -1166.082520\n",
      "Train Epoch: 1293 [101376/101520 (100%)] Loss: -1163.743042\n",
      "    epoch          : 1293\n",
      "    loss           : -1166.2693472244032\n",
      "    ess            : 1.9670067450509\n",
      "    log_marginal   : 1166.2998359718515\n",
      "    log_joint      : 1374.753592793067\n",
      "    val_loss       : -1165.1415272588315\n",
      "    val_ess        : 1.9686918932458628\n",
      "    val_log_marginal: 1165.1682978091033\n",
      "    val_log_joint  : 1373.6989958389945\n",
      "Train Epoch: 1294 [0/101520 (0%)] Loss: -1167.234619\n",
      "Train Epoch: 1294 [11264/101520 (11%)] Loss: -1168.278809\n",
      "Train Epoch: 1294 [22528/101520 (22%)] Loss: -1160.561035\n",
      "Train Epoch: 1294 [33792/101520 (33%)] Loss: -1160.052002\n",
      "Train Epoch: 1294 [45056/101520 (44%)] Loss: -1162.663086\n",
      "Train Epoch: 1294 [56320/101520 (55%)] Loss: -1168.868042\n",
      "Train Epoch: 1294 [67584/101520 (67%)] Loss: -1161.680786\n",
      "Train Epoch: 1294 [78848/101520 (78%)] Loss: -1162.850342\n",
      "Train Epoch: 1294 [90112/101520 (89%)] Loss: -1163.986084\n",
      "Train Epoch: 1294 [101376/101520 (100%)] Loss: -1163.274048\n",
      "    epoch          : 1294\n",
      "    loss           : -1166.3407927086605\n",
      "    ess            : 1.967211770052886\n",
      "    log_marginal   : 1166.371416408213\n",
      "    log_joint      : 1374.7863493492855\n",
      "    val_loss       : -1163.905660878057\n",
      "    val_ess        : 1.962566904399706\n",
      "    val_log_marginal: 1163.939453125\n",
      "    val_log_joint  : 1372.4392355213995\n",
      "Train Epoch: 1295 [0/101520 (0%)] Loss: -1162.347778\n",
      "Train Epoch: 1295 [11264/101520 (11%)] Loss: -1161.983643\n",
      "Train Epoch: 1295 [22528/101520 (22%)] Loss: -1172.834473\n",
      "Train Epoch: 1295 [33792/101520 (33%)] Loss: -1172.519897\n",
      "Train Epoch: 1295 [45056/101520 (44%)] Loss: -1173.647217\n",
      "Train Epoch: 1295 [56320/101520 (55%)] Loss: -1171.663208\n",
      "Train Epoch: 1295 [67584/101520 (67%)] Loss: -1163.121826\n",
      "Train Epoch: 1295 [78848/101520 (78%)] Loss: -1167.846069\n",
      "Train Epoch: 1295 [90112/101520 (89%)] Loss: -1163.750122\n",
      "Train Epoch: 1295 [101376/101520 (100%)] Loss: -1178.209961\n",
      "    epoch          : 1295\n",
      "    loss           : -1166.4452033114792\n",
      "    ess            : 1.9660189918537236\n",
      "    log_marginal   : 1166.4764385894316\n",
      "    log_joint      : 1374.830621613929\n",
      "    val_loss       : -1166.7053647248642\n",
      "    val_ess        : 1.962609042292056\n",
      "    val_log_marginal: 1166.7405422044837\n",
      "    val_log_joint  : 1375.1249097741168\n",
      "Train Epoch: 1296 [0/101520 (0%)] Loss: -1164.064453\n",
      "Train Epoch: 1296 [11264/101520 (11%)] Loss: -1172.999023\n",
      "Train Epoch: 1296 [22528/101520 (22%)] Loss: -1174.164917\n",
      "Train Epoch: 1296 [33792/101520 (33%)] Loss: -1160.941406\n",
      "Train Epoch: 1296 [45056/101520 (44%)] Loss: -1162.522217\n",
      "Train Epoch: 1296 [56320/101520 (55%)] Loss: -1170.524292\n",
      "Train Epoch: 1296 [67584/101520 (67%)] Loss: -1164.626221\n",
      "Train Epoch: 1296 [78848/101520 (78%)] Loss: -1162.963013\n",
      "Train Epoch: 1296 [90112/101520 (89%)] Loss: -1169.458496\n",
      "Train Epoch: 1296 [101376/101520 (100%)] Loss: -1161.921021\n",
      "    epoch          : 1296\n",
      "    loss           : -1166.4162352288788\n",
      "    ess            : 1.9671431600148954\n",
      "    log_marginal   : 1166.4471134971734\n",
      "    log_joint      : 1374.8381893598853\n",
      "    val_loss       : -1164.2624352496603\n",
      "    val_ess        : 1.9671302826508232\n",
      "    val_log_marginal: 1164.2926609205163\n",
      "    val_log_joint  : 1372.6046673318615\n",
      "Train Epoch: 1297 [0/101520 (0%)] Loss: -1166.455200\n",
      "Train Epoch: 1297 [11264/101520 (11%)] Loss: -1174.731812\n",
      "Train Epoch: 1297 [22528/101520 (22%)] Loss: -1158.223877\n",
      "Train Epoch: 1297 [33792/101520 (33%)] Loss: -1164.881714\n",
      "Train Epoch: 1297 [45056/101520 (44%)] Loss: -1169.349365\n",
      "Train Epoch: 1297 [56320/101520 (55%)] Loss: -1156.916260\n",
      "Train Epoch: 1297 [67584/101520 (67%)] Loss: -1165.942627\n",
      "Train Epoch: 1297 [78848/101520 (78%)] Loss: -1169.169800\n",
      "Train Epoch: 1297 [90112/101520 (89%)] Loss: -1169.059326\n",
      "Train Epoch: 1297 [101376/101520 (100%)] Loss: -1189.336670\n",
      "    epoch          : 1297\n",
      "    loss           : -1166.5609560252435\n",
      "    ess            : 1.9662582766470598\n",
      "    log_marginal   : 1166.5927108687972\n",
      "    log_joint      : 1374.922377389879\n",
      "    val_loss       : -1163.0322902513587\n",
      "    val_ess        : 1.9649129691331282\n",
      "    val_log_marginal: 1163.0649679432745\n",
      "    val_log_joint  : 1371.2420495074728\n",
      "Train Epoch: 1298 [0/101520 (0%)] Loss: -1171.141113\n",
      "Train Epoch: 1298 [11264/101520 (11%)] Loss: -1165.968994\n",
      "Train Epoch: 1298 [22528/101520 (22%)] Loss: -1165.871582\n",
      "Train Epoch: 1298 [33792/101520 (33%)] Loss: -1163.843140\n",
      "Train Epoch: 1298 [45056/101520 (44%)] Loss: -1162.054077\n",
      "Train Epoch: 1298 [56320/101520 (55%)] Loss: -1164.307617\n",
      "Train Epoch: 1298 [67584/101520 (67%)] Loss: -1166.598877\n",
      "Train Epoch: 1298 [78848/101520 (78%)] Loss: -1164.541260\n",
      "Train Epoch: 1298 [90112/101520 (89%)] Loss: -1166.483032\n",
      "Train Epoch: 1298 [101376/101520 (100%)] Loss: -1172.875122\n",
      "    epoch          : 1298\n",
      "    loss           : -1166.4922635639134\n",
      "    ess            : 1.9661489467525004\n",
      "    log_marginal   : 1166.5247753660883\n",
      "    log_joint      : 1374.9369773673052\n",
      "    val_loss       : -1166.0021813434103\n",
      "    val_ess        : 1.970006621402243\n",
      "    val_log_marginal: 1166.0292384935462\n",
      "    val_log_joint  : 1374.4476477581522\n",
      "Train Epoch: 1299 [0/101520 (0%)] Loss: -1166.995850\n",
      "Train Epoch: 1299 [11264/101520 (11%)] Loss: -1168.540161\n",
      "Train Epoch: 1299 [22528/101520 (22%)] Loss: -1160.969482\n",
      "Train Epoch: 1299 [33792/101520 (33%)] Loss: -1167.737061\n",
      "Train Epoch: 1299 [45056/101520 (44%)] Loss: -1173.800781\n",
      "Train Epoch: 1299 [56320/101520 (55%)] Loss: -1160.041260\n",
      "Train Epoch: 1299 [67584/101520 (67%)] Loss: -1169.324585\n",
      "Train Epoch: 1299 [78848/101520 (78%)] Loss: -1173.395264\n",
      "Train Epoch: 1299 [90112/101520 (89%)] Loss: -1163.586426\n",
      "Train Epoch: 1299 [101376/101520 (100%)] Loss: -1156.482422\n",
      "    epoch          : 1299\n",
      "    loss           : -1166.458691774301\n",
      "    ess            : 1.967072015431658\n",
      "    log_marginal   : 1166.4895154483354\n",
      "    log_joint      : 1374.9388949140232\n",
      "    val_loss       : -1166.7443476137908\n",
      "    val_ess        : 1.9683161714802617\n",
      "    val_log_marginal: 1166.7726307744565\n",
      "    val_log_joint  : 1374.9651568868885\n",
      "Train Epoch: 1300 [0/101520 (0%)] Loss: -1163.697021\n",
      "Train Epoch: 1300 [11264/101520 (11%)] Loss: -1166.460693\n",
      "Train Epoch: 1300 [22528/101520 (22%)] Loss: -1167.798828\n",
      "Train Epoch: 1300 [33792/101520 (33%)] Loss: -1166.149536\n",
      "Train Epoch: 1300 [45056/101520 (44%)] Loss: -1169.208374\n",
      "Train Epoch: 1300 [56320/101520 (55%)] Loss: -1167.220581\n",
      "Train Epoch: 1300 [67584/101520 (67%)] Loss: -1166.114258\n",
      "Train Epoch: 1300 [78848/101520 (78%)] Loss: -1164.198120\n",
      "Train Epoch: 1300 [90112/101520 (89%)] Loss: -1159.589844\n",
      "Train Epoch: 1300 [101376/101520 (100%)] Loss: -1171.007812\n",
      "    epoch          : 1300\n",
      "    loss           : -1166.5701137523556\n",
      "    ess            : 1.967747321080922\n",
      "    log_marginal   : 1166.5999216050957\n",
      "    log_joint      : 1375.078684437814\n",
      "    val_loss       : -1164.7315036939538\n",
      "    val_ess        : 1.967110364333443\n",
      "    val_log_marginal: 1164.764892578125\n",
      "    val_log_joint  : 1373.2658107591712\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1300.pth ...\n",
      "Train Epoch: 1301 [0/101520 (0%)] Loss: -1162.798096\n",
      "Train Epoch: 1301 [11264/101520 (11%)] Loss: -1159.587769\n",
      "Train Epoch: 1301 [22528/101520 (22%)] Loss: -1167.528564\n",
      "Train Epoch: 1301 [33792/101520 (33%)] Loss: -1177.267578\n",
      "Train Epoch: 1301 [45056/101520 (44%)] Loss: -1165.431641\n",
      "Train Epoch: 1301 [56320/101520 (55%)] Loss: -1168.750000\n",
      "Train Epoch: 1301 [67584/101520 (67%)] Loss: -1174.355347\n",
      "Train Epoch: 1301 [78848/101520 (78%)] Loss: -1161.604370\n",
      "Train Epoch: 1301 [90112/101520 (89%)] Loss: -1165.321655\n",
      "Train Epoch: 1301 [101376/101520 (100%)] Loss: -1168.611084\n",
      "    epoch          : 1301\n",
      "    loss           : -1166.622729737555\n",
      "    ess            : 1.9670319467333692\n",
      "    log_marginal   : 1166.6519020885678\n",
      "    log_joint      : 1375.0443096831816\n",
      "    val_loss       : -1164.4013831097147\n",
      "    val_ess        : 1.9687430651291558\n",
      "    val_log_marginal: 1164.4304358440897\n",
      "    val_log_joint  : 1372.873582922894\n",
      "Train Epoch: 1302 [0/101520 (0%)] Loss: -1166.788696\n",
      "Train Epoch: 1302 [11264/101520 (11%)] Loss: -1165.898193\n",
      "Train Epoch: 1302 [22528/101520 (22%)] Loss: -1162.777100\n",
      "Train Epoch: 1302 [33792/101520 (33%)] Loss: -1169.252441\n",
      "Train Epoch: 1302 [45056/101520 (44%)] Loss: -1167.114258\n",
      "Train Epoch: 1302 [56320/101520 (55%)] Loss: -1172.167236\n",
      "Train Epoch: 1302 [67584/101520 (67%)] Loss: -1165.926025\n",
      "Train Epoch: 1302 [78848/101520 (78%)] Loss: -1167.909668\n",
      "Train Epoch: 1302 [90112/101520 (89%)] Loss: -1164.224365\n",
      "Train Epoch: 1302 [101376/101520 (100%)] Loss: -1167.622192\n",
      "    epoch          : 1302\n",
      "    loss           : -1166.681866976484\n",
      "    ess            : 1.9674677920700916\n",
      "    log_marginal   : 1166.7120097558104\n",
      "    log_joint      : 1375.1215918459484\n",
      "    val_loss       : -1167.0955810546875\n",
      "    val_ess        : 1.9686267583266548\n",
      "    val_log_marginal: 1167.1241879670517\n",
      "    val_log_joint  : 1375.5627653702445\n",
      "Train Epoch: 1303 [0/101520 (0%)] Loss: -1164.351807\n",
      "Train Epoch: 1303 [11264/101520 (11%)] Loss: -1167.727295\n",
      "Train Epoch: 1303 [22528/101520 (22%)] Loss: -1173.735718\n",
      "Train Epoch: 1303 [33792/101520 (33%)] Loss: -1165.469849\n",
      "Train Epoch: 1303 [45056/101520 (44%)] Loss: -1166.669189\n",
      "Train Epoch: 1303 [56320/101520 (55%)] Loss: -1174.219971\n",
      "Train Epoch: 1303 [67584/101520 (67%)] Loss: -1168.840088\n",
      "Train Epoch: 1303 [78848/101520 (78%)] Loss: -1169.236328\n",
      "Train Epoch: 1303 [90112/101520 (89%)] Loss: -1169.090576\n",
      "Train Epoch: 1303 [101376/101520 (100%)] Loss: -1171.925293\n",
      "    epoch          : 1303\n",
      "    loss           : -1166.6900812657036\n",
      "    ess            : 1.9672320691784422\n",
      "    log_marginal   : 1166.7199572079146\n",
      "    log_joint      : 1375.1419082718278\n",
      "    val_loss       : -1164.6218580163043\n",
      "    val_ess        : 1.9661998645119045\n",
      "    val_log_marginal: 1164.6502844769022\n",
      "    val_log_joint  : 1373.2401653787365\n",
      "Train Epoch: 1304 [0/101520 (0%)] Loss: -1167.408936\n",
      "Train Epoch: 1304 [11264/101520 (11%)] Loss: -1167.785767\n",
      "Train Epoch: 1304 [22528/101520 (22%)] Loss: -1169.295410\n",
      "Train Epoch: 1304 [33792/101520 (33%)] Loss: -1156.307495\n",
      "Train Epoch: 1304 [45056/101520 (44%)] Loss: -1161.798340\n",
      "Train Epoch: 1304 [56320/101520 (55%)] Loss: -1170.751709\n",
      "Train Epoch: 1304 [67584/101520 (67%)] Loss: -1163.226074\n",
      "Train Epoch: 1304 [78848/101520 (78%)] Loss: -1165.105225\n",
      "Train Epoch: 1304 [90112/101520 (89%)] Loss: -1169.140869\n",
      "Train Epoch: 1304 [101376/101520 (100%)] Loss: -1172.513184\n",
      "    epoch          : 1304\n",
      "    loss           : -1166.7926878042556\n",
      "    ess            : 1.9673603791088314\n",
      "    log_marginal   : 1166.8230563216473\n",
      "    log_joint      : 1375.2259858864636\n",
      "    val_loss       : -1163.8365319293478\n",
      "    val_ess        : 1.9666375180949336\n",
      "    val_log_marginal: 1163.8670495074728\n",
      "    val_log_joint  : 1372.2119352921195\n",
      "Train Epoch: 1305 [0/101520 (0%)] Loss: -1168.303345\n",
      "Train Epoch: 1305 [11264/101520 (11%)] Loss: -1159.637695\n",
      "Train Epoch: 1305 [22528/101520 (22%)] Loss: -1164.179688\n",
      "Train Epoch: 1305 [33792/101520 (33%)] Loss: -1163.851562\n",
      "Train Epoch: 1305 [45056/101520 (44%)] Loss: -1161.521973\n",
      "Train Epoch: 1305 [56320/101520 (55%)] Loss: -1169.431763\n",
      "Train Epoch: 1305 [67584/101520 (67%)] Loss: -1170.459961\n",
      "Train Epoch: 1305 [78848/101520 (78%)] Loss: -1162.202148\n",
      "Train Epoch: 1305 [90112/101520 (89%)] Loss: -1163.937012\n",
      "Train Epoch: 1305 [101376/101520 (100%)] Loss: -1166.801270\n",
      "    epoch          : 1305\n",
      "    loss           : -1166.772699557357\n",
      "    ess            : 1.9674472431441647\n",
      "    log_marginal   : 1166.802609851013\n",
      "    log_joint      : 1375.2176795844457\n",
      "    val_loss       : -1163.0316586701767\n",
      "    val_ess        : 1.9672486108282339\n",
      "    val_log_marginal: 1163.0610988451087\n",
      "    val_log_joint  : 1371.3434209408967\n",
      "Train Epoch: 1306 [0/101520 (0%)] Loss: -1165.510498\n",
      "Train Epoch: 1306 [11264/101520 (11%)] Loss: -1171.833740\n",
      "Train Epoch: 1306 [22528/101520 (22%)] Loss: -1174.854370\n",
      "Train Epoch: 1306 [33792/101520 (33%)] Loss: -1159.201904\n",
      "Train Epoch: 1306 [45056/101520 (44%)] Loss: -1167.777588\n",
      "Train Epoch: 1306 [56320/101520 (55%)] Loss: -1167.450562\n",
      "Train Epoch: 1306 [67584/101520 (67%)] Loss: -1163.786621\n",
      "Train Epoch: 1306 [78848/101520 (78%)] Loss: -1168.234863\n",
      "Train Epoch: 1306 [90112/101520 (89%)] Loss: -1169.554688\n",
      "Train Epoch: 1306 [101376/101520 (100%)] Loss: -1173.190186\n",
      "    epoch          : 1306\n",
      "    loss           : -1166.8225569988615\n",
      "    ess            : 1.966855691305956\n",
      "    log_marginal   : 1166.8534322000628\n",
      "    log_joint      : 1375.2661439521828\n",
      "    val_loss       : -1165.6977432914402\n",
      "    val_ess        : 1.9685837445051775\n",
      "    val_log_marginal: 1165.7257133152175\n",
      "    val_log_joint  : 1374.0937765370245\n",
      "Train Epoch: 1307 [0/101520 (0%)] Loss: -1175.791016\n",
      "Train Epoch: 1307 [11264/101520 (11%)] Loss: -1167.130737\n",
      "Train Epoch: 1307 [22528/101520 (22%)] Loss: -1170.038818\n",
      "Train Epoch: 1307 [33792/101520 (33%)] Loss: -1165.822021\n",
      "Train Epoch: 1307 [45056/101520 (44%)] Loss: -1165.369873\n",
      "Train Epoch: 1307 [56320/101520 (55%)] Loss: -1168.343262\n",
      "Train Epoch: 1307 [67584/101520 (67%)] Loss: -1165.780029\n",
      "Train Epoch: 1307 [78848/101520 (78%)] Loss: -1162.289551\n",
      "Train Epoch: 1307 [90112/101520 (89%)] Loss: -1159.895020\n",
      "Train Epoch: 1307 [101376/101520 (100%)] Loss: -1158.717651\n",
      "    epoch          : 1307\n",
      "    loss           : -1166.8228630947708\n",
      "    ess            : 1.966673712634561\n",
      "    log_marginal   : 1166.8549086987673\n",
      "    log_joint      : 1375.2738000304255\n",
      "    val_loss       : -1164.6263480808425\n",
      "    val_ess        : 1.9686593646588533\n",
      "    val_log_marginal: 1164.6560695482337\n",
      "    val_log_joint  : 1373.3087158203125\n",
      "Train Epoch: 1308 [0/101520 (0%)] Loss: -1167.555664\n",
      "Train Epoch: 1308 [11264/101520 (11%)] Loss: -1163.281128\n",
      "Train Epoch: 1308 [22528/101520 (22%)] Loss: -1169.218750\n",
      "Train Epoch: 1308 [33792/101520 (33%)] Loss: -1158.991211\n",
      "Train Epoch: 1308 [45056/101520 (44%)] Loss: -1161.296631\n",
      "Train Epoch: 1308 [56320/101520 (55%)] Loss: -1164.766602\n",
      "Train Epoch: 1308 [67584/101520 (67%)] Loss: -1168.970215\n",
      "Train Epoch: 1308 [78848/101520 (78%)] Loss: -1166.860718\n",
      "Train Epoch: 1308 [90112/101520 (89%)] Loss: -1165.941162\n",
      "Train Epoch: 1308 [101376/101520 (100%)] Loss: -1169.668457\n",
      "    epoch          : 1308\n",
      "    loss           : -1166.9308996152638\n",
      "    ess            : 1.9669379798611204\n",
      "    log_marginal   : 1166.9615601199357\n",
      "    log_joint      : 1375.369858938246\n",
      "    val_loss       : -1166.8939262058425\n",
      "    val_ess        : 1.9646309821502022\n",
      "    val_log_marginal: 1166.927198327106\n",
      "    val_log_joint  : 1375.3380445397418\n",
      "Train Epoch: 1309 [0/101520 (0%)] Loss: -1169.173340\n",
      "Train Epoch: 1309 [11264/101520 (11%)] Loss: -1164.423828\n",
      "Train Epoch: 1309 [22528/101520 (22%)] Loss: -1166.513672\n",
      "Train Epoch: 1309 [33792/101520 (33%)] Loss: -1167.255493\n",
      "Train Epoch: 1309 [45056/101520 (44%)] Loss: -1165.701904\n",
      "Train Epoch: 1309 [56320/101520 (55%)] Loss: -1160.355835\n",
      "Train Epoch: 1309 [67584/101520 (67%)] Loss: -1168.607788\n",
      "Train Epoch: 1309 [78848/101520 (78%)] Loss: -1166.367676\n",
      "Train Epoch: 1309 [90112/101520 (89%)] Loss: -1174.893799\n",
      "Train Epoch: 1309 [101376/101520 (100%)] Loss: -1171.376709\n",
      "    epoch          : 1309\n",
      "    loss           : -1167.0121039769158\n",
      "    ess            : 1.9674433978957746\n",
      "    log_marginal   : 1167.0423669862987\n",
      "    log_joint      : 1375.443382684909\n",
      "    val_loss       : -1166.716743800951\n",
      "    val_ess        : 1.968584444211877\n",
      "    val_log_marginal: 1166.745992909307\n",
      "    val_log_joint  : 1375.0578719429348\n",
      "Train Epoch: 1310 [0/101520 (0%)] Loss: -1167.135986\n",
      "Train Epoch: 1310 [11264/101520 (11%)] Loss: -1167.626953\n",
      "Train Epoch: 1310 [22528/101520 (22%)] Loss: -1168.595947\n",
      "Train Epoch: 1310 [33792/101520 (33%)] Loss: -1164.218994\n",
      "Train Epoch: 1310 [45056/101520 (44%)] Loss: -1169.267334\n",
      "Train Epoch: 1310 [56320/101520 (55%)] Loss: -1162.177246\n",
      "Train Epoch: 1310 [67584/101520 (67%)] Loss: -1163.031372\n",
      "Train Epoch: 1310 [78848/101520 (78%)] Loss: -1175.143677\n",
      "Train Epoch: 1310 [90112/101520 (89%)] Loss: -1163.546387\n",
      "Train Epoch: 1310 [101376/101520 (100%)] Loss: -1176.522583\n",
      "    epoch          : 1310\n",
      "    loss           : -1167.040531024262\n",
      "    ess            : 1.9669386136471925\n",
      "    log_marginal   : 1167.0719527814856\n",
      "    log_joint      : 1375.5078726150282\n",
      "    val_loss       : -1164.290575110394\n",
      "    val_ess        : 1.965121170748835\n",
      "    val_log_marginal: 1164.3222390879755\n",
      "    val_log_joint  : 1372.6413733440897\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1310.pth ...\n",
      "Train Epoch: 1311 [0/101520 (0%)] Loss: -1165.379272\n",
      "Train Epoch: 1311 [11264/101520 (11%)] Loss: -1168.172119\n",
      "Train Epoch: 1311 [22528/101520 (22%)] Loss: -1178.875732\n",
      "Train Epoch: 1311 [33792/101520 (33%)] Loss: -1167.011963\n",
      "Train Epoch: 1311 [45056/101520 (44%)] Loss: -1169.908325\n",
      "Train Epoch: 1311 [56320/101520 (55%)] Loss: -1169.369019\n",
      "Train Epoch: 1311 [67584/101520 (67%)] Loss: -1167.794312\n",
      "Train Epoch: 1311 [78848/101520 (78%)] Loss: -1167.631836\n",
      "Train Epoch: 1311 [90112/101520 (89%)] Loss: -1167.573730\n",
      "Train Epoch: 1311 [101376/101520 (100%)] Loss: -1167.895142\n",
      "    epoch          : 1311\n",
      "    loss           : -1167.1013692731235\n",
      "    ess            : 1.9672161736080993\n",
      "    log_marginal   : 1167.131907094064\n",
      "    log_joint      : 1375.4867569046403\n",
      "    val_loss       : -1166.8539932914402\n",
      "    val_ess        : 1.965448612752168\n",
      "    val_log_marginal: 1166.8822339928668\n",
      "    val_log_joint  : 1375.4805112092392\n",
      "Train Epoch: 1312 [0/101520 (0%)] Loss: -1165.518921\n",
      "Train Epoch: 1312 [11264/101520 (11%)] Loss: -1169.721191\n",
      "Train Epoch: 1312 [22528/101520 (22%)] Loss: -1166.400391\n",
      "Train Epoch: 1312 [33792/101520 (33%)] Loss: -1166.226074\n",
      "Train Epoch: 1312 [45056/101520 (44%)] Loss: -1156.830566\n",
      "Train Epoch: 1312 [56320/101520 (55%)] Loss: -1168.262207\n",
      "Train Epoch: 1312 [67584/101520 (67%)] Loss: -1172.761353\n",
      "Train Epoch: 1312 [78848/101520 (78%)] Loss: -1164.341553\n",
      "Train Epoch: 1312 [90112/101520 (89%)] Loss: -1169.411133\n",
      "Train Epoch: 1312 [101376/101520 (100%)] Loss: -1168.858154\n",
      "    epoch          : 1312\n",
      "    loss           : -1167.0633863899577\n",
      "    ess            : 1.9680325355961095\n",
      "    log_marginal   : 1167.093664734807\n",
      "    log_joint      : 1375.5240956982177\n",
      "    val_loss       : -1165.891744862432\n",
      "    val_ess        : 1.96620163710221\n",
      "    val_log_marginal: 1165.9260519276495\n",
      "    val_log_joint  : 1374.6761793053668\n",
      "Train Epoch: 1313 [0/101520 (0%)] Loss: -1171.289673\n",
      "Train Epoch: 1313 [11264/101520 (11%)] Loss: -1165.526367\n",
      "Train Epoch: 1313 [22528/101520 (22%)] Loss: -1165.395020\n",
      "Train Epoch: 1313 [33792/101520 (33%)] Loss: -1167.733765\n",
      "Train Epoch: 1313 [45056/101520 (44%)] Loss: -1168.955322\n",
      "Train Epoch: 1313 [56320/101520 (55%)] Loss: -1170.345703\n",
      "Train Epoch: 1313 [67584/101520 (67%)] Loss: -1171.526611\n",
      "Train Epoch: 1313 [78848/101520 (78%)] Loss: -1166.628174\n",
      "Train Epoch: 1313 [90112/101520 (89%)] Loss: -1158.642090\n",
      "Train Epoch: 1313 [101376/101520 (100%)] Loss: -1164.116211\n",
      "    epoch          : 1313\n",
      "    loss           : -1167.1161017489792\n",
      "    ess            : 1.9672578123945688\n",
      "    log_marginal   : 1167.146748145022\n",
      "    log_joint      : 1375.5614424662374\n",
      "    val_loss       : -1165.6364905315897\n",
      "    val_ess        : 1.9696649261142896\n",
      "    val_log_marginal: 1165.6657555621603\n",
      "    val_log_joint  : 1373.8236083984375\n",
      "Train Epoch: 1314 [0/101520 (0%)] Loss: -1174.446289\n",
      "Train Epoch: 1314 [11264/101520 (11%)] Loss: -1153.911499\n",
      "Train Epoch: 1314 [22528/101520 (22%)] Loss: -1168.407471\n",
      "Train Epoch: 1314 [33792/101520 (33%)] Loss: -1169.336182\n",
      "Train Epoch: 1314 [45056/101520 (44%)] Loss: -1170.992432\n",
      "Train Epoch: 1314 [56320/101520 (55%)] Loss: -1168.145508\n",
      "Train Epoch: 1314 [67584/101520 (67%)] Loss: -1165.385010\n",
      "Train Epoch: 1314 [78848/101520 (78%)] Loss: -1170.509644\n",
      "Train Epoch: 1314 [90112/101520 (89%)] Loss: -1162.198975\n",
      "Train Epoch: 1314 [101376/101520 (100%)] Loss: -1171.828735\n",
      "    epoch          : 1314\n",
      "    loss           : -1167.1890470418498\n",
      "    ess            : 1.966536508133663\n",
      "    log_marginal   : 1167.219940645611\n",
      "    log_joint      : 1375.6357587498037\n",
      "    val_loss       : -1167.012058423913\n",
      "    val_ess        : 1.968573554702427\n",
      "    val_log_marginal: 1167.044481360394\n",
      "    val_log_joint  : 1375.4584536345108\n",
      "Train Epoch: 1315 [0/101520 (0%)] Loss: -1171.520996\n",
      "Train Epoch: 1315 [11264/101520 (11%)] Loss: -1167.638428\n",
      "Train Epoch: 1315 [22528/101520 (22%)] Loss: -1175.623779\n",
      "Train Epoch: 1315 [33792/101520 (33%)] Loss: -1166.769775\n",
      "Train Epoch: 1315 [45056/101520 (44%)] Loss: -1168.918213\n",
      "Train Epoch: 1315 [56320/101520 (55%)] Loss: -1170.497070\n",
      "Train Epoch: 1315 [67584/101520 (67%)] Loss: -1172.887817\n",
      "Train Epoch: 1315 [78848/101520 (78%)] Loss: -1171.779053\n",
      "Train Epoch: 1315 [90112/101520 (89%)] Loss: -1170.826660\n",
      "Train Epoch: 1315 [101376/101520 (100%)] Loss: -1171.942139\n",
      "    epoch          : 1315\n",
      "    loss           : -1167.271056822197\n",
      "    ess            : 1.9669761759551925\n",
      "    log_marginal   : 1167.301679294912\n",
      "    log_joint      : 1375.7393369435065\n",
      "    val_loss       : -1167.1578209918478\n",
      "    val_ess        : 1.9680471834929094\n",
      "    val_log_marginal: 1167.1876379925272\n",
      "    val_log_joint  : 1375.4728791610055\n",
      "Train Epoch: 1316 [0/101520 (0%)] Loss: -1165.117920\n",
      "Train Epoch: 1316 [11264/101520 (11%)] Loss: -1171.556152\n",
      "Train Epoch: 1316 [22528/101520 (22%)] Loss: -1170.194092\n",
      "Train Epoch: 1316 [33792/101520 (33%)] Loss: -1168.150391\n",
      "Train Epoch: 1316 [45056/101520 (44%)] Loss: -1173.975952\n",
      "Train Epoch: 1316 [56320/101520 (55%)] Loss: -1167.736206\n",
      "Train Epoch: 1316 [67584/101520 (67%)] Loss: -1162.901733\n",
      "Train Epoch: 1316 [78848/101520 (78%)] Loss: -1169.505615\n",
      "Train Epoch: 1316 [90112/101520 (89%)] Loss: -1168.322754\n",
      "Train Epoch: 1316 [101376/101520 (100%)] Loss: -1169.362305\n",
      "    epoch          : 1316\n",
      "    loss           : -1167.3879008077497\n",
      "    ess            : 1.9669156649603916\n",
      "    log_marginal   : 1167.4192814659234\n",
      "    log_joint      : 1375.787790269708\n",
      "    val_loss       : -1166.1931311565897\n",
      "    val_ess        : 1.9648195816122966\n",
      "    val_log_marginal: 1166.225294030231\n",
      "    val_log_joint  : 1374.5314622961957\n",
      "Train Epoch: 1317 [0/101520 (0%)] Loss: -1165.276978\n",
      "Train Epoch: 1317 [11264/101520 (11%)] Loss: -1160.589722\n",
      "Train Epoch: 1317 [22528/101520 (22%)] Loss: -1169.652344\n",
      "Train Epoch: 1317 [33792/101520 (33%)] Loss: -1170.988159\n",
      "Train Epoch: 1317 [45056/101520 (44%)] Loss: -1172.236328\n",
      "Train Epoch: 1317 [56320/101520 (55%)] Loss: -1171.405762\n",
      "Train Epoch: 1317 [67584/101520 (67%)] Loss: -1163.201172\n",
      "Train Epoch: 1317 [78848/101520 (78%)] Loss: -1166.879883\n",
      "Train Epoch: 1317 [90112/101520 (89%)] Loss: -1171.265503\n",
      "Train Epoch: 1317 [101376/101520 (100%)] Loss: -1168.631470\n",
      "    epoch          : 1317\n",
      "    loss           : -1167.354301414298\n",
      "    ess            : 1.9671144341703635\n",
      "    log_marginal   : 1167.3849183662453\n",
      "    log_joint      : 1375.8050647524733\n",
      "    val_loss       : -1164.7586563773777\n",
      "    val_ess        : 1.9678032190903374\n",
      "    val_log_marginal: 1164.7887281334918\n",
      "    val_log_joint  : 1373.0841382897418\n",
      "Train Epoch: 1318 [0/101520 (0%)] Loss: -1171.058594\n",
      "Train Epoch: 1318 [11264/101520 (11%)] Loss: -1166.929932\n",
      "Train Epoch: 1318 [22528/101520 (22%)] Loss: -1170.634521\n",
      "Train Epoch: 1318 [33792/101520 (33%)] Loss: -1164.299072\n",
      "Train Epoch: 1318 [45056/101520 (44%)] Loss: -1165.758057\n",
      "Train Epoch: 1318 [56320/101520 (55%)] Loss: -1167.401367\n",
      "Train Epoch: 1318 [67584/101520 (67%)] Loss: -1170.141846\n",
      "Train Epoch: 1318 [78848/101520 (78%)] Loss: -1170.106934\n",
      "Train Epoch: 1318 [90112/101520 (89%)] Loss: -1175.104126\n",
      "Train Epoch: 1318 [101376/101520 (100%)] Loss: -1173.890015\n",
      "    epoch          : 1318\n",
      "    loss           : -1167.4053617697864\n",
      "    ess            : 1.9665225780189937\n",
      "    log_marginal   : 1167.4359805619897\n",
      "    log_joint      : 1375.8760090736887\n",
      "    val_loss       : -1165.938375721807\n",
      "    val_ess        : 1.9683189236599465\n",
      "    val_log_marginal: 1165.9677415930707\n",
      "    val_log_joint  : 1374.7920664911685\n",
      "Train Epoch: 1319 [0/101520 (0%)] Loss: -1175.542725\n",
      "Train Epoch: 1319 [11264/101520 (11%)] Loss: -1170.473267\n",
      "Train Epoch: 1319 [22528/101520 (22%)] Loss: -1171.469238\n",
      "Train Epoch: 1319 [33792/101520 (33%)] Loss: -1166.081299\n",
      "Train Epoch: 1319 [45056/101520 (44%)] Loss: -1167.706299\n",
      "Train Epoch: 1319 [56320/101520 (55%)] Loss: -1167.601074\n",
      "Train Epoch: 1319 [67584/101520 (67%)] Loss: -1162.408325\n",
      "Train Epoch: 1319 [78848/101520 (78%)] Loss: -1160.057861\n",
      "Train Epoch: 1319 [90112/101520 (89%)] Loss: -1161.475708\n",
      "Train Epoch: 1319 [101376/101520 (100%)] Loss: -1172.388672\n",
      "    epoch          : 1319\n",
      "    loss           : -1167.4437182249137\n",
      "    ess            : 1.966715396948196\n",
      "    log_marginal   : 1167.4741486975895\n",
      "    log_joint      : 1375.9155886856156\n",
      "    val_loss       : -1165.57592242697\n",
      "    val_ess        : 1.9705478056617405\n",
      "    val_log_marginal: 1165.6017960258152\n",
      "    val_log_joint  : 1373.818454908288\n",
      "Train Epoch: 1320 [0/101520 (0%)] Loss: -1170.360596\n",
      "Train Epoch: 1320 [11264/101520 (11%)] Loss: -1166.370605\n",
      "Train Epoch: 1320 [22528/101520 (22%)] Loss: -1170.657959\n",
      "Train Epoch: 1320 [33792/101520 (33%)] Loss: -1169.168945\n",
      "Train Epoch: 1320 [45056/101520 (44%)] Loss: -1171.588867\n",
      "Train Epoch: 1320 [56320/101520 (55%)] Loss: -1158.884766\n",
      "Train Epoch: 1320 [67584/101520 (67%)] Loss: -1174.008545\n",
      "Train Epoch: 1320 [78848/101520 (78%)] Loss: -1160.900146\n",
      "Train Epoch: 1320 [90112/101520 (89%)] Loss: -1163.415283\n",
      "Train Epoch: 1320 [101376/101520 (100%)] Loss: -1165.399048\n",
      "    epoch          : 1320\n",
      "    loss           : -1167.5368425378847\n",
      "    ess            : 1.9672805208656656\n",
      "    log_marginal   : 1167.567782148045\n",
      "    log_joint      : 1375.935821073139\n",
      "    val_loss       : -1166.7195726477582\n",
      "    val_ess        : 1.9660941621531611\n",
      "    val_log_marginal: 1166.7499840777853\n",
      "    val_log_joint  : 1375.3082275390625\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1320.pth ...\n",
      "Train Epoch: 1321 [0/101520 (0%)] Loss: -1170.766113\n",
      "Train Epoch: 1321 [11264/101520 (11%)] Loss: -1169.714722\n",
      "Train Epoch: 1321 [22528/101520 (22%)] Loss: -1164.759155\n",
      "Train Epoch: 1321 [33792/101520 (33%)] Loss: -1168.706787\n",
      "Train Epoch: 1321 [45056/101520 (44%)] Loss: -1169.064941\n",
      "Train Epoch: 1321 [56320/101520 (55%)] Loss: -1164.880615\n",
      "Train Epoch: 1321 [67584/101520 (67%)] Loss: -1171.941040\n",
      "Train Epoch: 1321 [78848/101520 (78%)] Loss: -1169.243774\n",
      "Train Epoch: 1321 [90112/101520 (89%)] Loss: -1170.392578\n",
      "Train Epoch: 1321 [101376/101520 (100%)] Loss: -1165.602661\n",
      "    epoch          : 1321\n",
      "    loss           : -1167.599122934006\n",
      "    ess            : 1.9668602601966667\n",
      "    log_marginal   : 1167.6305361033685\n",
      "    log_joint      : 1375.9625372958542\n",
      "    val_loss       : -1165.9648596722147\n",
      "    val_ess        : 1.9645336659058281\n",
      "    val_log_marginal: 1166.0027757727582\n",
      "    val_log_joint  : 1374.5531483525815\n",
      "Train Epoch: 1322 [0/101520 (0%)] Loss: -1164.146484\n",
      "Train Epoch: 1322 [11264/101520 (11%)] Loss: -1168.760742\n",
      "Train Epoch: 1322 [22528/101520 (22%)] Loss: -1160.304199\n",
      "Train Epoch: 1322 [33792/101520 (33%)] Loss: -1177.075439\n",
      "Train Epoch: 1322 [45056/101520 (44%)] Loss: -1164.996094\n",
      "Train Epoch: 1322 [56320/101520 (55%)] Loss: -1167.276123\n",
      "Train Epoch: 1322 [67584/101520 (67%)] Loss: -1162.150391\n",
      "Train Epoch: 1322 [78848/101520 (78%)] Loss: -1171.969971\n",
      "Train Epoch: 1322 [90112/101520 (89%)] Loss: -1164.463989\n",
      "Train Epoch: 1322 [101376/101520 (100%)] Loss: -1170.376099\n",
      "    epoch          : 1322\n",
      "    loss           : -1167.637821676743\n",
      "    ess            : 1.9665713531887112\n",
      "    log_marginal   : 1167.6691827055197\n",
      "    log_joint      : 1376.0955810546875\n",
      "    val_loss       : -1165.98021930197\n",
      "    val_ess        : 1.9685711186865102\n",
      "    val_log_marginal: 1166.0079398777175\n",
      "    val_log_joint  : 1374.3921959918478\n",
      "Train Epoch: 1323 [0/101520 (0%)] Loss: -1164.039795\n",
      "Train Epoch: 1323 [11264/101520 (11%)] Loss: -1165.498779\n",
      "Train Epoch: 1323 [22528/101520 (22%)] Loss: -1173.542236\n",
      "Train Epoch: 1323 [33792/101520 (33%)] Loss: -1170.901123\n",
      "Train Epoch: 1323 [45056/101520 (44%)] Loss: -1169.120117\n",
      "Train Epoch: 1323 [56320/101520 (55%)] Loss: -1167.939453\n",
      "Train Epoch: 1323 [67584/101520 (67%)] Loss: -1169.078369\n",
      "Train Epoch: 1323 [78848/101520 (78%)] Loss: -1165.763428\n",
      "Train Epoch: 1323 [90112/101520 (89%)] Loss: -1172.234253\n",
      "Train Epoch: 1323 [101376/101520 (100%)] Loss: -1164.293457\n",
      "    epoch          : 1323\n",
      "    loss           : -1167.6434362976995\n",
      "    ess            : 1.9668272919391268\n",
      "    log_marginal   : 1167.6745366235475\n",
      "    log_joint      : 1376.0749757086212\n",
      "    val_loss       : -1164.0815270465353\n",
      "    val_ess        : 1.966285861056784\n",
      "    val_log_marginal: 1164.1123418393342\n",
      "    val_log_joint  : 1372.7262812075408\n",
      "Train Epoch: 1324 [0/101520 (0%)] Loss: -1169.230225\n",
      "Train Epoch: 1324 [11264/101520 (11%)] Loss: -1163.068481\n",
      "Train Epoch: 1324 [22528/101520 (22%)] Loss: -1168.336426\n",
      "Train Epoch: 1324 [33792/101520 (33%)] Loss: -1159.318359\n",
      "Train Epoch: 1324 [45056/101520 (44%)] Loss: -1165.085205\n",
      "Train Epoch: 1324 [56320/101520 (55%)] Loss: -1167.859131\n",
      "Train Epoch: 1324 [67584/101520 (67%)] Loss: -1168.145752\n",
      "Train Epoch: 1324 [78848/101520 (78%)] Loss: -1177.897095\n",
      "Train Epoch: 1324 [90112/101520 (89%)] Loss: -1164.608154\n",
      "Train Epoch: 1324 [101376/101520 (100%)] Loss: -1176.227173\n",
      "    epoch          : 1324\n",
      "    loss           : -1167.6663431905622\n",
      "    ess            : 1.9663299903198703\n",
      "    log_marginal   : 1167.6980526411353\n",
      "    log_joint      : 1376.1618891577025\n",
      "    val_loss       : -1168.192335045856\n",
      "    val_ess        : 1.9632933658102285\n",
      "    val_log_marginal: 1168.2287809952445\n",
      "    val_log_joint  : 1376.469137440557\n",
      "Train Epoch: 1325 [0/101520 (0%)] Loss: -1160.885986\n",
      "Train Epoch: 1325 [11264/101520 (11%)] Loss: -1167.714355\n",
      "Train Epoch: 1325 [22528/101520 (22%)] Loss: -1167.051880\n",
      "Train Epoch: 1325 [33792/101520 (33%)] Loss: -1166.739258\n",
      "Train Epoch: 1325 [45056/101520 (44%)] Loss: -1160.111084\n",
      "Train Epoch: 1325 [56320/101520 (55%)] Loss: -1160.899780\n",
      "Train Epoch: 1325 [67584/101520 (67%)] Loss: -1169.480713\n",
      "Train Epoch: 1325 [78848/101520 (78%)] Loss: -1168.702759\n",
      "Train Epoch: 1325 [90112/101520 (89%)] Loss: -1170.585693\n",
      "Train Epoch: 1325 [101376/101520 (100%)] Loss: -1169.804565\n",
      "    epoch          : 1325\n",
      "    loss           : -1167.7839968887406\n",
      "    ess            : 1.9674708789317452\n",
      "    log_marginal   : 1167.8143365754554\n",
      "    log_joint      : 1376.1345030818154\n",
      "    val_loss       : -1165.9899477751358\n",
      "    val_ess        : 1.9677659843278967\n",
      "    val_log_marginal: 1166.0213941491168\n",
      "    val_log_joint  : 1374.539603855299\n",
      "Train Epoch: 1326 [0/101520 (0%)] Loss: -1166.786377\n",
      "Train Epoch: 1326 [11264/101520 (11%)] Loss: -1170.849243\n",
      "Train Epoch: 1326 [22528/101520 (22%)] Loss: -1158.755371\n",
      "Train Epoch: 1326 [33792/101520 (33%)] Loss: -1170.609619\n",
      "Train Epoch: 1326 [45056/101520 (44%)] Loss: -1167.986328\n",
      "Train Epoch: 1326 [56320/101520 (55%)] Loss: -1168.984863\n",
      "Train Epoch: 1326 [67584/101520 (67%)] Loss: -1169.700195\n",
      "Train Epoch: 1326 [78848/101520 (78%)] Loss: -1173.018799\n",
      "Train Epoch: 1326 [90112/101520 (89%)] Loss: -1172.635498\n",
      "Train Epoch: 1326 [101376/101520 (100%)] Loss: -1163.250244\n",
      "    epoch          : 1326\n",
      "    loss           : -1167.7261674583856\n",
      "    ess            : 1.9672709547694605\n",
      "    log_marginal   : 1167.756621854389\n",
      "    log_joint      : 1376.1274567417163\n",
      "    val_loss       : -1166.5062362007473\n",
      "    val_ess        : 1.9701486566792363\n",
      "    val_log_marginal: 1166.5337497877038\n",
      "    val_log_joint  : 1374.9668393342392\n",
      "Train Epoch: 1327 [0/101520 (0%)] Loss: -1167.991699\n",
      "Train Epoch: 1327 [11264/101520 (11%)] Loss: -1169.882324\n",
      "Train Epoch: 1327 [22528/101520 (22%)] Loss: -1161.158203\n",
      "Train Epoch: 1327 [33792/101520 (33%)] Loss: -1166.828369\n",
      "Train Epoch: 1327 [45056/101520 (44%)] Loss: -1168.356201\n",
      "Train Epoch: 1327 [56320/101520 (55%)] Loss: -1173.184448\n",
      "Train Epoch: 1327 [67584/101520 (67%)] Loss: -1174.692627\n",
      "Train Epoch: 1327 [78848/101520 (78%)] Loss: -1161.601196\n",
      "Train Epoch: 1327 [90112/101520 (89%)] Loss: -1166.893555\n",
      "Train Epoch: 1327 [101376/101520 (100%)] Loss: -1173.165161\n",
      "    epoch          : 1327\n",
      "    loss           : -1167.863129735592\n",
      "    ess            : 1.967422838187098\n",
      "    log_marginal   : 1167.8938049623116\n",
      "    log_joint      : 1376.2086678509736\n",
      "    val_loss       : -1166.0233048148777\n",
      "    val_ess        : 1.9656529115593953\n",
      "    val_log_marginal: 1166.0546238111413\n",
      "    val_log_joint  : 1374.3235606317935\n",
      "Train Epoch: 1328 [0/101520 (0%)] Loss: -1165.497192\n",
      "Train Epoch: 1328 [11264/101520 (11%)] Loss: -1169.535278\n",
      "Train Epoch: 1328 [22528/101520 (22%)] Loss: -1171.670166\n",
      "Train Epoch: 1328 [33792/101520 (33%)] Loss: -1163.213623\n",
      "Train Epoch: 1328 [45056/101520 (44%)] Loss: -1175.503784\n",
      "Train Epoch: 1328 [56320/101520 (55%)] Loss: -1174.191772\n",
      "Train Epoch: 1328 [67584/101520 (67%)] Loss: -1170.926514\n",
      "Train Epoch: 1328 [78848/101520 (78%)] Loss: -1167.658081\n",
      "Train Epoch: 1328 [90112/101520 (89%)] Loss: -1167.213379\n",
      "Train Epoch: 1328 [101376/101520 (100%)] Loss: -1166.684204\n",
      "    epoch          : 1328\n",
      "    loss           : -1167.774947123312\n",
      "    ess            : 1.9667133272592745\n",
      "    log_marginal   : 1167.8056321647298\n",
      "    log_joint      : 1376.2266005319566\n",
      "    val_loss       : -1165.929692807405\n",
      "    val_ess        : 1.9680185628973919\n",
      "    val_log_marginal: 1165.9592868970788\n",
      "    val_log_joint  : 1374.4553541100543\n",
      "Train Epoch: 1329 [0/101520 (0%)] Loss: -1166.143311\n",
      "Train Epoch: 1329 [11264/101520 (11%)] Loss: -1162.609863\n",
      "Train Epoch: 1329 [22528/101520 (22%)] Loss: -1168.807617\n",
      "Train Epoch: 1329 [33792/101520 (33%)] Loss: -1169.440308\n",
      "Train Epoch: 1329 [45056/101520 (44%)] Loss: -1163.842773\n",
      "Train Epoch: 1329 [56320/101520 (55%)] Loss: -1169.742065\n",
      "Train Epoch: 1329 [67584/101520 (67%)] Loss: -1171.537109\n",
      "Train Epoch: 1329 [78848/101520 (78%)] Loss: -1168.243164\n",
      "Train Epoch: 1329 [90112/101520 (89%)] Loss: -1163.056641\n",
      "Train Epoch: 1329 [101376/101520 (100%)] Loss: -1161.212524\n",
      "    epoch          : 1329\n",
      "    loss           : -1167.827022073257\n",
      "    ess            : 1.9664979951465549\n",
      "    log_marginal   : 1167.858498424741\n",
      "    log_joint      : 1376.2775240950848\n",
      "    val_loss       : -1165.8518331776495\n",
      "    val_ess        : 1.9658037164936895\n",
      "    val_log_marginal: 1165.88355022928\n",
      "    val_log_joint  : 1374.3962296195652\n",
      "Train Epoch: 1330 [0/101520 (0%)] Loss: -1166.643555\n",
      "Train Epoch: 1330 [11264/101520 (11%)] Loss: -1158.415771\n",
      "Train Epoch: 1330 [22528/101520 (22%)] Loss: -1170.699463\n",
      "Train Epoch: 1330 [33792/101520 (33%)] Loss: -1164.787720\n",
      "Train Epoch: 1330 [45056/101520 (44%)] Loss: -1164.082397\n",
      "Train Epoch: 1330 [56320/101520 (55%)] Loss: -1163.295898\n",
      "Train Epoch: 1330 [67584/101520 (67%)] Loss: -1159.017334\n",
      "Train Epoch: 1330 [78848/101520 (78%)] Loss: -1160.563232\n",
      "Train Epoch: 1330 [90112/101520 (89%)] Loss: -1168.824097\n",
      "Train Epoch: 1330 [101376/101520 (100%)] Loss: -1161.073730\n",
      "    epoch          : 1330\n",
      "    loss           : -1167.8436119808025\n",
      "    ess            : 1.966940009414251\n",
      "    log_marginal   : 1167.874859527128\n",
      "    log_joint      : 1376.3104284851995\n",
      "    val_loss       : -1166.6166089928668\n",
      "    val_ess        : 1.9676547205966453\n",
      "    val_log_marginal: 1166.6476413892663\n",
      "    val_log_joint  : 1375.0425123131793\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1330.pth ...\n",
      "Train Epoch: 1331 [0/101520 (0%)] Loss: -1168.932617\n",
      "Train Epoch: 1331 [11264/101520 (11%)] Loss: -1166.495728\n",
      "Train Epoch: 1331 [22528/101520 (22%)] Loss: -1162.247559\n",
      "Train Epoch: 1331 [33792/101520 (33%)] Loss: -1164.483154\n",
      "Train Epoch: 1331 [45056/101520 (44%)] Loss: -1165.002441\n",
      "Train Epoch: 1331 [56320/101520 (55%)] Loss: -1165.790039\n",
      "Train Epoch: 1331 [67584/101520 (67%)] Loss: -1169.558838\n",
      "Train Epoch: 1331 [78848/101520 (78%)] Loss: -1174.770264\n",
      "Train Epoch: 1331 [90112/101520 (89%)] Loss: -1166.627808\n",
      "Train Epoch: 1331 [101376/101520 (100%)] Loss: -1159.612183\n",
      "    epoch          : 1331\n",
      "    loss           : -1167.848349413081\n",
      "    ess            : 1.966850264587594\n",
      "    log_marginal   : 1167.879629470595\n",
      "    log_joint      : 1376.3353185605763\n",
      "    val_loss       : -1166.7875127377717\n",
      "    val_ess        : 1.9688674107841824\n",
      "    val_log_marginal: 1166.81615680197\n",
      "    val_log_joint  : 1375.0139319378397\n",
      "Train Epoch: 1332 [0/101520 (0%)] Loss: -1161.099121\n",
      "Train Epoch: 1332 [11264/101520 (11%)] Loss: -1169.190674\n",
      "Train Epoch: 1332 [22528/101520 (22%)] Loss: -1174.264893\n",
      "Train Epoch: 1332 [33792/101520 (33%)] Loss: -1168.209717\n",
      "Train Epoch: 1332 [45056/101520 (44%)] Loss: -1163.924072\n",
      "Train Epoch: 1332 [56320/101520 (55%)] Loss: -1167.325439\n",
      "Train Epoch: 1332 [67584/101520 (67%)] Loss: -1165.709839\n",
      "Train Epoch: 1332 [78848/101520 (78%)] Loss: -1165.812500\n",
      "Train Epoch: 1332 [90112/101520 (89%)] Loss: -1159.724854\n",
      "Train Epoch: 1332 [101376/101520 (100%)] Loss: -1155.674194\n",
      "    epoch          : 1332\n",
      "    loss           : -1167.9760625637955\n",
      "    ess            : 1.9675213110506835\n",
      "    log_marginal   : 1168.0066034518295\n",
      "    log_joint      : 1376.3807931257852\n",
      "    val_loss       : -1166.8300038213315\n",
      "    val_ess        : 1.966069801993992\n",
      "    val_log_marginal: 1166.8620446246603\n",
      "    val_log_joint  : 1374.9700556216033\n",
      "Train Epoch: 1333 [0/101520 (0%)] Loss: -1172.232422\n",
      "Train Epoch: 1333 [11264/101520 (11%)] Loss: -1168.145752\n",
      "Train Epoch: 1333 [22528/101520 (22%)] Loss: -1169.021729\n",
      "Train Epoch: 1333 [33792/101520 (33%)] Loss: -1172.875732\n",
      "Train Epoch: 1333 [45056/101520 (44%)] Loss: -1164.072021\n",
      "Train Epoch: 1333 [56320/101520 (55%)] Loss: -1165.171753\n",
      "Train Epoch: 1333 [67584/101520 (67%)] Loss: -1165.920654\n",
      "Train Epoch: 1333 [78848/101520 (78%)] Loss: -1159.087402\n",
      "Train Epoch: 1333 [90112/101520 (89%)] Loss: -1167.548584\n",
      "Train Epoch: 1333 [101376/101520 (100%)] Loss: -1156.059814\n",
      "    epoch          : 1333\n",
      "    loss           : -1167.9841431277482\n",
      "    ess            : 1.96657029647923\n",
      "    log_marginal   : 1168.0154802331972\n",
      "    log_joint      : 1376.4015800437735\n",
      "    val_loss       : -1167.3866762907608\n",
      "    val_ess        : 1.9672506581182065\n",
      "    val_log_marginal: 1167.4178307574728\n",
      "    val_log_joint  : 1375.8013332201087\n",
      "Train Epoch: 1334 [0/101520 (0%)] Loss: -1167.372437\n",
      "Train Epoch: 1334 [11264/101520 (11%)] Loss: -1167.270264\n",
      "Train Epoch: 1334 [22528/101520 (22%)] Loss: -1170.646484\n",
      "Train Epoch: 1334 [33792/101520 (33%)] Loss: -1167.251465\n",
      "Train Epoch: 1334 [45056/101520 (44%)] Loss: -1164.229736\n",
      "Train Epoch: 1334 [56320/101520 (55%)] Loss: -1162.523315\n",
      "Train Epoch: 1334 [67584/101520 (67%)] Loss: -1165.829590\n",
      "Train Epoch: 1334 [78848/101520 (78%)] Loss: -1163.508057\n",
      "Train Epoch: 1334 [90112/101520 (89%)] Loss: -1172.869873\n",
      "Train Epoch: 1334 [101376/101520 (100%)] Loss: -1164.739868\n",
      "    epoch          : 1334\n",
      "    loss           : -1168.078483236495\n",
      "    ess            : 1.9674968246239513\n",
      "    log_marginal   : 1168.108368379986\n",
      "    log_joint      : 1376.4727249528894\n",
      "    val_loss       : -1166.253025220788\n",
      "    val_ess        : 1.9679507172625998\n",
      "    val_log_marginal: 1166.2806980298913\n",
      "    val_log_joint  : 1374.9300165591033\n",
      "Train Epoch: 1335 [0/101520 (0%)] Loss: -1171.730469\n",
      "Train Epoch: 1335 [11264/101520 (11%)] Loss: -1169.584473\n",
      "Train Epoch: 1335 [22528/101520 (22%)] Loss: -1172.488403\n",
      "Train Epoch: 1335 [33792/101520 (33%)] Loss: -1169.226562\n",
      "Train Epoch: 1335 [45056/101520 (44%)] Loss: -1165.276611\n",
      "Train Epoch: 1335 [56320/101520 (55%)] Loss: -1174.162231\n",
      "Train Epoch: 1335 [67584/101520 (67%)] Loss: -1170.165771\n",
      "Train Epoch: 1335 [78848/101520 (78%)] Loss: -1167.334473\n",
      "Train Epoch: 1335 [90112/101520 (89%)] Loss: -1169.493896\n",
      "Train Epoch: 1335 [101376/101520 (100%)] Loss: -1158.999634\n",
      "    epoch          : 1335\n",
      "    loss           : -1168.0311138210584\n",
      "    ess            : 1.966832113625416\n",
      "    log_marginal   : 1168.061471910333\n",
      "    log_joint      : 1376.4630415259894\n",
      "    val_loss       : -1163.8541206691575\n",
      "    val_ess        : 1.9664636539376301\n",
      "    val_log_marginal: 1163.8840968919837\n",
      "    val_log_joint  : 1372.5242017663043\n",
      "Train Epoch: 1336 [0/101520 (0%)] Loss: -1174.507812\n",
      "Train Epoch: 1336 [11264/101520 (11%)] Loss: -1174.840576\n",
      "Train Epoch: 1336 [22528/101520 (22%)] Loss: -1166.905396\n",
      "Train Epoch: 1336 [33792/101520 (33%)] Loss: -1172.225464\n",
      "Train Epoch: 1336 [45056/101520 (44%)] Loss: -1169.231567\n",
      "Train Epoch: 1336 [56320/101520 (55%)] Loss: -1163.527710\n",
      "Train Epoch: 1336 [67584/101520 (67%)] Loss: -1163.580811\n",
      "Train Epoch: 1336 [78848/101520 (78%)] Loss: -1165.214844\n",
      "Train Epoch: 1336 [90112/101520 (89%)] Loss: -1175.183228\n",
      "Train Epoch: 1336 [101376/101520 (100%)] Loss: -1171.440063\n",
      "    epoch          : 1336\n",
      "    loss           : -1168.083053818899\n",
      "    ess            : 1.9667918065085483\n",
      "    log_marginal   : 1168.1135401126728\n",
      "    log_joint      : 1376.4977094947394\n",
      "    val_loss       : -1166.4856700067935\n",
      "    val_ess        : 1.971145738726077\n",
      "    val_log_marginal: 1166.5113525390625\n",
      "    val_log_joint  : 1375.0204175866168\n",
      "Train Epoch: 1337 [0/101520 (0%)] Loss: -1165.695068\n",
      "Train Epoch: 1337 [11264/101520 (11%)] Loss: -1167.626709\n",
      "Train Epoch: 1337 [22528/101520 (22%)] Loss: -1172.614990\n",
      "Train Epoch: 1337 [33792/101520 (33%)] Loss: -1167.944092\n",
      "Train Epoch: 1337 [45056/101520 (44%)] Loss: -1169.596680\n",
      "Train Epoch: 1337 [56320/101520 (55%)] Loss: -1170.301025\n",
      "Train Epoch: 1337 [67584/101520 (67%)] Loss: -1165.909912\n",
      "Train Epoch: 1337 [78848/101520 (78%)] Loss: -1167.275757\n",
      "Train Epoch: 1337 [90112/101520 (89%)] Loss: -1167.330811\n",
      "Train Epoch: 1337 [101376/101520 (100%)] Loss: -1171.041382\n",
      "    epoch          : 1337\n",
      "    loss           : -1168.1392276323022\n",
      "    ess            : 1.9657561347712225\n",
      "    log_marginal   : 1168.171946156564\n",
      "    log_joint      : 1376.535441489675\n",
      "    val_loss       : -1166.3788850203805\n",
      "    val_ess        : 1.9695616286733877\n",
      "    val_log_marginal: 1166.4044507897418\n",
      "    val_log_joint  : 1374.503561268682\n",
      "Train Epoch: 1338 [0/101520 (0%)] Loss: -1182.596191\n",
      "Train Epoch: 1338 [11264/101520 (11%)] Loss: -1172.140015\n",
      "Train Epoch: 1338 [22528/101520 (22%)] Loss: -1172.476440\n",
      "Train Epoch: 1338 [33792/101520 (33%)] Loss: -1168.523804\n",
      "Train Epoch: 1338 [45056/101520 (44%)] Loss: -1172.120117\n",
      "Train Epoch: 1338 [56320/101520 (55%)] Loss: -1172.163574\n",
      "Train Epoch: 1338 [67584/101520 (67%)] Loss: -1163.306641\n",
      "Train Epoch: 1338 [78848/101520 (78%)] Loss: -1163.944946\n",
      "Train Epoch: 1338 [90112/101520 (89%)] Loss: -1161.663330\n",
      "Train Epoch: 1338 [101376/101520 (100%)] Loss: -1161.422485\n",
      "    epoch          : 1338\n",
      "    loss           : -1168.1151037168263\n",
      "    ess            : 1.966789029351431\n",
      "    log_marginal   : 1168.1451471223304\n",
      "    log_joint      : 1376.5833550074592\n",
      "    val_loss       : -1166.689750339674\n",
      "    val_ess        : 1.9655326553013013\n",
      "    val_log_marginal: 1166.7227464758832\n",
      "    val_log_joint  : 1375.214408542799\n",
      "Train Epoch: 1339 [0/101520 (0%)] Loss: -1161.829834\n",
      "Train Epoch: 1339 [11264/101520 (11%)] Loss: -1165.122314\n",
      "Train Epoch: 1339 [22528/101520 (22%)] Loss: -1169.769775\n",
      "Train Epoch: 1339 [33792/101520 (33%)] Loss: -1167.278809\n",
      "Train Epoch: 1339 [45056/101520 (44%)] Loss: -1169.792969\n",
      "Train Epoch: 1339 [56320/101520 (55%)] Loss: -1164.079346\n",
      "Train Epoch: 1339 [67584/101520 (67%)] Loss: -1170.244629\n",
      "Train Epoch: 1339 [78848/101520 (78%)] Loss: -1168.976318\n",
      "Train Epoch: 1339 [90112/101520 (89%)] Loss: -1174.980713\n",
      "Train Epoch: 1339 [101376/101520 (100%)] Loss: -1157.199707\n",
      "    epoch          : 1339\n",
      "    loss           : -1168.1767320489164\n",
      "    ess            : 1.9661344050163\n",
      "    log_marginal   : 1168.2086089627826\n",
      "    log_joint      : 1376.601309158095\n",
      "    val_loss       : -1165.9123110563858\n",
      "    val_ess        : 1.9673436102659807\n",
      "    val_log_marginal: 1165.9407905910325\n",
      "    val_log_joint  : 1374.4303986922555\n",
      "Train Epoch: 1340 [0/101520 (0%)] Loss: -1166.345215\n",
      "Train Epoch: 1340 [11264/101520 (11%)] Loss: -1172.202393\n",
      "Train Epoch: 1340 [22528/101520 (22%)] Loss: -1176.867432\n",
      "Train Epoch: 1340 [33792/101520 (33%)] Loss: -1169.774414\n",
      "Train Epoch: 1340 [45056/101520 (44%)] Loss: -1172.372070\n",
      "Train Epoch: 1340 [56320/101520 (55%)] Loss: -1172.947266\n",
      "Train Epoch: 1340 [67584/101520 (67%)] Loss: -1173.123291\n",
      "Train Epoch: 1340 [78848/101520 (78%)] Loss: -1169.270874\n",
      "Train Epoch: 1340 [90112/101520 (89%)] Loss: -1163.110840\n",
      "Train Epoch: 1340 [101376/101520 (100%)] Loss: -1162.314453\n",
      "    epoch          : 1340\n",
      "    loss           : -1168.250254568742\n",
      "    ess            : 1.9665031564894633\n",
      "    log_marginal   : 1168.281768338764\n",
      "    log_joint      : 1376.680680011385\n",
      "    val_loss       : -1166.650242017663\n",
      "    val_ess        : 1.9684428080268528\n",
      "    val_log_marginal: 1166.6809453549592\n",
      "    val_log_joint  : 1375.1959706182065\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1340.pth ...\n",
      "Train Epoch: 1341 [0/101520 (0%)] Loss: -1163.162842\n",
      "Train Epoch: 1341 [11264/101520 (11%)] Loss: -1176.148193\n",
      "Train Epoch: 1341 [22528/101520 (22%)] Loss: -1171.048096\n",
      "Train Epoch: 1341 [33792/101520 (33%)] Loss: -1165.759399\n",
      "Train Epoch: 1341 [45056/101520 (44%)] Loss: -1170.289551\n",
      "Train Epoch: 1341 [56320/101520 (55%)] Loss: -1168.703491\n",
      "Train Epoch: 1341 [67584/101520 (67%)] Loss: -1163.714844\n",
      "Train Epoch: 1341 [78848/101520 (78%)] Loss: -1170.889771\n",
      "Train Epoch: 1341 [90112/101520 (89%)] Loss: -1167.191650\n",
      "Train Epoch: 1341 [101376/101520 (100%)] Loss: -1177.493408\n",
      "    epoch          : 1341\n",
      "    loss           : -1168.331796310655\n",
      "    ess            : 1.966189134061037\n",
      "    log_marginal   : 1168.363685492894\n",
      "    log_joint      : 1376.816864473736\n",
      "    val_loss       : -1167.0154870074728\n",
      "    val_ess        : 1.968356287997702\n",
      "    val_log_marginal: 1167.0446087381115\n",
      "    val_log_joint  : 1375.1272238026495\n",
      "Train Epoch: 1342 [0/101520 (0%)] Loss: -1169.184814\n",
      "Train Epoch: 1342 [11264/101520 (11%)] Loss: -1166.834229\n",
      "Train Epoch: 1342 [22528/101520 (22%)] Loss: -1161.244751\n",
      "Train Epoch: 1342 [33792/101520 (33%)] Loss: -1174.599121\n",
      "Train Epoch: 1342 [45056/101520 (44%)] Loss: -1168.915894\n",
      "Train Epoch: 1342 [56320/101520 (55%)] Loss: -1170.089600\n",
      "Train Epoch: 1342 [67584/101520 (67%)] Loss: -1167.328491\n",
      "Train Epoch: 1342 [78848/101520 (78%)] Loss: -1170.236816\n",
      "Train Epoch: 1342 [90112/101520 (89%)] Loss: -1169.916382\n",
      "Train Epoch: 1342 [101376/101520 (100%)] Loss: -1161.223511\n",
      "    epoch          : 1342\n",
      "    loss           : -1168.354671919166\n",
      "    ess            : 1.9672261746085469\n",
      "    log_marginal   : 1168.3849987240892\n",
      "    log_joint      : 1376.7688387003377\n",
      "    val_loss       : -1166.8594864555027\n",
      "    val_ess        : 1.9647346320359602\n",
      "    val_log_marginal: 1166.893846594769\n",
      "    val_log_joint  : 1375.6394255264945\n",
      "Train Epoch: 1343 [0/101520 (0%)] Loss: -1175.543823\n",
      "Train Epoch: 1343 [11264/101520 (11%)] Loss: -1168.534668\n",
      "Train Epoch: 1343 [22528/101520 (22%)] Loss: -1167.817261\n",
      "Train Epoch: 1343 [33792/101520 (33%)] Loss: -1170.641357\n",
      "Train Epoch: 1343 [45056/101520 (44%)] Loss: -1166.907471\n",
      "Train Epoch: 1343 [56320/101520 (55%)] Loss: -1163.447998\n",
      "Train Epoch: 1343 [67584/101520 (67%)] Loss: -1166.401123\n",
      "Train Epoch: 1343 [78848/101520 (78%)] Loss: -1169.791504\n",
      "Train Epoch: 1343 [90112/101520 (89%)] Loss: -1164.345947\n",
      "Train Epoch: 1343 [101376/101520 (100%)] Loss: -1177.616821\n",
      "    epoch          : 1343\n",
      "    loss           : -1168.4405750677215\n",
      "    ess            : 1.9665436672804943\n",
      "    log_marginal   : 1168.4726715854663\n",
      "    log_joint      : 1376.8801244994504\n",
      "    val_loss       : -1168.2469163977582\n",
      "    val_ess        : 1.9601779087730076\n",
      "    val_log_marginal: 1168.2894711701767\n",
      "    val_log_joint  : 1376.8012483016305\n",
      "Train Epoch: 1344 [0/101520 (0%)] Loss: -1168.124023\n",
      "Train Epoch: 1344 [11264/101520 (11%)] Loss: -1166.805908\n",
      "Train Epoch: 1344 [22528/101520 (22%)] Loss: -1168.090332\n",
      "Train Epoch: 1344 [33792/101520 (33%)] Loss: -1165.693848\n",
      "Train Epoch: 1344 [45056/101520 (44%)] Loss: -1165.534912\n",
      "Train Epoch: 1344 [56320/101520 (55%)] Loss: -1163.542480\n",
      "Train Epoch: 1344 [67584/101520 (67%)] Loss: -1169.486450\n",
      "Train Epoch: 1344 [78848/101520 (78%)] Loss: -1172.543701\n",
      "Train Epoch: 1344 [90112/101520 (89%)] Loss: -1174.866455\n",
      "Train Epoch: 1344 [101376/101520 (100%)] Loss: -1170.687256\n",
      "    epoch          : 1344\n",
      "    loss           : -1168.550929083896\n",
      "    ess            : 1.967035999849214\n",
      "    log_marginal   : 1168.581899364989\n",
      "    log_joint      : 1376.9703792399498\n",
      "    val_loss       : -1166.3455439028533\n",
      "    val_ess        : 1.967009601385697\n",
      "    val_log_marginal: 1166.3774520210598\n",
      "    val_log_joint  : 1374.7610606317935\n",
      "Train Epoch: 1345 [0/101520 (0%)] Loss: -1164.994507\n",
      "Train Epoch: 1345 [11264/101520 (11%)] Loss: -1162.335327\n",
      "Train Epoch: 1345 [22528/101520 (22%)] Loss: -1165.625244\n",
      "Train Epoch: 1345 [33792/101520 (33%)] Loss: -1170.968872\n",
      "Train Epoch: 1345 [45056/101520 (44%)] Loss: -1171.138184\n",
      "Train Epoch: 1345 [56320/101520 (55%)] Loss: -1161.652100\n",
      "Train Epoch: 1345 [67584/101520 (67%)] Loss: -1163.100342\n",
      "Train Epoch: 1345 [78848/101520 (78%)] Loss: -1161.038330\n",
      "Train Epoch: 1345 [90112/101520 (89%)] Loss: -1164.616455\n",
      "Train Epoch: 1345 [101376/101520 (100%)] Loss: -1161.424194\n",
      "    epoch          : 1345\n",
      "    loss           : -1168.485554604075\n",
      "    ess            : 1.966291646262509\n",
      "    log_marginal   : 1168.5168861887562\n",
      "    log_joint      : 1376.896872669009\n",
      "    val_loss       : -1167.2954950747283\n",
      "    val_ess        : 1.9675788723904153\n",
      "    val_log_marginal: 1167.3236402428668\n",
      "    val_log_joint  : 1376.0814368206522\n",
      "Train Epoch: 1346 [0/101520 (0%)] Loss: -1171.205933\n",
      "Train Epoch: 1346 [11264/101520 (11%)] Loss: -1172.733276\n",
      "Train Epoch: 1346 [22528/101520 (22%)] Loss: -1166.579468\n",
      "Train Epoch: 1346 [33792/101520 (33%)] Loss: -1165.448730\n",
      "Train Epoch: 1346 [45056/101520 (44%)] Loss: -1176.449463\n",
      "Train Epoch: 1346 [56320/101520 (55%)] Loss: -1168.599487\n",
      "Train Epoch: 1346 [67584/101520 (67%)] Loss: -1164.126709\n",
      "Train Epoch: 1346 [78848/101520 (78%)] Loss: -1163.383789\n",
      "Train Epoch: 1346 [90112/101520 (89%)] Loss: -1165.339722\n",
      "Train Epoch: 1346 [101376/101520 (100%)] Loss: -1157.653442\n",
      "    epoch          : 1346\n",
      "    loss           : -1168.5183583935302\n",
      "    ess            : 1.966225728317721\n",
      "    log_marginal   : 1168.5502708856784\n",
      "    log_joint      : 1376.9313866696766\n",
      "    val_loss       : -1168.654052734375\n",
      "    val_ess        : 1.9695980186047761\n",
      "    val_log_marginal: 1168.6824314283288\n",
      "    val_log_joint  : 1377.353420091712\n",
      "Train Epoch: 1347 [0/101520 (0%)] Loss: -1168.717285\n",
      "Train Epoch: 1347 [11264/101520 (11%)] Loss: -1166.370850\n",
      "Train Epoch: 1347 [22528/101520 (22%)] Loss: -1170.273926\n",
      "Train Epoch: 1347 [33792/101520 (33%)] Loss: -1169.844604\n",
      "Train Epoch: 1347 [45056/101520 (44%)] Loss: -1162.539795\n",
      "Train Epoch: 1347 [56320/101520 (55%)] Loss: -1169.062500\n",
      "Train Epoch: 1347 [67584/101520 (67%)] Loss: -1164.950684\n",
      "Train Epoch: 1347 [78848/101520 (78%)] Loss: -1168.425293\n",
      "Train Epoch: 1347 [90112/101520 (89%)] Loss: -1169.440186\n",
      "Train Epoch: 1347 [101376/101520 (100%)] Loss: -1169.220337\n",
      "    epoch          : 1347\n",
      "    loss           : -1168.5771067250314\n",
      "    ess            : 1.9665674126926977\n",
      "    log_marginal   : 1168.6091277922817\n",
      "    log_joint      : 1377.0624165750628\n",
      "    val_loss       : -1165.7310313349185\n",
      "    val_ess        : 1.9677734478660251\n",
      "    val_log_marginal: 1165.7602804432745\n",
      "    val_log_joint  : 1374.3193571671195\n",
      "Train Epoch: 1348 [0/101520 (0%)] Loss: -1170.375000\n",
      "Train Epoch: 1348 [11264/101520 (11%)] Loss: -1174.413818\n",
      "Train Epoch: 1348 [22528/101520 (22%)] Loss: -1167.279419\n",
      "Train Epoch: 1348 [33792/101520 (33%)] Loss: -1167.415649\n",
      "Train Epoch: 1348 [45056/101520 (44%)] Loss: -1160.135742\n",
      "Train Epoch: 1348 [56320/101520 (55%)] Loss: -1171.983887\n",
      "Train Epoch: 1348 [67584/101520 (67%)] Loss: -1166.953125\n",
      "Train Epoch: 1348 [78848/101520 (78%)] Loss: -1172.613403\n",
      "Train Epoch: 1348 [90112/101520 (89%)] Loss: -1169.543945\n",
      "Train Epoch: 1348 [101376/101520 (100%)] Loss: -1177.528809\n",
      "    epoch          : 1348\n",
      "    loss           : -1168.6577688245918\n",
      "    ess            : 1.967108170590808\n",
      "    log_marginal   : 1168.6894169332993\n",
      "    log_joint      : 1377.126468524262\n",
      "    val_loss       : -1165.9803838315217\n",
      "    val_ess        : 1.9678656225619109\n",
      "    val_log_marginal: 1166.010789954144\n",
      "    val_log_joint  : 1374.4956426205842\n",
      "Train Epoch: 1349 [0/101520 (0%)] Loss: -1171.191406\n",
      "Train Epoch: 1349 [11264/101520 (11%)] Loss: -1176.917480\n",
      "Train Epoch: 1349 [22528/101520 (22%)] Loss: -1165.885986\n",
      "Train Epoch: 1349 [33792/101520 (33%)] Loss: -1171.560913\n",
      "Train Epoch: 1349 [45056/101520 (44%)] Loss: -1167.050293\n",
      "Train Epoch: 1349 [56320/101520 (55%)] Loss: -1162.255981\n",
      "Train Epoch: 1349 [67584/101520 (67%)] Loss: -1162.931030\n",
      "Train Epoch: 1349 [78848/101520 (78%)] Loss: -1164.895508\n",
      "Train Epoch: 1349 [90112/101520 (89%)] Loss: -1168.753906\n",
      "Train Epoch: 1349 [101376/101520 (100%)] Loss: -1173.342651\n",
      "    epoch          : 1349\n",
      "    loss           : -1168.712951353447\n",
      "    ess            : 1.9671892592655353\n",
      "    log_marginal   : 1168.743770120132\n",
      "    log_joint      : 1377.2103161069017\n",
      "    val_loss       : -1166.3468972911005\n",
      "    val_ess        : 1.9682895204295283\n",
      "    val_log_marginal: 1166.3780199133832\n",
      "    val_log_joint  : 1374.8448327105978\n",
      "Train Epoch: 1350 [0/101520 (0%)] Loss: -1169.911133\n",
      "Train Epoch: 1350 [11264/101520 (11%)] Loss: -1168.499268\n",
      "Train Epoch: 1350 [22528/101520 (22%)] Loss: -1171.308350\n",
      "Train Epoch: 1350 [33792/101520 (33%)] Loss: -1167.479980\n",
      "Train Epoch: 1350 [45056/101520 (44%)] Loss: -1170.472900\n",
      "Train Epoch: 1350 [56320/101520 (55%)] Loss: -1168.835205\n",
      "Train Epoch: 1350 [67584/101520 (67%)] Loss: -1166.531494\n",
      "Train Epoch: 1350 [78848/101520 (78%)] Loss: -1166.906128\n",
      "Train Epoch: 1350 [90112/101520 (89%)] Loss: -1169.920044\n",
      "Train Epoch: 1350 [101376/101520 (100%)] Loss: -1161.334229\n",
      "    epoch          : 1350\n",
      "    loss           : -1168.752470236927\n",
      "    ess            : 1.9673790769960413\n",
      "    log_marginal   : 1168.7826694507694\n",
      "    log_joint      : 1377.1384872359847\n",
      "    val_loss       : -1167.3019117272418\n",
      "    val_ess        : 1.9652855396270752\n",
      "    val_log_marginal: 1167.3325460682745\n",
      "    val_log_joint  : 1375.5476498811142\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1350.pth ...\n",
      "Train Epoch: 1351 [0/101520 (0%)] Loss: -1169.682251\n",
      "Train Epoch: 1351 [11264/101520 (11%)] Loss: -1182.061279\n",
      "Train Epoch: 1351 [22528/101520 (22%)] Loss: -1168.393799\n",
      "Train Epoch: 1351 [33792/101520 (33%)] Loss: -1169.760498\n",
      "Train Epoch: 1351 [45056/101520 (44%)] Loss: -1177.720337\n",
      "Train Epoch: 1351 [56320/101520 (55%)] Loss: -1163.894897\n",
      "Train Epoch: 1351 [67584/101520 (67%)] Loss: -1168.449463\n",
      "Train Epoch: 1351 [78848/101520 (78%)] Loss: -1166.991943\n",
      "Train Epoch: 1351 [90112/101520 (89%)] Loss: -1178.197998\n",
      "Train Epoch: 1351 [101376/101520 (100%)] Loss: -1166.440918\n",
      "    epoch          : 1351\n",
      "    loss           : -1168.7369666938207\n",
      "    ess            : 1.966856715667188\n",
      "    log_marginal   : 1168.7679584445666\n",
      "    log_joint      : 1377.1510512768923\n",
      "    val_loss       : -1168.348489512568\n",
      "    val_ess        : 1.9656355018201082\n",
      "    val_log_marginal: 1168.3810929008152\n",
      "    val_log_joint  : 1376.881151282269\n",
      "Train Epoch: 1352 [0/101520 (0%)] Loss: -1176.675293\n",
      "Train Epoch: 1352 [11264/101520 (11%)] Loss: -1175.090454\n",
      "Train Epoch: 1352 [22528/101520 (22%)] Loss: -1169.621582\n",
      "Train Epoch: 1352 [33792/101520 (33%)] Loss: -1177.121826\n",
      "Train Epoch: 1352 [45056/101520 (44%)] Loss: -1168.890381\n",
      "Train Epoch: 1352 [56320/101520 (55%)] Loss: -1167.373291\n",
      "Train Epoch: 1352 [67584/101520 (67%)] Loss: -1171.109863\n",
      "Train Epoch: 1352 [78848/101520 (78%)] Loss: -1165.972168\n",
      "Train Epoch: 1352 [90112/101520 (89%)] Loss: -1172.111572\n",
      "Train Epoch: 1352 [101376/101520 (100%)] Loss: -1192.941650\n",
      "    epoch          : 1352\n",
      "    loss           : -1168.927294553824\n",
      "    ess            : 1.9665524845746294\n",
      "    log_marginal   : 1168.9592401705795\n",
      "    log_joint      : 1377.3631186940563\n",
      "    val_loss       : -1167.65161663553\n",
      "    val_ess        : 1.9680592391801917\n",
      "    val_log_marginal: 1167.6803509256115\n",
      "    val_log_joint  : 1376.1014775815217\n",
      "Train Epoch: 1353 [0/101520 (0%)] Loss: -1166.798340\n",
      "Train Epoch: 1353 [11264/101520 (11%)] Loss: -1168.596924\n",
      "Train Epoch: 1353 [22528/101520 (22%)] Loss: -1165.229736\n",
      "Train Epoch: 1353 [33792/101520 (33%)] Loss: -1168.689453\n",
      "Train Epoch: 1353 [45056/101520 (44%)] Loss: -1172.234375\n",
      "Train Epoch: 1353 [56320/101520 (55%)] Loss: -1172.644531\n",
      "Train Epoch: 1353 [67584/101520 (67%)] Loss: -1167.977783\n",
      "Train Epoch: 1353 [78848/101520 (78%)] Loss: -1172.213379\n",
      "Train Epoch: 1353 [90112/101520 (89%)] Loss: -1170.285400\n",
      "Train Epoch: 1353 [101376/101520 (100%)] Loss: -1168.221802\n",
      "    epoch          : 1353\n",
      "    loss           : -1168.842208478918\n",
      "    ess            : 1.9676007331915237\n",
      "    log_marginal   : 1168.8710428362515\n",
      "    log_joint      : 1377.277538203714\n",
      "    val_loss       : -1167.15625\n",
      "    val_ess        : 1.9671969724738079\n",
      "    val_log_marginal: 1167.1870701002038\n",
      "    val_log_joint  : 1375.3556810461957\n",
      "Train Epoch: 1354 [0/101520 (0%)] Loss: -1162.216431\n",
      "Train Epoch: 1354 [11264/101520 (11%)] Loss: -1168.611084\n",
      "Train Epoch: 1354 [22528/101520 (22%)] Loss: -1166.802612\n",
      "Train Epoch: 1354 [33792/101520 (33%)] Loss: -1174.862793\n",
      "Train Epoch: 1354 [45056/101520 (44%)] Loss: -1170.137329\n",
      "Train Epoch: 1354 [56320/101520 (55%)] Loss: -1169.788818\n",
      "Train Epoch: 1354 [67584/101520 (67%)] Loss: -1173.346191\n",
      "Train Epoch: 1354 [78848/101520 (78%)] Loss: -1169.530518\n",
      "Train Epoch: 1354 [90112/101520 (89%)] Loss: -1168.986938\n",
      "Train Epoch: 1354 [101376/101520 (100%)] Loss: -1165.038208\n",
      "    epoch          : 1354\n",
      "    loss           : -1168.9210223480684\n",
      "    ess            : 1.9671957882205446\n",
      "    log_marginal   : 1168.952044769747\n",
      "    log_joint      : 1377.295289312775\n",
      "    val_loss       : -1166.7736604110055\n",
      "    val_ess        : 1.9655224603155386\n",
      "    val_log_marginal: 1166.8056905995245\n",
      "    val_log_joint  : 1375.0987920346467\n",
      "Train Epoch: 1355 [0/101520 (0%)] Loss: -1179.057983\n",
      "Train Epoch: 1355 [11264/101520 (11%)] Loss: -1168.927979\n",
      "Train Epoch: 1355 [22528/101520 (22%)] Loss: -1164.835205\n",
      "Train Epoch: 1355 [33792/101520 (33%)] Loss: -1166.703491\n",
      "Train Epoch: 1355 [45056/101520 (44%)] Loss: -1170.578857\n",
      "Train Epoch: 1355 [56320/101520 (55%)] Loss: -1167.508545\n",
      "Train Epoch: 1355 [67584/101520 (67%)] Loss: -1171.243896\n",
      "Train Epoch: 1355 [78848/101520 (78%)] Loss: -1160.003418\n",
      "Train Epoch: 1355 [90112/101520 (89%)] Loss: -1171.135498\n",
      "Train Epoch: 1355 [101376/101520 (100%)] Loss: -1167.102417\n",
      "    epoch          : 1355\n",
      "    loss           : -1168.922725198257\n",
      "    ess            : 1.9664118146177512\n",
      "    log_marginal   : 1168.9550860994425\n",
      "    log_joint      : 1377.346435546875\n",
      "    val_loss       : -1168.168897545856\n",
      "    val_ess        : 1.9683339492134426\n",
      "    val_log_marginal: 1168.199462890625\n",
      "    val_log_joint  : 1376.7313073199728\n",
      "Train Epoch: 1356 [0/101520 (0%)] Loss: -1168.666748\n",
      "Train Epoch: 1356 [11264/101520 (11%)] Loss: -1162.056152\n",
      "Train Epoch: 1356 [22528/101520 (22%)] Loss: -1166.512207\n",
      "Train Epoch: 1356 [33792/101520 (33%)] Loss: -1171.883545\n",
      "Train Epoch: 1356 [45056/101520 (44%)] Loss: -1171.700195\n",
      "Train Epoch: 1356 [56320/101520 (55%)] Loss: -1167.554565\n",
      "Train Epoch: 1356 [67584/101520 (67%)] Loss: -1170.047974\n",
      "Train Epoch: 1356 [78848/101520 (78%)] Loss: -1167.580566\n",
      "Train Epoch: 1356 [90112/101520 (89%)] Loss: -1170.032471\n",
      "Train Epoch: 1356 [101376/101520 (100%)] Loss: -1155.535400\n",
      "    epoch          : 1356\n",
      "    loss           : -1168.920203434163\n",
      "    ess            : 1.967514185450185\n",
      "    log_marginal   : 1168.9500646543263\n",
      "    log_joint      : 1377.3800533428864\n",
      "    val_loss       : -1166.854200280231\n",
      "    val_ess        : 1.9656264263650645\n",
      "    val_log_marginal: 1166.8951840608017\n",
      "    val_log_joint  : 1375.3372961956522\n",
      "Train Epoch: 1357 [0/101520 (0%)] Loss: -1180.907959\n",
      "Train Epoch: 1357 [11264/101520 (11%)] Loss: -1161.626099\n",
      "Train Epoch: 1357 [22528/101520 (22%)] Loss: -1172.714355\n",
      "Train Epoch: 1357 [33792/101520 (33%)] Loss: -1163.955688\n",
      "Train Epoch: 1357 [45056/101520 (44%)] Loss: -1167.417480\n",
      "Train Epoch: 1357 [56320/101520 (55%)] Loss: -1167.722168\n",
      "Train Epoch: 1357 [67584/101520 (67%)] Loss: -1169.830566\n",
      "Train Epoch: 1357 [78848/101520 (78%)] Loss: -1176.816406\n",
      "Train Epoch: 1357 [90112/101520 (89%)] Loss: -1168.522095\n",
      "Train Epoch: 1357 [101376/101520 (100%)] Loss: -1168.725708\n",
      "    epoch          : 1357\n",
      "    loss           : -1169.0124769354586\n",
      "    ess            : 1.9677999786396123\n",
      "    log_marginal   : 1169.0421461555827\n",
      "    log_joint      : 1377.466941028384\n",
      "    val_loss       : -1166.433588442595\n",
      "    val_ess        : 1.9693207792613818\n",
      "    val_log_marginal: 1166.4619352921195\n",
      "    val_log_joint  : 1374.9085640285325\n",
      "Train Epoch: 1358 [0/101520 (0%)] Loss: -1174.526489\n",
      "Train Epoch: 1358 [11264/101520 (11%)] Loss: -1169.734497\n",
      "Train Epoch: 1358 [22528/101520 (22%)] Loss: -1172.866699\n",
      "Train Epoch: 1358 [33792/101520 (33%)] Loss: -1170.344238\n",
      "Train Epoch: 1358 [45056/101520 (44%)] Loss: -1169.229492\n",
      "Train Epoch: 1358 [56320/101520 (55%)] Loss: -1168.866699\n",
      "Train Epoch: 1358 [67584/101520 (67%)] Loss: -1175.224854\n",
      "Train Epoch: 1358 [78848/101520 (78%)] Loss: -1168.340332\n",
      "Train Epoch: 1358 [90112/101520 (89%)] Loss: -1167.389160\n",
      "Train Epoch: 1358 [101376/101520 (100%)] Loss: -1165.388306\n",
      "    epoch          : 1358\n",
      "    loss           : -1169.042883484807\n",
      "    ess            : 1.9670991016991775\n",
      "    log_marginal   : 1169.0737421237045\n",
      "    log_joint      : 1377.438750760639\n",
      "    val_loss       : -1168.4595151154892\n",
      "    val_ess        : 1.9695406115573386\n",
      "    val_log_marginal: 1168.4876762058425\n",
      "    val_log_joint  : 1376.9244384765625\n",
      "Train Epoch: 1359 [0/101520 (0%)] Loss: -1172.168335\n",
      "Train Epoch: 1359 [11264/101520 (11%)] Loss: -1168.895264\n",
      "Train Epoch: 1359 [22528/101520 (22%)] Loss: -1171.254272\n",
      "Train Epoch: 1359 [33792/101520 (33%)] Loss: -1170.604492\n",
      "Train Epoch: 1359 [45056/101520 (44%)] Loss: -1170.854492\n",
      "Train Epoch: 1359 [56320/101520 (55%)] Loss: -1166.739502\n",
      "Train Epoch: 1359 [67584/101520 (67%)] Loss: -1171.226318\n",
      "Train Epoch: 1359 [78848/101520 (78%)] Loss: -1170.129517\n",
      "Train Epoch: 1359 [90112/101520 (89%)] Loss: -1163.209717\n",
      "Train Epoch: 1359 [101376/101520 (100%)] Loss: -1158.019287\n",
      "    epoch          : 1359\n",
      "    loss           : -1168.918159523202\n",
      "    ess            : 1.9663843689252383\n",
      "    log_marginal   : 1168.9495960024733\n",
      "    log_joint      : 1377.40689347617\n",
      "    val_loss       : -1168.2336319633152\n",
      "    val_ess        : 1.9650158104689226\n",
      "    val_log_marginal: 1168.2697170091712\n",
      "    val_log_joint  : 1376.644435716712\n",
      "Train Epoch: 1360 [0/101520 (0%)] Loss: -1170.552490\n",
      "Train Epoch: 1360 [11264/101520 (11%)] Loss: -1171.354492\n",
      "Train Epoch: 1360 [22528/101520 (22%)] Loss: -1173.357178\n",
      "Train Epoch: 1360 [33792/101520 (33%)] Loss: -1170.509033\n",
      "Train Epoch: 1360 [45056/101520 (44%)] Loss: -1168.462036\n",
      "Train Epoch: 1360 [56320/101520 (55%)] Loss: -1165.427856\n",
      "Train Epoch: 1360 [67584/101520 (67%)] Loss: -1171.532593\n",
      "Train Epoch: 1360 [78848/101520 (78%)] Loss: -1168.092529\n",
      "Train Epoch: 1360 [90112/101520 (89%)] Loss: -1168.315552\n",
      "Train Epoch: 1360 [101376/101520 (100%)] Loss: -1179.515869\n",
      "    epoch          : 1360\n",
      "    loss           : -1169.0899118394707\n",
      "    ess            : 1.9666752306061175\n",
      "    log_marginal   : 1169.1222175329774\n",
      "    log_joint      : 1377.5729207561244\n",
      "    val_loss       : -1167.8147078804348\n",
      "    val_ess        : 1.969429876493371\n",
      "    val_log_marginal: 1167.843458092731\n",
      "    val_log_joint  : 1376.622171153193\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1360.pth ...\n",
      "Train Epoch: 1361 [0/101520 (0%)] Loss: -1173.854248\n",
      "Train Epoch: 1361 [11264/101520 (11%)] Loss: -1164.005615\n",
      "Train Epoch: 1361 [22528/101520 (22%)] Loss: -1161.540771\n",
      "Train Epoch: 1361 [33792/101520 (33%)] Loss: -1169.659668\n",
      "Train Epoch: 1361 [45056/101520 (44%)] Loss: -1173.868530\n",
      "Train Epoch: 1361 [56320/101520 (55%)] Loss: -1166.264893\n",
      "Train Epoch: 1361 [67584/101520 (67%)] Loss: -1173.807373\n",
      "Train Epoch: 1361 [78848/101520 (78%)] Loss: -1164.899414\n",
      "Train Epoch: 1361 [90112/101520 (89%)] Loss: -1163.043701\n",
      "Train Epoch: 1361 [101376/101520 (100%)] Loss: -1166.060425\n",
      "    epoch          : 1361\n",
      "    loss           : -1169.058151475149\n",
      "    ess            : 1.9668707392323557\n",
      "    log_marginal   : 1169.0884292065798\n",
      "    log_joint      : 1377.505073585702\n",
      "    val_loss       : -1167.328905188519\n",
      "    val_ess        : 1.9693050384521484\n",
      "    val_log_marginal: 1167.3569495159647\n",
      "    val_log_joint  : 1375.7265837296195\n",
      "Train Epoch: 1362 [0/101520 (0%)] Loss: -1172.586182\n",
      "Train Epoch: 1362 [11264/101520 (11%)] Loss: -1166.932129\n",
      "Train Epoch: 1362 [22528/101520 (22%)] Loss: -1167.552246\n",
      "Train Epoch: 1362 [33792/101520 (33%)] Loss: -1173.025146\n",
      "Train Epoch: 1362 [45056/101520 (44%)] Loss: -1162.637451\n",
      "Train Epoch: 1362 [56320/101520 (55%)] Loss: -1177.715210\n",
      "Train Epoch: 1362 [67584/101520 (67%)] Loss: -1169.673828\n",
      "Train Epoch: 1362 [78848/101520 (78%)] Loss: -1164.782471\n",
      "Train Epoch: 1362 [90112/101520 (89%)] Loss: -1163.557251\n",
      "Train Epoch: 1362 [101376/101520 (100%)] Loss: -1180.347534\n",
      "    epoch          : 1362\n",
      "    loss           : -1169.184805251845\n",
      "    ess            : 1.966609068851375\n",
      "    log_marginal   : 1169.2152866382694\n",
      "    log_joint      : 1377.6598802116048\n",
      "    val_loss       : -1169.4321554432745\n",
      "    val_ess        : 1.9673998096714849\n",
      "    val_log_marginal: 1169.461574388587\n",
      "    val_log_joint  : 1377.913478685462\n",
      "Train Epoch: 1363 [0/101520 (0%)] Loss: -1167.748291\n",
      "Train Epoch: 1363 [11264/101520 (11%)] Loss: -1161.213257\n",
      "Train Epoch: 1363 [22528/101520 (22%)] Loss: -1174.376709\n",
      "Train Epoch: 1363 [33792/101520 (33%)] Loss: -1170.302246\n",
      "Train Epoch: 1363 [45056/101520 (44%)] Loss: -1167.162476\n",
      "Train Epoch: 1363 [56320/101520 (55%)] Loss: -1173.364136\n",
      "Train Epoch: 1363 [67584/101520 (67%)] Loss: -1168.236084\n",
      "Train Epoch: 1363 [78848/101520 (78%)] Loss: -1168.254639\n",
      "Train Epoch: 1363 [90112/101520 (89%)] Loss: -1165.438232\n",
      "Train Epoch: 1363 [101376/101520 (100%)] Loss: -1166.091431\n",
      "    epoch          : 1363\n",
      "    loss           : -1169.168706692643\n",
      "    ess            : 1.9671657966009934\n",
      "    log_marginal   : 1169.198969088607\n",
      "    log_joint      : 1377.6122451858903\n",
      "    val_loss       : -1168.7962646484375\n",
      "    val_ess        : 1.9666547412457673\n",
      "    val_log_marginal: 1168.8259436565897\n",
      "    val_log_joint  : 1377.393559994905\n",
      "Train Epoch: 1364 [0/101520 (0%)] Loss: -1178.292969\n",
      "Train Epoch: 1364 [11264/101520 (11%)] Loss: -1163.764893\n",
      "Train Epoch: 1364 [22528/101520 (22%)] Loss: -1170.738525\n",
      "Train Epoch: 1364 [33792/101520 (33%)] Loss: -1163.891357\n",
      "Train Epoch: 1364 [45056/101520 (44%)] Loss: -1167.536865\n",
      "Train Epoch: 1364 [56320/101520 (55%)] Loss: -1160.822021\n",
      "Train Epoch: 1364 [67584/101520 (67%)] Loss: -1177.720947\n",
      "Train Epoch: 1364 [78848/101520 (78%)] Loss: -1170.670654\n",
      "Train Epoch: 1364 [90112/101520 (89%)] Loss: -1169.987793\n",
      "Train Epoch: 1364 [101376/101520 (100%)] Loss: -1175.550293\n",
      "    epoch          : 1364\n",
      "    loss           : -1169.2811193418263\n",
      "    ess            : 1.9662365452129038\n",
      "    log_marginal   : 1169.312722670972\n",
      "    log_joint      : 1377.6494901264134\n",
      "    val_loss       : -1168.6682712720788\n",
      "    val_ess        : 1.966925719509954\n",
      "    val_log_marginal: 1168.6990860648777\n",
      "    val_log_joint  : 1376.9987899116848\n",
      "Train Epoch: 1365 [0/101520 (0%)] Loss: -1172.184814\n",
      "Train Epoch: 1365 [11264/101520 (11%)] Loss: -1166.414551\n",
      "Train Epoch: 1365 [22528/101520 (22%)] Loss: -1171.672119\n",
      "Train Epoch: 1365 [33792/101520 (33%)] Loss: -1175.214355\n",
      "Train Epoch: 1365 [45056/101520 (44%)] Loss: -1168.103760\n",
      "Train Epoch: 1365 [56320/101520 (55%)] Loss: -1174.890137\n",
      "Train Epoch: 1365 [67584/101520 (67%)] Loss: -1161.788086\n",
      "Train Epoch: 1365 [78848/101520 (78%)] Loss: -1174.217529\n",
      "Train Epoch: 1365 [90112/101520 (89%)] Loss: -1165.917480\n",
      "Train Epoch: 1365 [101376/101520 (100%)] Loss: -1176.865356\n",
      "    epoch          : 1365\n",
      "    loss           : -1169.252695974992\n",
      "    ess            : 1.966166373473316\n",
      "    log_marginal   : 1169.285221885796\n",
      "    log_joint      : 1377.7060755437342\n",
      "    val_loss       : -1168.8762313179348\n",
      "    val_ess        : 1.9685670863027158\n",
      "    val_log_marginal: 1168.9067170516305\n",
      "    val_log_joint  : 1377.4763608186142\n",
      "Train Epoch: 1366 [0/101520 (0%)] Loss: -1167.157471\n",
      "Train Epoch: 1366 [11264/101520 (11%)] Loss: -1167.405273\n",
      "Train Epoch: 1366 [22528/101520 (22%)] Loss: -1169.718506\n",
      "Train Epoch: 1366 [33792/101520 (33%)] Loss: -1162.929199\n",
      "Train Epoch: 1366 [45056/101520 (44%)] Loss: -1173.112549\n",
      "Train Epoch: 1366 [56320/101520 (55%)] Loss: -1168.257324\n",
      "Train Epoch: 1366 [67584/101520 (67%)] Loss: -1165.998901\n",
      "Train Epoch: 1366 [78848/101520 (78%)] Loss: -1173.555664\n",
      "Train Epoch: 1366 [90112/101520 (89%)] Loss: -1178.912231\n",
      "Train Epoch: 1366 [101376/101520 (100%)] Loss: -1169.638672\n",
      "    epoch          : 1366\n",
      "    loss           : -1169.2778130152717\n",
      "    ess            : 1.9663222237447997\n",
      "    log_marginal   : 1169.3087477180827\n",
      "    log_joint      : 1377.7194069713803\n",
      "    val_loss       : -1167.8626602836277\n",
      "    val_ess        : 1.9674019243406213\n",
      "    val_log_marginal: 1167.8921110733695\n",
      "    val_log_joint  : 1376.4825121008832\n",
      "Train Epoch: 1367 [0/101520 (0%)] Loss: -1176.167236\n",
      "Train Epoch: 1367 [11264/101520 (11%)] Loss: -1171.159302\n",
      "Train Epoch: 1367 [22528/101520 (22%)] Loss: -1166.750977\n",
      "Train Epoch: 1367 [33792/101520 (33%)] Loss: -1175.869141\n",
      "Train Epoch: 1367 [45056/101520 (44%)] Loss: -1171.064697\n",
      "Train Epoch: 1367 [56320/101520 (55%)] Loss: -1168.200806\n",
      "Train Epoch: 1367 [67584/101520 (67%)] Loss: -1167.633789\n",
      "Train Epoch: 1367 [78848/101520 (78%)] Loss: -1169.609375\n",
      "Train Epoch: 1367 [90112/101520 (89%)] Loss: -1155.994629\n",
      "Train Epoch: 1367 [101376/101520 (100%)] Loss: -1165.222656\n",
      "    epoch          : 1367\n",
      "    loss           : -1169.3268607441505\n",
      "    ess            : 1.9672174489677852\n",
      "    log_marginal   : 1169.3574881242148\n",
      "    log_joint      : 1377.7158387150596\n",
      "    val_loss       : -1167.4866624915082\n",
      "    val_ess        : 1.965249258538951\n",
      "    val_log_marginal: 1167.520805027174\n",
      "    val_log_joint  : 1375.7974481997283\n",
      "Train Epoch: 1368 [0/101520 (0%)] Loss: -1168.803223\n",
      "Train Epoch: 1368 [11264/101520 (11%)] Loss: -1166.751709\n",
      "Train Epoch: 1368 [22528/101520 (22%)] Loss: -1167.095947\n",
      "Train Epoch: 1368 [33792/101520 (33%)] Loss: -1166.582275\n",
      "Train Epoch: 1368 [45056/101520 (44%)] Loss: -1163.108154\n",
      "Train Epoch: 1368 [56320/101520 (55%)] Loss: -1166.468750\n",
      "Train Epoch: 1368 [67584/101520 (67%)] Loss: -1167.977051\n",
      "Train Epoch: 1368 [78848/101520 (78%)] Loss: -1166.523804\n",
      "Train Epoch: 1368 [90112/101520 (89%)] Loss: -1174.953857\n",
      "Train Epoch: 1368 [101376/101520 (100%)] Loss: -1179.988892\n",
      "    epoch          : 1368\n",
      "    loss           : -1169.4381281407036\n",
      "    ess            : 1.9658092553891129\n",
      "    log_marginal   : 1169.470733182514\n",
      "    log_joint      : 1377.852485695077\n",
      "    val_loss       : -1169.112840735394\n",
      "    val_ess        : 1.969464711520983\n",
      "    val_log_marginal: 1169.1400093410325\n",
      "    val_log_joint  : 1377.1961563773777\n",
      "Train Epoch: 1369 [0/101520 (0%)] Loss: -1169.290771\n",
      "Train Epoch: 1369 [11264/101520 (11%)] Loss: -1169.502319\n",
      "Train Epoch: 1369 [22528/101520 (22%)] Loss: -1172.465332\n",
      "Train Epoch: 1369 [33792/101520 (33%)] Loss: -1172.498779\n",
      "Train Epoch: 1369 [45056/101520 (44%)] Loss: -1175.165527\n",
      "Train Epoch: 1369 [56320/101520 (55%)] Loss: -1170.794678\n",
      "Train Epoch: 1369 [67584/101520 (67%)] Loss: -1171.080078\n",
      "Train Epoch: 1369 [78848/101520 (78%)] Loss: -1173.377686\n",
      "Train Epoch: 1369 [90112/101520 (89%)] Loss: -1173.415039\n",
      "Train Epoch: 1369 [101376/101520 (100%)] Loss: -1172.284546\n",
      "    epoch          : 1369\n",
      "    loss           : -1169.434107181415\n",
      "    ess            : 1.9670177176969135\n",
      "    log_marginal   : 1169.4648462036746\n",
      "    log_joint      : 1377.8769948374686\n",
      "    val_loss       : -1167.8455014436142\n",
      "    val_ess        : 1.9663011457609094\n",
      "    val_log_marginal: 1167.8776483950408\n",
      "    val_log_joint  : 1376.1074643342392\n",
      "Train Epoch: 1370 [0/101520 (0%)] Loss: -1172.690796\n",
      "Train Epoch: 1370 [11264/101520 (11%)] Loss: -1163.150879\n",
      "Train Epoch: 1370 [22528/101520 (22%)] Loss: -1169.559570\n",
      "Train Epoch: 1370 [33792/101520 (33%)] Loss: -1168.128174\n",
      "Train Epoch: 1370 [45056/101520 (44%)] Loss: -1166.728027\n",
      "Train Epoch: 1370 [56320/101520 (55%)] Loss: -1167.049561\n",
      "Train Epoch: 1370 [67584/101520 (67%)] Loss: -1170.380005\n",
      "Train Epoch: 1370 [78848/101520 (78%)] Loss: -1176.752441\n",
      "Train Epoch: 1370 [90112/101520 (89%)] Loss: -1170.427490\n",
      "Train Epoch: 1370 [101376/101520 (100%)] Loss: -1166.584839\n",
      "    epoch          : 1370\n",
      "    loss           : -1169.44963403443\n",
      "    ess            : 1.9670262983695945\n",
      "    log_marginal   : 1169.480130142902\n",
      "    log_joint      : 1377.900841487712\n",
      "    val_loss       : -1169.4398671025815\n",
      "    val_ess        : 1.9674858891445657\n",
      "    val_log_marginal: 1169.4680918817935\n",
      "    val_log_joint  : 1377.5722921620245\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1370.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1371 [0/101520 (0%)] Loss: -1169.414551\n",
      "Train Epoch: 1371 [11264/101520 (11%)] Loss: -1165.275146\n",
      "Train Epoch: 1371 [22528/101520 (22%)] Loss: -1172.295654\n",
      "Train Epoch: 1371 [33792/101520 (33%)] Loss: -1169.739746\n",
      "Train Epoch: 1371 [45056/101520 (44%)] Loss: -1167.601807\n",
      "Train Epoch: 1371 [56320/101520 (55%)] Loss: -1172.183594\n",
      "Train Epoch: 1371 [67584/101520 (67%)] Loss: -1165.803711\n",
      "Train Epoch: 1371 [78848/101520 (78%)] Loss: -1164.969971\n",
      "Train Epoch: 1371 [90112/101520 (89%)] Loss: -1176.673340\n",
      "Train Epoch: 1371 [101376/101520 (100%)] Loss: -1174.394287\n",
      "    epoch          : 1371\n",
      "    loss           : -1169.433834210113\n",
      "    ess            : 1.9662047906137592\n",
      "    log_marginal   : 1169.4653240568075\n",
      "    log_joint      : 1377.8939116971576\n",
      "    val_loss       : -1169.769435716712\n",
      "    val_ess        : 1.9686742865520974\n",
      "    val_log_marginal: 1169.7985521399457\n",
      "    val_log_joint  : 1378.3227326766305\n",
      "Train Epoch: 1372 [0/101520 (0%)] Loss: -1170.125610\n",
      "Train Epoch: 1372 [11264/101520 (11%)] Loss: -1167.815552\n",
      "Train Epoch: 1372 [22528/101520 (22%)] Loss: -1174.155151\n",
      "Train Epoch: 1372 [33792/101520 (33%)] Loss: -1175.875732\n",
      "Train Epoch: 1372 [45056/101520 (44%)] Loss: -1168.224609\n",
      "Train Epoch: 1372 [56320/101520 (55%)] Loss: -1168.395752\n",
      "Train Epoch: 1372 [67584/101520 (67%)] Loss: -1169.539307\n",
      "Train Epoch: 1372 [78848/101520 (78%)] Loss: -1179.078369\n",
      "Train Epoch: 1372 [90112/101520 (89%)] Loss: -1165.425049\n",
      "Train Epoch: 1372 [101376/101520 (100%)] Loss: -1153.336060\n",
      "    epoch          : 1372\n",
      "    loss           : -1169.453543964942\n",
      "    ess            : 1.9668833814074647\n",
      "    log_marginal   : 1169.4846399968592\n",
      "    log_joint      : 1377.8325968220006\n",
      "    val_loss       : -1168.5528989045517\n",
      "    val_ess        : 1.9630351844041243\n",
      "    val_log_marginal: 1168.5874448029892\n",
      "    val_log_joint  : 1376.8727390455163\n",
      "Train Epoch: 1373 [0/101520 (0%)] Loss: -1176.685791\n",
      "Train Epoch: 1373 [11264/101520 (11%)] Loss: -1170.255493\n",
      "Train Epoch: 1373 [22528/101520 (22%)] Loss: -1169.022583\n",
      "Train Epoch: 1373 [33792/101520 (33%)] Loss: -1166.966309\n",
      "Train Epoch: 1373 [45056/101520 (44%)] Loss: -1168.074463\n",
      "Train Epoch: 1373 [56320/101520 (55%)] Loss: -1171.215088\n",
      "Train Epoch: 1373 [67584/101520 (67%)] Loss: -1171.691162\n",
      "Train Epoch: 1373 [78848/101520 (78%)] Loss: -1174.135620\n",
      "Train Epoch: 1373 [90112/101520 (89%)] Loss: -1168.784424\n",
      "Train Epoch: 1373 [101376/101520 (100%)] Loss: -1164.228149\n",
      "    epoch          : 1373\n",
      "    loss           : -1169.481726871663\n",
      "    ess            : 1.967027495853865\n",
      "    log_marginal   : 1169.512524168695\n",
      "    log_joint      : 1377.9083877640153\n",
      "    val_loss       : -1168.9686756963315\n",
      "    val_ess        : 1.9672053430391394\n",
      "    val_log_marginal: 1168.9987049932065\n",
      "    val_log_joint  : 1377.5061884341033\n",
      "Train Epoch: 1374 [0/101520 (0%)] Loss: -1163.125732\n",
      "Train Epoch: 1374 [11264/101520 (11%)] Loss: -1172.418213\n",
      "Train Epoch: 1374 [22528/101520 (22%)] Loss: -1174.711914\n",
      "Train Epoch: 1374 [33792/101520 (33%)] Loss: -1169.218262\n",
      "Train Epoch: 1374 [45056/101520 (44%)] Loss: -1171.712280\n",
      "Train Epoch: 1374 [56320/101520 (55%)] Loss: -1174.604736\n",
      "Train Epoch: 1374 [67584/101520 (67%)] Loss: -1165.495728\n",
      "Train Epoch: 1374 [78848/101520 (78%)] Loss: -1173.203125\n",
      "Train Epoch: 1374 [90112/101520 (89%)] Loss: -1163.233154\n",
      "Train Epoch: 1374 [101376/101520 (100%)] Loss: -1165.143066\n",
      "    epoch          : 1374\n",
      "    loss           : -1169.5127732166693\n",
      "    ess            : 1.9669268945952756\n",
      "    log_marginal   : 1169.5427926988457\n",
      "    log_joint      : 1377.97987557416\n",
      "    val_loss       : -1167.9560918393342\n",
      "    val_ess        : 1.9654723664988643\n",
      "    val_log_marginal: 1167.9883449388587\n",
      "    val_log_joint  : 1376.5577021059783\n",
      "Train Epoch: 1375 [0/101520 (0%)] Loss: -1173.623535\n",
      "Train Epoch: 1375 [11264/101520 (11%)] Loss: -1164.631836\n",
      "Train Epoch: 1375 [22528/101520 (22%)] Loss: -1171.243408\n",
      "Train Epoch: 1375 [33792/101520 (33%)] Loss: -1170.364990\n",
      "Train Epoch: 1375 [45056/101520 (44%)] Loss: -1172.259033\n",
      "Train Epoch: 1375 [56320/101520 (55%)] Loss: -1174.636963\n",
      "Train Epoch: 1375 [67584/101520 (67%)] Loss: -1166.143311\n",
      "Train Epoch: 1375 [78848/101520 (78%)] Loss: -1173.746338\n",
      "Train Epoch: 1375 [90112/101520 (89%)] Loss: -1171.058838\n",
      "Train Epoch: 1375 [101376/101520 (100%)] Loss: -1166.763428\n",
      "    epoch          : 1375\n",
      "    loss           : -1169.6481160686244\n",
      "    ess            : 1.9670205984882374\n",
      "    log_marginal   : 1169.678506055669\n",
      "    log_joint      : 1378.1074372104663\n",
      "    val_loss       : -1167.4756018597147\n",
      "    val_ess        : 1.9667311906814575\n",
      "    val_log_marginal: 1167.5058275305707\n",
      "    val_log_joint  : 1376.055366847826\n",
      "Train Epoch: 1376 [0/101520 (0%)] Loss: -1176.168945\n",
      "Train Epoch: 1376 [11264/101520 (11%)] Loss: -1161.572144\n",
      "Train Epoch: 1376 [22528/101520 (22%)] Loss: -1170.788452\n",
      "Train Epoch: 1376 [33792/101520 (33%)] Loss: -1170.212524\n",
      "Train Epoch: 1376 [45056/101520 (44%)] Loss: -1172.749878\n",
      "Train Epoch: 1376 [56320/101520 (55%)] Loss: -1169.168091\n",
      "Train Epoch: 1376 [67584/101520 (67%)] Loss: -1166.092529\n",
      "Train Epoch: 1376 [78848/101520 (78%)] Loss: -1169.230957\n",
      "Train Epoch: 1376 [90112/101520 (89%)] Loss: -1174.047119\n",
      "Train Epoch: 1376 [101376/101520 (100%)] Loss: -1174.200073\n",
      "    epoch          : 1376\n",
      "    loss           : -1169.646623621035\n",
      "    ess            : 1.9670001615831\n",
      "    log_marginal   : 1169.6778693271042\n",
      "    log_joint      : 1378.08876229291\n",
      "    val_loss       : -1169.577493418818\n",
      "    val_ess        : 1.969834161841351\n",
      "    val_log_marginal: 1169.6046514096467\n",
      "    val_log_joint  : 1377.9399732506793\n",
      "Train Epoch: 1377 [0/101520 (0%)] Loss: -1175.106201\n",
      "Train Epoch: 1377 [11264/101520 (11%)] Loss: -1166.144043\n",
      "Train Epoch: 1377 [22528/101520 (22%)] Loss: -1172.028320\n",
      "Train Epoch: 1377 [33792/101520 (33%)] Loss: -1170.756836\n",
      "Train Epoch: 1377 [45056/101520 (44%)] Loss: -1168.779297\n",
      "Train Epoch: 1377 [56320/101520 (55%)] Loss: -1164.852783\n",
      "Train Epoch: 1377 [67584/101520 (67%)] Loss: -1177.333740\n",
      "Train Epoch: 1377 [78848/101520 (78%)] Loss: -1171.267456\n",
      "Train Epoch: 1377 [90112/101520 (89%)] Loss: -1168.173096\n",
      "Train Epoch: 1377 [101376/101520 (100%)] Loss: -1165.429810\n",
      "    epoch          : 1377\n",
      "    loss           : -1169.6504016665358\n",
      "    ess            : 1.9672309747293366\n",
      "    log_marginal   : 1169.6800426694017\n",
      "    log_joint      : 1378.129922071294\n",
      "    val_loss       : -1168.1427320397418\n",
      "    val_ess        : 1.96264938686205\n",
      "    val_log_marginal: 1168.1753195057745\n",
      "    val_log_joint  : 1376.4589578379755\n",
      "Train Epoch: 1378 [0/101520 (0%)] Loss: -1172.054199\n",
      "Train Epoch: 1378 [11264/101520 (11%)] Loss: -1171.389160\n",
      "Train Epoch: 1378 [22528/101520 (22%)] Loss: -1168.136475\n",
      "Train Epoch: 1378 [33792/101520 (33%)] Loss: -1186.998047\n",
      "Train Epoch: 1378 [45056/101520 (44%)] Loss: -1167.339111\n",
      "Train Epoch: 1378 [56320/101520 (55%)] Loss: -1183.734131\n",
      "Train Epoch: 1378 [67584/101520 (67%)] Loss: -1166.914917\n",
      "Train Epoch: 1378 [78848/101520 (78%)] Loss: -1160.229004\n",
      "Train Epoch: 1378 [90112/101520 (89%)] Loss: -1171.762939\n",
      "Train Epoch: 1378 [101376/101520 (100%)] Loss: -1165.158691\n",
      "    epoch          : 1378\n",
      "    loss           : -1169.6865835525282\n",
      "    ess            : 1.9668366513659608\n",
      "    log_marginal   : 1169.7179844535176\n",
      "    log_joint      : 1378.215593961016\n",
      "    val_loss       : -1167.279493248981\n",
      "    val_ess        : 1.9709247039711995\n",
      "    val_log_marginal: 1167.3044009001358\n",
      "    val_log_joint  : 1375.6635317595108\n",
      "Train Epoch: 1379 [0/101520 (0%)] Loss: -1169.079590\n",
      "Train Epoch: 1379 [11264/101520 (11%)] Loss: -1169.624512\n",
      "Train Epoch: 1379 [22528/101520 (22%)] Loss: -1168.989258\n",
      "Train Epoch: 1379 [33792/101520 (33%)] Loss: -1165.724854\n",
      "Train Epoch: 1379 [45056/101520 (44%)] Loss: -1163.529053\n",
      "Train Epoch: 1379 [56320/101520 (55%)] Loss: -1177.184082\n",
      "Train Epoch: 1379 [67584/101520 (67%)] Loss: -1170.994385\n",
      "Train Epoch: 1379 [78848/101520 (78%)] Loss: -1165.225342\n",
      "Train Epoch: 1379 [90112/101520 (89%)] Loss: -1168.315430\n",
      "Train Epoch: 1379 [101376/101520 (100%)] Loss: -1164.486938\n",
      "    epoch          : 1379\n",
      "    loss           : -1169.827048450259\n",
      "    ess            : 1.966805818092883\n",
      "    log_marginal   : 1169.858765875275\n",
      "    log_joint      : 1378.2289388838724\n",
      "    val_loss       : -1166.2344068444293\n",
      "    val_ess        : 1.9687492173650991\n",
      "    val_log_marginal: 1166.2648607336957\n",
      "    val_log_joint  : 1374.8176694123642\n",
      "Train Epoch: 1380 [0/101520 (0%)] Loss: -1174.238525\n",
      "Train Epoch: 1380 [11264/101520 (11%)] Loss: -1166.705811\n",
      "Train Epoch: 1380 [22528/101520 (22%)] Loss: -1178.094238\n",
      "Train Epoch: 1380 [33792/101520 (33%)] Loss: -1169.348267\n",
      "Train Epoch: 1380 [45056/101520 (44%)] Loss: -1167.428223\n",
      "Train Epoch: 1380 [56320/101520 (55%)] Loss: -1175.984131\n",
      "Train Epoch: 1380 [67584/101520 (67%)] Loss: -1173.346924\n",
      "Train Epoch: 1380 [78848/101520 (78%)] Loss: -1173.369629\n",
      "Train Epoch: 1380 [90112/101520 (89%)] Loss: -1168.594482\n",
      "Train Epoch: 1380 [101376/101520 (100%)] Loss: -1164.174927\n",
      "    epoch          : 1380\n",
      "    loss           : -1169.758723426704\n",
      "    ess            : 1.9668037729646692\n",
      "    log_marginal   : 1169.7900672797582\n",
      "    log_joint      : 1378.239935026696\n",
      "    val_loss       : -1168.6344896399457\n",
      "    val_ess        : 1.963525917219079\n",
      "    val_log_marginal: 1168.6695078974185\n",
      "    val_log_joint  : 1376.9284880264945\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1380.pth ...\n",
      "Train Epoch: 1381 [0/101520 (0%)] Loss: -1174.001953\n",
      "Train Epoch: 1381 [11264/101520 (11%)] Loss: -1176.708130\n",
      "Train Epoch: 1381 [22528/101520 (22%)] Loss: -1168.040283\n",
      "Train Epoch: 1381 [33792/101520 (33%)] Loss: -1173.362061\n",
      "Train Epoch: 1381 [45056/101520 (44%)] Loss: -1164.335571\n",
      "Train Epoch: 1381 [56320/101520 (55%)] Loss: -1175.368042\n",
      "Train Epoch: 1381 [67584/101520 (67%)] Loss: -1168.383301\n",
      "Train Epoch: 1381 [78848/101520 (78%)] Loss: -1179.041626\n",
      "Train Epoch: 1381 [90112/101520 (89%)] Loss: -1166.133057\n",
      "Train Epoch: 1381 [101376/101520 (100%)] Loss: -1169.344482\n",
      "    epoch          : 1381\n",
      "    loss           : -1169.8674641518137\n",
      "    ess            : 1.9669766006757265\n",
      "    log_marginal   : 1169.897671340099\n",
      "    log_joint      : 1378.3685940689777\n",
      "    val_loss       : -1167.5566034731658\n",
      "    val_ess        : 1.9676425457000732\n",
      "    val_log_marginal: 1167.58666461447\n",
      "    val_log_joint  : 1376.374612559443\n",
      "Train Epoch: 1382 [0/101520 (0%)] Loss: -1175.047119\n",
      "Train Epoch: 1382 [11264/101520 (11%)] Loss: -1165.796143\n",
      "Train Epoch: 1382 [22528/101520 (22%)] Loss: -1167.546753\n",
      "Train Epoch: 1382 [33792/101520 (33%)] Loss: -1174.737915\n",
      "Train Epoch: 1382 [45056/101520 (44%)] Loss: -1172.995239\n",
      "Train Epoch: 1382 [56320/101520 (55%)] Loss: -1166.004761\n",
      "Train Epoch: 1382 [67584/101520 (67%)] Loss: -1170.021973\n",
      "Train Epoch: 1382 [78848/101520 (78%)] Loss: -1171.827637\n",
      "Train Epoch: 1382 [90112/101520 (89%)] Loss: -1167.257324\n",
      "Train Epoch: 1382 [101376/101520 (100%)] Loss: -1171.127441\n",
      "    epoch          : 1382\n",
      "    loss           : -1169.9385428117148\n",
      "    ess            : 1.9668349812378236\n",
      "    log_marginal   : 1169.9695327222048\n",
      "    log_joint      : 1378.3826793881517\n",
      "    val_loss       : -1168.9602475373642\n",
      "    val_ess        : 1.9674106525338215\n",
      "    val_log_marginal: 1168.991651452106\n",
      "    val_log_joint  : 1377.379298997962\n",
      "Train Epoch: 1383 [0/101520 (0%)] Loss: -1169.834473\n",
      "Train Epoch: 1383 [11264/101520 (11%)] Loss: -1172.863037\n",
      "Train Epoch: 1383 [22528/101520 (22%)] Loss: -1164.542358\n",
      "Train Epoch: 1383 [33792/101520 (33%)] Loss: -1171.251953\n",
      "Train Epoch: 1383 [45056/101520 (44%)] Loss: -1168.107422\n",
      "Train Epoch: 1383 [56320/101520 (55%)] Loss: -1172.301270\n",
      "Train Epoch: 1383 [67584/101520 (67%)] Loss: -1174.323242\n",
      "Train Epoch: 1383 [78848/101520 (78%)] Loss: -1171.340576\n",
      "Train Epoch: 1383 [90112/101520 (89%)] Loss: -1169.771729\n",
      "Train Epoch: 1383 [101376/101520 (100%)] Loss: -1169.061646\n",
      "    epoch          : 1383\n",
      "    loss           : -1170.014921408802\n",
      "    ess            : 1.9670630028499432\n",
      "    log_marginal   : 1170.0446305011385\n",
      "    log_joint      : 1378.404218357412\n",
      "    val_loss       : -1168.0207254161005\n",
      "    val_ess        : 1.9694666603337163\n",
      "    val_log_marginal: 1168.049125339674\n",
      "    val_log_joint  : 1376.373391856318\n",
      "Train Epoch: 1384 [0/101520 (0%)] Loss: -1167.227051\n",
      "Train Epoch: 1384 [11264/101520 (11%)] Loss: -1174.186401\n",
      "Train Epoch: 1384 [22528/101520 (22%)] Loss: -1166.649780\n",
      "Train Epoch: 1384 [33792/101520 (33%)] Loss: -1164.380981\n",
      "Train Epoch: 1384 [45056/101520 (44%)] Loss: -1170.002197\n",
      "Train Epoch: 1384 [56320/101520 (55%)] Loss: -1165.764771\n",
      "Train Epoch: 1384 [67584/101520 (67%)] Loss: -1175.648071\n",
      "Train Epoch: 1384 [78848/101520 (78%)] Loss: -1170.620605\n",
      "Train Epoch: 1384 [90112/101520 (89%)] Loss: -1166.414062\n",
      "Train Epoch: 1384 [101376/101520 (100%)] Loss: -1166.861816\n",
      "    epoch          : 1384\n",
      "    loss           : -1169.919973402167\n",
      "    ess            : 1.9668627809639552\n",
      "    log_marginal   : 1169.9512730890783\n",
      "    log_joint      : 1378.394696873037\n",
      "    val_loss       : -1166.8950036090353\n",
      "    val_ess        : 1.966585169667783\n",
      "    val_log_marginal: 1166.9253513502038\n",
      "    val_log_joint  : 1375.608101222826\n",
      "Train Epoch: 1385 [0/101520 (0%)] Loss: -1175.093872\n",
      "Train Epoch: 1385 [11264/101520 (11%)] Loss: -1169.247437\n",
      "Train Epoch: 1385 [22528/101520 (22%)] Loss: -1169.901367\n",
      "Train Epoch: 1385 [33792/101520 (33%)] Loss: -1167.601929\n",
      "Train Epoch: 1385 [45056/101520 (44%)] Loss: -1171.035156\n",
      "Train Epoch: 1385 [56320/101520 (55%)] Loss: -1164.305420\n",
      "Train Epoch: 1385 [67584/101520 (67%)] Loss: -1167.941650\n",
      "Train Epoch: 1385 [78848/101520 (78%)] Loss: -1169.350830\n",
      "Train Epoch: 1385 [90112/101520 (89%)] Loss: -1169.270874\n",
      "Train Epoch: 1385 [101376/101520 (100%)] Loss: -1166.520142\n",
      "    epoch          : 1385\n",
      "    loss           : -1169.9822071784704\n",
      "    ess            : 1.9676444734161223\n",
      "    log_marginal   : 1170.0126450121702\n",
      "    log_joint      : 1378.4417031446294\n",
      "    val_loss       : -1169.0672235903533\n",
      "    val_ess        : 1.9687751894411833\n",
      "    val_log_marginal: 1169.100246263587\n",
      "    val_log_joint  : 1377.4591064453125\n",
      "Train Epoch: 1386 [0/101520 (0%)] Loss: -1164.902588\n",
      "Train Epoch: 1386 [11264/101520 (11%)] Loss: -1170.772095\n",
      "Train Epoch: 1386 [22528/101520 (22%)] Loss: -1167.720581\n",
      "Train Epoch: 1386 [33792/101520 (33%)] Loss: -1167.516846\n",
      "Train Epoch: 1386 [45056/101520 (44%)] Loss: -1158.679443\n",
      "Train Epoch: 1386 [56320/101520 (55%)] Loss: -1172.314941\n",
      "Train Epoch: 1386 [67584/101520 (67%)] Loss: -1171.952271\n",
      "Train Epoch: 1386 [78848/101520 (78%)] Loss: -1170.692383\n",
      "Train Epoch: 1386 [90112/101520 (89%)] Loss: -1166.479980\n",
      "Train Epoch: 1386 [101376/101520 (100%)] Loss: -1162.833130\n",
      "    epoch          : 1386\n",
      "    loss           : -1170.049332355135\n",
      "    ess            : 1.9665263070533023\n",
      "    log_marginal   : 1170.0802603103407\n",
      "    log_joint      : 1378.4223804569724\n",
      "    val_loss       : -1168.1610638162365\n",
      "    val_ess        : 1.968205830325251\n",
      "    val_log_marginal: 1168.190084706182\n",
      "    val_log_joint  : 1376.5410474694293\n",
      "Train Epoch: 1387 [0/101520 (0%)] Loss: -1183.929810\n",
      "Train Epoch: 1387 [11264/101520 (11%)] Loss: -1166.579590\n",
      "Train Epoch: 1387 [22528/101520 (22%)] Loss: -1168.255249\n",
      "Train Epoch: 1387 [33792/101520 (33%)] Loss: -1169.112793\n",
      "Train Epoch: 1387 [45056/101520 (44%)] Loss: -1173.290283\n",
      "Train Epoch: 1387 [56320/101520 (55%)] Loss: -1163.061279\n",
      "Train Epoch: 1387 [67584/101520 (67%)] Loss: -1174.056519\n",
      "Train Epoch: 1387 [78848/101520 (78%)] Loss: -1172.324829\n",
      "Train Epoch: 1387 [90112/101520 (89%)] Loss: -1171.210571\n",
      "Train Epoch: 1387 [101376/101520 (100%)] Loss: -1173.350952\n",
      "    epoch          : 1387\n",
      "    loss           : -1170.0391091198178\n",
      "    ess            : 1.9670054529180478\n",
      "    log_marginal   : 1170.0703327428157\n",
      "    log_joint      : 1378.5060041418028\n",
      "    val_loss       : -1168.3262780230978\n",
      "    val_ess        : 1.9656137590822966\n",
      "    val_log_marginal: 1168.3581224524457\n",
      "    val_log_joint  : 1376.956059994905\n",
      "Train Epoch: 1388 [0/101520 (0%)] Loss: -1170.925537\n",
      "Train Epoch: 1388 [11264/101520 (11%)] Loss: -1171.691528\n",
      "Train Epoch: 1388 [22528/101520 (22%)] Loss: -1169.124512\n",
      "Train Epoch: 1388 [33792/101520 (33%)] Loss: -1170.687500\n",
      "Train Epoch: 1388 [45056/101520 (44%)] Loss: -1158.507080\n",
      "Train Epoch: 1388 [56320/101520 (55%)] Loss: -1172.834717\n",
      "Train Epoch: 1388 [67584/101520 (67%)] Loss: -1165.131348\n",
      "Train Epoch: 1388 [78848/101520 (78%)] Loss: -1167.782471\n",
      "Train Epoch: 1388 [90112/101520 (89%)] Loss: -1177.095337\n",
      "Train Epoch: 1388 [101376/101520 (100%)] Loss: -1176.325928\n",
      "    epoch          : 1388\n",
      "    loss           : -1170.1596017195352\n",
      "    ess            : 1.9674387301631908\n",
      "    log_marginal   : 1170.1909915789886\n",
      "    log_joint      : 1378.590881040947\n",
      "    val_loss       : -1169.575736667799\n",
      "    val_ess        : 1.966834192690642\n",
      "    val_log_marginal: 1169.6040941321332\n",
      "    val_log_joint  : 1378.277004076087\n",
      "Train Epoch: 1389 [0/101520 (0%)] Loss: -1174.825928\n",
      "Train Epoch: 1389 [11264/101520 (11%)] Loss: -1171.760132\n",
      "Train Epoch: 1389 [22528/101520 (22%)] Loss: -1170.341309\n",
      "Train Epoch: 1389 [33792/101520 (33%)] Loss: -1173.003662\n",
      "Train Epoch: 1389 [45056/101520 (44%)] Loss: -1171.065918\n",
      "Train Epoch: 1389 [56320/101520 (55%)] Loss: -1171.467041\n",
      "Train Epoch: 1389 [67584/101520 (67%)] Loss: -1169.100342\n",
      "Train Epoch: 1389 [78848/101520 (78%)] Loss: -1161.612671\n",
      "Train Epoch: 1389 [90112/101520 (89%)] Loss: -1167.180664\n",
      "Train Epoch: 1389 [101376/101520 (100%)] Loss: -1184.652954\n",
      "    epoch          : 1389\n",
      "    loss           : -1170.226961835545\n",
      "    ess            : 1.9664880246972318\n",
      "    log_marginal   : 1170.2590344299624\n",
      "    log_joint      : 1378.7447098775126\n",
      "    val_loss       : -1167.8856041949728\n",
      "    val_ess        : 1.9684577195540718\n",
      "    val_log_marginal: 1167.9167586616848\n",
      "    val_log_joint  : 1376.223043690557\n",
      "Train Epoch: 1390 [0/101520 (0%)] Loss: -1171.576172\n",
      "Train Epoch: 1390 [11264/101520 (11%)] Loss: -1169.964844\n",
      "Train Epoch: 1390 [22528/101520 (22%)] Loss: -1160.807007\n",
      "Train Epoch: 1390 [33792/101520 (33%)] Loss: -1174.077881\n",
      "Train Epoch: 1390 [45056/101520 (44%)] Loss: -1167.402466\n",
      "Train Epoch: 1390 [56320/101520 (55%)] Loss: -1164.674194\n",
      "Train Epoch: 1390 [67584/101520 (67%)] Loss: -1171.762329\n",
      "Train Epoch: 1390 [78848/101520 (78%)] Loss: -1176.855835\n",
      "Train Epoch: 1390 [90112/101520 (89%)] Loss: -1170.281250\n",
      "Train Epoch: 1390 [101376/101520 (100%)] Loss: -1170.044067\n",
      "    epoch          : 1390\n",
      "    loss           : -1170.2100504966238\n",
      "    ess            : 1.9668137074714929\n",
      "    log_marginal   : 1170.2409373527796\n",
      "    log_joint      : 1378.661006448257\n",
      "    val_loss       : -1169.0054825492527\n",
      "    val_ess        : 1.9672448375950689\n",
      "    val_log_marginal: 1169.033203125\n",
      "    val_log_joint  : 1377.7906494140625\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1390.pth ...\n",
      "Train Epoch: 1391 [0/101520 (0%)] Loss: -1174.144043\n",
      "Train Epoch: 1391 [11264/101520 (11%)] Loss: -1173.818848\n",
      "Train Epoch: 1391 [22528/101520 (22%)] Loss: -1170.766113\n",
      "Train Epoch: 1391 [33792/101520 (33%)] Loss: -1170.032837\n",
      "Train Epoch: 1391 [45056/101520 (44%)] Loss: -1172.875977\n",
      "Train Epoch: 1391 [56320/101520 (55%)] Loss: -1169.195435\n",
      "Train Epoch: 1391 [67584/101520 (67%)] Loss: -1169.509277\n",
      "Train Epoch: 1391 [78848/101520 (78%)] Loss: -1165.958008\n",
      "Train Epoch: 1391 [90112/101520 (89%)] Loss: -1172.196533\n",
      "Train Epoch: 1391 [101376/101520 (100%)] Loss: -1173.779785\n",
      "    epoch          : 1391\n",
      "    loss           : -1170.3133023516018\n",
      "    ess            : 1.9661739639301397\n",
      "    log_marginal   : 1170.3460214892823\n",
      "    log_joint      : 1378.737808917635\n",
      "    val_loss       : -1170.6139446756115\n",
      "    val_ess        : 1.9670022052267324\n",
      "    val_log_marginal: 1170.6462030825408\n",
      "    val_log_joint  : 1379.1314591117527\n",
      "Train Epoch: 1392 [0/101520 (0%)] Loss: -1173.677856\n",
      "Train Epoch: 1392 [11264/101520 (11%)] Loss: -1172.838135\n",
      "Train Epoch: 1392 [22528/101520 (22%)] Loss: -1175.932373\n",
      "Train Epoch: 1392 [33792/101520 (33%)] Loss: -1167.095947\n",
      "Train Epoch: 1392 [45056/101520 (44%)] Loss: -1171.580566\n",
      "Train Epoch: 1392 [56320/101520 (55%)] Loss: -1178.296143\n",
      "Train Epoch: 1392 [67584/101520 (67%)] Loss: -1171.494751\n",
      "Train Epoch: 1392 [78848/101520 (78%)] Loss: -1166.388916\n",
      "Train Epoch: 1392 [90112/101520 (89%)] Loss: -1169.227051\n",
      "Train Epoch: 1392 [101376/101520 (100%)] Loss: -1167.652588\n",
      "    epoch          : 1392\n",
      "    loss           : -1170.2275255672896\n",
      "    ess            : 1.9668708716205616\n",
      "    log_marginal   : 1170.258938736652\n",
      "    log_joint      : 1378.7083329243876\n",
      "    val_loss       : -1169.6289858610733\n",
      "    val_ess        : 1.9671754215074622\n",
      "    val_log_marginal: 1169.6573645550272\n",
      "    val_log_joint  : 1378.1825588060462\n",
      "Train Epoch: 1393 [0/101520 (0%)] Loss: -1173.340942\n",
      "Train Epoch: 1393 [11264/101520 (11%)] Loss: -1162.609619\n",
      "Train Epoch: 1393 [22528/101520 (22%)] Loss: -1163.246582\n",
      "Train Epoch: 1393 [33792/101520 (33%)] Loss: -1172.988770\n",
      "Train Epoch: 1393 [45056/101520 (44%)] Loss: -1177.036621\n",
      "Train Epoch: 1393 [56320/101520 (55%)] Loss: -1168.207642\n",
      "Train Epoch: 1393 [67584/101520 (67%)] Loss: -1169.061768\n",
      "Train Epoch: 1393 [78848/101520 (78%)] Loss: -1172.909912\n",
      "Train Epoch: 1393 [90112/101520 (89%)] Loss: -1173.246582\n",
      "Train Epoch: 1393 [101376/101520 (100%)] Loss: -1182.254517\n",
      "    epoch          : 1393\n",
      "    loss           : -1170.40103410117\n",
      "    ess            : 1.9661940300284917\n",
      "    log_marginal   : 1170.432995053392\n",
      "    log_joint      : 1378.851019011071\n",
      "    val_loss       : -1171.4116476307745\n",
      "    val_ess        : 1.968213957288991\n",
      "    val_log_marginal: 1171.44164508322\n",
      "    val_log_joint  : 1379.8136517068615\n",
      "Train Epoch: 1394 [0/101520 (0%)] Loss: -1170.303467\n",
      "Train Epoch: 1394 [11264/101520 (11%)] Loss: -1176.593628\n",
      "Train Epoch: 1394 [22528/101520 (22%)] Loss: -1166.931641\n",
      "Train Epoch: 1394 [33792/101520 (33%)] Loss: -1164.013550\n",
      "Train Epoch: 1394 [45056/101520 (44%)] Loss: -1172.409058\n",
      "Train Epoch: 1394 [56320/101520 (55%)] Loss: -1169.069458\n",
      "Train Epoch: 1394 [67584/101520 (67%)] Loss: -1173.449829\n",
      "Train Epoch: 1394 [78848/101520 (78%)] Loss: -1170.952148\n",
      "Train Epoch: 1394 [90112/101520 (89%)] Loss: -1170.833862\n",
      "Train Epoch: 1394 [101376/101520 (100%)] Loss: -1173.611572\n",
      "    epoch          : 1394\n",
      "    loss           : -1170.3858004622723\n",
      "    ess            : 1.9671610042677452\n",
      "    log_marginal   : 1170.4170664111573\n",
      "    log_joint      : 1378.8091832453283\n",
      "    val_loss       : -1171.8794369904892\n",
      "    val_ess        : 1.966395378112793\n",
      "    val_log_marginal: 1171.911281419837\n",
      "    val_log_joint  : 1380.083501401155\n",
      "Train Epoch: 1395 [0/101520 (0%)] Loss: -1169.857788\n",
      "Train Epoch: 1395 [11264/101520 (11%)] Loss: -1167.514771\n",
      "Train Epoch: 1395 [22528/101520 (22%)] Loss: -1172.228638\n",
      "Train Epoch: 1395 [33792/101520 (33%)] Loss: -1168.697266\n",
      "Train Epoch: 1395 [45056/101520 (44%)] Loss: -1166.852417\n",
      "Train Epoch: 1395 [56320/101520 (55%)] Loss: -1166.183838\n",
      "Train Epoch: 1395 [67584/101520 (67%)] Loss: -1179.790894\n",
      "Train Epoch: 1395 [78848/101520 (78%)] Loss: -1169.280884\n",
      "Train Epoch: 1395 [90112/101520 (89%)] Loss: -1174.123779\n",
      "Train Epoch: 1395 [101376/101520 (100%)] Loss: -1163.704224\n",
      "    epoch          : 1395\n",
      "    loss           : -1170.3927425211998\n",
      "    ess            : 1.9668368442573738\n",
      "    log_marginal   : 1170.4249193967887\n",
      "    log_joint      : 1378.887185561597\n",
      "    val_loss       : -1168.634914232337\n",
      "    val_ess        : 1.9691502322321353\n",
      "    val_log_marginal: 1168.6625605044158\n",
      "    val_log_joint  : 1377.1409487516983\n",
      "Train Epoch: 1396 [0/101520 (0%)] Loss: -1171.563477\n",
      "Train Epoch: 1396 [11264/101520 (11%)] Loss: -1168.910645\n",
      "Train Epoch: 1396 [22528/101520 (22%)] Loss: -1169.802002\n",
      "Train Epoch: 1396 [33792/101520 (33%)] Loss: -1172.178101\n",
      "Train Epoch: 1396 [45056/101520 (44%)] Loss: -1168.610352\n",
      "Train Epoch: 1396 [56320/101520 (55%)] Loss: -1173.214111\n",
      "Train Epoch: 1396 [67584/101520 (67%)] Loss: -1166.095093\n",
      "Train Epoch: 1396 [78848/101520 (78%)] Loss: -1169.194824\n",
      "Train Epoch: 1396 [90112/101520 (89%)] Loss: -1165.817627\n",
      "Train Epoch: 1396 [101376/101520 (100%)] Loss: -1154.419067\n",
      "    epoch          : 1396\n",
      "    loss           : -1170.442136831619\n",
      "    ess            : 1.9661578874492167\n",
      "    log_marginal   : 1170.4751872153738\n",
      "    log_joint      : 1378.9088631634736\n",
      "    val_loss       : -1168.4536504330842\n",
      "    val_ess        : 1.968357283136119\n",
      "    val_log_marginal: 1168.4833188264267\n",
      "    val_log_joint  : 1377.0192021908967\n",
      "Train Epoch: 1397 [0/101520 (0%)] Loss: -1167.920410\n",
      "Train Epoch: 1397 [11264/101520 (11%)] Loss: -1164.935791\n",
      "Train Epoch: 1397 [22528/101520 (22%)] Loss: -1168.913330\n",
      "Train Epoch: 1397 [33792/101520 (33%)] Loss: -1173.933838\n",
      "Train Epoch: 1397 [45056/101520 (44%)] Loss: -1171.437744\n",
      "Train Epoch: 1397 [56320/101520 (55%)] Loss: -1177.447876\n",
      "Train Epoch: 1397 [67584/101520 (67%)] Loss: -1170.648926\n",
      "Train Epoch: 1397 [78848/101520 (78%)] Loss: -1174.461914\n",
      "Train Epoch: 1397 [90112/101520 (89%)] Loss: -1173.990601\n",
      "Train Epoch: 1397 [101376/101520 (100%)] Loss: -1182.155640\n",
      "    epoch          : 1397\n",
      "    loss           : -1170.5711602445822\n",
      "    ess            : 1.9662344611469824\n",
      "    log_marginal   : 1170.6032917271907\n",
      "    log_joint      : 1378.9931935065954\n",
      "    val_loss       : -1170.6314697265625\n",
      "    val_ess        : 1.9661642623984295\n",
      "    val_log_marginal: 1170.6603685461957\n",
      "    val_log_joint  : 1379.128518809443\n",
      "Train Epoch: 1398 [0/101520 (0%)] Loss: -1162.774902\n",
      "Train Epoch: 1398 [11264/101520 (11%)] Loss: -1173.574951\n",
      "Train Epoch: 1398 [22528/101520 (22%)] Loss: -1168.545654\n",
      "Train Epoch: 1398 [33792/101520 (33%)] Loss: -1175.001709\n",
      "Train Epoch: 1398 [45056/101520 (44%)] Loss: -1175.129639\n",
      "Train Epoch: 1398 [56320/101520 (55%)] Loss: -1169.870850\n",
      "Train Epoch: 1398 [67584/101520 (67%)] Loss: -1162.671143\n",
      "Train Epoch: 1398 [78848/101520 (78%)] Loss: -1177.007324\n",
      "Train Epoch: 1398 [90112/101520 (89%)] Loss: -1173.879761\n",
      "Train Epoch: 1398 [101376/101520 (100%)] Loss: -1165.864868\n",
      "    epoch          : 1398\n",
      "    loss           : -1170.578060591041\n",
      "    ess            : 1.9667335562969572\n",
      "    log_marginal   : 1170.6089388593357\n",
      "    log_joint      : 1379.0271216924466\n",
      "    val_loss       : -1168.0097974694293\n",
      "    val_ess        : 1.965508072272591\n",
      "    val_log_marginal: 1168.0428307574728\n",
      "    val_log_joint  : 1376.549852454144\n",
      "Train Epoch: 1399 [0/101520 (0%)] Loss: -1170.102295\n",
      "Train Epoch: 1399 [11264/101520 (11%)] Loss: -1170.470215\n",
      "Train Epoch: 1399 [22528/101520 (22%)] Loss: -1172.031006\n",
      "Train Epoch: 1399 [33792/101520 (33%)] Loss: -1167.428467\n",
      "Train Epoch: 1399 [45056/101520 (44%)] Loss: -1173.926758\n",
      "Train Epoch: 1399 [56320/101520 (55%)] Loss: -1167.426636\n",
      "Train Epoch: 1399 [67584/101520 (67%)] Loss: -1164.806152\n",
      "Train Epoch: 1399 [78848/101520 (78%)] Loss: -1164.735840\n",
      "Train Epoch: 1399 [90112/101520 (89%)] Loss: -1174.177856\n",
      "Train Epoch: 1399 [101376/101520 (100%)] Loss: -1159.181396\n",
      "    epoch          : 1399\n",
      "    loss           : -1170.5448973382538\n",
      "    ess            : 1.967021414382973\n",
      "    log_marginal   : 1170.5758694596027\n",
      "    log_joint      : 1379.0073285126805\n",
      "    val_loss       : -1167.9831277598505\n",
      "    val_ess        : 1.9643930248592212\n",
      "    val_log_marginal: 1168.014940344769\n",
      "    val_log_joint  : 1376.6582721212635\n",
      "Train Epoch: 1400 [0/101520 (0%)] Loss: -1172.998779\n",
      "Train Epoch: 1400 [11264/101520 (11%)] Loss: -1169.755615\n",
      "Train Epoch: 1400 [22528/101520 (22%)] Loss: -1175.139526\n",
      "Train Epoch: 1400 [33792/101520 (33%)] Loss: -1168.110474\n",
      "Train Epoch: 1400 [45056/101520 (44%)] Loss: -1167.093018\n",
      "Train Epoch: 1400 [56320/101520 (55%)] Loss: -1170.687256\n",
      "Train Epoch: 1400 [67584/101520 (67%)] Loss: -1166.167114\n",
      "Train Epoch: 1400 [78848/101520 (78%)] Loss: -1171.891479\n",
      "Train Epoch: 1400 [90112/101520 (89%)] Loss: -1164.171021\n",
      "Train Epoch: 1400 [101376/101520 (100%)] Loss: -1186.506348\n",
      "    epoch          : 1400\n",
      "    loss           : -1170.6694323669126\n",
      "    ess            : 1.9671350825372054\n",
      "    log_marginal   : 1170.6987795422424\n",
      "    log_joint      : 1379.144746559948\n",
      "    val_loss       : -1168.5863886294158\n",
      "    val_ess        : 1.968709541403729\n",
      "    val_log_marginal: 1168.6150008491848\n",
      "    val_log_joint  : 1376.7882557744565\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1400.pth ...\n",
      "Train Epoch: 1401 [0/101520 (0%)] Loss: -1172.058838\n",
      "Train Epoch: 1401 [11264/101520 (11%)] Loss: -1170.765381\n",
      "Train Epoch: 1401 [22528/101520 (22%)] Loss: -1174.836182\n",
      "Train Epoch: 1401 [33792/101520 (33%)] Loss: -1169.379639\n",
      "Train Epoch: 1401 [45056/101520 (44%)] Loss: -1163.431519\n",
      "Train Epoch: 1401 [56320/101520 (55%)] Loss: -1166.857300\n",
      "Train Epoch: 1401 [67584/101520 (67%)] Loss: -1170.624756\n",
      "Train Epoch: 1401 [78848/101520 (78%)] Loss: -1162.147827\n",
      "Train Epoch: 1401 [90112/101520 (89%)] Loss: -1165.790771\n",
      "Train Epoch: 1401 [101376/101520 (100%)] Loss: -1170.681641\n",
      "    epoch          : 1401\n",
      "    loss           : -1170.7311054785646\n",
      "    ess            : 1.9672684975005874\n",
      "    log_marginal   : 1170.7614758362124\n",
      "    log_joint      : 1379.126429265468\n",
      "    val_loss       : -1169.1131061056385\n",
      "    val_ess        : 1.9672672489415044\n",
      "    val_log_marginal: 1169.142530358356\n",
      "    val_log_joint  : 1377.5632005774457\n",
      "Train Epoch: 1402 [0/101520 (0%)] Loss: -1167.620972\n",
      "Train Epoch: 1402 [11264/101520 (11%)] Loss: -1170.567627\n",
      "Train Epoch: 1402 [22528/101520 (22%)] Loss: -1172.464844\n",
      "Train Epoch: 1402 [33792/101520 (33%)] Loss: -1169.496948\n",
      "Train Epoch: 1402 [45056/101520 (44%)] Loss: -1172.401733\n",
      "Train Epoch: 1402 [56320/101520 (55%)] Loss: -1172.093018\n",
      "Train Epoch: 1402 [67584/101520 (67%)] Loss: -1170.155762\n",
      "Train Epoch: 1402 [78848/101520 (78%)] Loss: -1166.817383\n",
      "Train Epoch: 1402 [90112/101520 (89%)] Loss: -1170.065308\n",
      "Train Epoch: 1402 [101376/101520 (100%)] Loss: -1174.268433\n",
      "    epoch          : 1402\n",
      "    loss           : -1170.7383830774968\n",
      "    ess            : 1.967010823925536\n",
      "    log_marginal   : 1170.769579710074\n",
      "    log_joint      : 1379.1423806041928\n",
      "    val_loss       : -1171.8656908118207\n",
      "    val_ess        : 1.9684555686038474\n",
      "    val_log_marginal: 1171.8936289911685\n",
      "    val_log_joint  : 1380.2406377377717\n",
      "Train Epoch: 1403 [0/101520 (0%)] Loss: -1167.083618\n",
      "Train Epoch: 1403 [11264/101520 (11%)] Loss: -1171.498291\n",
      "Train Epoch: 1403 [22528/101520 (22%)] Loss: -1166.988770\n",
      "Train Epoch: 1403 [33792/101520 (33%)] Loss: -1174.904907\n",
      "Train Epoch: 1403 [45056/101520 (44%)] Loss: -1169.128174\n",
      "Train Epoch: 1403 [56320/101520 (55%)] Loss: -1173.932007\n",
      "Train Epoch: 1403 [67584/101520 (67%)] Loss: -1173.339478\n",
      "Train Epoch: 1403 [78848/101520 (78%)] Loss: -1167.626831\n",
      "Train Epoch: 1403 [90112/101520 (89%)] Loss: -1166.887451\n",
      "Train Epoch: 1403 [101376/101520 (100%)] Loss: -1179.671875\n",
      "    epoch          : 1403\n",
      "    loss           : -1170.6915620583386\n",
      "    ess            : 1.9668124608658066\n",
      "    log_marginal   : 1170.7227580774968\n",
      "    log_joint      : 1379.1606813363694\n",
      "    val_loss       : -1169.35669476053\n",
      "    val_ess        : 1.969522061555282\n",
      "    val_log_marginal: 1169.3861508576767\n",
      "    val_log_joint  : 1378.0337232506793\n",
      "Train Epoch: 1404 [0/101520 (0%)] Loss: -1165.162598\n",
      "Train Epoch: 1404 [11264/101520 (11%)] Loss: -1172.319092\n",
      "Train Epoch: 1404 [22528/101520 (22%)] Loss: -1172.254517\n",
      "Train Epoch: 1404 [33792/101520 (33%)] Loss: -1170.733643\n",
      "Train Epoch: 1404 [45056/101520 (44%)] Loss: -1166.276367\n",
      "Train Epoch: 1404 [56320/101520 (55%)] Loss: -1170.072876\n",
      "Train Epoch: 1404 [67584/101520 (67%)] Loss: -1167.601929\n",
      "Train Epoch: 1404 [78848/101520 (78%)] Loss: -1180.336060\n",
      "Train Epoch: 1404 [90112/101520 (89%)] Loss: -1173.613770\n",
      "Train Epoch: 1404 [101376/101520 (100%)] Loss: -1158.937500\n",
      "    epoch          : 1404\n",
      "    loss           : -1170.6919988124214\n",
      "    ess            : 1.9663836488771678\n",
      "    log_marginal   : 1170.724292237555\n",
      "    log_joint      : 1379.2001082070508\n",
      "    val_loss       : -1171.2807988705842\n",
      "    val_ess        : 1.9674236670784329\n",
      "    val_log_marginal: 1171.3122399371603\n",
      "    val_log_joint  : 1379.767434825068\n",
      "Train Epoch: 1405 [0/101520 (0%)] Loss: -1175.710938\n",
      "Train Epoch: 1405 [11264/101520 (11%)] Loss: -1167.822632\n",
      "Train Epoch: 1405 [22528/101520 (22%)] Loss: -1174.999023\n",
      "Train Epoch: 1405 [33792/101520 (33%)] Loss: -1172.490234\n",
      "Train Epoch: 1405 [45056/101520 (44%)] Loss: -1170.650635\n",
      "Train Epoch: 1405 [56320/101520 (55%)] Loss: -1167.875732\n",
      "Train Epoch: 1405 [67584/101520 (67%)] Loss: -1173.668213\n",
      "Train Epoch: 1405 [78848/101520 (78%)] Loss: -1175.884521\n",
      "Train Epoch: 1405 [90112/101520 (89%)] Loss: -1176.763428\n",
      "Train Epoch: 1405 [101376/101520 (100%)] Loss: -1173.397583\n",
      "    epoch          : 1405\n",
      "    loss           : -1170.877480665044\n",
      "    ess            : 1.9671831634176435\n",
      "    log_marginal   : 1170.9088975149184\n",
      "    log_joint      : 1379.3029417105056\n",
      "    val_loss       : -1169.7775719684103\n",
      "    val_ess        : 1.967524476673292\n",
      "    val_log_marginal: 1169.8098462975543\n",
      "    val_log_joint  : 1378.331834876019\n",
      "Train Epoch: 1406 [0/101520 (0%)] Loss: -1167.739014\n",
      "Train Epoch: 1406 [11264/101520 (11%)] Loss: -1168.301636\n",
      "Train Epoch: 1406 [22528/101520 (22%)] Loss: -1178.083740\n",
      "Train Epoch: 1406 [33792/101520 (33%)] Loss: -1170.880859\n",
      "Train Epoch: 1406 [45056/101520 (44%)] Loss: -1171.291260\n",
      "Train Epoch: 1406 [56320/101520 (55%)] Loss: -1166.058105\n",
      "Train Epoch: 1406 [67584/101520 (67%)] Loss: -1177.137695\n",
      "Train Epoch: 1406 [78848/101520 (78%)] Loss: -1165.309204\n",
      "Train Epoch: 1406 [90112/101520 (89%)] Loss: -1176.792725\n",
      "Train Epoch: 1406 [101376/101520 (100%)] Loss: -1178.267212\n",
      "    epoch          : 1406\n",
      "    loss           : -1170.9516233511306\n",
      "    ess            : 1.967402486345876\n",
      "    log_marginal   : 1170.9818986288867\n",
      "    log_joint      : 1379.341601194449\n",
      "    val_loss       : -1170.5162778108017\n",
      "    val_ess        : 1.9674056820247485\n",
      "    val_log_marginal: 1170.5458294412365\n",
      "    val_log_joint  : 1378.8924507472825\n",
      "Train Epoch: 1407 [0/101520 (0%)] Loss: -1174.043945\n",
      "Train Epoch: 1407 [11264/101520 (11%)] Loss: -1166.004150\n",
      "Train Epoch: 1407 [22528/101520 (22%)] Loss: -1170.076416\n",
      "Train Epoch: 1407 [33792/101520 (33%)] Loss: -1172.818115\n",
      "Train Epoch: 1407 [45056/101520 (44%)] Loss: -1173.413330\n",
      "Train Epoch: 1407 [56320/101520 (55%)] Loss: -1168.933350\n",
      "Train Epoch: 1407 [67584/101520 (67%)] Loss: -1174.216309\n",
      "Train Epoch: 1407 [78848/101520 (78%)] Loss: -1167.110229\n",
      "Train Epoch: 1407 [90112/101520 (89%)] Loss: -1166.916992\n",
      "Train Epoch: 1407 [101376/101520 (100%)] Loss: -1173.111572\n",
      "    epoch          : 1407\n",
      "    loss           : -1170.8717310919833\n",
      "    ess            : 1.966228011265472\n",
      "    log_marginal   : 1170.9026148810458\n",
      "    log_joint      : 1379.348402780504\n",
      "    val_loss       : -1170.5266007133152\n",
      "    val_ess        : 1.9681031030157339\n",
      "    val_log_marginal: 1170.5568582286005\n",
      "    val_log_joint  : 1379.116842518682\n",
      "Train Epoch: 1408 [0/101520 (0%)] Loss: -1176.650635\n",
      "Train Epoch: 1408 [11264/101520 (11%)] Loss: -1171.151001\n",
      "Train Epoch: 1408 [22528/101520 (22%)] Loss: -1166.583862\n",
      "Train Epoch: 1408 [33792/101520 (33%)] Loss: -1171.512329\n",
      "Train Epoch: 1408 [45056/101520 (44%)] Loss: -1171.132080\n",
      "Train Epoch: 1408 [56320/101520 (55%)] Loss: -1174.956787\n",
      "Train Epoch: 1408 [67584/101520 (67%)] Loss: -1176.935059\n",
      "Train Epoch: 1408 [78848/101520 (78%)] Loss: -1169.573975\n",
      "Train Epoch: 1408 [90112/101520 (89%)] Loss: -1171.161377\n",
      "Train Epoch: 1408 [101376/101520 (100%)] Loss: -1162.140381\n",
      "    epoch          : 1408\n",
      "    loss           : -1170.9728549976444\n",
      "    ess            : 1.966045593496543\n",
      "    log_marginal   : 1171.0060397200848\n",
      "    log_joint      : 1379.397938790633\n",
      "    val_loss       : -1168.9111434273098\n",
      "    val_ess        : 1.966642571532208\n",
      "    val_log_marginal: 1168.943502674932\n",
      "    val_log_joint  : 1377.2118238366168\n",
      "Train Epoch: 1409 [0/101520 (0%)] Loss: -1169.087402\n",
      "Train Epoch: 1409 [11264/101520 (11%)] Loss: -1169.824707\n",
      "Train Epoch: 1409 [22528/101520 (22%)] Loss: -1176.312988\n",
      "Train Epoch: 1409 [33792/101520 (33%)] Loss: -1175.301880\n",
      "Train Epoch: 1409 [45056/101520 (44%)] Loss: -1170.883667\n",
      "Train Epoch: 1409 [56320/101520 (55%)] Loss: -1163.036377\n",
      "Train Epoch: 1409 [67584/101520 (67%)] Loss: -1168.078247\n",
      "Train Epoch: 1409 [78848/101520 (78%)] Loss: -1171.571777\n",
      "Train Epoch: 1409 [90112/101520 (89%)] Loss: -1172.257446\n",
      "Train Epoch: 1409 [101376/101520 (100%)] Loss: -1181.153442\n",
      "    epoch          : 1409\n",
      "    loss           : -1171.0816539975267\n",
      "    ess            : 1.9667833582240732\n",
      "    log_marginal   : 1171.1122684957993\n",
      "    log_joint      : 1379.5235221517744\n",
      "    val_loss       : -1169.2859045940897\n",
      "    val_ess        : 1.9679315245669822\n",
      "    val_log_marginal: 1169.3141824473505\n",
      "    val_log_joint  : 1377.7627749235733\n",
      "Train Epoch: 1410 [0/101520 (0%)] Loss: -1167.427246\n",
      "Train Epoch: 1410 [11264/101520 (11%)] Loss: -1167.860229\n",
      "Train Epoch: 1410 [22528/101520 (22%)] Loss: -1165.155151\n",
      "Train Epoch: 1410 [33792/101520 (33%)] Loss: -1164.483521\n",
      "Train Epoch: 1410 [45056/101520 (44%)] Loss: -1171.229370\n",
      "Train Epoch: 1410 [56320/101520 (55%)] Loss: -1177.851807\n",
      "Train Epoch: 1410 [67584/101520 (67%)] Loss: -1168.257324\n",
      "Train Epoch: 1410 [78848/101520 (78%)] Loss: -1166.168701\n",
      "Train Epoch: 1410 [90112/101520 (89%)] Loss: -1168.711914\n",
      "Train Epoch: 1410 [101376/101520 (100%)] Loss: -1172.128296\n",
      "    epoch          : 1410\n",
      "    loss           : -1171.0487802783448\n",
      "    ess            : 1.9670805122385073\n",
      "    log_marginal   : 1171.0798793773556\n",
      "    log_joint      : 1379.515532373783\n",
      "    val_loss       : -1169.7131188434103\n",
      "    val_ess        : 1.967529042907383\n",
      "    val_log_marginal: 1169.7434878141983\n",
      "    val_log_joint  : 1377.9868960173233\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1410.pth ...\n",
      "Train Epoch: 1411 [0/101520 (0%)] Loss: -1165.338379\n",
      "Train Epoch: 1411 [11264/101520 (11%)] Loss: -1176.601074\n",
      "Train Epoch: 1411 [22528/101520 (22%)] Loss: -1171.078125\n",
      "Train Epoch: 1411 [33792/101520 (33%)] Loss: -1175.390991\n",
      "Train Epoch: 1411 [45056/101520 (44%)] Loss: -1168.342285\n",
      "Train Epoch: 1411 [56320/101520 (55%)] Loss: -1168.980591\n",
      "Train Epoch: 1411 [67584/101520 (67%)] Loss: -1168.695923\n",
      "Train Epoch: 1411 [78848/101520 (78%)] Loss: -1170.221558\n",
      "Train Epoch: 1411 [90112/101520 (89%)] Loss: -1168.907471\n",
      "Train Epoch: 1411 [101376/101520 (100%)] Loss: -1167.428833\n",
      "    epoch          : 1411\n",
      "    loss           : -1171.1499422159627\n",
      "    ess            : 1.9669509005905994\n",
      "    log_marginal   : 1171.1802635001177\n",
      "    log_joint      : 1379.594546830834\n",
      "    val_loss       : -1170.7400592306385\n",
      "    val_ess        : 1.964403028073518\n",
      "    val_log_marginal: 1170.773389733356\n",
      "    val_log_joint  : 1379.1674220872962\n",
      "Train Epoch: 1412 [0/101520 (0%)] Loss: -1168.909180\n",
      "Train Epoch: 1412 [11264/101520 (11%)] Loss: -1171.305298\n",
      "Train Epoch: 1412 [22528/101520 (22%)] Loss: -1167.037109\n",
      "Train Epoch: 1412 [33792/101520 (33%)] Loss: -1167.548584\n",
      "Train Epoch: 1412 [45056/101520 (44%)] Loss: -1168.807495\n",
      "Train Epoch: 1412 [56320/101520 (55%)] Loss: -1172.664062\n",
      "Train Epoch: 1412 [67584/101520 (67%)] Loss: -1168.237549\n",
      "Train Epoch: 1412 [78848/101520 (78%)] Loss: -1164.197266\n",
      "Train Epoch: 1412 [90112/101520 (89%)] Loss: -1175.421509\n",
      "Train Epoch: 1412 [101376/101520 (100%)] Loss: -1178.648682\n",
      "    epoch          : 1412\n",
      "    loss           : -1171.177228304609\n",
      "    ess            : 1.9666000496802019\n",
      "    log_marginal   : 1171.2088960427136\n",
      "    log_joint      : 1379.6371236063128\n",
      "    val_loss       : -1170.4991083559783\n",
      "    val_ess        : 1.9678527842397275\n",
      "    val_log_marginal: 1170.5308466372283\n",
      "    val_log_joint  : 1379.0645751953125\n",
      "Train Epoch: 1413 [0/101520 (0%)] Loss: -1172.720947\n",
      "Train Epoch: 1413 [11264/101520 (11%)] Loss: -1171.247070\n",
      "Train Epoch: 1413 [22528/101520 (22%)] Loss: -1160.985352\n",
      "Train Epoch: 1413 [33792/101520 (33%)] Loss: -1176.576172\n",
      "Train Epoch: 1413 [45056/101520 (44%)] Loss: -1177.228271\n",
      "Train Epoch: 1413 [56320/101520 (55%)] Loss: -1165.918945\n",
      "Train Epoch: 1413 [67584/101520 (67%)] Loss: -1173.058105\n",
      "Train Epoch: 1413 [78848/101520 (78%)] Loss: -1171.861572\n",
      "Train Epoch: 1413 [90112/101520 (89%)] Loss: -1175.994507\n",
      "Train Epoch: 1413 [101376/101520 (100%)] Loss: -1177.519165\n",
      "    epoch          : 1413\n",
      "    loss           : -1171.3086219672582\n",
      "    ess            : 1.9665289841704632\n",
      "    log_marginal   : 1171.339751123783\n",
      "    log_joint      : 1379.716547213607\n",
      "    val_loss       : -1171.1237049932065\n",
      "    val_ess        : 1.966817135396211\n",
      "    val_log_marginal: 1171.155857252038\n",
      "    val_log_joint  : 1379.2252728006115\n",
      "Train Epoch: 1414 [0/101520 (0%)] Loss: -1174.064575\n",
      "Train Epoch: 1414 [11264/101520 (11%)] Loss: -1171.133423\n",
      "Train Epoch: 1414 [22528/101520 (22%)] Loss: -1165.057861\n",
      "Train Epoch: 1414 [33792/101520 (33%)] Loss: -1179.552612\n",
      "Train Epoch: 1414 [45056/101520 (44%)] Loss: -1166.963623\n",
      "Train Epoch: 1414 [56320/101520 (55%)] Loss: -1171.128540\n",
      "Train Epoch: 1414 [67584/101520 (67%)] Loss: -1166.979248\n",
      "Train Epoch: 1414 [78848/101520 (78%)] Loss: -1173.510986\n",
      "Train Epoch: 1414 [90112/101520 (89%)] Loss: -1172.508911\n",
      "Train Epoch: 1414 [101376/101520 (100%)] Loss: -1167.949951\n",
      "    epoch          : 1414\n",
      "    loss           : -1171.3540873311872\n",
      "    ess            : 1.9657957919278937\n",
      "    log_marginal   : 1171.3867696637485\n",
      "    log_joint      : 1379.7299203537218\n",
      "    val_loss       : -1170.9297087296195\n",
      "    val_ess        : 1.9676903589912083\n",
      "    val_log_marginal: 1170.9600299337635\n",
      "    val_log_joint  : 1379.3318561056385\n",
      "Train Epoch: 1415 [0/101520 (0%)] Loss: -1172.434937\n",
      "Train Epoch: 1415 [11264/101520 (11%)] Loss: -1174.701172\n",
      "Train Epoch: 1415 [22528/101520 (22%)] Loss: -1171.434570\n",
      "Train Epoch: 1415 [33792/101520 (33%)] Loss: -1171.742432\n",
      "Train Epoch: 1415 [45056/101520 (44%)] Loss: -1171.916870\n",
      "Train Epoch: 1415 [56320/101520 (55%)] Loss: -1170.512695\n",
      "Train Epoch: 1415 [67584/101520 (67%)] Loss: -1169.950439\n",
      "Train Epoch: 1415 [78848/101520 (78%)] Loss: -1177.357300\n",
      "Train Epoch: 1415 [90112/101520 (89%)] Loss: -1173.605835\n",
      "Train Epoch: 1415 [101376/101520 (100%)] Loss: -1156.950562\n",
      "    epoch          : 1415\n",
      "    loss           : -1171.2343400351367\n",
      "    ess            : 1.966717098825541\n",
      "    log_marginal   : 1171.2652483609454\n",
      "    log_joint      : 1379.6841452133715\n",
      "    val_loss       : -1168.8004681131115\n",
      "    val_ess        : 1.96945205460424\n",
      "    val_log_marginal: 1168.8306247877038\n",
      "    val_log_joint  : 1377.2376072095788\n",
      "Train Epoch: 1416 [0/101520 (0%)] Loss: -1170.293945\n",
      "Train Epoch: 1416 [11264/101520 (11%)] Loss: -1173.919067\n",
      "Train Epoch: 1416 [22528/101520 (22%)] Loss: -1168.597412\n",
      "Train Epoch: 1416 [33792/101520 (33%)] Loss: -1172.523193\n",
      "Train Epoch: 1416 [45056/101520 (44%)] Loss: -1174.251709\n",
      "Train Epoch: 1416 [56320/101520 (55%)] Loss: -1176.559570\n",
      "Train Epoch: 1416 [67584/101520 (67%)] Loss: -1171.806519\n",
      "Train Epoch: 1416 [78848/101520 (78%)] Loss: -1166.641113\n",
      "Train Epoch: 1416 [90112/101520 (89%)] Loss: -1171.501465\n",
      "Train Epoch: 1416 [101376/101520 (100%)] Loss: -1181.302368\n",
      "    epoch          : 1416\n",
      "    loss           : -1171.3712053921954\n",
      "    ess            : 1.9666798623962018\n",
      "    log_marginal   : 1171.4015095006282\n",
      "    log_joint      : 1379.8298830578674\n",
      "    val_loss       : -1169.984380307405\n",
      "    val_ess        : 1.9650780118030051\n",
      "    val_log_marginal: 1170.021585215693\n",
      "    val_log_joint  : 1378.321825110394\n",
      "Train Epoch: 1417 [0/101520 (0%)] Loss: -1173.339111\n",
      "Train Epoch: 1417 [11264/101520 (11%)] Loss: -1168.047363\n",
      "Train Epoch: 1417 [22528/101520 (22%)] Loss: -1169.526855\n",
      "Train Epoch: 1417 [33792/101520 (33%)] Loss: -1173.856689\n",
      "Train Epoch: 1417 [45056/101520 (44%)] Loss: -1169.136475\n",
      "Train Epoch: 1417 [56320/101520 (55%)] Loss: -1169.926514\n",
      "Train Epoch: 1417 [67584/101520 (67%)] Loss: -1168.745361\n",
      "Train Epoch: 1417 [78848/101520 (78%)] Loss: -1168.955933\n",
      "Train Epoch: 1417 [90112/101520 (89%)] Loss: -1170.718872\n",
      "Train Epoch: 1417 [101376/101520 (100%)] Loss: -1169.076782\n",
      "    epoch          : 1417\n",
      "    loss           : -1171.3986822540437\n",
      "    ess            : 1.9665263417977183\n",
      "    log_marginal   : 1171.4307652765783\n",
      "    log_joint      : 1379.846847150793\n",
      "    val_loss       : -1169.8241338315217\n",
      "    val_ess        : 1.964883415595345\n",
      "    val_log_marginal: 1169.8661684782608\n",
      "    val_log_joint  : 1378.5937765370245\n",
      "Train Epoch: 1418 [0/101520 (0%)] Loss: -1174.393066\n",
      "Train Epoch: 1418 [11264/101520 (11%)] Loss: -1168.976318\n",
      "Train Epoch: 1418 [22528/101520 (22%)] Loss: -1169.324097\n",
      "Train Epoch: 1418 [33792/101520 (33%)] Loss: -1168.378418\n",
      "Train Epoch: 1418 [45056/101520 (44%)] Loss: -1169.354370\n",
      "Train Epoch: 1418 [56320/101520 (55%)] Loss: -1168.088013\n",
      "Train Epoch: 1418 [67584/101520 (67%)] Loss: -1172.007568\n",
      "Train Epoch: 1418 [78848/101520 (78%)] Loss: -1170.105835\n",
      "Train Epoch: 1418 [90112/101520 (89%)] Loss: -1171.069580\n",
      "Train Epoch: 1418 [101376/101520 (100%)] Loss: -1177.349854\n",
      "    epoch          : 1418\n",
      "    loss           : -1171.4033515968515\n",
      "    ess            : 1.966712981612239\n",
      "    log_marginal   : 1171.433946465727\n",
      "    log_joint      : 1379.9440598991048\n",
      "    val_loss       : -1170.381544030231\n",
      "    val_ess        : 1.9642695758653723\n",
      "    val_log_marginal: 1170.4161270805027\n",
      "    val_log_joint  : 1378.9048966117527\n",
      "Train Epoch: 1419 [0/101520 (0%)] Loss: -1177.542725\n",
      "Train Epoch: 1419 [11264/101520 (11%)] Loss: -1171.944824\n",
      "Train Epoch: 1419 [22528/101520 (22%)] Loss: -1161.842896\n",
      "Train Epoch: 1419 [33792/101520 (33%)] Loss: -1167.810303\n",
      "Train Epoch: 1419 [45056/101520 (44%)] Loss: -1171.710693\n",
      "Train Epoch: 1419 [56320/101520 (55%)] Loss: -1176.347412\n",
      "Train Epoch: 1419 [67584/101520 (67%)] Loss: -1172.511353\n",
      "Train Epoch: 1419 [78848/101520 (78%)] Loss: -1166.588989\n",
      "Train Epoch: 1419 [90112/101520 (89%)] Loss: -1175.119385\n",
      "Train Epoch: 1419 [101376/101520 (100%)] Loss: -1158.470703\n",
      "    epoch          : 1419\n",
      "    loss           : -1171.4392611249607\n",
      "    ess            : 1.9666571527270216\n",
      "    log_marginal   : 1171.4705608118718\n",
      "    log_joint      : 1379.872993507577\n",
      "    val_loss       : -1171.305472995924\n",
      "    val_ess        : 1.9680971736493318\n",
      "    val_log_marginal: 1171.3343558933425\n",
      "    val_log_joint  : 1379.8709186056385\n",
      "Train Epoch: 1420 [0/101520 (0%)] Loss: -1176.231201\n",
      "Train Epoch: 1420 [11264/101520 (11%)] Loss: -1172.498901\n",
      "Train Epoch: 1420 [22528/101520 (22%)] Loss: -1172.560425\n",
      "Train Epoch: 1420 [33792/101520 (33%)] Loss: -1171.591797\n",
      "Train Epoch: 1420 [45056/101520 (44%)] Loss: -1166.488037\n",
      "Train Epoch: 1420 [56320/101520 (55%)] Loss: -1173.835571\n",
      "Train Epoch: 1420 [67584/101520 (67%)] Loss: -1173.458984\n",
      "Train Epoch: 1420 [78848/101520 (78%)] Loss: -1171.899292\n",
      "Train Epoch: 1420 [90112/101520 (89%)] Loss: -1172.656616\n",
      "Train Epoch: 1420 [101376/101520 (100%)] Loss: -1165.125610\n",
      "    epoch          : 1420\n",
      "    loss           : -1171.5524215314856\n",
      "    ess            : 1.9667027625606288\n",
      "    log_marginal   : 1171.584000323885\n",
      "    log_joint      : 1380.011340884108\n",
      "    val_loss       : -1170.175786557405\n",
      "    val_ess        : 1.9645947373431663\n",
      "    val_log_marginal: 1170.2097274116848\n",
      "    val_log_joint  : 1378.615621815557\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1420.pth ...\n",
      "Train Epoch: 1421 [0/101520 (0%)] Loss: -1169.601318\n",
      "Train Epoch: 1421 [11264/101520 (11%)] Loss: -1169.455811\n",
      "Train Epoch: 1421 [22528/101520 (22%)] Loss: -1168.676514\n",
      "Train Epoch: 1421 [33792/101520 (33%)] Loss: -1180.989258\n",
      "Train Epoch: 1421 [45056/101520 (44%)] Loss: -1173.813477\n",
      "Train Epoch: 1421 [56320/101520 (55%)] Loss: -1167.308350\n",
      "Train Epoch: 1421 [67584/101520 (67%)] Loss: -1173.759766\n",
      "Train Epoch: 1421 [78848/101520 (78%)] Loss: -1165.887451\n",
      "Train Epoch: 1421 [90112/101520 (89%)] Loss: -1177.324707\n",
      "Train Epoch: 1421 [101376/101520 (100%)] Loss: -1170.311890\n",
      "    epoch          : 1421\n",
      "    loss           : -1171.5317217189463\n",
      "    ess            : 1.967026192938263\n",
      "    log_marginal   : 1171.5625662492148\n",
      "    log_joint      : 1379.9732451319096\n",
      "    val_loss       : -1169.341303286345\n",
      "    val_ess        : 1.967924107675967\n",
      "    val_log_marginal: 1169.3724046790082\n",
      "    val_log_joint  : 1377.6202233355978\n",
      "Train Epoch: 1422 [0/101520 (0%)] Loss: -1172.599121\n",
      "Train Epoch: 1422 [11264/101520 (11%)] Loss: -1168.992676\n",
      "Train Epoch: 1422 [22528/101520 (22%)] Loss: -1164.586792\n",
      "Train Epoch: 1422 [33792/101520 (33%)] Loss: -1171.447510\n",
      "Train Epoch: 1422 [45056/101520 (44%)] Loss: -1177.138428\n",
      "Train Epoch: 1422 [56320/101520 (55%)] Loss: -1166.593750\n",
      "Train Epoch: 1422 [67584/101520 (67%)] Loss: -1178.002930\n",
      "Train Epoch: 1422 [78848/101520 (78%)] Loss: -1170.380127\n",
      "Train Epoch: 1422 [90112/101520 (89%)] Loss: -1179.572021\n",
      "Train Epoch: 1422 [101376/101520 (100%)] Loss: -1167.108521\n",
      "    epoch          : 1422\n",
      "    loss           : -1171.584998356038\n",
      "    ess            : 1.96748300952528\n",
      "    log_marginal   : 1171.6158888927057\n",
      "    log_joint      : 1380.0517510648947\n",
      "    val_loss       : -1169.3626549762228\n",
      "    val_ess        : 1.9671140339063562\n",
      "    val_log_marginal: 1169.3941119650135\n",
      "    val_log_joint  : 1377.7080237347147\n",
      "Train Epoch: 1423 [0/101520 (0%)] Loss: -1168.242798\n",
      "Train Epoch: 1423 [11264/101520 (11%)] Loss: -1176.692627\n",
      "Train Epoch: 1423 [22528/101520 (22%)] Loss: -1175.806396\n",
      "Train Epoch: 1423 [33792/101520 (33%)] Loss: -1171.846436\n",
      "Train Epoch: 1423 [45056/101520 (44%)] Loss: -1177.776611\n",
      "Train Epoch: 1423 [56320/101520 (55%)] Loss: -1171.227295\n",
      "Train Epoch: 1423 [67584/101520 (67%)] Loss: -1171.024414\n",
      "Train Epoch: 1423 [78848/101520 (78%)] Loss: -1161.612305\n",
      "Train Epoch: 1423 [90112/101520 (89%)] Loss: -1167.091309\n",
      "Train Epoch: 1423 [101376/101520 (100%)] Loss: -1161.491943\n",
      "    epoch          : 1423\n",
      "    loss           : -1171.601359458425\n",
      "    ess            : 1.9671793457251698\n",
      "    log_marginal   : 1171.63254320921\n",
      "    log_joint      : 1380.045620558849\n",
      "    val_loss       : -1170.892042077106\n",
      "    val_ess        : 1.9626936342405237\n",
      "    val_log_marginal: 1170.9307012143342\n",
      "    val_log_joint  : 1379.515678074049\n",
      "Train Epoch: 1424 [0/101520 (0%)] Loss: -1170.677246\n",
      "Train Epoch: 1424 [11264/101520 (11%)] Loss: -1177.908691\n",
      "Train Epoch: 1424 [22528/101520 (22%)] Loss: -1168.349976\n",
      "Train Epoch: 1424 [33792/101520 (33%)] Loss: -1171.083374\n",
      "Train Epoch: 1424 [45056/101520 (44%)] Loss: -1166.493774\n",
      "Train Epoch: 1424 [56320/101520 (55%)] Loss: -1168.902588\n",
      "Train Epoch: 1424 [67584/101520 (67%)] Loss: -1168.406250\n",
      "Train Epoch: 1424 [78848/101520 (78%)] Loss: -1174.071045\n",
      "Train Epoch: 1424 [90112/101520 (89%)] Loss: -1185.131958\n",
      "Train Epoch: 1424 [101376/101520 (100%)] Loss: -1181.854126\n",
      "    epoch          : 1424\n",
      "    loss           : -1171.641061754083\n",
      "    ess            : 1.9665874991584662\n",
      "    log_marginal   : 1171.672742987398\n",
      "    log_joint      : 1380.146076451594\n",
      "    val_loss       : -1170.3684453549592\n",
      "    val_ess        : 1.9685149607451067\n",
      "    val_log_marginal: 1170.3990372367527\n",
      "    val_log_joint  : 1378.8069219174592\n",
      "Train Epoch: 1425 [0/101520 (0%)] Loss: -1173.127441\n",
      "Train Epoch: 1425 [11264/101520 (11%)] Loss: -1173.121094\n",
      "Train Epoch: 1425 [22528/101520 (22%)] Loss: -1169.500854\n",
      "Train Epoch: 1425 [33792/101520 (33%)] Loss: -1172.141113\n",
      "Train Epoch: 1425 [45056/101520 (44%)] Loss: -1170.854980\n",
      "Train Epoch: 1425 [56320/101520 (55%)] Loss: -1177.954834\n",
      "Train Epoch: 1425 [67584/101520 (67%)] Loss: -1173.020996\n",
      "Train Epoch: 1425 [78848/101520 (78%)] Loss: -1176.531494\n",
      "Train Epoch: 1425 [90112/101520 (89%)] Loss: -1173.243896\n",
      "Train Epoch: 1425 [101376/101520 (100%)] Loss: -1168.444946\n",
      "    epoch          : 1425\n",
      "    loss           : -1171.7073766047033\n",
      "    ess            : 1.9667555099755676\n",
      "    log_marginal   : 1171.738921045658\n",
      "    log_joint      : 1380.1983660980684\n",
      "    val_loss       : -1171.939309825068\n",
      "    val_ess        : 1.9654022455215454\n",
      "    val_log_marginal: 1171.973638119905\n",
      "    val_log_joint  : 1380.4065737516983\n",
      "Train Epoch: 1426 [0/101520 (0%)] Loss: -1176.392944\n",
      "Train Epoch: 1426 [11264/101520 (11%)] Loss: -1179.827637\n",
      "Train Epoch: 1426 [22528/101520 (22%)] Loss: -1181.628052\n",
      "Train Epoch: 1426 [33792/101520 (33%)] Loss: -1171.353027\n",
      "Train Epoch: 1426 [45056/101520 (44%)] Loss: -1176.083984\n",
      "Train Epoch: 1426 [56320/101520 (55%)] Loss: -1171.202148\n",
      "Train Epoch: 1426 [67584/101520 (67%)] Loss: -1168.535156\n",
      "Train Epoch: 1426 [78848/101520 (78%)] Loss: -1163.150513\n",
      "Train Epoch: 1426 [90112/101520 (89%)] Loss: -1171.738892\n",
      "Train Epoch: 1426 [101376/101520 (100%)] Loss: -1193.860962\n",
      "    epoch          : 1426\n",
      "    loss           : -1171.866811474364\n",
      "    ess            : 1.9672135701730622\n",
      "    log_marginal   : 1171.897883582954\n",
      "    log_joint      : 1380.3156818025675\n",
      "    val_loss       : -1170.4170611837635\n",
      "    val_ess        : 1.9663593302602353\n",
      "    val_log_marginal: 1170.4460289996603\n",
      "    val_log_joint  : 1378.8036313264267\n",
      "Train Epoch: 1427 [0/101520 (0%)] Loss: -1170.975952\n",
      "Train Epoch: 1427 [11264/101520 (11%)] Loss: -1170.246704\n",
      "Train Epoch: 1427 [22528/101520 (22%)] Loss: -1170.523682\n",
      "Train Epoch: 1427 [33792/101520 (33%)] Loss: -1172.355469\n",
      "Train Epoch: 1427 [45056/101520 (44%)] Loss: -1165.482300\n",
      "Train Epoch: 1427 [56320/101520 (55%)] Loss: -1172.082520\n",
      "Train Epoch: 1427 [67584/101520 (67%)] Loss: -1171.660645\n",
      "Train Epoch: 1427 [78848/101520 (78%)] Loss: -1169.939941\n",
      "Train Epoch: 1427 [90112/101520 (89%)] Loss: -1173.548828\n",
      "Train Epoch: 1427 [101376/101520 (100%)] Loss: -1169.320801\n",
      "    epoch          : 1427\n",
      "    loss           : -1171.8271005908448\n",
      "    ess            : 1.9659942831825372\n",
      "    log_marginal   : 1171.8596461310458\n",
      "    log_joint      : 1380.2844919175957\n",
      "    val_loss       : -1171.6533362347147\n",
      "    val_ess        : 1.9678282063940298\n",
      "    val_log_marginal: 1171.6852443529212\n",
      "    val_log_joint  : 1380.2145571501358\n",
      "Train Epoch: 1428 [0/101520 (0%)] Loss: -1172.197632\n",
      "Train Epoch: 1428 [11264/101520 (11%)] Loss: -1172.840088\n",
      "Train Epoch: 1428 [22528/101520 (22%)] Loss: -1167.205566\n",
      "Train Epoch: 1428 [33792/101520 (33%)] Loss: -1169.178955\n",
      "Train Epoch: 1428 [45056/101520 (44%)] Loss: -1171.702637\n",
      "Train Epoch: 1428 [56320/101520 (55%)] Loss: -1166.140625\n",
      "Train Epoch: 1428 [67584/101520 (67%)] Loss: -1176.788330\n",
      "Train Epoch: 1428 [78848/101520 (78%)] Loss: -1180.449829\n",
      "Train Epoch: 1428 [90112/101520 (89%)] Loss: -1165.952881\n",
      "Train Epoch: 1428 [101376/101520 (100%)] Loss: -1155.041626\n",
      "    epoch          : 1428\n",
      "    loss           : -1171.844350536864\n",
      "    ess            : 1.9664226740448918\n",
      "    log_marginal   : 1171.875183412178\n",
      "    log_joint      : 1380.240191435694\n",
      "    val_loss       : -1169.4919168223505\n",
      "    val_ess        : 1.9649949177451755\n",
      "    val_log_marginal: 1169.5234481148098\n",
      "    val_log_joint  : 1377.8344885784647\n",
      "Train Epoch: 1429 [0/101520 (0%)] Loss: -1174.568115\n",
      "Train Epoch: 1429 [11264/101520 (11%)] Loss: -1176.880493\n",
      "Train Epoch: 1429 [22528/101520 (22%)] Loss: -1164.578491\n",
      "Train Epoch: 1429 [33792/101520 (33%)] Loss: -1172.971924\n",
      "Train Epoch: 1429 [45056/101520 (44%)] Loss: -1168.176758\n",
      "Train Epoch: 1429 [56320/101520 (55%)] Loss: -1172.443115\n",
      "Train Epoch: 1429 [67584/101520 (67%)] Loss: -1175.873047\n",
      "Train Epoch: 1429 [78848/101520 (78%)] Loss: -1176.745850\n",
      "Train Epoch: 1429 [90112/101520 (89%)] Loss: -1171.960571\n",
      "Train Epoch: 1429 [101376/101520 (100%)] Loss: -1163.346802\n",
      "    epoch          : 1429\n",
      "    loss           : -1171.885196858315\n",
      "    ess            : 1.9673985015207798\n",
      "    log_marginal   : 1171.915592979546\n",
      "    log_joint      : 1380.3531776313207\n",
      "    val_loss       : -1170.3294253141983\n",
      "    val_ess        : 1.9686537618222444\n",
      "    val_log_marginal: 1170.3596828294837\n",
      "    val_log_joint  : 1378.5418170431385\n",
      "Train Epoch: 1430 [0/101520 (0%)] Loss: -1171.328857\n",
      "Train Epoch: 1430 [11264/101520 (11%)] Loss: -1176.966309\n",
      "Train Epoch: 1430 [22528/101520 (22%)] Loss: -1164.905762\n",
      "Train Epoch: 1430 [33792/101520 (33%)] Loss: -1165.861816\n",
      "Train Epoch: 1430 [45056/101520 (44%)] Loss: -1173.027344\n",
      "Train Epoch: 1430 [56320/101520 (55%)] Loss: -1178.280273\n",
      "Train Epoch: 1430 [67584/101520 (67%)] Loss: -1175.491943\n",
      "Train Epoch: 1430 [78848/101520 (78%)] Loss: -1174.147827\n",
      "Train Epoch: 1430 [90112/101520 (89%)] Loss: -1175.758423\n",
      "Train Epoch: 1430 [101376/101520 (100%)] Loss: -1169.724365\n",
      "    epoch          : 1430\n",
      "    loss           : -1171.9485243600816\n",
      "    ess            : 1.9664906383159773\n",
      "    log_marginal   : 1171.980518436911\n",
      "    log_joint      : 1380.3667666948022\n",
      "    val_loss       : -1169.054692807405\n",
      "    val_ess        : 1.9667051615922346\n",
      "    val_log_marginal: 1169.0861604110055\n",
      "    val_log_joint  : 1377.9007515285325\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1430.pth ...\n",
      "Train Epoch: 1431 [0/101520 (0%)] Loss: -1165.180664\n",
      "Train Epoch: 1431 [11264/101520 (11%)] Loss: -1173.928955\n",
      "Train Epoch: 1431 [22528/101520 (22%)] Loss: -1174.382446\n",
      "Train Epoch: 1431 [33792/101520 (33%)] Loss: -1174.230469\n",
      "Train Epoch: 1431 [45056/101520 (44%)] Loss: -1175.771973\n",
      "Train Epoch: 1431 [56320/101520 (55%)] Loss: -1171.215088\n",
      "Train Epoch: 1431 [67584/101520 (67%)] Loss: -1172.666870\n",
      "Train Epoch: 1431 [78848/101520 (78%)] Loss: -1174.732178\n",
      "Train Epoch: 1431 [90112/101520 (89%)] Loss: -1169.963867\n",
      "Train Epoch: 1431 [101376/101520 (100%)] Loss: -1183.651245\n",
      "    epoch          : 1431\n",
      "    loss           : -1171.9733543204302\n",
      "    ess            : 1.966639445654711\n",
      "    log_marginal   : 1172.005410352544\n",
      "    log_joint      : 1380.4091717130575\n",
      "    val_loss       : -1172.125244140625\n",
      "    val_ess        : 1.9680237148119055\n",
      "    val_log_marginal: 1172.1546949303668\n",
      "    val_log_joint  : 1380.5985001273777\n",
      "Train Epoch: 1432 [0/101520 (0%)] Loss: -1166.776855\n",
      "Train Epoch: 1432 [11264/101520 (11%)] Loss: -1170.509888\n",
      "Train Epoch: 1432 [22528/101520 (22%)] Loss: -1170.606201\n",
      "Train Epoch: 1432 [33792/101520 (33%)] Loss: -1171.115234\n",
      "Train Epoch: 1432 [45056/101520 (44%)] Loss: -1172.171875\n",
      "Train Epoch: 1432 [56320/101520 (55%)] Loss: -1168.629150\n",
      "Train Epoch: 1432 [67584/101520 (67%)] Loss: -1177.663574\n",
      "Train Epoch: 1432 [78848/101520 (78%)] Loss: -1173.420898\n",
      "Train Epoch: 1432 [90112/101520 (89%)] Loss: -1172.777832\n",
      "Train Epoch: 1432 [101376/101520 (100%)] Loss: -1174.269165\n",
      "    epoch          : 1432\n",
      "    loss           : -1172.0919870347832\n",
      "    ess            : 1.966882960880222\n",
      "    log_marginal   : 1172.123328434163\n",
      "    log_joint      : 1380.4821286608826\n",
      "    val_loss       : -1169.3435164741848\n",
      "    val_ess        : 1.9669185203054678\n",
      "    val_log_marginal: 1169.375387440557\n",
      "    val_log_joint  : 1378.1460279381793\n",
      "Train Epoch: 1433 [0/101520 (0%)] Loss: -1175.725586\n",
      "Train Epoch: 1433 [11264/101520 (11%)] Loss: -1168.981567\n",
      "Train Epoch: 1433 [22528/101520 (22%)] Loss: -1167.648193\n",
      "Train Epoch: 1433 [33792/101520 (33%)] Loss: -1171.776611\n",
      "Train Epoch: 1433 [45056/101520 (44%)] Loss: -1170.867188\n",
      "Train Epoch: 1433 [56320/101520 (55%)] Loss: -1168.468384\n",
      "Train Epoch: 1433 [67584/101520 (67%)] Loss: -1175.929443\n",
      "Train Epoch: 1433 [78848/101520 (78%)] Loss: -1166.905762\n",
      "Train Epoch: 1433 [90112/101520 (89%)] Loss: -1175.648926\n",
      "Train Epoch: 1433 [101376/101520 (100%)] Loss: -1178.322510\n",
      "    epoch          : 1433\n",
      "    loss           : -1172.0804977033606\n",
      "    ess            : 1.9670491194605229\n",
      "    log_marginal   : 1172.110604904405\n",
      "    log_joint      : 1380.502831540515\n",
      "    val_loss       : -1169.918356190557\n",
      "    val_ess        : 1.9636833408604497\n",
      "    val_log_marginal: 1169.9495265794837\n",
      "    val_log_joint  : 1378.0812245244565\n",
      "Train Epoch: 1434 [0/101520 (0%)] Loss: -1168.357666\n",
      "Train Epoch: 1434 [11264/101520 (11%)] Loss: -1175.364380\n",
      "Train Epoch: 1434 [22528/101520 (22%)] Loss: -1169.540039\n",
      "Train Epoch: 1434 [33792/101520 (33%)] Loss: -1176.404053\n",
      "Train Epoch: 1434 [45056/101520 (44%)] Loss: -1180.102295\n",
      "Train Epoch: 1434 [56320/101520 (55%)] Loss: -1168.497314\n",
      "Train Epoch: 1434 [67584/101520 (67%)] Loss: -1164.367432\n",
      "Train Epoch: 1434 [78848/101520 (78%)] Loss: -1172.969360\n",
      "Train Epoch: 1434 [90112/101520 (89%)] Loss: -1169.678955\n",
      "Train Epoch: 1434 [101376/101520 (100%)] Loss: -1179.880249\n",
      "    epoch          : 1434\n",
      "    loss           : -1172.0822318379005\n",
      "    ess            : 1.9661262514603197\n",
      "    log_marginal   : 1172.1144774163788\n",
      "    log_joint      : 1380.5549107843908\n",
      "    val_loss       : -1171.2033001443615\n",
      "    val_ess        : 1.9688145907028862\n",
      "    val_log_marginal: 1171.2325810971467\n",
      "    val_log_joint  : 1379.6299040421195\n",
      "Train Epoch: 1435 [0/101520 (0%)] Loss: -1164.858765\n",
      "Train Epoch: 1435 [11264/101520 (11%)] Loss: -1175.662109\n",
      "Train Epoch: 1435 [22528/101520 (22%)] Loss: -1174.870605\n",
      "Train Epoch: 1435 [33792/101520 (33%)] Loss: -1175.330811\n",
      "Train Epoch: 1435 [45056/101520 (44%)] Loss: -1168.211304\n",
      "Train Epoch: 1435 [56320/101520 (55%)] Loss: -1166.818604\n",
      "Train Epoch: 1435 [67584/101520 (67%)] Loss: -1179.264404\n",
      "Train Epoch: 1435 [78848/101520 (78%)] Loss: -1167.994873\n",
      "Train Epoch: 1435 [90112/101520 (89%)] Loss: -1165.660767\n",
      "Train Epoch: 1435 [101376/101520 (100%)] Loss: -1177.689087\n",
      "    epoch          : 1435\n",
      "    loss           : -1172.1430921698336\n",
      "    ess            : 1.966813665538577\n",
      "    log_marginal   : 1172.1744654669833\n",
      "    log_joint      : 1380.5711314139055\n",
      "    val_loss       : -1169.704982591712\n",
      "    val_ess        : 1.9649004728897759\n",
      "    val_log_marginal: 1169.741402004076\n",
      "    val_log_joint  : 1378.405613111413\n",
      "Train Epoch: 1436 [0/101520 (0%)] Loss: -1168.755737\n",
      "Train Epoch: 1436 [11264/101520 (11%)] Loss: -1173.514648\n",
      "Train Epoch: 1436 [22528/101520 (22%)] Loss: -1171.361084\n",
      "Train Epoch: 1436 [33792/101520 (33%)] Loss: -1171.818848\n",
      "Train Epoch: 1436 [45056/101520 (44%)] Loss: -1174.270142\n",
      "Train Epoch: 1436 [56320/101520 (55%)] Loss: -1169.816650\n",
      "Train Epoch: 1436 [67584/101520 (67%)] Loss: -1169.168457\n",
      "Train Epoch: 1436 [78848/101520 (78%)] Loss: -1172.045898\n",
      "Train Epoch: 1436 [90112/101520 (89%)] Loss: -1163.586670\n",
      "Train Epoch: 1436 [101376/101520 (100%)] Loss: -1184.938110\n",
      "    epoch          : 1436\n",
      "    loss           : -1172.121608408252\n",
      "    ess            : 1.9663141205083186\n",
      "    log_marginal   : 1172.1531007086212\n",
      "    log_joint      : 1380.621314580716\n",
      "    val_loss       : -1169.4781653362772\n",
      "    val_ess        : 1.9663676282633906\n",
      "    val_log_marginal: 1169.5086510699728\n",
      "    val_log_joint  : 1377.9612665591033\n",
      "Train Epoch: 1437 [0/101520 (0%)] Loss: -1169.699219\n",
      "Train Epoch: 1437 [11264/101520 (11%)] Loss: -1170.889648\n",
      "Train Epoch: 1437 [22528/101520 (22%)] Loss: -1171.363525\n",
      "Train Epoch: 1437 [33792/101520 (33%)] Loss: -1170.795166\n",
      "Train Epoch: 1437 [45056/101520 (44%)] Loss: -1167.463257\n",
      "Train Epoch: 1437 [56320/101520 (55%)] Loss: -1171.093384\n",
      "Train Epoch: 1437 [67584/101520 (67%)] Loss: -1164.033203\n",
      "Train Epoch: 1437 [78848/101520 (78%)] Loss: -1173.094238\n",
      "Train Epoch: 1437 [90112/101520 (89%)] Loss: -1174.612915\n",
      "Train Epoch: 1437 [101376/101520 (100%)] Loss: -1166.015015\n",
      "    epoch          : 1437\n",
      "    loss           : -1172.1992715040044\n",
      "    ess            : 1.967475344787291\n",
      "    log_marginal   : 1172.2297265134266\n",
      "    log_joint      : 1380.627177636228\n",
      "    val_loss       : -1170.9621900475543\n",
      "    val_ess        : 1.9704276582469111\n",
      "    val_log_marginal: 1170.990239682405\n",
      "    val_log_joint  : 1379.4180112092392\n",
      "Train Epoch: 1438 [0/101520 (0%)] Loss: -1163.949707\n",
      "Train Epoch: 1438 [11264/101520 (11%)] Loss: -1174.675781\n",
      "Train Epoch: 1438 [22528/101520 (22%)] Loss: -1169.164429\n",
      "Train Epoch: 1438 [33792/101520 (33%)] Loss: -1178.242920\n",
      "Train Epoch: 1438 [45056/101520 (44%)] Loss: -1177.589355\n",
      "Train Epoch: 1438 [56320/101520 (55%)] Loss: -1176.441895\n",
      "Train Epoch: 1438 [67584/101520 (67%)] Loss: -1176.097656\n",
      "Train Epoch: 1438 [78848/101520 (78%)] Loss: -1171.566895\n",
      "Train Epoch: 1438 [90112/101520 (89%)] Loss: -1172.400757\n",
      "Train Epoch: 1438 [101376/101520 (100%)] Loss: -1159.872192\n",
      "    epoch          : 1438\n",
      "    loss           : -1172.091718357412\n",
      "    ess            : 1.9670917149165168\n",
      "    log_marginal   : 1172.123048715256\n",
      "    log_joint      : 1380.5807363232177\n",
      "    val_loss       : -1171.3049740998642\n",
      "    val_ess        : 1.9670961732449739\n",
      "    val_log_marginal: 1171.3361975628397\n",
      "    val_log_joint  : 1379.8111094599185\n",
      "Train Epoch: 1439 [0/101520 (0%)] Loss: -1172.670654\n",
      "Train Epoch: 1439 [11264/101520 (11%)] Loss: -1174.559937\n",
      "Train Epoch: 1439 [22528/101520 (22%)] Loss: -1170.215332\n",
      "Train Epoch: 1439 [33792/101520 (33%)] Loss: -1171.649048\n",
      "Train Epoch: 1439 [45056/101520 (44%)] Loss: -1174.206909\n",
      "Train Epoch: 1439 [56320/101520 (55%)] Loss: -1169.023682\n",
      "Train Epoch: 1439 [67584/101520 (67%)] Loss: -1167.100586\n",
      "Train Epoch: 1439 [78848/101520 (78%)] Loss: -1175.696533\n",
      "Train Epoch: 1439 [90112/101520 (89%)] Loss: -1168.751709\n",
      "Train Epoch: 1439 [101376/101520 (100%)] Loss: -1168.539062\n",
      "    epoch          : 1439\n",
      "    loss           : -1172.2544031191112\n",
      "    ess            : 1.9664439532025975\n",
      "    log_marginal   : 1172.2863070233982\n",
      "    log_joint      : 1380.7750385226916\n",
      "    val_loss       : -1170.8208326256793\n",
      "    val_ess        : 1.9674031112505042\n",
      "    val_log_marginal: 1170.8513502038043\n",
      "    val_log_joint  : 1379.192430579144\n",
      "Train Epoch: 1440 [0/101520 (0%)] Loss: -1174.991577\n",
      "Train Epoch: 1440 [11264/101520 (11%)] Loss: -1174.111328\n",
      "Train Epoch: 1440 [22528/101520 (22%)] Loss: -1169.004639\n",
      "Train Epoch: 1440 [33792/101520 (33%)] Loss: -1165.835693\n",
      "Train Epoch: 1440 [45056/101520 (44%)] Loss: -1169.785278\n",
      "Train Epoch: 1440 [56320/101520 (55%)] Loss: -1178.287109\n",
      "Train Epoch: 1440 [67584/101520 (67%)] Loss: -1172.677368\n",
      "Train Epoch: 1440 [78848/101520 (78%)] Loss: -1166.184570\n",
      "Train Epoch: 1440 [90112/101520 (89%)] Loss: -1172.438843\n",
      "Train Epoch: 1440 [101376/101520 (100%)] Loss: -1170.969971\n",
      "    epoch          : 1440\n",
      "    loss           : -1172.4151341423917\n",
      "    ess            : 1.966376539450794\n",
      "    log_marginal   : 1172.4475625196294\n",
      "    log_joint      : 1380.8697939158685\n",
      "    val_loss       : -1170.5352464758832\n",
      "    val_ess        : 1.969180967496789\n",
      "    val_log_marginal: 1170.5643735139267\n",
      "    val_log_joint  : 1379.0030358355978\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1440.pth ...\n",
      "Train Epoch: 1441 [0/101520 (0%)] Loss: -1174.273804\n",
      "Train Epoch: 1441 [11264/101520 (11%)] Loss: -1175.896973\n",
      "Train Epoch: 1441 [22528/101520 (22%)] Loss: -1178.109863\n",
      "Train Epoch: 1441 [33792/101520 (33%)] Loss: -1169.290771\n",
      "Train Epoch: 1441 [45056/101520 (44%)] Loss: -1174.216309\n",
      "Train Epoch: 1441 [56320/101520 (55%)] Loss: -1171.859375\n",
      "Train Epoch: 1441 [67584/101520 (67%)] Loss: -1162.661621\n",
      "Train Epoch: 1441 [78848/101520 (78%)] Loss: -1170.316406\n",
      "Train Epoch: 1441 [90112/101520 (89%)] Loss: -1175.299316\n",
      "Train Epoch: 1441 [101376/101520 (100%)] Loss: -1168.614746\n",
      "    epoch          : 1441\n",
      "    loss           : -1172.4956950278738\n",
      "    ess            : 1.9671747133360435\n",
      "    log_marginal   : 1172.5267352386934\n",
      "    log_joint      : 1380.9141275223776\n",
      "    val_loss       : -1170.5696703040082\n",
      "    val_ess        : 1.969167128853176\n",
      "    val_log_marginal: 1170.5973749575408\n",
      "    val_log_joint  : 1379.459079908288\n",
      "Train Epoch: 1442 [0/101520 (0%)] Loss: -1181.294312\n",
      "Train Epoch: 1442 [11264/101520 (11%)] Loss: -1168.573364\n",
      "Train Epoch: 1442 [22528/101520 (22%)] Loss: -1178.344727\n",
      "Train Epoch: 1442 [33792/101520 (33%)] Loss: -1173.501099\n",
      "Train Epoch: 1442 [45056/101520 (44%)] Loss: -1172.151978\n",
      "Train Epoch: 1442 [56320/101520 (55%)] Loss: -1172.445801\n",
      "Train Epoch: 1442 [67584/101520 (67%)] Loss: -1175.879150\n",
      "Train Epoch: 1442 [78848/101520 (78%)] Loss: -1171.574463\n",
      "Train Epoch: 1442 [90112/101520 (89%)] Loss: -1165.286499\n",
      "Train Epoch: 1442 [101376/101520 (100%)] Loss: -1174.866821\n",
      "    epoch          : 1442\n",
      "    loss           : -1172.559076510482\n",
      "    ess            : 1.9670270561572893\n",
      "    log_marginal   : 1172.5901437117227\n",
      "    log_joint      : 1380.9677206834956\n",
      "    val_loss       : -1171.4239873471467\n",
      "    val_ess        : 1.9671860259512197\n",
      "    val_log_marginal: 1171.4553859544837\n",
      "    val_log_joint  : 1379.9565164317255\n",
      "Train Epoch: 1443 [0/101520 (0%)] Loss: -1166.174194\n",
      "Train Epoch: 1443 [11264/101520 (11%)] Loss: -1169.905518\n",
      "Train Epoch: 1443 [22528/101520 (22%)] Loss: -1167.934326\n",
      "Train Epoch: 1443 [33792/101520 (33%)] Loss: -1169.405273\n",
      "Train Epoch: 1443 [45056/101520 (44%)] Loss: -1173.225586\n",
      "Train Epoch: 1443 [56320/101520 (55%)] Loss: -1168.377930\n",
      "Train Epoch: 1443 [67584/101520 (67%)] Loss: -1170.437500\n",
      "Train Epoch: 1443 [78848/101520 (78%)] Loss: -1172.913574\n",
      "Train Epoch: 1443 [90112/101520 (89%)] Loss: -1168.655640\n",
      "Train Epoch: 1443 [101376/101520 (100%)] Loss: -1177.729614\n",
      "    epoch          : 1443\n",
      "    loss           : -1172.5489090962626\n",
      "    ess            : 1.9663661107346042\n",
      "    log_marginal   : 1172.5814883744897\n",
      "    log_joint      : 1380.9999171884815\n",
      "    val_loss       : -1170.7081617272418\n",
      "    val_ess        : 1.9683198617852253\n",
      "    val_log_marginal: 1170.737453294837\n",
      "    val_log_joint  : 1379.006246815557\n",
      "Train Epoch: 1444 [0/101520 (0%)] Loss: -1174.111816\n",
      "Train Epoch: 1444 [11264/101520 (11%)] Loss: -1172.963867\n",
      "Train Epoch: 1444 [22528/101520 (22%)] Loss: -1172.329956\n",
      "Train Epoch: 1444 [33792/101520 (33%)] Loss: -1174.894043\n",
      "Train Epoch: 1444 [45056/101520 (44%)] Loss: -1165.256592\n",
      "Train Epoch: 1444 [56320/101520 (55%)] Loss: -1173.272217\n",
      "Train Epoch: 1444 [67584/101520 (67%)] Loss: -1171.893555\n",
      "Train Epoch: 1444 [78848/101520 (78%)] Loss: -1172.906250\n",
      "Train Epoch: 1444 [90112/101520 (89%)] Loss: -1171.394287\n",
      "Train Epoch: 1444 [101376/101520 (100%)] Loss: -1163.171021\n",
      "    epoch          : 1444\n",
      "    loss           : -1172.5179087576555\n",
      "    ess            : 1.966865574894239\n",
      "    log_marginal   : 1172.5498169558732\n",
      "    log_joint      : 1380.9163462576555\n",
      "    val_loss       : -1171.997362219769\n",
      "    val_ess        : 1.967221975326538\n",
      "    val_log_marginal: 1172.0273809018342\n",
      "    val_log_joint  : 1380.2060812245245\n",
      "Train Epoch: 1445 [0/101520 (0%)] Loss: -1168.955322\n",
      "Train Epoch: 1445 [11264/101520 (11%)] Loss: -1171.738525\n",
      "Train Epoch: 1445 [22528/101520 (22%)] Loss: -1172.480957\n",
      "Train Epoch: 1445 [33792/101520 (33%)] Loss: -1177.839355\n",
      "Train Epoch: 1445 [45056/101520 (44%)] Loss: -1176.742798\n",
      "Train Epoch: 1445 [56320/101520 (55%)] Loss: -1179.438354\n",
      "Train Epoch: 1445 [67584/101520 (67%)] Loss: -1173.009399\n",
      "Train Epoch: 1445 [78848/101520 (78%)] Loss: -1171.326294\n",
      "Train Epoch: 1445 [90112/101520 (89%)] Loss: -1169.230347\n",
      "Train Epoch: 1445 [101376/101520 (100%)] Loss: -1163.024292\n",
      "    epoch          : 1445\n",
      "    loss           : -1172.4350181081188\n",
      "    ess            : 1.9659862500339298\n",
      "    log_marginal   : 1172.4675360444803\n",
      "    log_joint      : 1380.8624445469536\n",
      "    val_loss       : -1171.4606349779212\n",
      "    val_ess        : 1.9634162343066672\n",
      "    val_log_marginal: 1171.4994692595108\n",
      "    val_log_joint  : 1379.8008237092392\n",
      "Train Epoch: 1446 [0/101520 (0%)] Loss: -1168.667603\n",
      "Train Epoch: 1446 [11264/101520 (11%)] Loss: -1176.866699\n",
      "Train Epoch: 1446 [22528/101520 (22%)] Loss: -1164.038452\n",
      "Train Epoch: 1446 [33792/101520 (33%)] Loss: -1166.178833\n",
      "Train Epoch: 1446 [45056/101520 (44%)] Loss: -1176.937866\n",
      "Train Epoch: 1446 [56320/101520 (55%)] Loss: -1167.018555\n",
      "Train Epoch: 1446 [67584/101520 (67%)] Loss: -1164.302734\n",
      "Train Epoch: 1446 [78848/101520 (78%)] Loss: -1175.433838\n",
      "Train Epoch: 1446 [90112/101520 (89%)] Loss: -1176.423584\n",
      "Train Epoch: 1446 [101376/101520 (100%)] Loss: -1178.591431\n",
      "    epoch          : 1446\n",
      "    loss           : -1172.4928923180355\n",
      "    ess            : 1.966640054880075\n",
      "    log_marginal   : 1172.5244612957365\n",
      "    log_joint      : 1380.9679635972832\n",
      "    val_loss       : -1170.7460247537365\n",
      "    val_ess        : 1.9659943373306938\n",
      "    val_log_marginal: 1170.778813901155\n",
      "    val_log_joint  : 1379.0325343919837\n",
      "Train Epoch: 1447 [0/101520 (0%)] Loss: -1172.314575\n",
      "Train Epoch: 1447 [11264/101520 (11%)] Loss: -1168.947510\n",
      "Train Epoch: 1447 [22528/101520 (22%)] Loss: -1172.760742\n",
      "Train Epoch: 1447 [33792/101520 (33%)] Loss: -1183.406982\n",
      "Train Epoch: 1447 [45056/101520 (44%)] Loss: -1168.654053\n",
      "Train Epoch: 1447 [56320/101520 (55%)] Loss: -1177.398071\n",
      "Train Epoch: 1447 [67584/101520 (67%)] Loss: -1180.799927\n",
      "Train Epoch: 1447 [78848/101520 (78%)] Loss: -1173.539307\n",
      "Train Epoch: 1447 [90112/101520 (89%)] Loss: -1177.635864\n",
      "Train Epoch: 1447 [101376/101520 (100%)] Loss: -1161.069458\n",
      "    epoch          : 1447\n",
      "    loss           : -1172.5197189561086\n",
      "    ess            : 1.9679584527135494\n",
      "    log_marginal   : 1172.5497642018688\n",
      "    log_joint      : 1380.9951638073178\n",
      "    val_loss       : -1170.8356615149457\n",
      "    val_ess        : 1.9655313543651416\n",
      "    val_log_marginal: 1170.8696979025135\n",
      "    val_log_joint  : 1379.4366295855978\n",
      "Train Epoch: 1448 [0/101520 (0%)] Loss: -1171.832642\n",
      "Train Epoch: 1448 [11264/101520 (11%)] Loss: -1170.803833\n",
      "Train Epoch: 1448 [22528/101520 (22%)] Loss: -1174.451904\n",
      "Train Epoch: 1448 [33792/101520 (33%)] Loss: -1170.952393\n",
      "Train Epoch: 1448 [45056/101520 (44%)] Loss: -1174.743408\n",
      "Train Epoch: 1448 [56320/101520 (55%)] Loss: -1169.747314\n",
      "Train Epoch: 1448 [67584/101520 (67%)] Loss: -1173.782471\n",
      "Train Epoch: 1448 [78848/101520 (78%)] Loss: -1172.718018\n",
      "Train Epoch: 1448 [90112/101520 (89%)] Loss: -1179.443604\n",
      "Train Epoch: 1448 [101376/101520 (100%)] Loss: -1165.822754\n",
      "    epoch          : 1448\n",
      "    loss           : -1172.684326171875\n",
      "    ess            : 1.966700738398873\n",
      "    log_marginal   : 1172.7151240823257\n",
      "    log_joint      : 1381.0639482814463\n",
      "    val_loss       : -1170.9371815557065\n",
      "    val_ess        : 1.9645942190419072\n",
      "    val_log_marginal: 1170.9706712805707\n",
      "    val_log_joint  : 1379.3362559442935\n",
      "Train Epoch: 1449 [0/101520 (0%)] Loss: -1170.459961\n",
      "Train Epoch: 1449 [11264/101520 (11%)] Loss: -1170.317383\n",
      "Train Epoch: 1449 [22528/101520 (22%)] Loss: -1176.777100\n",
      "Train Epoch: 1449 [33792/101520 (33%)] Loss: -1171.695801\n",
      "Train Epoch: 1449 [45056/101520 (44%)] Loss: -1172.328003\n",
      "Train Epoch: 1449 [56320/101520 (55%)] Loss: -1176.295288\n",
      "Train Epoch: 1449 [67584/101520 (67%)] Loss: -1169.014404\n",
      "Train Epoch: 1449 [78848/101520 (78%)] Loss: -1172.827148\n",
      "Train Epoch: 1449 [90112/101520 (89%)] Loss: -1177.646484\n",
      "Train Epoch: 1449 [101376/101520 (100%)] Loss: -1150.571411\n",
      "    epoch          : 1449\n",
      "    loss           : -1172.5667313618876\n",
      "    ess            : 1.9659477166794053\n",
      "    log_marginal   : 1172.5988941288474\n",
      "    log_joint      : 1381.044811459642\n",
      "    val_loss       : -1170.7111497961957\n",
      "    val_ess        : 1.965792469356371\n",
      "    val_log_marginal: 1170.7423467221467\n",
      "    val_log_joint  : 1379.0451819378397\n",
      "Train Epoch: 1450 [0/101520 (0%)] Loss: -1171.812988\n",
      "Train Epoch: 1450 [11264/101520 (11%)] Loss: -1163.167969\n",
      "Train Epoch: 1450 [22528/101520 (22%)] Loss: -1168.793945\n",
      "Train Epoch: 1450 [33792/101520 (33%)] Loss: -1178.575684\n",
      "Train Epoch: 1450 [45056/101520 (44%)] Loss: -1173.245361\n",
      "Train Epoch: 1450 [56320/101520 (55%)] Loss: -1169.707520\n",
      "Train Epoch: 1450 [67584/101520 (67%)] Loss: -1177.648193\n",
      "Train Epoch: 1450 [78848/101520 (78%)] Loss: -1166.007935\n",
      "Train Epoch: 1450 [90112/101520 (89%)] Loss: -1169.279663\n",
      "Train Epoch: 1450 [101376/101520 (100%)] Loss: -1164.338745\n",
      "    epoch          : 1450\n",
      "    loss           : -1172.6790912570666\n",
      "    ess            : 1.966957836294893\n",
      "    log_marginal   : 1172.7097020748272\n",
      "    log_joint      : 1381.189693585113\n",
      "    val_loss       : -1170.9100394870925\n",
      "    val_ess        : 1.9685447941655698\n",
      "    val_log_marginal: 1170.9390816066575\n",
      "    val_log_joint  : 1379.0462328040082\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1450.pth ...\n",
      "Train Epoch: 1451 [0/101520 (0%)] Loss: -1173.381348\n",
      "Train Epoch: 1451 [11264/101520 (11%)] Loss: -1165.817383\n",
      "Train Epoch: 1451 [22528/101520 (22%)] Loss: -1169.347046\n",
      "Train Epoch: 1451 [33792/101520 (33%)] Loss: -1165.784424\n",
      "Train Epoch: 1451 [45056/101520 (44%)] Loss: -1184.148682\n",
      "Train Epoch: 1451 [56320/101520 (55%)] Loss: -1171.486206\n",
      "Train Epoch: 1451 [67584/101520 (67%)] Loss: -1179.771973\n",
      "Train Epoch: 1451 [78848/101520 (78%)] Loss: -1175.129395\n",
      "Train Epoch: 1451 [90112/101520 (89%)] Loss: -1173.171875\n",
      "Train Epoch: 1451 [101376/101520 (100%)] Loss: -1181.963989\n",
      "    epoch          : 1451\n",
      "    loss           : -1172.8027705667007\n",
      "    ess            : 1.9662539869097608\n",
      "    log_marginal   : 1172.8346057680983\n",
      "    log_joint      : 1381.2962290701555\n",
      "    val_loss       : -1172.2390295940897\n",
      "    val_ess        : 1.967904842418173\n",
      "    val_log_marginal: 1172.2677798063858\n",
      "    val_log_joint  : 1380.66943359375\n",
      "Train Epoch: 1452 [0/101520 (0%)] Loss: -1174.454102\n",
      "Train Epoch: 1452 [11264/101520 (11%)] Loss: -1177.471924\n",
      "Train Epoch: 1452 [22528/101520 (22%)] Loss: -1175.248047\n",
      "Train Epoch: 1452 [33792/101520 (33%)] Loss: -1172.260254\n",
      "Train Epoch: 1452 [45056/101520 (44%)] Loss: -1172.325684\n",
      "Train Epoch: 1452 [56320/101520 (55%)] Loss: -1180.154297\n",
      "Train Epoch: 1452 [67584/101520 (67%)] Loss: -1171.916870\n",
      "Train Epoch: 1452 [78848/101520 (78%)] Loss: -1180.595703\n",
      "Train Epoch: 1452 [90112/101520 (89%)] Loss: -1174.276489\n",
      "Train Epoch: 1452 [101376/101520 (100%)] Loss: -1169.241089\n",
      "    epoch          : 1452\n",
      "    loss           : -1172.8052316023477\n",
      "    ess            : 1.9659320834893077\n",
      "    log_marginal   : 1172.837242241481\n",
      "    log_joint      : 1381.27757194174\n",
      "    val_loss       : -1170.3937298318615\n",
      "    val_ess        : 1.9631165162376736\n",
      "    val_log_marginal: 1170.4288436226223\n",
      "    val_log_joint  : 1379.045362389606\n",
      "Train Epoch: 1453 [0/101520 (0%)] Loss: -1169.516113\n",
      "Train Epoch: 1453 [11264/101520 (11%)] Loss: -1176.138428\n",
      "Train Epoch: 1453 [22528/101520 (22%)] Loss: -1175.526123\n",
      "Train Epoch: 1453 [33792/101520 (33%)] Loss: -1177.609375\n",
      "Train Epoch: 1453 [45056/101520 (44%)] Loss: -1170.796631\n",
      "Train Epoch: 1453 [56320/101520 (55%)] Loss: -1172.164062\n",
      "Train Epoch: 1453 [67584/101520 (67%)] Loss: -1172.663574\n",
      "Train Epoch: 1453 [78848/101520 (78%)] Loss: -1177.422852\n",
      "Train Epoch: 1453 [90112/101520 (89%)] Loss: -1174.256104\n",
      "Train Epoch: 1453 [101376/101520 (100%)] Loss: -1178.478271\n",
      "    epoch          : 1453\n",
      "    loss           : -1172.7919179638427\n",
      "    ess            : 1.967478960003685\n",
      "    log_marginal   : 1172.822377880614\n",
      "    log_joint      : 1381.2471653923917\n",
      "    val_loss       : -1172.2958241338315\n",
      "    val_ess        : 1.9654600257458894\n",
      "    val_log_marginal: 1172.32689898947\n",
      "    val_log_joint  : 1380.848876953125\n",
      "Train Epoch: 1454 [0/101520 (0%)] Loss: -1177.277100\n",
      "Train Epoch: 1454 [11264/101520 (11%)] Loss: -1179.133911\n",
      "Train Epoch: 1454 [22528/101520 (22%)] Loss: -1179.162598\n",
      "Train Epoch: 1454 [33792/101520 (33%)] Loss: -1176.442871\n",
      "Train Epoch: 1454 [45056/101520 (44%)] Loss: -1172.967407\n",
      "Train Epoch: 1454 [56320/101520 (55%)] Loss: -1173.264526\n",
      "Train Epoch: 1454 [67584/101520 (67%)] Loss: -1168.876343\n",
      "Train Epoch: 1454 [78848/101520 (78%)] Loss: -1173.097656\n",
      "Train Epoch: 1454 [90112/101520 (89%)] Loss: -1176.513672\n",
      "Train Epoch: 1454 [101376/101520 (100%)] Loss: -1158.876099\n",
      "    epoch          : 1454\n",
      "    loss           : -1172.8266478878768\n",
      "    ess            : 1.9666026195689061\n",
      "    log_marginal   : 1172.8580806866364\n",
      "    log_joint      : 1381.213608938246\n",
      "    val_loss       : -1171.1289487092392\n",
      "    val_ess        : 1.96597261014192\n",
      "    val_log_marginal: 1171.1603260869565\n",
      "    val_log_joint  : 1379.608939792799\n",
      "Train Epoch: 1455 [0/101520 (0%)] Loss: -1168.560059\n",
      "Train Epoch: 1455 [11264/101520 (11%)] Loss: -1172.601318\n",
      "Train Epoch: 1455 [22528/101520 (22%)] Loss: -1172.832275\n",
      "Train Epoch: 1455 [33792/101520 (33%)] Loss: -1166.956299\n",
      "Train Epoch: 1455 [45056/101520 (44%)] Loss: -1170.747437\n",
      "Train Epoch: 1455 [56320/101520 (55%)] Loss: -1180.385132\n",
      "Train Epoch: 1455 [67584/101520 (67%)] Loss: -1164.201660\n",
      "Train Epoch: 1455 [78848/101520 (78%)] Loss: -1177.988770\n",
      "Train Epoch: 1455 [90112/101520 (89%)] Loss: -1172.572998\n",
      "Train Epoch: 1455 [101376/101520 (100%)] Loss: -1175.347900\n",
      "    epoch          : 1455\n",
      "    loss           : -1172.873273226484\n",
      "    ess            : 1.9670348760470673\n",
      "    log_marginal   : 1172.9033644786432\n",
      "    log_joint      : 1381.3692590148005\n",
      "    val_loss       : -1173.516845703125\n",
      "    val_ess        : 1.9650390510973723\n",
      "    val_log_marginal: 1173.5472359035325\n",
      "    val_log_joint  : 1382.137642238451\n",
      "Train Epoch: 1456 [0/101520 (0%)] Loss: -1170.407227\n",
      "Train Epoch: 1456 [11264/101520 (11%)] Loss: -1173.960815\n",
      "Train Epoch: 1456 [22528/101520 (22%)] Loss: -1167.583008\n",
      "Train Epoch: 1456 [33792/101520 (33%)] Loss: -1178.677856\n",
      "Train Epoch: 1456 [45056/101520 (44%)] Loss: -1178.214600\n",
      "Train Epoch: 1456 [56320/101520 (55%)] Loss: -1169.270752\n",
      "Train Epoch: 1456 [67584/101520 (67%)] Loss: -1175.798096\n",
      "Train Epoch: 1456 [78848/101520 (78%)] Loss: -1172.095093\n",
      "Train Epoch: 1456 [90112/101520 (89%)] Loss: -1178.090820\n",
      "Train Epoch: 1456 [101376/101520 (100%)] Loss: -1174.218506\n",
      "    epoch          : 1456\n",
      "    loss           : -1172.8944214480607\n",
      "    ess            : 1.9660970943057956\n",
      "    log_marginal   : 1172.926984163984\n",
      "    log_joint      : 1381.376958645768\n",
      "    val_loss       : -1173.5355224609375\n",
      "    val_ess        : 1.968882415605628\n",
      "    val_log_marginal: 1173.5635827105978\n",
      "    val_log_joint  : 1381.757865574049\n",
      "Train Epoch: 1457 [0/101520 (0%)] Loss: -1177.693237\n",
      "Train Epoch: 1457 [11264/101520 (11%)] Loss: -1172.901611\n",
      "Train Epoch: 1457 [22528/101520 (22%)] Loss: -1165.617920\n",
      "Train Epoch: 1457 [33792/101520 (33%)] Loss: -1176.969727\n",
      "Train Epoch: 1457 [45056/101520 (44%)] Loss: -1176.532227\n",
      "Train Epoch: 1457 [56320/101520 (55%)] Loss: -1167.320312\n",
      "Train Epoch: 1457 [67584/101520 (67%)] Loss: -1168.355957\n",
      "Train Epoch: 1457 [78848/101520 (78%)] Loss: -1179.532593\n",
      "Train Epoch: 1457 [90112/101520 (89%)] Loss: -1165.422363\n",
      "Train Epoch: 1457 [101376/101520 (100%)] Loss: -1177.443115\n",
      "    epoch          : 1457\n",
      "    loss           : -1172.95127676959\n",
      "    ess            : 1.965958190922761\n",
      "    log_marginal   : 1172.98387751747\n",
      "    log_joint      : 1381.3946907388506\n",
      "    val_loss       : -1171.7291259765625\n",
      "    val_ess        : 1.9675977748373281\n",
      "    val_log_marginal: 1171.7602379840353\n",
      "    val_log_joint  : 1380.0386591372283\n",
      "Train Epoch: 1458 [0/101520 (0%)] Loss: -1166.752808\n",
      "Train Epoch: 1458 [11264/101520 (11%)] Loss: -1174.794189\n",
      "Train Epoch: 1458 [22528/101520 (22%)] Loss: -1183.900146\n",
      "Train Epoch: 1458 [33792/101520 (33%)] Loss: -1178.998047\n",
      "Train Epoch: 1458 [45056/101520 (44%)] Loss: -1174.396973\n",
      "Train Epoch: 1458 [56320/101520 (55%)] Loss: -1168.621338\n",
      "Train Epoch: 1458 [67584/101520 (67%)] Loss: -1178.410400\n",
      "Train Epoch: 1458 [78848/101520 (78%)] Loss: -1172.293457\n",
      "Train Epoch: 1458 [90112/101520 (89%)] Loss: -1168.602783\n",
      "Train Epoch: 1458 [101376/101520 (100%)] Loss: -1182.693481\n",
      "    epoch          : 1458\n",
      "    loss           : -1173.054753135796\n",
      "    ess            : 1.9667332010652552\n",
      "    log_marginal   : 1173.0872502159234\n",
      "    log_joint      : 1381.4848393579225\n",
      "    val_loss       : -1172.9881697944973\n",
      "    val_ess        : 1.9662045499552852\n",
      "    val_log_marginal: 1173.0219514266305\n",
      "    val_log_joint  : 1381.3513077445652\n",
      "Train Epoch: 1459 [0/101520 (0%)] Loss: -1177.724976\n",
      "Train Epoch: 1459 [11264/101520 (11%)] Loss: -1172.289917\n",
      "Train Epoch: 1459 [22528/101520 (22%)] Loss: -1171.273682\n",
      "Train Epoch: 1459 [33792/101520 (33%)] Loss: -1169.437866\n",
      "Train Epoch: 1459 [45056/101520 (44%)] Loss: -1172.013916\n",
      "Train Epoch: 1459 [56320/101520 (55%)] Loss: -1168.042480\n",
      "Train Epoch: 1459 [67584/101520 (67%)] Loss: -1175.641113\n",
      "Train Epoch: 1459 [78848/101520 (78%)] Loss: -1173.134766\n",
      "Train Epoch: 1459 [90112/101520 (89%)] Loss: -1172.411377\n",
      "Train Epoch: 1459 [101376/101520 (100%)] Loss: -1173.378662\n",
      "    epoch          : 1459\n",
      "    loss           : -1173.0482607127435\n",
      "    ess            : 1.9667740545081134\n",
      "    log_marginal   : 1173.079680629711\n",
      "    log_joint      : 1381.5743126030543\n",
      "    val_loss       : -1171.223680579144\n",
      "    val_ess        : 1.9671365333640056\n",
      "    val_log_marginal: 1171.25439453125\n",
      "    val_log_joint  : 1379.7481211786685\n",
      "Train Epoch: 1460 [0/101520 (0%)] Loss: -1172.049561\n",
      "Train Epoch: 1460 [11264/101520 (11%)] Loss: -1171.795410\n",
      "Train Epoch: 1460 [22528/101520 (22%)] Loss: -1167.082397\n",
      "Train Epoch: 1460 [33792/101520 (33%)] Loss: -1172.866699\n",
      "Train Epoch: 1460 [45056/101520 (44%)] Loss: -1178.047363\n",
      "Train Epoch: 1460 [56320/101520 (55%)] Loss: -1176.862549\n",
      "Train Epoch: 1460 [67584/101520 (67%)] Loss: -1175.193481\n",
      "Train Epoch: 1460 [78848/101520 (78%)] Loss: -1166.851318\n",
      "Train Epoch: 1460 [90112/101520 (89%)] Loss: -1171.840088\n",
      "Train Epoch: 1460 [101376/101520 (100%)] Loss: -1173.138062\n",
      "    epoch          : 1460\n",
      "    loss           : -1173.1072513446136\n",
      "    ess            : 1.9668609568821125\n",
      "    log_marginal   : 1173.1387902648005\n",
      "    log_joint      : 1381.5813589431532\n",
      "    val_loss       : -1171.9510657269022\n",
      "    val_ess        : 1.969674271085988\n",
      "    val_log_marginal: 1171.9778734290082\n",
      "    val_log_joint  : 1380.385444972826\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1460.pth ...\n",
      "Train Epoch: 1461 [0/101520 (0%)] Loss: -1174.370117\n",
      "Train Epoch: 1461 [11264/101520 (11%)] Loss: -1172.421387\n",
      "Train Epoch: 1461 [22528/101520 (22%)] Loss: -1173.852295\n",
      "Train Epoch: 1461 [33792/101520 (33%)] Loss: -1172.932861\n",
      "Train Epoch: 1461 [45056/101520 (44%)] Loss: -1175.461914\n",
      "Train Epoch: 1461 [56320/101520 (55%)] Loss: -1181.188232\n",
      "Train Epoch: 1461 [67584/101520 (67%)] Loss: -1171.003662\n",
      "Train Epoch: 1461 [78848/101520 (78%)] Loss: -1176.792480\n",
      "Train Epoch: 1461 [90112/101520 (89%)] Loss: -1180.427246\n",
      "Train Epoch: 1461 [101376/101520 (100%)] Loss: -1171.945801\n",
      "    epoch          : 1461\n",
      "    loss           : -1173.191288473618\n",
      "    ess            : 1.9661889016328744\n",
      "    log_marginal   : 1173.223595393962\n",
      "    log_joint      : 1381.5709338930983\n",
      "    val_loss       : -1171.4109311311142\n",
      "    val_ess        : 1.9699285911477131\n",
      "    val_log_marginal: 1171.4387843919837\n",
      "    val_log_joint  : 1379.6516750169837\n",
      "Train Epoch: 1462 [0/101520 (0%)] Loss: -1181.280273\n",
      "Train Epoch: 1462 [11264/101520 (11%)] Loss: -1172.438232\n",
      "Train Epoch: 1462 [22528/101520 (22%)] Loss: -1175.197632\n",
      "Train Epoch: 1462 [33792/101520 (33%)] Loss: -1170.313232\n",
      "Train Epoch: 1462 [45056/101520 (44%)] Loss: -1168.453613\n",
      "Train Epoch: 1462 [56320/101520 (55%)] Loss: -1177.704346\n",
      "Train Epoch: 1462 [67584/101520 (67%)] Loss: -1174.070801\n",
      "Train Epoch: 1462 [78848/101520 (78%)] Loss: -1176.399170\n",
      "Train Epoch: 1462 [90112/101520 (89%)] Loss: -1174.531616\n",
      "Train Epoch: 1462 [101376/101520 (100%)] Loss: -1178.105713\n",
      "    epoch          : 1462\n",
      "    loss           : -1173.201449753651\n",
      "    ess            : 1.9667816785112697\n",
      "    log_marginal   : 1173.2329500284627\n",
      "    log_joint      : 1381.652434535961\n",
      "    val_loss       : -1170.9278670601223\n",
      "    val_ess        : 1.9662507927936057\n",
      "    val_log_marginal: 1170.9615106997283\n",
      "    val_log_joint  : 1379.545553456182\n",
      "Train Epoch: 1463 [0/101520 (0%)] Loss: -1171.112061\n",
      "Train Epoch: 1463 [11264/101520 (11%)] Loss: -1168.128418\n",
      "Train Epoch: 1463 [22528/101520 (22%)] Loss: -1176.267212\n",
      "Train Epoch: 1463 [33792/101520 (33%)] Loss: -1176.301270\n",
      "Train Epoch: 1463 [45056/101520 (44%)] Loss: -1164.881958\n",
      "Train Epoch: 1463 [56320/101520 (55%)] Loss: -1172.724365\n",
      "Train Epoch: 1463 [67584/101520 (67%)] Loss: -1170.102905\n",
      "Train Epoch: 1463 [78848/101520 (78%)] Loss: -1175.072998\n",
      "Train Epoch: 1463 [90112/101520 (89%)] Loss: -1183.213623\n",
      "Train Epoch: 1463 [101376/101520 (100%)] Loss: -1164.241333\n",
      "    epoch          : 1463\n",
      "    loss           : -1173.1005043528187\n",
      "    ess            : 1.9660534038016544\n",
      "    log_marginal   : 1173.1329787364557\n",
      "    log_joint      : 1381.6656494140625\n",
      "    val_loss       : -1172.5244459069293\n",
      "    val_ess        : 1.9695458360340283\n",
      "    val_log_marginal: 1172.552293860394\n",
      "    val_log_joint  : 1380.8347008746603\n",
      "Train Epoch: 1464 [0/101520 (0%)] Loss: -1173.649292\n",
      "Train Epoch: 1464 [11264/101520 (11%)] Loss: -1175.385254\n",
      "Train Epoch: 1464 [22528/101520 (22%)] Loss: -1167.929810\n",
      "Train Epoch: 1464 [33792/101520 (33%)] Loss: -1170.172485\n",
      "Train Epoch: 1464 [45056/101520 (44%)] Loss: -1175.624756\n",
      "Train Epoch: 1464 [56320/101520 (55%)] Loss: -1172.578979\n",
      "Train Epoch: 1464 [67584/101520 (67%)] Loss: -1179.270996\n",
      "Train Epoch: 1464 [78848/101520 (78%)] Loss: -1180.190674\n",
      "Train Epoch: 1464 [90112/101520 (89%)] Loss: -1173.091919\n",
      "Train Epoch: 1464 [101376/101520 (100%)] Loss: -1166.115234\n",
      "    epoch          : 1464\n",
      "    loss           : -1173.3036134039337\n",
      "    ess            : 1.9660066983208584\n",
      "    log_marginal   : 1173.3365490784\n",
      "    log_joint      : 1381.772402049309\n",
      "    val_loss       : -1173.1578422214675\n",
      "    val_ess        : 1.9675090261127637\n",
      "    val_log_marginal: 1173.187982973845\n",
      "    val_log_joint  : 1381.4046153192935\n",
      "Train Epoch: 1465 [0/101520 (0%)] Loss: -1179.554688\n",
      "Train Epoch: 1465 [11264/101520 (11%)] Loss: -1164.415527\n",
      "Train Epoch: 1465 [22528/101520 (22%)] Loss: -1175.193359\n",
      "Train Epoch: 1465 [33792/101520 (33%)] Loss: -1173.272461\n",
      "Train Epoch: 1465 [45056/101520 (44%)] Loss: -1172.735840\n",
      "Train Epoch: 1465 [56320/101520 (55%)] Loss: -1169.999756\n",
      "Train Epoch: 1465 [67584/101520 (67%)] Loss: -1169.870605\n",
      "Train Epoch: 1465 [78848/101520 (78%)] Loss: -1175.550049\n",
      "Train Epoch: 1465 [90112/101520 (89%)] Loss: -1180.698120\n",
      "Train Epoch: 1465 [101376/101520 (100%)] Loss: -1168.290405\n",
      "    epoch          : 1465\n",
      "    loss           : -1173.4015855645414\n",
      "    ess            : 1.9658204269169564\n",
      "    log_marginal   : 1173.4338869641174\n",
      "    log_joint      : 1381.8485941671247\n",
      "    val_loss       : -1170.5466520889945\n",
      "    val_ess        : 1.965963576150977\n",
      "    val_log_marginal: 1170.5790325662365\n",
      "    val_log_joint  : 1378.6589567764945\n",
      "Train Epoch: 1466 [0/101520 (0%)] Loss: -1173.418457\n",
      "Train Epoch: 1466 [11264/101520 (11%)] Loss: -1171.764648\n",
      "Train Epoch: 1466 [22528/101520 (22%)] Loss: -1170.565430\n",
      "Train Epoch: 1466 [33792/101520 (33%)] Loss: -1181.486084\n",
      "Train Epoch: 1466 [45056/101520 (44%)] Loss: -1177.689331\n",
      "Train Epoch: 1466 [56320/101520 (55%)] Loss: -1168.527588\n",
      "Train Epoch: 1466 [67584/101520 (67%)] Loss: -1166.817871\n",
      "Train Epoch: 1466 [78848/101520 (78%)] Loss: -1174.739746\n",
      "Train Epoch: 1466 [90112/101520 (89%)] Loss: -1172.373413\n",
      "Train Epoch: 1466 [101376/101520 (100%)] Loss: -1178.843384\n",
      "    epoch          : 1466\n",
      "    loss           : -1173.4659068045305\n",
      "    ess            : 1.9663202690718762\n",
      "    log_marginal   : 1173.497068472244\n",
      "    log_joint      : 1381.8891129230135\n",
      "    val_loss       : -1170.7403405230978\n",
      "    val_ess        : 1.9689317630684895\n",
      "    val_log_marginal: 1170.7693295686142\n",
      "    val_log_joint  : 1378.9278989045517\n",
      "Train Epoch: 1467 [0/101520 (0%)] Loss: -1171.219971\n",
      "Train Epoch: 1467 [11264/101520 (11%)] Loss: -1173.752319\n",
      "Train Epoch: 1467 [22528/101520 (22%)] Loss: -1170.301636\n",
      "Train Epoch: 1467 [33792/101520 (33%)] Loss: -1179.707153\n",
      "Train Epoch: 1467 [45056/101520 (44%)] Loss: -1176.344482\n",
      "Train Epoch: 1467 [56320/101520 (55%)] Loss: -1174.093018\n",
      "Train Epoch: 1467 [67584/101520 (67%)] Loss: -1182.625122\n",
      "Train Epoch: 1467 [78848/101520 (78%)] Loss: -1171.127930\n",
      "Train Epoch: 1467 [90112/101520 (89%)] Loss: -1175.732300\n",
      "Train Epoch: 1467 [101376/101520 (100%)] Loss: -1175.653687\n",
      "    epoch          : 1467\n",
      "    loss           : -1173.4792081746623\n",
      "    ess            : 1.9664294683753545\n",
      "    log_marginal   : 1173.5106912737515\n",
      "    log_joint      : 1381.8580837537295\n",
      "    val_loss       : -1171.7599726137908\n",
      "    val_ess        : 1.970130899678106\n",
      "    val_log_marginal: 1171.787109375\n",
      "    val_log_joint  : 1380.1679262907608\n",
      "Train Epoch: 1468 [0/101520 (0%)] Loss: -1171.592285\n",
      "Train Epoch: 1468 [11264/101520 (11%)] Loss: -1173.908569\n",
      "Train Epoch: 1468 [22528/101520 (22%)] Loss: -1169.233521\n",
      "Train Epoch: 1468 [33792/101520 (33%)] Loss: -1167.851929\n",
      "Train Epoch: 1468 [45056/101520 (44%)] Loss: -1178.694458\n",
      "Train Epoch: 1468 [56320/101520 (55%)] Loss: -1174.164795\n",
      "Train Epoch: 1468 [67584/101520 (67%)] Loss: -1168.055176\n",
      "Train Epoch: 1468 [78848/101520 (78%)] Loss: -1171.904297\n",
      "Train Epoch: 1468 [90112/101520 (89%)] Loss: -1169.795776\n",
      "Train Epoch: 1468 [101376/101520 (100%)] Loss: -1173.559692\n",
      "    epoch          : 1468\n",
      "    loss           : -1173.4216958817526\n",
      "    ess            : 1.9664328469702945\n",
      "    log_marginal   : 1173.453349511228\n",
      "    log_joint      : 1381.9143544872802\n",
      "    val_loss       : -1172.337593410326\n",
      "    val_ess        : 1.9653859967770784\n",
      "    val_log_marginal: 1172.372749660326\n",
      "    val_log_joint  : 1381.2752154806385\n",
      "Train Epoch: 1469 [0/101520 (0%)] Loss: -1176.604492\n",
      "Train Epoch: 1469 [11264/101520 (11%)] Loss: -1180.223389\n",
      "Train Epoch: 1469 [22528/101520 (22%)] Loss: -1173.744629\n",
      "Train Epoch: 1469 [33792/101520 (33%)] Loss: -1174.801758\n",
      "Train Epoch: 1469 [45056/101520 (44%)] Loss: -1172.729736\n",
      "Train Epoch: 1469 [56320/101520 (55%)] Loss: -1170.460327\n",
      "Train Epoch: 1469 [67584/101520 (67%)] Loss: -1167.868652\n",
      "Train Epoch: 1469 [78848/101520 (78%)] Loss: -1176.884644\n",
      "Train Epoch: 1469 [90112/101520 (89%)] Loss: -1170.507324\n",
      "Train Epoch: 1469 [101376/101520 (100%)] Loss: -1175.200562\n",
      "    epoch          : 1469\n",
      "    loss           : -1173.496170427332\n",
      "    ess            : 1.966950624432396\n",
      "    log_marginal   : 1173.5267174495525\n",
      "    log_joint      : 1381.9384090864478\n",
      "    val_loss       : -1172.604497494905\n",
      "    val_ess        : 1.9686945469483086\n",
      "    val_log_marginal: 1172.6331734035325\n",
      "    val_log_joint  : 1380.981344471807\n",
      "Train Epoch: 1470 [0/101520 (0%)] Loss: -1176.466187\n",
      "Train Epoch: 1470 [11264/101520 (11%)] Loss: -1176.979492\n",
      "Train Epoch: 1470 [22528/101520 (22%)] Loss: -1171.473145\n",
      "Train Epoch: 1470 [33792/101520 (33%)] Loss: -1168.978271\n",
      "Train Epoch: 1470 [45056/101520 (44%)] Loss: -1175.493896\n",
      "Train Epoch: 1470 [56320/101520 (55%)] Loss: -1173.946167\n",
      "Train Epoch: 1470 [67584/101520 (67%)] Loss: -1174.215210\n",
      "Train Epoch: 1470 [78848/101520 (78%)] Loss: -1167.508057\n",
      "Train Epoch: 1470 [90112/101520 (89%)] Loss: -1170.282227\n",
      "Train Epoch: 1470 [101376/101520 (100%)] Loss: -1178.575073\n",
      "    epoch          : 1470\n",
      "    loss           : -1173.512543798092\n",
      "    ess            : 1.9663362383243426\n",
      "    log_marginal   : 1173.5441011208386\n",
      "    log_joint      : 1381.942422071294\n",
      "    val_loss       : -1173.930128014606\n",
      "    val_ess        : 1.9688498870186184\n",
      "    val_log_marginal: 1173.9581458050272\n",
      "    val_log_joint  : 1382.3035835597825\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1470.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1471 [0/101520 (0%)] Loss: -1164.257202\n",
      "Train Epoch: 1471 [11264/101520 (11%)] Loss: -1177.000732\n",
      "Train Epoch: 1471 [22528/101520 (22%)] Loss: -1174.844727\n",
      "Train Epoch: 1471 [33792/101520 (33%)] Loss: -1176.300659\n",
      "Train Epoch: 1471 [45056/101520 (44%)] Loss: -1175.081543\n",
      "Train Epoch: 1471 [56320/101520 (55%)] Loss: -1176.247314\n",
      "Train Epoch: 1471 [67584/101520 (67%)] Loss: -1179.612549\n",
      "Train Epoch: 1471 [78848/101520 (78%)] Loss: -1163.385742\n",
      "Train Epoch: 1471 [90112/101520 (89%)] Loss: -1166.524170\n",
      "Train Epoch: 1471 [101376/101520 (100%)] Loss: -1175.349731\n",
      "    epoch          : 1471\n",
      "    loss           : -1173.554746388191\n",
      "    ess            : 1.967267331166483\n",
      "    log_marginal   : 1173.5851523241206\n",
      "    log_joint      : 1382.0203587517667\n",
      "    val_loss       : -1173.1891028362772\n",
      "    val_ess        : 1.964099391647007\n",
      "    val_log_marginal: 1173.2236593495245\n",
      "    val_log_joint  : 1381.6892249065897\n",
      "Train Epoch: 1472 [0/101520 (0%)] Loss: -1171.694336\n",
      "Train Epoch: 1472 [11264/101520 (11%)] Loss: -1174.865234\n",
      "Train Epoch: 1472 [22528/101520 (22%)] Loss: -1167.113281\n",
      "Train Epoch: 1472 [33792/101520 (33%)] Loss: -1176.460693\n",
      "Train Epoch: 1472 [45056/101520 (44%)] Loss: -1178.079468\n",
      "Train Epoch: 1472 [56320/101520 (55%)] Loss: -1168.666260\n",
      "Train Epoch: 1472 [67584/101520 (67%)] Loss: -1177.712769\n",
      "Train Epoch: 1472 [78848/101520 (78%)] Loss: -1177.681396\n",
      "Train Epoch: 1472 [90112/101520 (89%)] Loss: -1163.235352\n",
      "Train Epoch: 1472 [101376/101520 (100%)] Loss: -1171.341309\n",
      "    epoch          : 1472\n",
      "    loss           : -1173.5391367236573\n",
      "    ess            : 1.9668824181484816\n",
      "    log_marginal   : 1173.570770723736\n",
      "    log_joint      : 1382.0136086928785\n",
      "    val_loss       : -1172.4695673403533\n",
      "    val_ess        : 1.9668801038161567\n",
      "    val_log_marginal: 1172.4995754076087\n",
      "    val_log_joint  : 1380.8095915421195\n",
      "Train Epoch: 1473 [0/101520 (0%)] Loss: -1174.020752\n",
      "Train Epoch: 1473 [11264/101520 (11%)] Loss: -1178.485596\n",
      "Train Epoch: 1473 [22528/101520 (22%)] Loss: -1170.514404\n",
      "Train Epoch: 1473 [33792/101520 (33%)] Loss: -1176.008667\n",
      "Train Epoch: 1473 [45056/101520 (44%)] Loss: -1175.877930\n",
      "Train Epoch: 1473 [56320/101520 (55%)] Loss: -1171.921387\n",
      "Train Epoch: 1473 [67584/101520 (67%)] Loss: -1173.501709\n",
      "Train Epoch: 1473 [78848/101520 (78%)] Loss: -1167.189697\n",
      "Train Epoch: 1473 [90112/101520 (89%)] Loss: -1175.699585\n",
      "Train Epoch: 1473 [101376/101520 (100%)] Loss: -1182.777344\n",
      "    epoch          : 1473\n",
      "    loss           : -1173.5430012611887\n",
      "    ess            : 1.9673581339007047\n",
      "    log_marginal   : 1173.5743003346813\n",
      "    log_joint      : 1382.0746659322\n",
      "    val_loss       : -1171.5275082795517\n",
      "    val_ess        : 1.9661352375279302\n",
      "    val_log_marginal: 1171.5577339504075\n",
      "    val_log_joint  : 1380.0536260190217\n",
      "Train Epoch: 1474 [0/101520 (0%)] Loss: -1174.323730\n",
      "Train Epoch: 1474 [11264/101520 (11%)] Loss: -1177.024780\n",
      "Train Epoch: 1474 [22528/101520 (22%)] Loss: -1170.871826\n",
      "Train Epoch: 1474 [33792/101520 (33%)] Loss: -1173.729248\n",
      "Train Epoch: 1474 [45056/101520 (44%)] Loss: -1179.547119\n",
      "Train Epoch: 1474 [56320/101520 (55%)] Loss: -1168.473633\n",
      "Train Epoch: 1474 [67584/101520 (67%)] Loss: -1180.776367\n",
      "Train Epoch: 1474 [78848/101520 (78%)] Loss: -1171.880859\n",
      "Train Epoch: 1474 [90112/101520 (89%)] Loss: -1174.425903\n",
      "Train Epoch: 1474 [101376/101520 (100%)] Loss: -1180.268433\n",
      "    epoch          : 1474\n",
      "    loss           : -1173.6677135678392\n",
      "    ess            : 1.9680788792557453\n",
      "    log_marginal   : 1173.6974220467573\n",
      "    log_joint      : 1382.0938493738222\n",
      "    val_loss       : -1172.2566767153533\n",
      "    val_ess        : 1.9640710509341697\n",
      "    val_log_marginal: 1172.2923583984375\n",
      "    val_log_joint  : 1380.675239894701\n",
      "Train Epoch: 1475 [0/101520 (0%)] Loss: -1183.771729\n",
      "Train Epoch: 1475 [11264/101520 (11%)] Loss: -1181.343994\n",
      "Train Epoch: 1475 [22528/101520 (22%)] Loss: -1175.322998\n",
      "Train Epoch: 1475 [33792/101520 (33%)] Loss: -1171.139160\n",
      "Train Epoch: 1475 [45056/101520 (44%)] Loss: -1162.721924\n",
      "Train Epoch: 1475 [56320/101520 (55%)] Loss: -1174.549683\n",
      "Train Epoch: 1475 [67584/101520 (67%)] Loss: -1180.542480\n",
      "Train Epoch: 1475 [78848/101520 (78%)] Loss: -1171.967285\n",
      "Train Epoch: 1475 [90112/101520 (89%)] Loss: -1175.270752\n",
      "Train Epoch: 1475 [101376/101520 (100%)] Loss: -1179.199707\n",
      "    epoch          : 1475\n",
      "    loss           : -1173.7809193673445\n",
      "    ess            : 1.9660904940648294\n",
      "    log_marginal   : 1173.8136998468908\n",
      "    log_joint      : 1382.1868356675957\n",
      "    val_loss       : -1171.7357336956522\n",
      "    val_ess        : 1.9668218830357427\n",
      "    val_log_marginal: 1171.7672490658967\n",
      "    val_log_joint  : 1380.3326946756115\n",
      "Train Epoch: 1476 [0/101520 (0%)] Loss: -1168.179932\n",
      "Train Epoch: 1476 [11264/101520 (11%)] Loss: -1170.861206\n",
      "Train Epoch: 1476 [22528/101520 (22%)] Loss: -1170.476440\n",
      "Train Epoch: 1476 [33792/101520 (33%)] Loss: -1167.209839\n",
      "Train Epoch: 1476 [45056/101520 (44%)] Loss: -1171.793945\n",
      "Train Epoch: 1476 [56320/101520 (55%)] Loss: -1168.043335\n",
      "Train Epoch: 1476 [67584/101520 (67%)] Loss: -1178.755249\n",
      "Train Epoch: 1476 [78848/101520 (78%)] Loss: -1175.357544\n",
      "Train Epoch: 1476 [90112/101520 (89%)] Loss: -1173.304688\n",
      "Train Epoch: 1476 [101376/101520 (100%)] Loss: -1192.792969\n",
      "    epoch          : 1476\n",
      "    loss           : -1173.8032747968357\n",
      "    ess            : 1.967310847948544\n",
      "    log_marginal   : 1173.8337304196766\n",
      "    log_joint      : 1382.2412238192917\n",
      "    val_loss       : -1173.4373620074728\n",
      "    val_ess        : 1.9701946144518645\n",
      "    val_log_marginal: 1173.464159094769\n",
      "    val_log_joint  : 1382.0362230383832\n",
      "Train Epoch: 1477 [0/101520 (0%)] Loss: -1171.176758\n",
      "Train Epoch: 1477 [11264/101520 (11%)] Loss: -1172.510010\n",
      "Train Epoch: 1477 [22528/101520 (22%)] Loss: -1176.142090\n",
      "Train Epoch: 1477 [33792/101520 (33%)] Loss: -1174.085205\n",
      "Train Epoch: 1477 [45056/101520 (44%)] Loss: -1176.223877\n",
      "Train Epoch: 1477 [56320/101520 (55%)] Loss: -1174.377319\n",
      "Train Epoch: 1477 [67584/101520 (67%)] Loss: -1172.421387\n",
      "Train Epoch: 1477 [78848/101520 (78%)] Loss: -1169.827637\n",
      "Train Epoch: 1477 [90112/101520 (89%)] Loss: -1180.516968\n",
      "Train Epoch: 1477 [101376/101520 (100%)] Loss: -1169.563232\n",
      "    epoch          : 1477\n",
      "    loss           : -1173.788164455088\n",
      "    ess            : 1.9674760390765702\n",
      "    log_marginal   : 1173.8186029022063\n",
      "    log_joint      : 1382.2342529296875\n",
      "    val_loss       : -1172.8568009086277\n",
      "    val_ess        : 1.9678755428480066\n",
      "    val_log_marginal: 1172.884521484375\n",
      "    val_log_joint  : 1381.3554262907608\n",
      "Train Epoch: 1478 [0/101520 (0%)] Loss: -1167.646484\n",
      "Train Epoch: 1478 [11264/101520 (11%)] Loss: -1176.974365\n",
      "Train Epoch: 1478 [22528/101520 (22%)] Loss: -1167.361572\n",
      "Train Epoch: 1478 [33792/101520 (33%)] Loss: -1170.128662\n",
      "Train Epoch: 1478 [45056/101520 (44%)] Loss: -1173.393066\n",
      "Train Epoch: 1478 [56320/101520 (55%)] Loss: -1169.315308\n",
      "Train Epoch: 1478 [67584/101520 (67%)] Loss: -1167.781128\n",
      "Train Epoch: 1478 [78848/101520 (78%)] Loss: -1167.273438\n",
      "Train Epoch: 1478 [90112/101520 (89%)] Loss: -1168.911255\n",
      "Train Epoch: 1478 [101376/101520 (100%)] Loss: -1168.840454\n",
      "    epoch          : 1478\n",
      "    loss           : -1173.737397313717\n",
      "    ess            : 1.9674252601125133\n",
      "    log_marginal   : 1173.767467709642\n",
      "    log_joint      : 1382.2437087782664\n",
      "    val_loss       : -1172.8143310546875\n",
      "    val_ess        : 1.968051552772522\n",
      "    val_log_marginal: 1172.8439835258152\n",
      "    val_log_joint  : 1381.544629967731\n",
      "Train Epoch: 1479 [0/101520 (0%)] Loss: -1171.285889\n",
      "Train Epoch: 1479 [11264/101520 (11%)] Loss: -1167.523804\n",
      "Train Epoch: 1479 [22528/101520 (22%)] Loss: -1177.058350\n",
      "Train Epoch: 1479 [33792/101520 (33%)] Loss: -1174.664795\n",
      "Train Epoch: 1479 [45056/101520 (44%)] Loss: -1174.343994\n",
      "Train Epoch: 1479 [56320/101520 (55%)] Loss: -1174.994629\n",
      "Train Epoch: 1479 [67584/101520 (67%)] Loss: -1175.531494\n",
      "Train Epoch: 1479 [78848/101520 (78%)] Loss: -1175.640503\n",
      "Train Epoch: 1479 [90112/101520 (89%)] Loss: -1176.702271\n",
      "Train Epoch: 1479 [101376/101520 (100%)] Loss: -1174.800171\n",
      "    epoch          : 1479\n",
      "    loss           : -1173.7562458287532\n",
      "    ess            : 1.9667768688058134\n",
      "    log_marginal   : 1173.7878528384108\n",
      "    log_joint      : 1382.222575892156\n",
      "    val_loss       : -1173.8004946501358\n",
      "    val_ess        : 1.9659815093745356\n",
      "    val_log_marginal: 1173.8330290421195\n",
      "    val_log_joint  : 1382.1028468919837\n",
      "Train Epoch: 1480 [0/101520 (0%)] Loss: -1181.006348\n",
      "Train Epoch: 1480 [11264/101520 (11%)] Loss: -1172.571777\n",
      "Train Epoch: 1480 [22528/101520 (22%)] Loss: -1166.874268\n",
      "Train Epoch: 1480 [33792/101520 (33%)] Loss: -1172.395020\n",
      "Train Epoch: 1480 [45056/101520 (44%)] Loss: -1172.967285\n",
      "Train Epoch: 1480 [56320/101520 (55%)] Loss: -1179.569824\n",
      "Train Epoch: 1480 [67584/101520 (67%)] Loss: -1172.247070\n",
      "Train Epoch: 1480 [78848/101520 (78%)] Loss: -1170.016113\n",
      "Train Epoch: 1480 [90112/101520 (89%)] Loss: -1177.206787\n",
      "Train Epoch: 1480 [101376/101520 (100%)] Loss: -1167.622192\n",
      "    epoch          : 1480\n",
      "    loss           : -1173.7796219868876\n",
      "    ess            : 1.9669413949976016\n",
      "    log_marginal   : 1173.8102272838803\n",
      "    log_joint      : 1382.2345400096185\n",
      "    val_loss       : -1171.8483515200408\n",
      "    val_ess        : 1.967427626900051\n",
      "    val_log_marginal: 1171.8787894870925\n",
      "    val_log_joint  : 1380.2586669921875\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1480.pth ...\n",
      "Train Epoch: 1481 [0/101520 (0%)] Loss: -1175.600708\n",
      "Train Epoch: 1481 [11264/101520 (11%)] Loss: -1180.045654\n",
      "Train Epoch: 1481 [22528/101520 (22%)] Loss: -1172.919189\n",
      "Train Epoch: 1481 [33792/101520 (33%)] Loss: -1171.108521\n",
      "Train Epoch: 1481 [45056/101520 (44%)] Loss: -1171.143311\n",
      "Train Epoch: 1481 [56320/101520 (55%)] Loss: -1176.925903\n",
      "Train Epoch: 1481 [67584/101520 (67%)] Loss: -1172.188599\n",
      "Train Epoch: 1481 [78848/101520 (78%)] Loss: -1171.622192\n",
      "Train Epoch: 1481 [90112/101520 (89%)] Loss: -1172.645264\n",
      "Train Epoch: 1481 [101376/101520 (100%)] Loss: -1174.166870\n",
      "    epoch          : 1481\n",
      "    loss           : -1173.814647578714\n",
      "    ess            : 1.9668869289321516\n",
      "    log_marginal   : 1173.8464649909704\n",
      "    log_joint      : 1382.2725823943938\n",
      "    val_loss       : -1172.8453316066575\n",
      "    val_ess        : 1.9636115198550017\n",
      "    val_log_marginal: 1172.8856307319973\n",
      "    val_log_joint  : 1381.072854746943\n",
      "Train Epoch: 1482 [0/101520 (0%)] Loss: -1175.482300\n",
      "Train Epoch: 1482 [11264/101520 (11%)] Loss: -1171.728760\n",
      "Train Epoch: 1482 [22528/101520 (22%)] Loss: -1170.138428\n",
      "Train Epoch: 1482 [33792/101520 (33%)] Loss: -1170.068115\n",
      "Train Epoch: 1482 [45056/101520 (44%)] Loss: -1176.145264\n",
      "Train Epoch: 1482 [56320/101520 (55%)] Loss: -1173.438965\n",
      "Train Epoch: 1482 [67584/101520 (67%)] Loss: -1169.423096\n",
      "Train Epoch: 1482 [78848/101520 (78%)] Loss: -1172.562256\n",
      "Train Epoch: 1482 [90112/101520 (89%)] Loss: -1167.193359\n",
      "Train Epoch: 1482 [101376/101520 (100%)] Loss: -1174.135864\n",
      "    epoch          : 1482\n",
      "    loss           : -1173.8722469770728\n",
      "    ess            : 1.9659942628151208\n",
      "    log_marginal   : 1173.9038558269865\n",
      "    log_joint      : 1382.296817952065\n",
      "    val_loss       : -1173.7996295431385\n",
      "    val_ess        : 1.9651011021240898\n",
      "    val_log_marginal: 1173.8392280910325\n",
      "    val_log_joint  : 1382.1559156334918\n",
      "Train Epoch: 1483 [0/101520 (0%)] Loss: -1162.437134\n",
      "Train Epoch: 1483 [11264/101520 (11%)] Loss: -1170.128906\n",
      "Train Epoch: 1483 [22528/101520 (22%)] Loss: -1178.406982\n",
      "Train Epoch: 1483 [33792/101520 (33%)] Loss: -1174.429443\n",
      "Train Epoch: 1483 [45056/101520 (44%)] Loss: -1174.673828\n",
      "Train Epoch: 1483 [56320/101520 (55%)] Loss: -1178.185059\n",
      "Train Epoch: 1483 [67584/101520 (67%)] Loss: -1174.656250\n",
      "Train Epoch: 1483 [78848/101520 (78%)] Loss: -1178.284424\n",
      "Train Epoch: 1483 [90112/101520 (89%)] Loss: -1166.427734\n",
      "Train Epoch: 1483 [101376/101520 (100%)] Loss: -1178.294556\n",
      "    epoch          : 1483\n",
      "    loss           : -1173.8763783517195\n",
      "    ess            : 1.9667887328258111\n",
      "    log_marginal   : 1173.9079227926743\n",
      "    log_joint      : 1382.3306541251177\n",
      "    val_loss       : -1171.5283096976902\n",
      "    val_ess        : 1.965731558592423\n",
      "    val_log_marginal: 1171.5578719429348\n",
      "    val_log_joint  : 1379.7746316661005\n",
      "Train Epoch: 1484 [0/101520 (0%)] Loss: -1171.354614\n",
      "Train Epoch: 1484 [11264/101520 (11%)] Loss: -1170.330566\n",
      "Train Epoch: 1484 [22528/101520 (22%)] Loss: -1172.883301\n",
      "Train Epoch: 1484 [33792/101520 (33%)] Loss: -1180.041992\n",
      "Train Epoch: 1484 [45056/101520 (44%)] Loss: -1173.606934\n",
      "Train Epoch: 1484 [56320/101520 (55%)] Loss: -1166.722534\n",
      "Train Epoch: 1484 [67584/101520 (67%)] Loss: -1173.834717\n",
      "Train Epoch: 1484 [78848/101520 (78%)] Loss: -1175.377686\n",
      "Train Epoch: 1484 [90112/101520 (89%)] Loss: -1175.273438\n",
      "Train Epoch: 1484 [101376/101520 (100%)] Loss: -1162.515625\n",
      "    epoch          : 1484\n",
      "    loss           : -1173.8366190081265\n",
      "    ess            : 1.9668845441473188\n",
      "    log_marginal   : 1173.8685370210428\n",
      "    log_joint      : 1382.3128840000786\n",
      "    val_loss       : -1172.355129076087\n",
      "    val_ess        : 1.9664012867471445\n",
      "    val_log_marginal: 1172.3882685122283\n",
      "    val_log_joint  : 1381.2660973590353\n",
      "Train Epoch: 1485 [0/101520 (0%)] Loss: -1172.788086\n",
      "Train Epoch: 1485 [11264/101520 (11%)] Loss: -1171.805176\n",
      "Train Epoch: 1485 [22528/101520 (22%)] Loss: -1179.730469\n",
      "Train Epoch: 1485 [33792/101520 (33%)] Loss: -1178.227295\n",
      "Train Epoch: 1485 [45056/101520 (44%)] Loss: -1176.010254\n",
      "Train Epoch: 1485 [56320/101520 (55%)] Loss: -1173.136353\n",
      "Train Epoch: 1485 [67584/101520 (67%)] Loss: -1167.179932\n",
      "Train Epoch: 1485 [78848/101520 (78%)] Loss: -1175.914795\n",
      "Train Epoch: 1485 [90112/101520 (89%)] Loss: -1171.615356\n",
      "Train Epoch: 1485 [101376/101520 (100%)] Loss: -1175.190552\n",
      "    epoch          : 1485\n",
      "    loss           : -1174.0162316710505\n",
      "    ess            : 1.966159527026229\n",
      "    log_marginal   : 1174.0491323806532\n",
      "    log_joint      : 1382.425752419323\n",
      "    val_loss       : -1172.9800070057745\n",
      "    val_ess        : 1.966666967972465\n",
      "    val_log_marginal: 1173.01171875\n",
      "    val_log_joint  : 1381.457572605299\n",
      "Train Epoch: 1486 [0/101520 (0%)] Loss: -1165.368530\n",
      "Train Epoch: 1486 [11264/101520 (11%)] Loss: -1171.487061\n",
      "Train Epoch: 1486 [22528/101520 (22%)] Loss: -1175.161133\n",
      "Train Epoch: 1486 [33792/101520 (33%)] Loss: -1177.337524\n",
      "Train Epoch: 1486 [45056/101520 (44%)] Loss: -1174.973633\n",
      "Train Epoch: 1486 [56320/101520 (55%)] Loss: -1181.835571\n",
      "Train Epoch: 1486 [67584/101520 (67%)] Loss: -1176.601318\n",
      "Train Epoch: 1486 [78848/101520 (78%)] Loss: -1168.978271\n",
      "Train Epoch: 1486 [90112/101520 (89%)] Loss: -1172.824097\n",
      "Train Epoch: 1486 [101376/101520 (100%)] Loss: -1154.077271\n",
      "    epoch          : 1486\n",
      "    loss           : -1173.941278045501\n",
      "    ess            : 1.9668597120735514\n",
      "    log_marginal   : 1173.973014486495\n",
      "    log_joint      : 1382.3851937912218\n",
      "    val_loss       : -1172.3025751528533\n",
      "    val_ess        : 1.9673966532168181\n",
      "    val_log_marginal: 1172.3331192680027\n",
      "    val_log_joint  : 1380.3733334748642\n",
      "Train Epoch: 1487 [0/101520 (0%)] Loss: -1174.819092\n",
      "Train Epoch: 1487 [11264/101520 (11%)] Loss: -1163.072266\n",
      "Train Epoch: 1487 [22528/101520 (22%)] Loss: -1174.339355\n",
      "Train Epoch: 1487 [33792/101520 (33%)] Loss: -1178.933594\n",
      "Train Epoch: 1487 [45056/101520 (44%)] Loss: -1181.525391\n",
      "Train Epoch: 1487 [56320/101520 (55%)] Loss: -1177.556274\n",
      "Train Epoch: 1487 [67584/101520 (67%)] Loss: -1170.400879\n",
      "Train Epoch: 1487 [78848/101520 (78%)] Loss: -1172.166382\n",
      "Train Epoch: 1487 [90112/101520 (89%)] Loss: -1173.834717\n",
      "Train Epoch: 1487 [101376/101520 (100%)] Loss: -1179.597412\n",
      "    epoch          : 1487\n",
      "    loss           : -1174.110617172778\n",
      "    ess            : 1.9662985268549704\n",
      "    log_marginal   : 1174.142521077065\n",
      "    log_joint      : 1382.554384471184\n",
      "    val_loss       : -1172.5757154381793\n",
      "    val_ess        : 1.9697382139123005\n",
      "    val_log_marginal: 1172.6051078464675\n",
      "    val_log_joint  : 1381.0628343665082\n",
      "Train Epoch: 1488 [0/101520 (0%)] Loss: -1177.782471\n",
      "Train Epoch: 1488 [11264/101520 (11%)] Loss: -1176.035278\n",
      "Train Epoch: 1488 [22528/101520 (22%)] Loss: -1165.679199\n",
      "Train Epoch: 1488 [33792/101520 (33%)] Loss: -1176.273193\n",
      "Train Epoch: 1488 [45056/101520 (44%)] Loss: -1176.632568\n",
      "Train Epoch: 1488 [56320/101520 (55%)] Loss: -1181.561646\n",
      "Train Epoch: 1488 [67584/101520 (67%)] Loss: -1171.230469\n",
      "Train Epoch: 1488 [78848/101520 (78%)] Loss: -1169.985718\n",
      "Train Epoch: 1488 [90112/101520 (89%)] Loss: -1167.177490\n",
      "Train Epoch: 1488 [101376/101520 (100%)] Loss: -1180.150146\n",
      "    epoch          : 1488\n",
      "    loss           : -1174.1938090108747\n",
      "    ess            : 1.9663169174338107\n",
      "    log_marginal   : 1174.2251062441112\n",
      "    log_joint      : 1382.6085205078125\n",
      "    val_loss       : -1172.182564113451\n",
      "    val_ess        : 1.9666222126587578\n",
      "    val_log_marginal: 1172.2130976137908\n",
      "    val_log_joint  : 1380.5561894955842\n",
      "Train Epoch: 1489 [0/101520 (0%)] Loss: -1174.110474\n",
      "Train Epoch: 1489 [11264/101520 (11%)] Loss: -1173.130615\n",
      "Train Epoch: 1489 [22528/101520 (22%)] Loss: -1171.290283\n",
      "Train Epoch: 1489 [33792/101520 (33%)] Loss: -1173.571045\n",
      "Train Epoch: 1489 [45056/101520 (44%)] Loss: -1172.963379\n",
      "Train Epoch: 1489 [56320/101520 (55%)] Loss: -1175.252686\n",
      "Train Epoch: 1489 [67584/101520 (67%)] Loss: -1167.930664\n",
      "Train Epoch: 1489 [78848/101520 (78%)] Loss: -1174.445068\n",
      "Train Epoch: 1489 [90112/101520 (89%)] Loss: -1181.877441\n",
      "Train Epoch: 1489 [101376/101520 (100%)] Loss: -1169.547363\n",
      "    epoch          : 1489\n",
      "    loss           : -1174.2210920324278\n",
      "    ess            : 1.9661490731502897\n",
      "    log_marginal   : 1174.2542730743562\n",
      "    log_joint      : 1382.634904871035\n",
      "    val_loss       : -1172.7249649711277\n",
      "    val_ess        : 1.9663277708965798\n",
      "    val_log_marginal: 1172.7540867017663\n",
      "    val_log_joint  : 1381.0876040251358\n",
      "Train Epoch: 1490 [0/101520 (0%)] Loss: -1177.076660\n",
      "Train Epoch: 1490 [11264/101520 (11%)] Loss: -1171.807861\n",
      "Train Epoch: 1490 [22528/101520 (22%)] Loss: -1168.966797\n",
      "Train Epoch: 1490 [33792/101520 (33%)] Loss: -1173.987305\n",
      "Train Epoch: 1490 [45056/101520 (44%)] Loss: -1168.214111\n",
      "Train Epoch: 1490 [56320/101520 (55%)] Loss: -1169.172852\n",
      "Train Epoch: 1490 [67584/101520 (67%)] Loss: -1169.729004\n",
      "Train Epoch: 1490 [78848/101520 (78%)] Loss: -1167.786987\n",
      "Train Epoch: 1490 [90112/101520 (89%)] Loss: -1184.123779\n",
      "Train Epoch: 1490 [101376/101520 (100%)] Loss: -1181.347290\n",
      "    epoch          : 1490\n",
      "    loss           : -1174.2666076966866\n",
      "    ess            : 1.9662651105142719\n",
      "    log_marginal   : 1174.2991201122802\n",
      "    log_joint      : 1382.6864688432397\n",
      "    val_loss       : -1172.6594503651495\n",
      "    val_ess        : 1.9689011418301126\n",
      "    val_log_marginal: 1172.6895805027175\n",
      "    val_log_joint  : 1381.2144244650135\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1490.pth ...\n",
      "Train Epoch: 1491 [0/101520 (0%)] Loss: -1174.460205\n",
      "Train Epoch: 1491 [11264/101520 (11%)] Loss: -1177.197021\n",
      "Train Epoch: 1491 [22528/101520 (22%)] Loss: -1180.509644\n",
      "Train Epoch: 1491 [33792/101520 (33%)] Loss: -1185.052490\n",
      "Train Epoch: 1491 [45056/101520 (44%)] Loss: -1171.937500\n",
      "Train Epoch: 1491 [56320/101520 (55%)] Loss: -1167.449463\n",
      "Train Epoch: 1491 [67584/101520 (67%)] Loss: -1169.731445\n",
      "Train Epoch: 1491 [78848/101520 (78%)] Loss: -1166.491699\n",
      "Train Epoch: 1491 [90112/101520 (89%)] Loss: -1174.699219\n",
      "Train Epoch: 1491 [101376/101520 (100%)] Loss: -1178.065918\n",
      "    epoch          : 1491\n",
      "    loss           : -1174.2751133597676\n",
      "    ess            : 1.9668649692631246\n",
      "    log_marginal   : 1174.3061228996546\n",
      "    log_joint      : 1382.7203780131124\n",
      "    val_loss       : -1173.026022206182\n",
      "    val_ess        : 1.9655934986860857\n",
      "    val_log_marginal: 1173.0660453464675\n",
      "    val_log_joint  : 1381.6257748811142\n",
      "Train Epoch: 1492 [0/101520 (0%)] Loss: -1168.564209\n",
      "Train Epoch: 1492 [11264/101520 (11%)] Loss: -1174.357422\n",
      "Train Epoch: 1492 [22528/101520 (22%)] Loss: -1173.599121\n",
      "Train Epoch: 1492 [33792/101520 (33%)] Loss: -1174.897827\n",
      "Train Epoch: 1492 [45056/101520 (44%)] Loss: -1177.962402\n",
      "Train Epoch: 1492 [56320/101520 (55%)] Loss: -1173.569214\n",
      "Train Epoch: 1492 [67584/101520 (67%)] Loss: -1163.810791\n",
      "Train Epoch: 1492 [78848/101520 (78%)] Loss: -1179.229004\n",
      "Train Epoch: 1492 [90112/101520 (89%)] Loss: -1173.831299\n",
      "Train Epoch: 1492 [101376/101520 (100%)] Loss: -1170.276978\n",
      "    epoch          : 1492\n",
      "    loss           : -1174.3496412727702\n",
      "    ess            : 1.9663824513928974\n",
      "    log_marginal   : 1174.382530940837\n",
      "    log_joint      : 1382.771749985278\n",
      "    val_loss       : -1173.7577488111413\n",
      "    val_ess        : 1.9664364586705747\n",
      "    val_log_marginal: 1173.78442913553\n",
      "    val_log_joint  : 1382.416164232337\n",
      "Train Epoch: 1493 [0/101520 (0%)] Loss: -1176.734619\n",
      "Train Epoch: 1493 [11264/101520 (11%)] Loss: -1174.005127\n",
      "Train Epoch: 1493 [22528/101520 (22%)] Loss: -1180.475220\n",
      "Train Epoch: 1493 [33792/101520 (33%)] Loss: -1164.130615\n",
      "Train Epoch: 1493 [45056/101520 (44%)] Loss: -1175.104004\n",
      "Train Epoch: 1493 [56320/101520 (55%)] Loss: -1179.375488\n",
      "Train Epoch: 1493 [67584/101520 (67%)] Loss: -1167.599854\n",
      "Train Epoch: 1493 [78848/101520 (78%)] Loss: -1171.714355\n",
      "Train Epoch: 1493 [90112/101520 (89%)] Loss: -1185.460571\n",
      "Train Epoch: 1493 [101376/101520 (100%)] Loss: -1174.881226\n",
      "    epoch          : 1493\n",
      "    loss           : -1174.3525028707993\n",
      "    ess            : 1.9670572220979623\n",
      "    log_marginal   : 1174.3837700465217\n",
      "    log_joint      : 1382.7858825376884\n",
      "    val_loss       : -1172.9989385190217\n",
      "    val_ess        : 1.9656614583471548\n",
      "    val_log_marginal: 1173.0270146908967\n",
      "    val_log_joint  : 1381.4193168308425\n",
      "Train Epoch: 1494 [0/101520 (0%)] Loss: -1181.452148\n",
      "Train Epoch: 1494 [11264/101520 (11%)] Loss: -1176.900879\n",
      "Train Epoch: 1494 [22528/101520 (22%)] Loss: -1177.912598\n",
      "Train Epoch: 1494 [33792/101520 (33%)] Loss: -1171.217041\n",
      "Train Epoch: 1494 [45056/101520 (44%)] Loss: -1164.809814\n",
      "Train Epoch: 1494 [56320/101520 (55%)] Loss: -1176.044434\n",
      "Train Epoch: 1494 [67584/101520 (67%)] Loss: -1168.895996\n",
      "Train Epoch: 1494 [78848/101520 (78%)] Loss: -1174.049805\n",
      "Train Epoch: 1494 [90112/101520 (89%)] Loss: -1176.904297\n",
      "Train Epoch: 1494 [101376/101520 (100%)] Loss: -1174.171021\n",
      "    epoch          : 1494\n",
      "    loss           : -1174.3577065012562\n",
      "    ess            : 1.96647751570946\n",
      "    log_marginal   : 1174.3901778178\n",
      "    log_joint      : 1382.8338218190563\n",
      "    val_loss       : -1172.8709876019022\n",
      "    val_ess        : 1.9679586576378865\n",
      "    val_log_marginal: 1172.9030443274457\n",
      "    val_log_joint  : 1381.4961627462635\n",
      "Train Epoch: 1495 [0/101520 (0%)] Loss: -1167.997681\n",
      "Train Epoch: 1495 [11264/101520 (11%)] Loss: -1173.850708\n",
      "Train Epoch: 1495 [22528/101520 (22%)] Loss: -1182.568359\n",
      "Train Epoch: 1495 [33792/101520 (33%)] Loss: -1176.025269\n",
      "Train Epoch: 1495 [45056/101520 (44%)] Loss: -1168.111572\n",
      "Train Epoch: 1495 [56320/101520 (55%)] Loss: -1176.420898\n",
      "Train Epoch: 1495 [67584/101520 (67%)] Loss: -1172.980957\n",
      "Train Epoch: 1495 [78848/101520 (78%)] Loss: -1177.939819\n",
      "Train Epoch: 1495 [90112/101520 (89%)] Loss: -1179.939453\n",
      "Train Epoch: 1495 [101376/101520 (100%)] Loss: -1175.560181\n",
      "    epoch          : 1495\n",
      "    loss           : -1174.4255444703988\n",
      "    ess            : 1.9669442368512178\n",
      "    log_marginal   : 1174.4564711987673\n",
      "    log_joint      : 1382.884671158527\n",
      "    val_loss       : -1172.4321872877038\n",
      "    val_ess        : 1.9697652225909026\n",
      "    val_log_marginal: 1172.4616433848505\n",
      "    val_log_joint  : 1380.5710555366848\n",
      "Train Epoch: 1496 [0/101520 (0%)] Loss: -1182.810913\n",
      "Train Epoch: 1496 [11264/101520 (11%)] Loss: -1172.160889\n",
      "Train Epoch: 1496 [22528/101520 (22%)] Loss: -1177.774170\n",
      "Train Epoch: 1496 [33792/101520 (33%)] Loss: -1176.883789\n",
      "Train Epoch: 1496 [45056/101520 (44%)] Loss: -1184.438110\n",
      "Train Epoch: 1496 [56320/101520 (55%)] Loss: -1180.526367\n",
      "Train Epoch: 1496 [67584/101520 (67%)] Loss: -1171.843994\n",
      "Train Epoch: 1496 [78848/101520 (78%)] Loss: -1172.545532\n",
      "Train Epoch: 1496 [90112/101520 (89%)] Loss: -1179.132935\n",
      "Train Epoch: 1496 [101376/101520 (100%)] Loss: -1175.738281\n",
      "    epoch          : 1496\n",
      "    loss           : -1174.480547267588\n",
      "    ess            : 1.966423765498789\n",
      "    log_marginal   : 1174.51280020709\n",
      "    log_joint      : 1382.9673869837468\n",
      "    val_loss       : -1174.3715608016305\n",
      "    val_ess        : 1.9663705981296042\n",
      "    val_log_marginal: 1174.4015051800272\n",
      "    val_log_joint  : 1382.8711627462635\n",
      "Train Epoch: 1497 [0/101520 (0%)] Loss: -1167.854492\n",
      "Train Epoch: 1497 [11264/101520 (11%)] Loss: -1168.720947\n",
      "Train Epoch: 1497 [22528/101520 (22%)] Loss: -1171.971924\n",
      "Train Epoch: 1497 [33792/101520 (33%)] Loss: -1174.160156\n",
      "Train Epoch: 1497 [45056/101520 (44%)] Loss: -1178.741211\n",
      "Train Epoch: 1497 [56320/101520 (55%)] Loss: -1179.228027\n",
      "Train Epoch: 1497 [67584/101520 (67%)] Loss: -1176.110840\n",
      "Train Epoch: 1497 [78848/101520 (78%)] Loss: -1179.186646\n",
      "Train Epoch: 1497 [90112/101520 (89%)] Loss: -1170.958130\n",
      "Train Epoch: 1497 [101376/101520 (100%)] Loss: -1167.192017\n",
      "    epoch          : 1497\n",
      "    loss           : -1174.4657411814935\n",
      "    ess            : 1.9666911950662507\n",
      "    log_marginal   : 1174.4975438717022\n",
      "    log_joint      : 1382.9356891881282\n",
      "    val_loss       : -1173.647848378057\n",
      "    val_ess        : 1.9666515122289243\n",
      "    val_log_marginal: 1173.679878566576\n",
      "    val_log_joint  : 1382.0821586277175\n",
      "Train Epoch: 1498 [0/101520 (0%)] Loss: -1180.434448\n",
      "Train Epoch: 1498 [11264/101520 (11%)] Loss: -1174.934570\n",
      "Train Epoch: 1498 [22528/101520 (22%)] Loss: -1176.853027\n",
      "Train Epoch: 1498 [33792/101520 (33%)] Loss: -1173.641357\n",
      "Train Epoch: 1498 [45056/101520 (44%)] Loss: -1171.761475\n",
      "Train Epoch: 1498 [56320/101520 (55%)] Loss: -1184.047363\n",
      "Train Epoch: 1498 [67584/101520 (67%)] Loss: -1175.646240\n",
      "Train Epoch: 1498 [78848/101520 (78%)] Loss: -1172.351562\n",
      "Train Epoch: 1498 [90112/101520 (89%)] Loss: -1176.427856\n",
      "Train Epoch: 1498 [101376/101520 (100%)] Loss: -1169.696899\n",
      "    epoch          : 1498\n",
      "    loss           : -1174.5651916810616\n",
      "    ess            : 1.967771286940455\n",
      "    log_marginal   : 1174.595303789455\n",
      "    log_joint      : 1382.9721121476523\n",
      "    val_loss       : -1173.3806417713995\n",
      "    val_ess        : 1.9668051315390545\n",
      "    val_log_marginal: 1173.4131602411685\n",
      "    val_log_joint  : 1382.1157651154892\n",
      "Train Epoch: 1499 [0/101520 (0%)] Loss: -1174.634766\n",
      "Train Epoch: 1499 [11264/101520 (11%)] Loss: -1172.875000\n",
      "Train Epoch: 1499 [22528/101520 (22%)] Loss: -1176.964600\n",
      "Train Epoch: 1499 [33792/101520 (33%)] Loss: -1173.789307\n",
      "Train Epoch: 1499 [45056/101520 (44%)] Loss: -1176.257202\n",
      "Train Epoch: 1499 [56320/101520 (55%)] Loss: -1181.323730\n",
      "Train Epoch: 1499 [67584/101520 (67%)] Loss: -1173.307861\n",
      "Train Epoch: 1499 [78848/101520 (78%)] Loss: -1178.138916\n",
      "Train Epoch: 1499 [90112/101520 (89%)] Loss: -1173.626709\n",
      "Train Epoch: 1499 [101376/101520 (100%)] Loss: -1190.053345\n",
      "    epoch          : 1499\n",
      "    loss           : -1174.578415147024\n",
      "    ess            : 1.9664923563674466\n",
      "    log_marginal   : 1174.6092602907113\n",
      "    log_joint      : 1383.032786000314\n",
      "    val_loss       : -1172.7840629245925\n",
      "    val_ess        : 1.9666967702948528\n",
      "    val_log_marginal: 1172.8162947944973\n",
      "    val_log_joint  : 1381.3237676205842\n",
      "Train Epoch: 1500 [0/101520 (0%)] Loss: -1179.577881\n",
      "Train Epoch: 1500 [11264/101520 (11%)] Loss: -1179.895630\n",
      "Train Epoch: 1500 [22528/101520 (22%)] Loss: -1182.093506\n",
      "Train Epoch: 1500 [33792/101520 (33%)] Loss: -1173.630981\n",
      "Train Epoch: 1500 [45056/101520 (44%)] Loss: -1175.743652\n",
      "Train Epoch: 1500 [56320/101520 (55%)] Loss: -1175.565918\n",
      "Train Epoch: 1500 [67584/101520 (67%)] Loss: -1174.051880\n",
      "Train Epoch: 1500 [78848/101520 (78%)] Loss: -1177.844727\n",
      "Train Epoch: 1500 [90112/101520 (89%)] Loss: -1173.049072\n",
      "Train Epoch: 1500 [101376/101520 (100%)] Loss: -1175.981689\n",
      "    epoch          : 1500\n",
      "    loss           : -1174.5941315464038\n",
      "    ess            : 1.966980112856956\n",
      "    log_marginal   : 1174.6254931885992\n",
      "    log_joint      : 1383.0768055364715\n",
      "    val_loss       : -1174.4405039911685\n",
      "    val_ess        : 1.9682875975318577\n",
      "    val_log_marginal: 1174.4693603515625\n",
      "    val_log_joint  : 1382.8453528362772\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1500.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1501 [0/101520 (0%)] Loss: -1172.956055\n",
      "Train Epoch: 1501 [11264/101520 (11%)] Loss: -1175.111328\n",
      "Train Epoch: 1501 [22528/101520 (22%)] Loss: -1165.844727\n",
      "Train Epoch: 1501 [33792/101520 (33%)] Loss: -1169.220459\n",
      "Train Epoch: 1501 [45056/101520 (44%)] Loss: -1172.664551\n",
      "Train Epoch: 1501 [56320/101520 (55%)] Loss: -1176.368408\n",
      "Train Epoch: 1501 [67584/101520 (67%)] Loss: -1169.480469\n",
      "Train Epoch: 1501 [78848/101520 (78%)] Loss: -1180.885010\n",
      "Train Epoch: 1501 [90112/101520 (89%)] Loss: -1171.745728\n",
      "Train Epoch: 1501 [101376/101520 (100%)] Loss: -1167.708252\n",
      "    epoch          : 1501\n",
      "    loss           : -1174.5872907015546\n",
      "    ess            : 1.9669600012314379\n",
      "    log_marginal   : 1174.6184229251728\n",
      "    log_joint      : 1383.114676777442\n",
      "    val_loss       : -1174.293064283288\n",
      "    val_ess        : 1.9681953813718713\n",
      "    val_log_marginal: 1174.322950280231\n",
      "    val_log_joint  : 1382.892429517663\n",
      "Train Epoch: 1502 [0/101520 (0%)] Loss: -1179.170776\n",
      "Train Epoch: 1502 [11264/101520 (11%)] Loss: -1168.394775\n",
      "Train Epoch: 1502 [22528/101520 (22%)] Loss: -1184.753906\n",
      "Train Epoch: 1502 [33792/101520 (33%)] Loss: -1172.093750\n",
      "Train Epoch: 1502 [45056/101520 (44%)] Loss: -1178.767822\n",
      "Train Epoch: 1502 [56320/101520 (55%)] Loss: -1172.822632\n",
      "Train Epoch: 1502 [67584/101520 (67%)] Loss: -1172.260620\n",
      "Train Epoch: 1502 [78848/101520 (78%)] Loss: -1177.793945\n",
      "Train Epoch: 1502 [90112/101520 (89%)] Loss: -1181.527832\n",
      "Train Epoch: 1502 [101376/101520 (100%)] Loss: -1167.162354\n",
      "    epoch          : 1502\n",
      "    loss           : -1174.684379539298\n",
      "    ess            : 1.966964168764239\n",
      "    log_marginal   : 1174.7152154817054\n",
      "    log_joint      : 1383.093411392902\n",
      "    val_loss       : -1172.05712890625\n",
      "    val_ess        : 1.9655927419662476\n",
      "    val_log_marginal: 1172.0933784816575\n",
      "    val_log_joint  : 1380.4113079568615\n",
      "Train Epoch: 1503 [0/101520 (0%)] Loss: -1175.365601\n",
      "Train Epoch: 1503 [11264/101520 (11%)] Loss: -1173.516113\n",
      "Train Epoch: 1503 [22528/101520 (22%)] Loss: -1178.179199\n",
      "Train Epoch: 1503 [33792/101520 (33%)] Loss: -1180.474609\n",
      "Train Epoch: 1503 [45056/101520 (44%)] Loss: -1172.837036\n",
      "Train Epoch: 1503 [56320/101520 (55%)] Loss: -1173.205811\n",
      "Train Epoch: 1503 [67584/101520 (67%)] Loss: -1180.775879\n",
      "Train Epoch: 1503 [78848/101520 (78%)] Loss: -1176.479736\n",
      "Train Epoch: 1503 [90112/101520 (89%)] Loss: -1174.422119\n",
      "Train Epoch: 1503 [101376/101520 (100%)] Loss: -1180.640625\n",
      "    epoch          : 1503\n",
      "    loss           : -1174.8384936155387\n",
      "    ess            : 1.9663777171666899\n",
      "    log_marginal   : 1174.870209813717\n",
      "    log_joint      : 1383.2067147259736\n",
      "    val_loss       : -1173.3481179942255\n",
      "    val_ess        : 1.9668401894362078\n",
      "    val_log_marginal: 1173.3800685716712\n",
      "    val_log_joint  : 1382.0245679772418\n",
      "Train Epoch: 1504 [0/101520 (0%)] Loss: -1173.830444\n",
      "Train Epoch: 1504 [11264/101520 (11%)] Loss: -1172.126953\n",
      "Train Epoch: 1504 [22528/101520 (22%)] Loss: -1180.461670\n",
      "Train Epoch: 1504 [33792/101520 (33%)] Loss: -1172.688232\n",
      "Train Epoch: 1504 [45056/101520 (44%)] Loss: -1176.774902\n",
      "Train Epoch: 1504 [56320/101520 (55%)] Loss: -1173.115112\n",
      "Train Epoch: 1504 [67584/101520 (67%)] Loss: -1179.984131\n",
      "Train Epoch: 1504 [78848/101520 (78%)] Loss: -1172.898315\n",
      "Train Epoch: 1504 [90112/101520 (89%)] Loss: -1179.097900\n",
      "Train Epoch: 1504 [101376/101520 (100%)] Loss: -1176.834595\n",
      "    epoch          : 1504\n",
      "    loss           : -1174.8042673082207\n",
      "    ess            : 1.9671978063918838\n",
      "    log_marginal   : 1174.834661589196\n",
      "    log_joint      : 1383.1518468808888\n",
      "    val_loss       : -1174.8148246433425\n",
      "    val_ess        : 1.9659538683683977\n",
      "    val_log_marginal: 1174.8483143682065\n",
      "    val_log_joint  : 1383.2087614639945\n",
      "Train Epoch: 1505 [0/101520 (0%)] Loss: -1178.200195\n",
      "Train Epoch: 1505 [11264/101520 (11%)] Loss: -1177.478882\n",
      "Train Epoch: 1505 [22528/101520 (22%)] Loss: -1179.352783\n",
      "Train Epoch: 1505 [33792/101520 (33%)] Loss: -1176.151611\n",
      "Train Epoch: 1505 [45056/101520 (44%)] Loss: -1177.661865\n",
      "Train Epoch: 1505 [56320/101520 (55%)] Loss: -1178.192261\n",
      "Train Epoch: 1505 [67584/101520 (67%)] Loss: -1180.925049\n",
      "Train Epoch: 1505 [78848/101520 (78%)] Loss: -1170.347900\n",
      "Train Epoch: 1505 [90112/101520 (89%)] Loss: -1175.352051\n",
      "Train Epoch: 1505 [101376/101520 (100%)] Loss: -1176.584595\n",
      "    epoch          : 1505\n",
      "    loss           : -1174.8359344329067\n",
      "    ess            : 1.9670760703446277\n",
      "    log_marginal   : 1174.8671936341866\n",
      "    log_joint      : 1383.197100001963\n",
      "    val_loss       : -1173.4553010360055\n",
      "    val_ess        : 1.9658241686613664\n",
      "    val_log_marginal: 1173.4889446756115\n",
      "    val_log_joint  : 1381.9415920091712\n",
      "Train Epoch: 1506 [0/101520 (0%)] Loss: -1178.157104\n",
      "Train Epoch: 1506 [11264/101520 (11%)] Loss: -1172.511475\n",
      "Train Epoch: 1506 [22528/101520 (22%)] Loss: -1174.368896\n",
      "Train Epoch: 1506 [33792/101520 (33%)] Loss: -1175.046021\n",
      "Train Epoch: 1506 [45056/101520 (44%)] Loss: -1175.223145\n",
      "Train Epoch: 1506 [56320/101520 (55%)] Loss: -1171.496094\n",
      "Train Epoch: 1506 [67584/101520 (67%)] Loss: -1172.430664\n",
      "Train Epoch: 1506 [78848/101520 (78%)] Loss: -1169.829590\n",
      "Train Epoch: 1506 [90112/101520 (89%)] Loss: -1181.793213\n",
      "Train Epoch: 1506 [101376/101520 (100%)] Loss: -1187.944702\n",
      "    epoch          : 1506\n",
      "    loss           : -1174.886918111063\n",
      "    ess            : 1.9672839360021466\n",
      "    log_marginal   : 1174.9182975423994\n",
      "    log_joint      : 1383.3743019295698\n",
      "    val_loss       : -1172.2396134086277\n",
      "    val_ess        : 1.967745998631353\n",
      "    val_log_marginal: 1172.269340183424\n",
      "    val_log_joint  : 1380.6196182914402\n",
      "Train Epoch: 1507 [0/101520 (0%)] Loss: -1172.281006\n",
      "Train Epoch: 1507 [11264/101520 (11%)] Loss: -1174.900024\n",
      "Train Epoch: 1507 [22528/101520 (22%)] Loss: -1173.387329\n",
      "Train Epoch: 1507 [33792/101520 (33%)] Loss: -1163.832397\n",
      "Train Epoch: 1507 [45056/101520 (44%)] Loss: -1170.850220\n",
      "Train Epoch: 1507 [56320/101520 (55%)] Loss: -1176.599609\n",
      "Train Epoch: 1507 [67584/101520 (67%)] Loss: -1169.865234\n",
      "Train Epoch: 1507 [78848/101520 (78%)] Loss: -1169.849365\n",
      "Train Epoch: 1507 [90112/101520 (89%)] Loss: -1176.354126\n",
      "Train Epoch: 1507 [101376/101520 (100%)] Loss: -1166.079712\n",
      "    epoch          : 1507\n",
      "    loss           : -1174.847766665358\n",
      "    ess            : 1.9669843229217145\n",
      "    log_marginal   : 1174.8788001285725\n",
      "    log_joint      : 1383.3236666732098\n",
      "    val_loss       : -1172.6810461956522\n",
      "    val_ess        : 1.965143286663553\n",
      "    val_log_marginal: 1172.7123917289402\n",
      "    val_log_joint  : 1381.0710343070652\n",
      "Train Epoch: 1508 [0/101520 (0%)] Loss: -1170.457031\n",
      "Train Epoch: 1508 [11264/101520 (11%)] Loss: -1168.437378\n",
      "Train Epoch: 1508 [22528/101520 (22%)] Loss: -1180.636719\n",
      "Train Epoch: 1508 [33792/101520 (33%)] Loss: -1178.467163\n",
      "Train Epoch: 1508 [45056/101520 (44%)] Loss: -1173.142578\n",
      "Train Epoch: 1508 [56320/101520 (55%)] Loss: -1180.828613\n",
      "Train Epoch: 1508 [67584/101520 (67%)] Loss: -1176.041260\n",
      "Train Epoch: 1508 [78848/101520 (78%)] Loss: -1171.976685\n",
      "Train Epoch: 1508 [90112/101520 (89%)] Loss: -1168.140381\n",
      "Train Epoch: 1508 [101376/101520 (100%)] Loss: -1169.577759\n",
      "    epoch          : 1508\n",
      "    loss           : -1174.9219492236573\n",
      "    ess            : 1.9677818726055587\n",
      "    log_marginal   : 1174.951611696176\n",
      "    log_joint      : 1383.3915549426822\n",
      "    val_loss       : -1172.6352379840353\n",
      "    val_ess        : 1.9676246435745903\n",
      "    val_log_marginal: 1172.664747155231\n",
      "    val_log_joint  : 1381.225538170856\n",
      "Train Epoch: 1509 [0/101520 (0%)] Loss: -1178.256348\n",
      "Train Epoch: 1509 [11264/101520 (11%)] Loss: -1177.986328\n",
      "Train Epoch: 1509 [22528/101520 (22%)] Loss: -1170.206787\n",
      "Train Epoch: 1509 [33792/101520 (33%)] Loss: -1176.367188\n",
      "Train Epoch: 1509 [45056/101520 (44%)] Loss: -1171.316895\n",
      "Train Epoch: 1509 [56320/101520 (55%)] Loss: -1176.354370\n",
      "Train Epoch: 1509 [67584/101520 (67%)] Loss: -1174.382568\n",
      "Train Epoch: 1509 [78848/101520 (78%)] Loss: -1170.254639\n",
      "Train Epoch: 1509 [90112/101520 (89%)] Loss: -1171.335205\n",
      "Train Epoch: 1509 [101376/101520 (100%)] Loss: -1173.038086\n",
      "    epoch          : 1509\n",
      "    loss           : -1174.9051114949748\n",
      "    ess            : 1.967233018659467\n",
      "    log_marginal   : 1174.9363351179727\n",
      "    log_joint      : 1383.422377389879\n",
      "    val_loss       : -1173.7314665421195\n",
      "    val_ess        : 1.9693354005398958\n",
      "    val_log_marginal: 1173.7596488620925\n",
      "    val_log_joint  : 1382.128226902174\n",
      "Train Epoch: 1510 [0/101520 (0%)] Loss: -1169.900269\n",
      "Train Epoch: 1510 [11264/101520 (11%)] Loss: -1184.504272\n",
      "Train Epoch: 1510 [22528/101520 (22%)] Loss: -1177.276123\n",
      "Train Epoch: 1510 [33792/101520 (33%)] Loss: -1167.073730\n",
      "Train Epoch: 1510 [45056/101520 (44%)] Loss: -1175.163086\n",
      "Train Epoch: 1510 [56320/101520 (55%)] Loss: -1185.194580\n",
      "Train Epoch: 1510 [67584/101520 (67%)] Loss: -1178.589111\n",
      "Train Epoch: 1510 [78848/101520 (78%)] Loss: -1177.319214\n",
      "Train Epoch: 1510 [90112/101520 (89%)] Loss: -1173.566650\n",
      "Train Epoch: 1510 [101376/101520 (100%)] Loss: -1165.138794\n",
      "    epoch          : 1510\n",
      "    loss           : -1174.9261627964038\n",
      "    ess            : 1.9673053906790574\n",
      "    log_marginal   : 1174.9572391989243\n",
      "    log_joint      : 1383.3917254730684\n",
      "    val_loss       : -1171.7635551120925\n",
      "    val_ess        : 1.9676086591637654\n",
      "    val_log_marginal: 1171.7935207201087\n",
      "    val_log_joint  : 1380.3742888077445\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1510.pth ...\n",
      "Train Epoch: 1511 [0/101520 (0%)] Loss: -1175.393311\n",
      "Train Epoch: 1511 [11264/101520 (11%)] Loss: -1176.448486\n",
      "Train Epoch: 1511 [22528/101520 (22%)] Loss: -1175.568115\n",
      "Train Epoch: 1511 [33792/101520 (33%)] Loss: -1172.160889\n",
      "Train Epoch: 1511 [45056/101520 (44%)] Loss: -1180.678955\n",
      "Train Epoch: 1511 [56320/101520 (55%)] Loss: -1174.567871\n",
      "Train Epoch: 1511 [67584/101520 (67%)] Loss: -1183.687622\n",
      "Train Epoch: 1511 [78848/101520 (78%)] Loss: -1174.399170\n",
      "Train Epoch: 1511 [90112/101520 (89%)] Loss: -1176.767944\n",
      "Train Epoch: 1511 [101376/101520 (100%)] Loss: -1159.465942\n",
      "    epoch          : 1511\n",
      "    loss           : -1174.936188510914\n",
      "    ess            : 1.9670414223742845\n",
      "    log_marginal   : 1174.9675170285018\n",
      "    log_joint      : 1383.3672365734924\n",
      "    val_loss       : -1173.3595872961957\n",
      "    val_ess        : 1.9689364899759707\n",
      "    val_log_marginal: 1173.387403405231\n",
      "    val_log_joint  : 1381.853812839674\n",
      "Train Epoch: 1512 [0/101520 (0%)] Loss: -1180.761963\n",
      "Train Epoch: 1512 [11264/101520 (11%)] Loss: -1169.745850\n",
      "Train Epoch: 1512 [22528/101520 (22%)] Loss: -1173.353027\n",
      "Train Epoch: 1512 [33792/101520 (33%)] Loss: -1181.313354\n",
      "Train Epoch: 1512 [45056/101520 (44%)] Loss: -1170.915283\n",
      "Train Epoch: 1512 [56320/101520 (55%)] Loss: -1174.379639\n",
      "Train Epoch: 1512 [67584/101520 (67%)] Loss: -1174.307251\n",
      "Train Epoch: 1512 [78848/101520 (78%)] Loss: -1175.188477\n",
      "Train Epoch: 1512 [90112/101520 (89%)] Loss: -1179.316895\n",
      "Train Epoch: 1512 [101376/101520 (100%)] Loss: -1180.342896\n",
      "    epoch          : 1512\n",
      "    loss           : -1175.0406831520886\n",
      "    ess            : 1.9671833053905161\n",
      "    log_marginal   : 1175.0716742894158\n",
      "    log_joint      : 1383.4888345536276\n",
      "    val_loss       : -1172.791939113451\n",
      "    val_ess        : 1.9676549279171487\n",
      "    val_log_marginal: 1172.8231784986413\n",
      "    val_log_joint  : 1381.4712020210598\n",
      "Train Epoch: 1513 [0/101520 (0%)] Loss: -1178.086670\n",
      "Train Epoch: 1513 [11264/101520 (11%)] Loss: -1171.728882\n",
      "Train Epoch: 1513 [22528/101520 (22%)] Loss: -1174.333618\n",
      "Train Epoch: 1513 [33792/101520 (33%)] Loss: -1177.842285\n",
      "Train Epoch: 1513 [45056/101520 (44%)] Loss: -1179.249268\n",
      "Train Epoch: 1513 [56320/101520 (55%)] Loss: -1173.305908\n",
      "Train Epoch: 1513 [67584/101520 (67%)] Loss: -1174.603271\n",
      "Train Epoch: 1513 [78848/101520 (78%)] Loss: -1174.434082\n",
      "Train Epoch: 1513 [90112/101520 (89%)] Loss: -1183.810303\n",
      "Train Epoch: 1513 [101376/101520 (100%)] Loss: -1167.928345\n",
      "    epoch          : 1513\n",
      "    loss           : -1175.066455936911\n",
      "    ess            : 1.9673971788368034\n",
      "    log_marginal   : 1175.09702197511\n",
      "    log_joint      : 1383.521249435655\n",
      "    val_loss       : -1173.4579228940217\n",
      "    val_ess        : 1.968387660772904\n",
      "    val_log_marginal: 1173.485542629076\n",
      "    val_log_joint  : 1381.93359375\n",
      "Train Epoch: 1514 [0/101520 (0%)] Loss: -1176.921143\n",
      "Train Epoch: 1514 [11264/101520 (11%)] Loss: -1175.634033\n",
      "Train Epoch: 1514 [22528/101520 (22%)] Loss: -1184.633789\n",
      "Train Epoch: 1514 [33792/101520 (33%)] Loss: -1177.732178\n",
      "Train Epoch: 1514 [45056/101520 (44%)] Loss: -1169.190796\n",
      "Train Epoch: 1514 [56320/101520 (55%)] Loss: -1171.164795\n",
      "Train Epoch: 1514 [67584/101520 (67%)] Loss: -1169.478394\n",
      "Train Epoch: 1514 [78848/101520 (78%)] Loss: -1168.637451\n",
      "Train Epoch: 1514 [90112/101520 (89%)] Loss: -1173.357666\n",
      "Train Epoch: 1514 [101376/101520 (100%)] Loss: -1172.653687\n",
      "    epoch          : 1514\n",
      "    loss           : -1175.0798315307004\n",
      "    ess            : 1.9663912285512417\n",
      "    log_marginal   : 1175.1117538375472\n",
      "    log_joint      : 1383.5867220624607\n",
      "    val_loss       : -1175.176126231318\n",
      "    val_ess        : 1.9627259606900422\n",
      "    val_log_marginal: 1175.2121316661005\n",
      "    val_log_joint  : 1383.8731795601223\n",
      "Train Epoch: 1515 [0/101520 (0%)] Loss: -1178.131348\n",
      "Train Epoch: 1515 [11264/101520 (11%)] Loss: -1169.508911\n",
      "Train Epoch: 1515 [22528/101520 (22%)] Loss: -1177.222534\n",
      "Train Epoch: 1515 [33792/101520 (33%)] Loss: -1171.928345\n",
      "Train Epoch: 1515 [45056/101520 (44%)] Loss: -1178.549194\n",
      "Train Epoch: 1515 [56320/101520 (55%)] Loss: -1173.941772\n",
      "Train Epoch: 1515 [67584/101520 (67%)] Loss: -1172.252319\n",
      "Train Epoch: 1515 [78848/101520 (78%)] Loss: -1173.682617\n",
      "Train Epoch: 1515 [90112/101520 (89%)] Loss: -1175.099121\n",
      "Train Epoch: 1515 [101376/101520 (100%)] Loss: -1185.485229\n",
      "    epoch          : 1515\n",
      "    loss           : -1175.1981875932397\n",
      "    ess            : 1.9669986909358346\n",
      "    log_marginal   : 1175.2292296443154\n",
      "    log_joint      : 1383.6711168145414\n",
      "    val_loss       : -1175.1610532014267\n",
      "    val_ess        : 1.9667397996653682\n",
      "    val_log_marginal: 1175.1905464504075\n",
      "    val_log_joint  : 1383.5723399286685\n",
      "Train Epoch: 1516 [0/101520 (0%)] Loss: -1171.412720\n",
      "Train Epoch: 1516 [11264/101520 (11%)] Loss: -1172.133057\n",
      "Train Epoch: 1516 [22528/101520 (22%)] Loss: -1169.905029\n",
      "Train Epoch: 1516 [33792/101520 (33%)] Loss: -1174.782104\n",
      "Train Epoch: 1516 [45056/101520 (44%)] Loss: -1176.887817\n",
      "Train Epoch: 1516 [56320/101520 (55%)] Loss: -1180.711670\n",
      "Train Epoch: 1516 [67584/101520 (67%)] Loss: -1174.656738\n",
      "Train Epoch: 1516 [78848/101520 (78%)] Loss: -1177.770752\n",
      "Train Epoch: 1516 [90112/101520 (89%)] Loss: -1176.409180\n",
      "Train Epoch: 1516 [101376/101520 (100%)] Loss: -1177.082397\n",
      "    epoch          : 1516\n",
      "    loss           : -1175.2974644953283\n",
      "    ess            : 1.9662863794882692\n",
      "    log_marginal   : 1175.329625422032\n",
      "    log_joint      : 1383.7184500382773\n",
      "    val_loss       : -1174.276563561481\n",
      "    val_ess        : 1.9664665149605793\n",
      "    val_log_marginal: 1174.305955969769\n",
      "    val_log_joint  : 1382.611131751019\n",
      "Train Epoch: 1517 [0/101520 (0%)] Loss: -1180.864014\n",
      "Train Epoch: 1517 [11264/101520 (11%)] Loss: -1168.112061\n",
      "Train Epoch: 1517 [22528/101520 (22%)] Loss: -1170.900879\n",
      "Train Epoch: 1517 [33792/101520 (33%)] Loss: -1167.791016\n",
      "Train Epoch: 1517 [45056/101520 (44%)] Loss: -1186.495850\n",
      "Train Epoch: 1517 [56320/101520 (55%)] Loss: -1171.223755\n",
      "Train Epoch: 1517 [67584/101520 (67%)] Loss: -1169.199097\n",
      "Train Epoch: 1517 [78848/101520 (78%)] Loss: -1171.402100\n",
      "Train Epoch: 1517 [90112/101520 (89%)] Loss: -1181.009399\n",
      "Train Epoch: 1517 [101376/101520 (100%)] Loss: -1179.558594\n",
      "    epoch          : 1517\n",
      "    loss           : -1175.252126109061\n",
      "    ess            : 1.9658157951268718\n",
      "    log_marginal   : 1175.284915789887\n",
      "    log_joint      : 1383.692420231038\n",
      "    val_loss       : -1173.3467433763587\n",
      "    val_ess        : 1.9668134088101594\n",
      "    val_log_marginal: 1173.378269361413\n",
      "    val_log_joint  : 1381.7173010784647\n",
      "Train Epoch: 1518 [0/101520 (0%)] Loss: -1177.077759\n",
      "Train Epoch: 1518 [11264/101520 (11%)] Loss: -1177.273682\n",
      "Train Epoch: 1518 [22528/101520 (22%)] Loss: -1174.677124\n",
      "Train Epoch: 1518 [33792/101520 (33%)] Loss: -1177.606201\n",
      "Train Epoch: 1518 [45056/101520 (44%)] Loss: -1181.147461\n",
      "Train Epoch: 1518 [56320/101520 (55%)] Loss: -1174.130005\n",
      "Train Epoch: 1518 [67584/101520 (67%)] Loss: -1165.740479\n",
      "Train Epoch: 1518 [78848/101520 (78%)] Loss: -1174.492432\n",
      "Train Epoch: 1518 [90112/101520 (89%)] Loss: -1172.285767\n",
      "Train Epoch: 1518 [101376/101520 (100%)] Loss: -1183.351685\n",
      "    epoch          : 1518\n",
      "    loss           : -1175.2734644904208\n",
      "    ess            : 1.9665586594960198\n",
      "    log_marginal   : 1175.305767730253\n",
      "    log_joint      : 1383.728212596184\n",
      "    val_loss       : -1175.1432203209918\n",
      "    val_ess        : 1.9658811040546582\n",
      "    val_log_marginal: 1175.172904636549\n",
      "    val_log_joint  : 1383.5590077275815\n",
      "Train Epoch: 1519 [0/101520 (0%)] Loss: -1173.972168\n",
      "Train Epoch: 1519 [11264/101520 (11%)] Loss: -1178.899902\n",
      "Train Epoch: 1519 [22528/101520 (22%)] Loss: -1170.232056\n",
      "Train Epoch: 1519 [33792/101520 (33%)] Loss: -1175.220947\n",
      "Train Epoch: 1519 [45056/101520 (44%)] Loss: -1170.873535\n",
      "Train Epoch: 1519 [56320/101520 (55%)] Loss: -1174.297607\n",
      "Train Epoch: 1519 [67584/101520 (67%)] Loss: -1172.665283\n",
      "Train Epoch: 1519 [78848/101520 (78%)] Loss: -1188.755615\n",
      "Train Epoch: 1519 [90112/101520 (89%)] Loss: -1168.911255\n",
      "Train Epoch: 1519 [101376/101520 (100%)] Loss: -1169.972290\n",
      "    epoch          : 1519\n",
      "    loss           : -1175.3106548366834\n",
      "    ess            : 1.9664611169441262\n",
      "    log_marginal   : 1175.3426421659076\n",
      "    log_joint      : 1383.7154706638662\n",
      "    val_loss       : -1174.2208092730978\n",
      "    val_ess        : 1.9686294638592263\n",
      "    val_log_marginal: 1174.2492888077445\n",
      "    val_log_joint  : 1382.5265741762908\n",
      "Train Epoch: 1520 [0/101520 (0%)] Loss: -1178.329224\n",
      "Train Epoch: 1520 [11264/101520 (11%)] Loss: -1177.625977\n",
      "Train Epoch: 1520 [22528/101520 (22%)] Loss: -1173.702393\n",
      "Train Epoch: 1520 [33792/101520 (33%)] Loss: -1172.225586\n",
      "Train Epoch: 1520 [45056/101520 (44%)] Loss: -1172.125244\n",
      "Train Epoch: 1520 [56320/101520 (55%)] Loss: -1171.810303\n",
      "Train Epoch: 1520 [67584/101520 (67%)] Loss: -1176.218750\n",
      "Train Epoch: 1520 [78848/101520 (78%)] Loss: -1177.380615\n",
      "Train Epoch: 1520 [90112/101520 (89%)] Loss: -1173.934937\n",
      "Train Epoch: 1520 [101376/101520 (100%)] Loss: -1179.167358\n",
      "    epoch          : 1520\n",
      "    loss           : -1175.3736363703283\n",
      "    ess            : 1.9671287920007754\n",
      "    log_marginal   : 1175.4041232775205\n",
      "    log_joint      : 1383.8284893706816\n",
      "    val_loss       : -1173.875440514606\n",
      "    val_ess        : 1.969797942949378\n",
      "    val_log_marginal: 1173.9048063858695\n",
      "    val_log_joint  : 1382.1800059442935\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1520.pth ...\n",
      "Train Epoch: 1521 [0/101520 (0%)] Loss: -1174.612183\n",
      "Train Epoch: 1521 [11264/101520 (11%)] Loss: -1179.116699\n",
      "Train Epoch: 1521 [22528/101520 (22%)] Loss: -1179.428467\n",
      "Train Epoch: 1521 [33792/101520 (33%)] Loss: -1175.132690\n",
      "Train Epoch: 1521 [45056/101520 (44%)] Loss: -1177.699219\n",
      "Train Epoch: 1521 [56320/101520 (55%)] Loss: -1170.534424\n",
      "Train Epoch: 1521 [67584/101520 (67%)] Loss: -1169.970703\n",
      "Train Epoch: 1521 [78848/101520 (78%)] Loss: -1182.429565\n",
      "Train Epoch: 1521 [90112/101520 (89%)] Loss: -1175.479614\n",
      "Train Epoch: 1521 [101376/101520 (100%)] Loss: -1186.435303\n",
      "    epoch          : 1521\n",
      "    loss           : -1175.5035756173445\n",
      "    ess            : 1.9661307502631566\n",
      "    log_marginal   : 1175.535865361966\n",
      "    log_joint      : 1383.8984970016097\n",
      "    val_loss       : -1175.041944420856\n",
      "    val_ess        : 1.968009808789129\n",
      "    val_log_marginal: 1175.0700099779212\n",
      "    val_log_joint  : 1383.6095979110055\n",
      "Train Epoch: 1522 [0/101520 (0%)] Loss: -1173.567627\n",
      "Train Epoch: 1522 [11264/101520 (11%)] Loss: -1176.845581\n",
      "Train Epoch: 1522 [22528/101520 (22%)] Loss: -1169.471191\n",
      "Train Epoch: 1522 [33792/101520 (33%)] Loss: -1177.171875\n",
      "Train Epoch: 1522 [45056/101520 (44%)] Loss: -1175.434082\n",
      "Train Epoch: 1522 [56320/101520 (55%)] Loss: -1177.783325\n",
      "Train Epoch: 1522 [67584/101520 (67%)] Loss: -1178.261353\n",
      "Train Epoch: 1522 [78848/101520 (78%)] Loss: -1176.376343\n",
      "Train Epoch: 1522 [90112/101520 (89%)] Loss: -1168.425537\n",
      "Train Epoch: 1522 [101376/101520 (100%)] Loss: -1177.105225\n",
      "    epoch          : 1522\n",
      "    loss           : -1175.4716465628926\n",
      "    ess            : 1.9660035755167056\n",
      "    log_marginal   : 1175.504512307632\n",
      "    log_joint      : 1383.9559688088882\n",
      "    val_loss       : -1173.7820938773777\n",
      "    val_ess        : 1.9675838688145513\n",
      "    val_log_marginal: 1173.8135084069293\n",
      "    val_log_joint  : 1381.7118344514267\n",
      "Train Epoch: 1523 [0/101520 (0%)] Loss: -1166.780273\n",
      "Train Epoch: 1523 [11264/101520 (11%)] Loss: -1176.771484\n",
      "Train Epoch: 1523 [22528/101520 (22%)] Loss: -1180.187256\n",
      "Train Epoch: 1523 [33792/101520 (33%)] Loss: -1177.135986\n",
      "Train Epoch: 1523 [45056/101520 (44%)] Loss: -1170.354858\n",
      "Train Epoch: 1523 [56320/101520 (55%)] Loss: -1175.148560\n",
      "Train Epoch: 1523 [67584/101520 (67%)] Loss: -1179.719849\n",
      "Train Epoch: 1523 [78848/101520 (78%)] Loss: -1177.419922\n",
      "Train Epoch: 1523 [90112/101520 (89%)] Loss: -1173.066650\n",
      "Train Epoch: 1523 [101376/101520 (100%)] Loss: -1178.253296\n",
      "    epoch          : 1523\n",
      "    loss           : -1175.54014395709\n",
      "    ess            : 1.9663298842894972\n",
      "    log_marginal   : 1175.5724024173603\n",
      "    log_joint      : 1383.9793903600032\n",
      "    val_loss       : -1176.0579940132473\n",
      "    val_ess        : 1.9672915831856106\n",
      "    val_log_marginal: 1176.0894191576087\n",
      "    val_log_joint  : 1384.6525188943615\n",
      "Train Epoch: 1524 [0/101520 (0%)] Loss: -1175.577515\n",
      "Train Epoch: 1524 [11264/101520 (11%)] Loss: -1174.177246\n",
      "Train Epoch: 1524 [22528/101520 (22%)] Loss: -1176.015625\n",
      "Train Epoch: 1524 [33792/101520 (33%)] Loss: -1174.969238\n",
      "Train Epoch: 1524 [45056/101520 (44%)] Loss: -1172.851318\n",
      "Train Epoch: 1524 [56320/101520 (55%)] Loss: -1180.815552\n",
      "Train Epoch: 1524 [67584/101520 (67%)] Loss: -1170.829468\n",
      "Train Epoch: 1524 [78848/101520 (78%)] Loss: -1170.960083\n",
      "Train Epoch: 1524 [90112/101520 (89%)] Loss: -1175.024902\n",
      "Train Epoch: 1524 [101376/101520 (100%)] Loss: -1167.618286\n",
      "    epoch          : 1524\n",
      "    loss           : -1175.5095460211212\n",
      "    ess            : 1.9669420060200906\n",
      "    log_marginal   : 1175.5406954204616\n",
      "    log_joint      : 1383.9838069743248\n",
      "    val_loss       : -1173.0766230044158\n",
      "    val_ess        : 1.9666355330011118\n",
      "    val_log_marginal: 1173.1076978600543\n",
      "    val_log_joint  : 1381.5471456776495\n",
      "Train Epoch: 1525 [0/101520 (0%)] Loss: -1174.419434\n",
      "Train Epoch: 1525 [11264/101520 (11%)] Loss: -1179.559570\n",
      "Train Epoch: 1525 [22528/101520 (22%)] Loss: -1185.402466\n",
      "Train Epoch: 1525 [33792/101520 (33%)] Loss: -1176.319336\n",
      "Train Epoch: 1525 [45056/101520 (44%)] Loss: -1184.057007\n",
      "Train Epoch: 1525 [56320/101520 (55%)] Loss: -1173.160645\n",
      "Train Epoch: 1525 [67584/101520 (67%)] Loss: -1176.647827\n",
      "Train Epoch: 1525 [78848/101520 (78%)] Loss: -1178.893921\n",
      "Train Epoch: 1525 [90112/101520 (89%)] Loss: -1166.833008\n",
      "Train Epoch: 1525 [101376/101520 (100%)] Loss: -1184.455688\n",
      "    epoch          : 1525\n",
      "    loss           : -1175.5682550938286\n",
      "    ess            : 1.9668889638766571\n",
      "    log_marginal   : 1175.5990315346262\n",
      "    log_joint      : 1384.026479443114\n",
      "    val_loss       : -1174.663473378057\n",
      "    val_ess        : 1.9679593780766362\n",
      "    val_log_marginal: 1174.692966627038\n",
      "    val_log_joint  : 1382.8075800356658\n",
      "Train Epoch: 1526 [0/101520 (0%)] Loss: -1178.719482\n",
      "Train Epoch: 1526 [11264/101520 (11%)] Loss: -1173.977173\n",
      "Train Epoch: 1526 [22528/101520 (22%)] Loss: -1177.919922\n",
      "Train Epoch: 1526 [33792/101520 (33%)] Loss: -1178.077393\n",
      "Train Epoch: 1526 [45056/101520 (44%)] Loss: -1173.353760\n",
      "Train Epoch: 1526 [56320/101520 (55%)] Loss: -1178.470947\n",
      "Train Epoch: 1526 [67584/101520 (67%)] Loss: -1171.676025\n",
      "Train Epoch: 1526 [78848/101520 (78%)] Loss: -1174.490845\n",
      "Train Epoch: 1526 [90112/101520 (89%)] Loss: -1167.310059\n",
      "Train Epoch: 1526 [101376/101520 (100%)] Loss: -1181.855103\n",
      "    epoch          : 1526\n",
      "    loss           : -1175.5167788404915\n",
      "    ess            : 1.9667926104224507\n",
      "    log_marginal   : 1175.5479766999058\n",
      "    log_joint      : 1384.005347783841\n",
      "    val_loss       : -1173.278511379076\n",
      "    val_ess        : 1.9667151648065317\n",
      "    val_log_marginal: 1173.30980914572\n",
      "    val_log_joint  : 1381.7416196076767\n",
      "Train Epoch: 1527 [0/101520 (0%)] Loss: -1181.380127\n",
      "Train Epoch: 1527 [11264/101520 (11%)] Loss: -1174.034424\n",
      "Train Epoch: 1527 [22528/101520 (22%)] Loss: -1178.886475\n",
      "Train Epoch: 1527 [33792/101520 (33%)] Loss: -1176.263916\n",
      "Train Epoch: 1527 [45056/101520 (44%)] Loss: -1184.591675\n",
      "Train Epoch: 1527 [56320/101520 (55%)] Loss: -1184.924438\n",
      "Train Epoch: 1527 [67584/101520 (67%)] Loss: -1178.151245\n",
      "Train Epoch: 1527 [78848/101520 (78%)] Loss: -1175.327026\n",
      "Train Epoch: 1527 [90112/101520 (89%)] Loss: -1178.306030\n",
      "Train Epoch: 1527 [101376/101520 (100%)] Loss: -1176.003418\n",
      "    epoch          : 1527\n",
      "    loss           : -1175.5717423788867\n",
      "    ess            : 1.9671358415229836\n",
      "    log_marginal   : 1175.6024507302136\n",
      "    log_joint      : 1384.029207929295\n",
      "    val_loss       : -1174.2839143172555\n",
      "    val_ess        : 1.966662163319795\n",
      "    val_log_marginal: 1174.317727793818\n",
      "    val_log_joint  : 1382.7060706097147\n",
      "Train Epoch: 1528 [0/101520 (0%)] Loss: -1175.152100\n",
      "Train Epoch: 1528 [11264/101520 (11%)] Loss: -1174.794189\n",
      "Train Epoch: 1528 [22528/101520 (22%)] Loss: -1188.886230\n",
      "Train Epoch: 1528 [33792/101520 (33%)] Loss: -1181.908081\n",
      "Train Epoch: 1528 [45056/101520 (44%)] Loss: -1174.280029\n",
      "Train Epoch: 1528 [56320/101520 (55%)] Loss: -1176.470215\n",
      "Train Epoch: 1528 [67584/101520 (67%)] Loss: -1178.008423\n",
      "Train Epoch: 1528 [78848/101520 (78%)] Loss: -1178.452637\n",
      "Train Epoch: 1528 [90112/101520 (89%)] Loss: -1171.903931\n",
      "Train Epoch: 1528 [101376/101520 (100%)] Loss: -1174.464600\n",
      "    epoch          : 1528\n",
      "    loss           : -1175.603330372566\n",
      "    ess            : 1.9664653521686344\n",
      "    log_marginal   : 1175.6357544558732\n",
      "    log_joint      : 1384.0653112976995\n",
      "    val_loss       : -1175.5903054942255\n",
      "    val_ess        : 1.965631718220918\n",
      "    val_log_marginal: 1175.6212211277175\n",
      "    val_log_joint  : 1383.8899244225543\n",
      "Train Epoch: 1529 [0/101520 (0%)] Loss: -1177.836792\n",
      "Train Epoch: 1529 [11264/101520 (11%)] Loss: -1169.787231\n",
      "Train Epoch: 1529 [22528/101520 (22%)] Loss: -1175.023926\n",
      "Train Epoch: 1529 [33792/101520 (33%)] Loss: -1173.467896\n",
      "Train Epoch: 1529 [45056/101520 (44%)] Loss: -1174.658813\n",
      "Train Epoch: 1529 [56320/101520 (55%)] Loss: -1174.722168\n",
      "Train Epoch: 1529 [67584/101520 (67%)] Loss: -1174.420898\n",
      "Train Epoch: 1529 [78848/101520 (78%)] Loss: -1176.744751\n",
      "Train Epoch: 1529 [90112/101520 (89%)] Loss: -1178.898438\n",
      "Train Epoch: 1529 [101376/101520 (100%)] Loss: -1168.612427\n",
      "    epoch          : 1529\n",
      "    loss           : -1175.5801302655857\n",
      "    ess            : 1.966456914666909\n",
      "    log_marginal   : 1175.6119059653738\n",
      "    log_joint      : 1384.0502739527717\n",
      "    val_loss       : -1173.719286047894\n",
      "    val_ess        : 1.969737954761671\n",
      "    val_log_marginal: 1173.7476010529892\n",
      "    val_log_joint  : 1381.9883555536685\n",
      "Train Epoch: 1530 [0/101520 (0%)] Loss: -1183.493286\n",
      "Train Epoch: 1530 [11264/101520 (11%)] Loss: -1168.477539\n",
      "Train Epoch: 1530 [22528/101520 (22%)] Loss: -1167.108765\n",
      "Train Epoch: 1530 [33792/101520 (33%)] Loss: -1169.552734\n",
      "Train Epoch: 1530 [45056/101520 (44%)] Loss: -1174.293701\n",
      "Train Epoch: 1530 [56320/101520 (55%)] Loss: -1170.125854\n",
      "Train Epoch: 1530 [67584/101520 (67%)] Loss: -1169.960449\n",
      "Train Epoch: 1530 [78848/101520 (78%)] Loss: -1178.920654\n",
      "Train Epoch: 1530 [90112/101520 (89%)] Loss: -1181.744141\n",
      "Train Epoch: 1530 [101376/101520 (100%)] Loss: -1179.371948\n",
      "    epoch          : 1530\n",
      "    loss           : -1175.638701932514\n",
      "    ess            : 1.9666137383810838\n",
      "    log_marginal   : 1175.670101606666\n",
      "    log_joint      : 1384.1275567289572\n",
      "    val_loss       : -1174.5373747452445\n",
      "    val_ess        : 1.9672396079353665\n",
      "    val_log_marginal: 1174.5680462381115\n",
      "    val_log_joint  : 1382.9535708220108\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1530.pth ...\n",
      "Train Epoch: 1531 [0/101520 (0%)] Loss: -1171.999878\n",
      "Train Epoch: 1531 [11264/101520 (11%)] Loss: -1177.658447\n",
      "Train Epoch: 1531 [22528/101520 (22%)] Loss: -1165.356934\n",
      "Train Epoch: 1531 [33792/101520 (33%)] Loss: -1170.881592\n",
      "Train Epoch: 1531 [45056/101520 (44%)] Loss: -1169.958496\n",
      "Train Epoch: 1531 [56320/101520 (55%)] Loss: -1182.878906\n",
      "Train Epoch: 1531 [67584/101520 (67%)] Loss: -1175.615234\n",
      "Train Epoch: 1531 [78848/101520 (78%)] Loss: -1172.978882\n",
      "Train Epoch: 1531 [90112/101520 (89%)] Loss: -1175.312866\n",
      "Train Epoch: 1531 [101376/101520 (100%)] Loss: -1175.665527\n",
      "    epoch          : 1531\n",
      "    loss           : -1175.6695182455244\n",
      "    ess            : 1.9671896965659443\n",
      "    log_marginal   : 1175.700065267745\n",
      "    log_joint      : 1384.177980355881\n",
      "    val_loss       : -1173.6255519701087\n",
      "    val_ess        : 1.969446415486543\n",
      "    val_log_marginal: 1173.6519191576087\n",
      "    val_log_joint  : 1382.0873598845108\n",
      "Train Epoch: 1532 [0/101520 (0%)] Loss: -1167.623779\n",
      "Train Epoch: 1532 [11264/101520 (11%)] Loss: -1179.107178\n",
      "Train Epoch: 1532 [22528/101520 (22%)] Loss: -1182.710693\n",
      "Train Epoch: 1532 [33792/101520 (33%)] Loss: -1176.233765\n",
      "Train Epoch: 1532 [45056/101520 (44%)] Loss: -1175.032471\n",
      "Train Epoch: 1532 [56320/101520 (55%)] Loss: -1175.241821\n",
      "Train Epoch: 1532 [67584/101520 (67%)] Loss: -1174.625000\n",
      "Train Epoch: 1532 [78848/101520 (78%)] Loss: -1173.817749\n",
      "Train Epoch: 1532 [90112/101520 (89%)] Loss: -1175.910156\n",
      "Train Epoch: 1532 [101376/101520 (100%)] Loss: -1180.987671\n",
      "    epoch          : 1532\n",
      "    loss           : -1175.8745184663553\n",
      "    ess            : 1.9669231745465916\n",
      "    log_marginal   : 1175.9062230095792\n",
      "    log_joint      : 1384.2687841060772\n",
      "    val_loss       : -1175.5233472741168\n",
      "    val_ess        : 1.9696012735366821\n",
      "    val_log_marginal: 1175.5530156674592\n",
      "    val_log_joint  : 1383.6409593665082\n",
      "Train Epoch: 1533 [0/101520 (0%)] Loss: -1175.942993\n",
      "Train Epoch: 1533 [11264/101520 (11%)] Loss: -1181.511475\n",
      "Train Epoch: 1533 [22528/101520 (22%)] Loss: -1178.924561\n",
      "Train Epoch: 1533 [33792/101520 (33%)] Loss: -1177.114990\n",
      "Train Epoch: 1533 [45056/101520 (44%)] Loss: -1177.900146\n",
      "Train Epoch: 1533 [56320/101520 (55%)] Loss: -1183.047607\n",
      "Train Epoch: 1533 [67584/101520 (67%)] Loss: -1168.080566\n",
      "Train Epoch: 1533 [78848/101520 (78%)] Loss: -1168.974121\n",
      "Train Epoch: 1533 [90112/101520 (89%)] Loss: -1174.193726\n",
      "Train Epoch: 1533 [101376/101520 (100%)] Loss: -1178.098267\n",
      "    epoch          : 1533\n",
      "    loss           : -1175.903263264565\n",
      "    ess            : 1.9664367311563924\n",
      "    log_marginal   : 1175.934345801272\n",
      "    log_joint      : 1384.3482359306297\n",
      "    val_loss       : -1175.4431736158288\n",
      "    val_ess        : 1.9622301537057627\n",
      "    val_log_marginal: 1175.4806969684103\n",
      "    val_log_joint  : 1383.9995966372283\n",
      "Train Epoch: 1534 [0/101520 (0%)] Loss: -1180.638672\n",
      "Train Epoch: 1534 [11264/101520 (11%)] Loss: -1181.886230\n",
      "Train Epoch: 1534 [22528/101520 (22%)] Loss: -1178.049805\n",
      "Train Epoch: 1534 [33792/101520 (33%)] Loss: -1175.806274\n",
      "Train Epoch: 1534 [45056/101520 (44%)] Loss: -1166.945068\n",
      "Train Epoch: 1534 [56320/101520 (55%)] Loss: -1172.317627\n",
      "Train Epoch: 1534 [67584/101520 (67%)] Loss: -1178.008301\n",
      "Train Epoch: 1534 [78848/101520 (78%)] Loss: -1177.879883\n",
      "Train Epoch: 1534 [90112/101520 (89%)] Loss: -1167.510010\n",
      "Train Epoch: 1534 [101376/101520 (100%)] Loss: -1178.839478\n",
      "    epoch          : 1534\n",
      "    loss           : -1175.9200648015467\n",
      "    ess            : 1.966896088878114\n",
      "    log_marginal   : 1175.951574891057\n",
      "    log_joint      : 1384.352582001806\n",
      "    val_loss       : -1174.7639425526495\n",
      "    val_ess        : 1.9653445793234783\n",
      "    val_log_marginal: 1174.8016198199728\n",
      "    val_log_joint  : 1383.5025475543478\n",
      "Train Epoch: 1535 [0/101520 (0%)] Loss: -1178.872070\n",
      "Train Epoch: 1535 [11264/101520 (11%)] Loss: -1177.195679\n",
      "Train Epoch: 1535 [22528/101520 (22%)] Loss: -1168.732422\n",
      "Train Epoch: 1535 [33792/101520 (33%)] Loss: -1185.268555\n",
      "Train Epoch: 1535 [45056/101520 (44%)] Loss: -1173.362671\n",
      "Train Epoch: 1535 [56320/101520 (55%)] Loss: -1182.545410\n",
      "Train Epoch: 1535 [67584/101520 (67%)] Loss: -1172.047363\n",
      "Train Epoch: 1535 [78848/101520 (78%)] Loss: -1184.163574\n",
      "Train Epoch: 1535 [90112/101520 (89%)] Loss: -1170.233765\n",
      "Train Epoch: 1535 [101376/101520 (100%)] Loss: -1169.990601\n",
      "    epoch          : 1535\n",
      "    loss           : -1175.8715722165516\n",
      "    ess            : 1.9663916814267335\n",
      "    log_marginal   : 1175.9037521592336\n",
      "    log_joint      : 1384.3217761169126\n",
      "    val_loss       : -1174.2425484035325\n",
      "    val_ess        : 1.9681456866471663\n",
      "    val_log_marginal: 1174.2708368716033\n",
      "    val_log_joint  : 1382.7448040506115\n",
      "Train Epoch: 1536 [0/101520 (0%)] Loss: -1177.164185\n",
      "Train Epoch: 1536 [11264/101520 (11%)] Loss: -1175.163330\n",
      "Train Epoch: 1536 [22528/101520 (22%)] Loss: -1169.695068\n",
      "Train Epoch: 1536 [33792/101520 (33%)] Loss: -1184.182617\n",
      "Train Epoch: 1536 [45056/101520 (44%)] Loss: -1179.901123\n",
      "Train Epoch: 1536 [56320/101520 (55%)] Loss: -1179.316406\n",
      "Train Epoch: 1536 [67584/101520 (67%)] Loss: -1174.766113\n",
      "Train Epoch: 1536 [78848/101520 (78%)] Loss: -1168.676758\n",
      "Train Epoch: 1536 [90112/101520 (89%)] Loss: -1181.123047\n",
      "Train Epoch: 1536 [101376/101520 (100%)] Loss: -1172.910400\n",
      "    epoch          : 1536\n",
      "    loss           : -1175.8034588224325\n",
      "    ess            : 1.9666516756891605\n",
      "    log_marginal   : 1175.8352737810144\n",
      "    log_joint      : 1384.2801170157427\n",
      "    val_loss       : -1176.3521569293478\n",
      "    val_ess        : 1.96878473136736\n",
      "    val_log_marginal: 1176.382472826087\n",
      "    val_log_joint  : 1384.7949802564538\n",
      "Train Epoch: 1537 [0/101520 (0%)] Loss: -1177.946777\n",
      "Train Epoch: 1537 [11264/101520 (11%)] Loss: -1176.510742\n",
      "Train Epoch: 1537 [22528/101520 (22%)] Loss: -1181.295898\n",
      "Train Epoch: 1537 [33792/101520 (33%)] Loss: -1180.424316\n",
      "Train Epoch: 1537 [45056/101520 (44%)] Loss: -1169.341064\n",
      "Train Epoch: 1537 [56320/101520 (55%)] Loss: -1177.705200\n",
      "Train Epoch: 1537 [67584/101520 (67%)] Loss: -1177.184814\n",
      "Train Epoch: 1537 [78848/101520 (78%)] Loss: -1170.806152\n",
      "Train Epoch: 1537 [90112/101520 (89%)] Loss: -1176.013062\n",
      "Train Epoch: 1537 [101376/101520 (100%)] Loss: -1177.890381\n",
      "    epoch          : 1537\n",
      "    loss           : -1175.9182760727465\n",
      "    ess            : 1.9669837951660156\n",
      "    log_marginal   : 1175.9501566671247\n",
      "    log_joint      : 1384.350614768177\n",
      "    val_loss       : -1174.988578464674\n",
      "    val_ess        : 1.9684558329374895\n",
      "    val_log_marginal: 1175.0176418138587\n",
      "    val_log_joint  : 1383.412990404212\n",
      "Train Epoch: 1538 [0/101520 (0%)] Loss: -1177.178467\n",
      "Train Epoch: 1538 [11264/101520 (11%)] Loss: -1175.551880\n",
      "Train Epoch: 1538 [22528/101520 (22%)] Loss: -1181.813965\n",
      "Train Epoch: 1538 [33792/101520 (33%)] Loss: -1179.261475\n",
      "Train Epoch: 1538 [45056/101520 (44%)] Loss: -1179.263916\n",
      "Train Epoch: 1538 [56320/101520 (55%)] Loss: -1177.656982\n",
      "Train Epoch: 1538 [67584/101520 (67%)] Loss: -1172.293457\n",
      "Train Epoch: 1538 [78848/101520 (78%)] Loss: -1176.250244\n",
      "Train Epoch: 1538 [90112/101520 (89%)] Loss: -1182.440430\n",
      "Train Epoch: 1538 [101376/101520 (100%)] Loss: -1171.876587\n",
      "    epoch          : 1538\n",
      "    loss           : -1175.8197340462077\n",
      "    ess            : 1.966489713994702\n",
      "    log_marginal   : 1175.851111637288\n",
      "    log_joint      : 1384.3196095222204\n",
      "    val_loss       : -1173.5968654466712\n",
      "    val_ess        : 1.9622874622759612\n",
      "    val_log_marginal: 1173.6303392493207\n",
      "    val_log_joint  : 1381.717624830163\n",
      "Train Epoch: 1539 [0/101520 (0%)] Loss: -1172.265259\n",
      "Train Epoch: 1539 [11264/101520 (11%)] Loss: -1175.484741\n",
      "Train Epoch: 1539 [22528/101520 (22%)] Loss: -1174.392578\n",
      "Train Epoch: 1539 [33792/101520 (33%)] Loss: -1179.402344\n",
      "Train Epoch: 1539 [45056/101520 (44%)] Loss: -1173.203125\n",
      "Train Epoch: 1539 [56320/101520 (55%)] Loss: -1178.150391\n",
      "Train Epoch: 1539 [67584/101520 (67%)] Loss: -1179.703735\n",
      "Train Epoch: 1539 [78848/101520 (78%)] Loss: -1179.861572\n",
      "Train Epoch: 1539 [90112/101520 (89%)] Loss: -1179.335449\n",
      "Train Epoch: 1539 [101376/101520 (100%)] Loss: -1177.657593\n",
      "    epoch          : 1539\n",
      "    loss           : -1175.979691548563\n",
      "    ess            : 1.9675641401329231\n",
      "    log_marginal   : 1176.0102992992306\n",
      "    log_joint      : 1384.4251942083465\n",
      "    val_loss       : -1174.491402004076\n",
      "    val_ess        : 1.967024564743042\n",
      "    val_log_marginal: 1174.5212986158288\n",
      "    val_log_joint  : 1382.8813370414402\n",
      "Train Epoch: 1540 [0/101520 (0%)] Loss: -1175.461182\n",
      "Train Epoch: 1540 [11264/101520 (11%)] Loss: -1175.568848\n",
      "Train Epoch: 1540 [22528/101520 (22%)] Loss: -1170.282471\n",
      "Train Epoch: 1540 [33792/101520 (33%)] Loss: -1174.679443\n",
      "Train Epoch: 1540 [45056/101520 (44%)] Loss: -1177.761230\n",
      "Train Epoch: 1540 [56320/101520 (55%)] Loss: -1171.242920\n",
      "Train Epoch: 1540 [67584/101520 (67%)] Loss: -1175.872192\n",
      "Train Epoch: 1540 [78848/101520 (78%)] Loss: -1170.907227\n",
      "Train Epoch: 1540 [90112/101520 (89%)] Loss: -1179.614380\n",
      "Train Epoch: 1540 [101376/101520 (100%)] Loss: -1172.225586\n",
      "    epoch          : 1540\n",
      "    loss           : -1175.9644370534313\n",
      "    ess            : 1.9666300083524617\n",
      "    log_marginal   : 1175.99665502807\n",
      "    log_joint      : 1384.5078198610238\n",
      "    val_loss       : -1175.6811417289402\n",
      "    val_ess        : 1.961348015329112\n",
      "    val_log_marginal: 1175.7259839928668\n",
      "    val_log_joint  : 1383.7616444463315\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1540.pth ...\n",
      "Train Epoch: 1541 [0/101520 (0%)] Loss: -1170.859863\n",
      "Train Epoch: 1541 [11264/101520 (11%)] Loss: -1173.283936\n",
      "Train Epoch: 1541 [22528/101520 (22%)] Loss: -1174.472168\n",
      "Train Epoch: 1541 [33792/101520 (33%)] Loss: -1179.043945\n",
      "Train Epoch: 1541 [45056/101520 (44%)] Loss: -1173.234619\n",
      "Train Epoch: 1541 [56320/101520 (55%)] Loss: -1177.833496\n",
      "Train Epoch: 1541 [67584/101520 (67%)] Loss: -1174.791260\n",
      "Train Epoch: 1541 [78848/101520 (78%)] Loss: -1171.531006\n",
      "Train Epoch: 1541 [90112/101520 (89%)] Loss: -1175.562988\n",
      "Train Epoch: 1541 [101376/101520 (100%)] Loss: -1177.707642\n",
      "    epoch          : 1541\n",
      "    loss           : -1176.0752075808732\n",
      "    ess            : 1.9673675059074134\n",
      "    log_marginal   : 1176.106773491481\n",
      "    log_joint      : 1384.5565486122016\n",
      "    val_loss       : -1171.616311778193\n",
      "    val_ess        : 1.9658272577368694\n",
      "    val_log_marginal: 1171.6490372367527\n",
      "    val_log_joint  : 1379.8756581182065\n",
      "Train Epoch: 1542 [0/101520 (0%)] Loss: -1177.269043\n",
      "Train Epoch: 1542 [11264/101520 (11%)] Loss: -1178.293945\n",
      "Train Epoch: 1542 [22528/101520 (22%)] Loss: -1177.377686\n",
      "Train Epoch: 1542 [33792/101520 (33%)] Loss: -1172.363770\n",
      "Train Epoch: 1542 [45056/101520 (44%)] Loss: -1180.499146\n",
      "Train Epoch: 1542 [56320/101520 (55%)] Loss: -1177.708984\n",
      "Train Epoch: 1542 [67584/101520 (67%)] Loss: -1169.845215\n",
      "Train Epoch: 1542 [78848/101520 (78%)] Loss: -1174.423340\n",
      "Train Epoch: 1542 [90112/101520 (89%)] Loss: -1180.148560\n",
      "Train Epoch: 1542 [101376/101520 (100%)] Loss: -1188.329224\n",
      "    epoch          : 1542\n",
      "    loss           : -1176.120004931886\n",
      "    ess            : 1.965660575646252\n",
      "    log_marginal   : 1176.15319701535\n",
      "    log_joint      : 1384.616041633951\n",
      "    val_loss       : -1175.0583071501358\n",
      "    val_ess        : 1.967440542967423\n",
      "    val_log_marginal: 1175.0875827955163\n",
      "    val_log_joint  : 1383.5702275815217\n",
      "Train Epoch: 1543 [0/101520 (0%)] Loss: -1184.157959\n",
      "Train Epoch: 1543 [11264/101520 (11%)] Loss: -1183.045898\n",
      "Train Epoch: 1543 [22528/101520 (22%)] Loss: -1173.923584\n",
      "Train Epoch: 1543 [33792/101520 (33%)] Loss: -1170.041748\n",
      "Train Epoch: 1543 [45056/101520 (44%)] Loss: -1172.648682\n",
      "Train Epoch: 1543 [56320/101520 (55%)] Loss: -1177.413208\n",
      "Train Epoch: 1543 [67584/101520 (67%)] Loss: -1175.972778\n",
      "Train Epoch: 1543 [78848/101520 (78%)] Loss: -1166.151489\n",
      "Train Epoch: 1543 [90112/101520 (89%)] Loss: -1177.762939\n",
      "Train Epoch: 1543 [101376/101520 (100%)] Loss: -1179.375366\n",
      "    epoch          : 1543\n",
      "    loss           : -1176.27748974364\n",
      "    ess            : 1.9664557052018055\n",
      "    log_marginal   : 1176.309137238929\n",
      "    log_joint      : 1384.65804793008\n",
      "    val_loss       : -1174.6351265285325\n",
      "    val_ess        : 1.9623315075169439\n",
      "    val_log_marginal: 1174.6743323284647\n",
      "    val_log_joint  : 1383.2264563519022\n",
      "Train Epoch: 1544 [0/101520 (0%)] Loss: -1182.944214\n",
      "Train Epoch: 1544 [11264/101520 (11%)] Loss: -1178.122681\n",
      "Train Epoch: 1544 [22528/101520 (22%)] Loss: -1173.528809\n",
      "Train Epoch: 1544 [33792/101520 (33%)] Loss: -1181.239258\n",
      "Train Epoch: 1544 [45056/101520 (44%)] Loss: -1169.403076\n",
      "Train Epoch: 1544 [56320/101520 (55%)] Loss: -1176.678833\n",
      "Train Epoch: 1544 [67584/101520 (67%)] Loss: -1175.831787\n",
      "Train Epoch: 1544 [78848/101520 (78%)] Loss: -1181.442139\n",
      "Train Epoch: 1544 [90112/101520 (89%)] Loss: -1174.644043\n",
      "Train Epoch: 1544 [101376/101520 (100%)] Loss: -1178.169312\n",
      "    epoch          : 1544\n",
      "    loss           : -1176.246380829931\n",
      "    ess            : 1.967224986708943\n",
      "    log_marginal   : 1176.2772339480607\n",
      "    log_joint      : 1384.68520458739\n",
      "    val_loss       : -1174.7798966117527\n",
      "    val_ess        : 1.9671855128329734\n",
      "    val_log_marginal: 1174.8094960088315\n",
      "    val_log_joint  : 1383.0719471807065\n",
      "Train Epoch: 1545 [0/101520 (0%)] Loss: -1183.195312\n",
      "Train Epoch: 1545 [11264/101520 (11%)] Loss: -1174.680054\n",
      "Train Epoch: 1545 [22528/101520 (22%)] Loss: -1173.247803\n",
      "Train Epoch: 1545 [33792/101520 (33%)] Loss: -1177.537842\n",
      "Train Epoch: 1545 [45056/101520 (44%)] Loss: -1168.564331\n",
      "Train Epoch: 1545 [56320/101520 (55%)] Loss: -1170.205688\n",
      "Train Epoch: 1545 [67584/101520 (67%)] Loss: -1182.350586\n",
      "Train Epoch: 1545 [78848/101520 (78%)] Loss: -1176.558594\n",
      "Train Epoch: 1545 [90112/101520 (89%)] Loss: -1175.483276\n",
      "Train Epoch: 1545 [101376/101520 (100%)] Loss: -1173.286865\n",
      "    epoch          : 1545\n",
      "    loss           : -1176.3109725875472\n",
      "    ess            : 1.9662218447306647\n",
      "    log_marginal   : 1176.3440327860003\n",
      "    log_joint      : 1384.731616456305\n",
      "    val_loss       : -1173.7994384765625\n",
      "    val_ess        : 1.9659861896349036\n",
      "    val_log_marginal: 1173.8317499575408\n",
      "    val_log_joint  : 1382.3542268172555\n",
      "Train Epoch: 1546 [0/101520 (0%)] Loss: -1176.233521\n",
      "Train Epoch: 1546 [11264/101520 (11%)] Loss: -1174.572266\n",
      "Train Epoch: 1546 [22528/101520 (22%)] Loss: -1175.869629\n",
      "Train Epoch: 1546 [33792/101520 (33%)] Loss: -1183.864746\n",
      "Train Epoch: 1546 [45056/101520 (44%)] Loss: -1172.213257\n",
      "Train Epoch: 1546 [56320/101520 (55%)] Loss: -1174.442505\n",
      "Train Epoch: 1546 [67584/101520 (67%)] Loss: -1173.636719\n",
      "Train Epoch: 1546 [78848/101520 (78%)] Loss: -1175.548584\n",
      "Train Epoch: 1546 [90112/101520 (89%)] Loss: -1173.886230\n",
      "Train Epoch: 1546 [101376/101520 (100%)] Loss: -1174.345337\n",
      "    epoch          : 1546\n",
      "    loss           : -1176.3166865823257\n",
      "    ess            : 1.9673319797420024\n",
      "    log_marginal   : 1176.347331751531\n",
      "    log_joint      : 1384.815951706776\n",
      "    val_loss       : -1174.7385996942935\n",
      "    val_ess        : 1.9662481287251348\n",
      "    val_log_marginal: 1174.7688094429348\n",
      "    val_log_joint  : 1383.3728770380435\n",
      "Train Epoch: 1547 [0/101520 (0%)] Loss: -1177.531006\n",
      "Train Epoch: 1547 [11264/101520 (11%)] Loss: -1174.099121\n",
      "Train Epoch: 1547 [22528/101520 (22%)] Loss: -1171.041626\n",
      "Train Epoch: 1547 [33792/101520 (33%)] Loss: -1175.802612\n",
      "Train Epoch: 1547 [45056/101520 (44%)] Loss: -1180.627197\n",
      "Train Epoch: 1547 [56320/101520 (55%)] Loss: -1179.485840\n",
      "Train Epoch: 1547 [67584/101520 (67%)] Loss: -1173.636719\n",
      "Train Epoch: 1547 [78848/101520 (78%)] Loss: -1180.301025\n",
      "Train Epoch: 1547 [90112/101520 (89%)] Loss: -1175.112305\n",
      "Train Epoch: 1547 [101376/101520 (100%)] Loss: -1177.081665\n",
      "    epoch          : 1547\n",
      "    loss           : -1176.3369490273633\n",
      "    ess            : 1.9667598182831578\n",
      "    log_marginal   : 1176.368349314934\n",
      "    log_joint      : 1384.7981552047347\n",
      "    val_loss       : -1173.872606360394\n",
      "    val_ess        : 1.9686492474182793\n",
      "    val_log_marginal: 1173.9005710767663\n",
      "    val_log_joint  : 1382.696389903193\n",
      "Train Epoch: 1548 [0/101520 (0%)] Loss: -1175.411621\n",
      "Train Epoch: 1548 [11264/101520 (11%)] Loss: -1174.823975\n",
      "Train Epoch: 1548 [22528/101520 (22%)] Loss: -1172.213867\n",
      "Train Epoch: 1548 [33792/101520 (33%)] Loss: -1170.943726\n",
      "Train Epoch: 1548 [45056/101520 (44%)] Loss: -1176.542603\n",
      "Train Epoch: 1548 [56320/101520 (55%)] Loss: -1181.493652\n",
      "Train Epoch: 1548 [67584/101520 (67%)] Loss: -1179.516113\n",
      "Train Epoch: 1548 [78848/101520 (78%)] Loss: -1182.644775\n",
      "Train Epoch: 1548 [90112/101520 (89%)] Loss: -1179.651367\n",
      "Train Epoch: 1548 [101376/101520 (100%)] Loss: -1182.852905\n",
      "    epoch          : 1548\n",
      "    loss           : -1176.4232091855763\n",
      "    ess            : 1.9657186311693047\n",
      "    log_marginal   : 1176.4562712242855\n",
      "    log_joint      : 1384.8696693918812\n",
      "    val_loss       : -1174.5193083389945\n",
      "    val_ess        : 1.9650490698607073\n",
      "    val_log_marginal: 1174.551614512568\n",
      "    val_log_joint  : 1383.0911812160325\n",
      "Train Epoch: 1549 [0/101520 (0%)] Loss: -1178.888428\n",
      "Train Epoch: 1549 [11264/101520 (11%)] Loss: -1175.408936\n",
      "Train Epoch: 1549 [22528/101520 (22%)] Loss: -1175.956543\n",
      "Train Epoch: 1549 [33792/101520 (33%)] Loss: -1177.112549\n",
      "Train Epoch: 1549 [45056/101520 (44%)] Loss: -1173.104004\n",
      "Train Epoch: 1549 [56320/101520 (55%)] Loss: -1179.213623\n",
      "Train Epoch: 1549 [67584/101520 (67%)] Loss: -1171.881470\n",
      "Train Epoch: 1549 [78848/101520 (78%)] Loss: -1185.142090\n",
      "Train Epoch: 1549 [90112/101520 (89%)] Loss: -1178.950684\n",
      "Train Epoch: 1549 [101376/101520 (100%)] Loss: -1167.544556\n",
      "    epoch          : 1549\n",
      "    loss           : -1176.348105272456\n",
      "    ess            : 1.9668978764184157\n",
      "    log_marginal   : 1176.3799938412767\n",
      "    log_joint      : 1384.7822192014762\n",
      "    val_loss       : -1175.7123651919158\n",
      "    val_ess        : 1.9674640271974646\n",
      "    val_log_marginal: 1175.7440557065217\n",
      "    val_log_joint  : 1383.7926396908967\n",
      "Train Epoch: 1550 [0/101520 (0%)] Loss: -1175.046387\n",
      "Train Epoch: 1550 [11264/101520 (11%)] Loss: -1183.771729\n",
      "Train Epoch: 1550 [22528/101520 (22%)] Loss: -1173.461792\n",
      "Train Epoch: 1550 [33792/101520 (33%)] Loss: -1178.365967\n",
      "Train Epoch: 1550 [45056/101520 (44%)] Loss: -1180.266479\n",
      "Train Epoch: 1550 [56320/101520 (55%)] Loss: -1174.623901\n",
      "Train Epoch: 1550 [67584/101520 (67%)] Loss: -1182.091431\n",
      "Train Epoch: 1550 [78848/101520 (78%)] Loss: -1174.885498\n",
      "Train Epoch: 1550 [90112/101520 (89%)] Loss: -1178.014526\n",
      "Train Epoch: 1550 [101376/101520 (100%)] Loss: -1184.878662\n",
      "    epoch          : 1550\n",
      "    loss           : -1176.4234103868955\n",
      "    ess            : 1.966201434782402\n",
      "    log_marginal   : 1176.4555737672738\n",
      "    log_joint      : 1384.8775487545147\n",
      "    val_loss       : -1173.9978929602582\n",
      "    val_ess        : 1.9687500051830127\n",
      "    val_log_marginal: 1174.0281823199728\n",
      "    val_log_joint  : 1382.5507494055707\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1550.pth ...\n",
      "Train Epoch: 1551 [0/101520 (0%)] Loss: -1174.795166\n",
      "Train Epoch: 1551 [11264/101520 (11%)] Loss: -1177.392334\n",
      "Train Epoch: 1551 [22528/101520 (22%)] Loss: -1182.360229\n",
      "Train Epoch: 1551 [33792/101520 (33%)] Loss: -1172.191284\n",
      "Train Epoch: 1551 [45056/101520 (44%)] Loss: -1184.003906\n",
      "Train Epoch: 1551 [56320/101520 (55%)] Loss: -1175.218750\n",
      "Train Epoch: 1551 [67584/101520 (67%)] Loss: -1172.999512\n",
      "Train Epoch: 1551 [78848/101520 (78%)] Loss: -1178.467041\n",
      "Train Epoch: 1551 [90112/101520 (89%)] Loss: -1175.384766\n",
      "Train Epoch: 1551 [101376/101520 (100%)] Loss: -1189.534546\n",
      "    epoch          : 1551\n",
      "    loss           : -1176.4476115931218\n",
      "    ess            : 1.9657171515364145\n",
      "    log_marginal   : 1176.4811435105214\n",
      "    log_joint      : 1384.9369362682553\n",
      "    val_loss       : -1176.057426120924\n",
      "    val_ess        : 1.9695124367009038\n",
      "    val_log_marginal: 1176.084817637568\n",
      "    val_log_joint  : 1384.5159912109375\n",
      "Train Epoch: 1552 [0/101520 (0%)] Loss: -1181.426636\n",
      "Train Epoch: 1552 [11264/101520 (11%)] Loss: -1177.919922\n",
      "Train Epoch: 1552 [22528/101520 (22%)] Loss: -1181.342163\n",
      "Train Epoch: 1552 [33792/101520 (33%)] Loss: -1177.131104\n",
      "Train Epoch: 1552 [45056/101520 (44%)] Loss: -1170.611328\n",
      "Train Epoch: 1552 [56320/101520 (55%)] Loss: -1170.965332\n",
      "Train Epoch: 1552 [67584/101520 (67%)] Loss: -1171.433838\n",
      "Train Epoch: 1552 [78848/101520 (78%)] Loss: -1168.718506\n",
      "Train Epoch: 1552 [90112/101520 (89%)] Loss: -1175.707275\n",
      "Train Epoch: 1552 [101376/101520 (100%)] Loss: -1182.873535\n",
      "    epoch          : 1552\n",
      "    loss           : -1176.4926923435537\n",
      "    ess            : 1.9666357232098604\n",
      "    log_marginal   : 1176.5242478260443\n",
      "    log_joint      : 1384.9366688177215\n",
      "    val_loss       : -1175.5730405061142\n",
      "    val_ess        : 1.9693277452303015\n",
      "    val_log_marginal: 1175.6033298658288\n",
      "    val_log_joint  : 1384.0420293393342\n",
      "Train Epoch: 1553 [0/101520 (0%)] Loss: -1180.172241\n",
      "Train Epoch: 1553 [11264/101520 (11%)] Loss: -1176.996460\n",
      "Train Epoch: 1553 [22528/101520 (22%)] Loss: -1169.690674\n",
      "Train Epoch: 1553 [33792/101520 (33%)] Loss: -1174.015137\n",
      "Train Epoch: 1553 [45056/101520 (44%)] Loss: -1178.974121\n",
      "Train Epoch: 1553 [56320/101520 (55%)] Loss: -1177.530151\n",
      "Train Epoch: 1553 [67584/101520 (67%)] Loss: -1176.063232\n",
      "Train Epoch: 1553 [78848/101520 (78%)] Loss: -1180.097900\n",
      "Train Epoch: 1553 [90112/101520 (89%)] Loss: -1170.363525\n",
      "Train Epoch: 1553 [101376/101520 (100%)] Loss: -1168.976562\n",
      "    epoch          : 1553\n",
      "    loss           : -1176.4799682003768\n",
      "    ess            : 1.9669612172859996\n",
      "    log_marginal   : 1176.5103968327967\n",
      "    log_joint      : 1384.913284071726\n",
      "    val_loss       : -1176.7551163383152\n",
      "    val_ess        : 1.9651752865832786\n",
      "    val_log_marginal: 1176.7861115828805\n",
      "    val_log_joint  : 1385.3980872112772\n",
      "Train Epoch: 1554 [0/101520 (0%)] Loss: -1179.500732\n",
      "Train Epoch: 1554 [11264/101520 (11%)] Loss: -1178.169434\n",
      "Train Epoch: 1554 [22528/101520 (22%)] Loss: -1180.435547\n",
      "Train Epoch: 1554 [33792/101520 (33%)] Loss: -1177.863647\n",
      "Train Epoch: 1554 [45056/101520 (44%)] Loss: -1167.773804\n",
      "Train Epoch: 1554 [56320/101520 (55%)] Loss: -1183.291504\n",
      "Train Epoch: 1554 [67584/101520 (67%)] Loss: -1177.844360\n",
      "Train Epoch: 1554 [78848/101520 (78%)] Loss: -1178.879028\n",
      "Train Epoch: 1554 [90112/101520 (89%)] Loss: -1173.215942\n",
      "Train Epoch: 1554 [101376/101520 (100%)] Loss: -1176.131470\n",
      "    epoch          : 1554\n",
      "    loss           : -1176.5252894109217\n",
      "    ess            : 1.9664979214644311\n",
      "    log_marginal   : 1176.557682823296\n",
      "    log_joint      : 1384.9239078694252\n",
      "    val_loss       : -1174.0638109290082\n",
      "    val_ess        : 1.9661379689755647\n",
      "    val_log_marginal: 1174.0950343919837\n",
      "    val_log_joint  : 1382.4697902513587\n",
      "Train Epoch: 1555 [0/101520 (0%)] Loss: -1179.936279\n",
      "Train Epoch: 1555 [11264/101520 (11%)] Loss: -1179.635254\n",
      "Train Epoch: 1555 [22528/101520 (22%)] Loss: -1174.275146\n",
      "Train Epoch: 1555 [33792/101520 (33%)] Loss: -1182.937378\n",
      "Train Epoch: 1555 [45056/101520 (44%)] Loss: -1186.366577\n",
      "Train Epoch: 1555 [56320/101520 (55%)] Loss: -1181.406982\n",
      "Train Epoch: 1555 [67584/101520 (67%)] Loss: -1170.661743\n",
      "Train Epoch: 1555 [78848/101520 (78%)] Loss: -1179.758179\n",
      "Train Epoch: 1555 [90112/101520 (89%)] Loss: -1171.526123\n",
      "Train Epoch: 1555 [101376/101520 (100%)] Loss: -1177.634766\n",
      "    epoch          : 1555\n",
      "    loss           : -1176.515609051115\n",
      "    ess            : 1.9663095072885255\n",
      "    log_marginal   : 1176.5479006359924\n",
      "    log_joint      : 1384.967495558849\n",
      "    val_loss       : -1175.732469641644\n",
      "    val_ess        : 1.966343667196191\n",
      "    val_log_marginal: 1175.7644308338995\n",
      "    val_log_joint  : 1384.3228918987772\n",
      "Train Epoch: 1556 [0/101520 (0%)] Loss: -1173.537842\n",
      "Train Epoch: 1556 [11264/101520 (11%)] Loss: -1174.317627\n",
      "Train Epoch: 1556 [22528/101520 (22%)] Loss: -1178.886719\n",
      "Train Epoch: 1556 [33792/101520 (33%)] Loss: -1172.361816\n",
      "Train Epoch: 1556 [45056/101520 (44%)] Loss: -1179.572632\n",
      "Train Epoch: 1556 [56320/101520 (55%)] Loss: -1181.362305\n",
      "Train Epoch: 1556 [67584/101520 (67%)] Loss: -1176.158691\n",
      "Train Epoch: 1556 [78848/101520 (78%)] Loss: -1172.056763\n",
      "Train Epoch: 1556 [90112/101520 (89%)] Loss: -1180.587402\n",
      "Train Epoch: 1556 [101376/101520 (100%)] Loss: -1171.431152\n",
      "    epoch          : 1556\n",
      "    loss           : -1176.550396636503\n",
      "    ess            : 1.9672187980095945\n",
      "    log_marginal   : 1176.5818754416614\n",
      "    log_joint      : 1385.0057925123665\n",
      "    val_loss       : -1175.9549135954483\n",
      "    val_ess        : 1.9685157692950705\n",
      "    val_log_marginal: 1175.983207370924\n",
      "    val_log_joint  : 1384.1400571076767\n",
      "Train Epoch: 1557 [0/101520 (0%)] Loss: -1179.724854\n",
      "Train Epoch: 1557 [11264/101520 (11%)] Loss: -1176.403564\n",
      "Train Epoch: 1557 [22528/101520 (22%)] Loss: -1175.674805\n",
      "Train Epoch: 1557 [33792/101520 (33%)] Loss: -1168.674561\n",
      "Train Epoch: 1557 [45056/101520 (44%)] Loss: -1182.200928\n",
      "Train Epoch: 1557 [56320/101520 (55%)] Loss: -1182.180176\n",
      "Train Epoch: 1557 [67584/101520 (67%)] Loss: -1179.122192\n",
      "Train Epoch: 1557 [78848/101520 (78%)] Loss: -1182.599609\n",
      "Train Epoch: 1557 [90112/101520 (89%)] Loss: -1173.721924\n",
      "Train Epoch: 1557 [101376/101520 (100%)] Loss: -1185.632324\n",
      "    epoch          : 1557\n",
      "    loss           : -1176.6530639035018\n",
      "    ess            : 1.9663408743077186\n",
      "    log_marginal   : 1176.6849273221576\n",
      "    log_joint      : 1385.10532275636\n",
      "    val_loss       : -1175.0711776069973\n",
      "    val_ess        : 1.96876931708792\n",
      "    val_log_marginal: 1175.1000711192255\n",
      "    val_log_joint  : 1383.699807871943\n",
      "Train Epoch: 1558 [0/101520 (0%)] Loss: -1180.665771\n",
      "Train Epoch: 1558 [11264/101520 (11%)] Loss: -1180.522217\n",
      "Train Epoch: 1558 [22528/101520 (22%)] Loss: -1174.797119\n",
      "Train Epoch: 1558 [33792/101520 (33%)] Loss: -1175.299683\n",
      "Train Epoch: 1558 [45056/101520 (44%)] Loss: -1174.645142\n",
      "Train Epoch: 1558 [56320/101520 (55%)] Loss: -1179.892700\n",
      "Train Epoch: 1558 [67584/101520 (67%)] Loss: -1181.954712\n",
      "Train Epoch: 1558 [78848/101520 (78%)] Loss: -1174.802246\n",
      "Train Epoch: 1558 [90112/101520 (89%)] Loss: -1180.273682\n",
      "Train Epoch: 1558 [101376/101520 (100%)] Loss: -1175.310181\n",
      "    epoch          : 1558\n",
      "    loss           : -1176.6607463587468\n",
      "    ess            : 1.9663434657619228\n",
      "    log_marginal   : 1176.6929581991992\n",
      "    log_joint      : 1385.0796321696373\n",
      "    val_loss       : -1175.8435430112092\n",
      "    val_ess        : 1.9651740271112192\n",
      "    val_log_marginal: 1175.8748938519022\n",
      "    val_log_joint  : 1384.0732103430707\n",
      "Train Epoch: 1559 [0/101520 (0%)] Loss: -1182.540283\n",
      "Train Epoch: 1559 [11264/101520 (11%)] Loss: -1180.983398\n",
      "Train Epoch: 1559 [22528/101520 (22%)] Loss: -1177.310303\n",
      "Train Epoch: 1559 [33792/101520 (33%)] Loss: -1176.688477\n",
      "Train Epoch: 1559 [45056/101520 (44%)] Loss: -1176.876709\n",
      "Train Epoch: 1559 [56320/101520 (55%)] Loss: -1174.954102\n",
      "Train Epoch: 1559 [67584/101520 (67%)] Loss: -1177.461060\n",
      "Train Epoch: 1559 [78848/101520 (78%)] Loss: -1178.203735\n",
      "Train Epoch: 1559 [90112/101520 (89%)] Loss: -1170.952393\n",
      "Train Epoch: 1559 [101376/101520 (100%)] Loss: -1181.889038\n",
      "    epoch          : 1559\n",
      "    loss           : -1176.6846715265783\n",
      "    ess            : 1.9669762382555247\n",
      "    log_marginal   : 1176.7161110729428\n",
      "    log_joint      : 1385.1445999528894\n",
      "    val_loss       : -1174.9381156589675\n",
      "    val_ess        : 1.9661302255547566\n",
      "    val_log_marginal: 1174.9719397503397\n",
      "    val_log_joint  : 1383.4491603685462\n",
      "Train Epoch: 1560 [0/101520 (0%)] Loss: -1173.566650\n",
      "Train Epoch: 1560 [11264/101520 (11%)] Loss: -1175.932251\n",
      "Train Epoch: 1560 [22528/101520 (22%)] Loss: -1181.000000\n",
      "Train Epoch: 1560 [33792/101520 (33%)] Loss: -1177.780762\n",
      "Train Epoch: 1560 [45056/101520 (44%)] Loss: -1172.007202\n",
      "Train Epoch: 1560 [56320/101520 (55%)] Loss: -1179.492676\n",
      "Train Epoch: 1560 [67584/101520 (67%)] Loss: -1178.462158\n",
      "Train Epoch: 1560 [78848/101520 (78%)] Loss: -1181.698364\n",
      "Train Epoch: 1560 [90112/101520 (89%)] Loss: -1175.325073\n",
      "Train Epoch: 1560 [101376/101520 (100%)] Loss: -1165.501343\n",
      "    epoch          : 1560\n",
      "    loss           : -1176.717099903816\n",
      "    ess            : 1.9661767506719234\n",
      "    log_marginal   : 1176.748834504554\n",
      "    log_joint      : 1385.1267525370995\n",
      "    val_loss       : -1173.6801173997962\n",
      "    val_ess        : 1.9676015169724175\n",
      "    val_log_marginal: 1173.7107995074728\n",
      "    val_log_joint  : 1382.014255689538\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1560.pth ...\n",
      "Train Epoch: 1561 [0/101520 (0%)] Loss: -1173.043335\n",
      "Train Epoch: 1561 [11264/101520 (11%)] Loss: -1175.712402\n",
      "Train Epoch: 1561 [22528/101520 (22%)] Loss: -1175.401123\n",
      "Train Epoch: 1561 [33792/101520 (33%)] Loss: -1175.150024\n",
      "Train Epoch: 1561 [45056/101520 (44%)] Loss: -1174.781006\n",
      "Train Epoch: 1561 [56320/101520 (55%)] Loss: -1173.948486\n",
      "Train Epoch: 1561 [67584/101520 (67%)] Loss: -1178.957031\n",
      "Train Epoch: 1561 [78848/101520 (78%)] Loss: -1177.665405\n",
      "Train Epoch: 1561 [90112/101520 (89%)] Loss: -1168.311768\n",
      "Train Epoch: 1561 [101376/101520 (100%)] Loss: -1165.594360\n",
      "    epoch          : 1561\n",
      "    loss           : -1176.7174747026147\n",
      "    ess            : 1.96647771459129\n",
      "    log_marginal   : 1176.7493786069017\n",
      "    log_joint      : 1385.1882581854586\n",
      "    val_loss       : -1175.4990818189538\n",
      "    val_ess        : 1.9652409294377202\n",
      "    val_log_marginal: 1175.532173488451\n",
      "    val_log_joint  : 1384.1219960088315\n",
      "Train Epoch: 1562 [0/101520 (0%)] Loss: -1178.346436\n",
      "Train Epoch: 1562 [11264/101520 (11%)] Loss: -1170.056396\n",
      "Train Epoch: 1562 [22528/101520 (22%)] Loss: -1182.482422\n",
      "Train Epoch: 1562 [33792/101520 (33%)] Loss: -1180.968872\n",
      "Train Epoch: 1562 [45056/101520 (44%)] Loss: -1182.364502\n",
      "Train Epoch: 1562 [56320/101520 (55%)] Loss: -1174.577881\n",
      "Train Epoch: 1562 [67584/101520 (67%)] Loss: -1177.316650\n",
      "Train Epoch: 1562 [78848/101520 (78%)] Loss: -1178.327026\n",
      "Train Epoch: 1562 [90112/101520 (89%)] Loss: -1179.423828\n",
      "Train Epoch: 1562 [101376/101520 (100%)] Loss: -1184.190063\n",
      "    epoch          : 1562\n",
      "    loss           : -1176.823751938403\n",
      "    ess            : 1.965941726262845\n",
      "    log_marginal   : 1176.856474143177\n",
      "    log_joint      : 1385.2355196392116\n",
      "    val_loss       : -1176.227443529212\n",
      "    val_ess        : 1.9657112204510232\n",
      "    val_log_marginal: 1176.2606307319973\n",
      "    val_log_joint  : 1385.0029721467392\n",
      "Train Epoch: 1563 [0/101520 (0%)] Loss: -1173.157959\n",
      "Train Epoch: 1563 [11264/101520 (11%)] Loss: -1178.902344\n",
      "Train Epoch: 1563 [22528/101520 (22%)] Loss: -1168.807373\n",
      "Train Epoch: 1563 [33792/101520 (33%)] Loss: -1170.614014\n",
      "Train Epoch: 1563 [45056/101520 (44%)] Loss: -1173.060059\n",
      "Train Epoch: 1563 [56320/101520 (55%)] Loss: -1181.918945\n",
      "Train Epoch: 1563 [67584/101520 (67%)] Loss: -1172.810303\n",
      "Train Epoch: 1563 [78848/101520 (78%)] Loss: -1178.389648\n",
      "Train Epoch: 1563 [90112/101520 (89%)] Loss: -1176.141113\n",
      "Train Epoch: 1563 [101376/101520 (100%)] Loss: -1180.534912\n",
      "    epoch          : 1563\n",
      "    loss           : -1176.8282206933104\n",
      "    ess            : 1.9665248579715364\n",
      "    log_marginal   : 1176.8596363163474\n",
      "    log_joint      : 1385.276313820077\n",
      "    val_loss       : -1176.2948157269022\n",
      "    val_ess        : 1.9629050182259602\n",
      "    val_log_marginal: 1176.329595151155\n",
      "    val_log_joint  : 1384.368211829144\n",
      "Train Epoch: 1564 [0/101520 (0%)] Loss: -1173.671509\n",
      "Train Epoch: 1564 [11264/101520 (11%)] Loss: -1174.353027\n",
      "Train Epoch: 1564 [22528/101520 (22%)] Loss: -1171.589111\n",
      "Train Epoch: 1564 [33792/101520 (33%)] Loss: -1178.848633\n",
      "Train Epoch: 1564 [45056/101520 (44%)] Loss: -1175.652832\n",
      "Train Epoch: 1564 [56320/101520 (55%)] Loss: -1176.278442\n",
      "Train Epoch: 1564 [67584/101520 (67%)] Loss: -1174.937012\n",
      "Train Epoch: 1564 [78848/101520 (78%)] Loss: -1178.373169\n",
      "Train Epoch: 1564 [90112/101520 (89%)] Loss: -1173.933594\n",
      "Train Epoch: 1564 [101376/101520 (100%)] Loss: -1167.324707\n",
      "    epoch          : 1564\n",
      "    loss           : -1176.802846630614\n",
      "    ess            : 1.9657770688809342\n",
      "    log_marginal   : 1176.8354982922424\n",
      "    log_joint      : 1385.2623597724953\n",
      "    val_loss       : -1175.300393809443\n",
      "    val_ess        : 1.9673397541046143\n",
      "    val_log_marginal: 1175.331049380095\n",
      "    val_log_joint  : 1383.7465554942255\n",
      "Train Epoch: 1565 [0/101520 (0%)] Loss: -1178.679810\n",
      "Train Epoch: 1565 [11264/101520 (11%)] Loss: -1177.541016\n",
      "Train Epoch: 1565 [22528/101520 (22%)] Loss: -1176.246582\n",
      "Train Epoch: 1565 [33792/101520 (33%)] Loss: -1178.136597\n",
      "Train Epoch: 1565 [45056/101520 (44%)] Loss: -1174.952148\n",
      "Train Epoch: 1565 [56320/101520 (55%)] Loss: -1175.264648\n",
      "Train Epoch: 1565 [67584/101520 (67%)] Loss: -1170.079102\n",
      "Train Epoch: 1565 [78848/101520 (78%)] Loss: -1169.391113\n",
      "Train Epoch: 1565 [90112/101520 (89%)] Loss: -1179.479492\n",
      "Train Epoch: 1565 [101376/101520 (100%)] Loss: -1172.831543\n",
      "    epoch          : 1565\n",
      "    loss           : -1176.862464789769\n",
      "    ess            : 1.9660675950984858\n",
      "    log_marginal   : 1176.8945692819566\n",
      "    log_joint      : 1385.3048874744818\n",
      "    val_loss       : -1175.5240903108017\n",
      "    val_ess        : 1.9700213463410088\n",
      "    val_log_marginal: 1175.5508396314538\n",
      "    val_log_joint  : 1383.944479237432\n",
      "Train Epoch: 1566 [0/101520 (0%)] Loss: -1171.961426\n",
      "Train Epoch: 1566 [11264/101520 (11%)] Loss: -1182.670166\n",
      "Train Epoch: 1566 [22528/101520 (22%)] Loss: -1171.349854\n",
      "Train Epoch: 1566 [33792/101520 (33%)] Loss: -1186.698242\n",
      "Train Epoch: 1566 [45056/101520 (44%)] Loss: -1170.424561\n",
      "Train Epoch: 1566 [56320/101520 (55%)] Loss: -1183.257446\n",
      "Train Epoch: 1566 [67584/101520 (67%)] Loss: -1170.935791\n",
      "Train Epoch: 1566 [78848/101520 (78%)] Loss: -1184.095093\n",
      "Train Epoch: 1566 [90112/101520 (89%)] Loss: -1174.234497\n",
      "Train Epoch: 1566 [101376/101520 (100%)] Loss: -1164.554077\n",
      "    epoch          : 1566\n",
      "    loss           : -1176.8323747644472\n",
      "    ess            : 1.9667616525487086\n",
      "    log_marginal   : 1176.8638008156015\n",
      "    log_joint      : 1385.3183047807397\n",
      "    val_loss       : -1174.9447552224865\n",
      "    val_ess        : 1.967435588007388\n",
      "    val_log_marginal: 1174.9727252462635\n",
      "    val_log_joint  : 1383.527391516644\n",
      "Train Epoch: 1567 [0/101520 (0%)] Loss: -1178.393066\n",
      "Train Epoch: 1567 [11264/101520 (11%)] Loss: -1171.765381\n",
      "Train Epoch: 1567 [22528/101520 (22%)] Loss: -1176.691406\n",
      "Train Epoch: 1567 [33792/101520 (33%)] Loss: -1173.459351\n",
      "Train Epoch: 1567 [45056/101520 (44%)] Loss: -1172.083252\n",
      "Train Epoch: 1567 [56320/101520 (55%)] Loss: -1166.664795\n",
      "Train Epoch: 1567 [67584/101520 (67%)] Loss: -1178.004883\n",
      "Train Epoch: 1567 [78848/101520 (78%)] Loss: -1175.466064\n",
      "Train Epoch: 1567 [90112/101520 (89%)] Loss: -1177.068848\n",
      "Train Epoch: 1567 [101376/101520 (100%)] Loss: -1175.396118\n",
      "    epoch          : 1567\n",
      "    loss           : -1176.945228461644\n",
      "    ess            : 1.966622235187933\n",
      "    log_marginal   : 1176.976680276382\n",
      "    log_joint      : 1385.4000195067133\n",
      "    val_loss       : -1176.4728367017663\n",
      "    val_ess        : 1.960983131242835\n",
      "    val_log_marginal: 1176.5135391898777\n",
      "    val_log_joint  : 1384.9136962890625\n",
      "Train Epoch: 1568 [0/101520 (0%)] Loss: -1172.692017\n",
      "Train Epoch: 1568 [11264/101520 (11%)] Loss: -1176.967163\n",
      "Train Epoch: 1568 [22528/101520 (22%)] Loss: -1173.390625\n",
      "Train Epoch: 1568 [33792/101520 (33%)] Loss: -1178.579712\n",
      "Train Epoch: 1568 [45056/101520 (44%)] Loss: -1173.779541\n",
      "Train Epoch: 1568 [56320/101520 (55%)] Loss: -1175.966431\n",
      "Train Epoch: 1568 [67584/101520 (67%)] Loss: -1170.992798\n",
      "Train Epoch: 1568 [78848/101520 (78%)] Loss: -1183.008179\n",
      "Train Epoch: 1568 [90112/101520 (89%)] Loss: -1178.636719\n",
      "Train Epoch: 1568 [101376/101520 (100%)] Loss: -1175.679932\n",
      "    epoch          : 1568\n",
      "    loss           : -1176.965018574317\n",
      "    ess            : 1.9665795348996493\n",
      "    log_marginal   : 1176.997271513819\n",
      "    log_joint      : 1385.4465123468908\n",
      "    val_loss       : -1175.6300951086957\n",
      "    val_ess        : 1.9664664631304534\n",
      "    val_log_marginal: 1175.6604375424592\n",
      "    val_log_joint  : 1384.0984470533288\n",
      "Train Epoch: 1569 [0/101520 (0%)] Loss: -1171.478027\n",
      "Train Epoch: 1569 [11264/101520 (11%)] Loss: -1175.683960\n",
      "Train Epoch: 1569 [22528/101520 (22%)] Loss: -1168.824463\n",
      "Train Epoch: 1569 [33792/101520 (33%)] Loss: -1175.344971\n",
      "Train Epoch: 1569 [45056/101520 (44%)] Loss: -1175.380249\n",
      "Train Epoch: 1569 [56320/101520 (55%)] Loss: -1170.042969\n",
      "Train Epoch: 1569 [67584/101520 (67%)] Loss: -1177.588623\n",
      "Train Epoch: 1569 [78848/101520 (78%)] Loss: -1178.498291\n",
      "Train Epoch: 1569 [90112/101520 (89%)] Loss: -1171.866943\n",
      "Train Epoch: 1569 [101376/101520 (100%)] Loss: -1188.526489\n",
      "    epoch          : 1569\n",
      "    loss           : -1177.0141399134343\n",
      "    ess            : 1.9661258710688683\n",
      "    log_marginal   : 1177.046262194763\n",
      "    log_joint      : 1385.4474410627356\n",
      "    val_loss       : -1175.8899774966033\n",
      "    val_ess        : 1.9673519341841987\n",
      "    val_log_marginal: 1175.9215884001358\n",
      "    val_log_joint  : 1384.5646335767663\n",
      "Train Epoch: 1570 [0/101520 (0%)] Loss: -1173.412476\n",
      "Train Epoch: 1570 [11264/101520 (11%)] Loss: -1169.918701\n",
      "Train Epoch: 1570 [22528/101520 (22%)] Loss: -1178.652832\n",
      "Train Epoch: 1570 [33792/101520 (33%)] Loss: -1178.933838\n",
      "Train Epoch: 1570 [45056/101520 (44%)] Loss: -1175.310181\n",
      "Train Epoch: 1570 [56320/101520 (55%)] Loss: -1171.886353\n",
      "Train Epoch: 1570 [67584/101520 (67%)] Loss: -1176.923096\n",
      "Train Epoch: 1570 [78848/101520 (78%)] Loss: -1175.816895\n",
      "Train Epoch: 1570 [90112/101520 (89%)] Loss: -1180.108643\n",
      "Train Epoch: 1570 [101376/101520 (100%)] Loss: -1187.760010\n",
      "    epoch          : 1570\n",
      "    loss           : -1177.0331706138113\n",
      "    ess            : 1.9670738083633346\n",
      "    log_marginal   : 1177.064543910961\n",
      "    log_joint      : 1385.513117957954\n",
      "    val_loss       : -1176.1996168053668\n",
      "    val_ess        : 1.9665193091268125\n",
      "    val_log_marginal: 1176.229486880095\n",
      "    val_log_joint  : 1384.603664232337\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1570.pth ...\n",
      "Train Epoch: 1571 [0/101520 (0%)] Loss: -1180.152588\n",
      "Train Epoch: 1571 [11264/101520 (11%)] Loss: -1178.098389\n",
      "Train Epoch: 1571 [22528/101520 (22%)] Loss: -1184.169067\n",
      "Train Epoch: 1571 [33792/101520 (33%)] Loss: -1179.165771\n",
      "Train Epoch: 1571 [45056/101520 (44%)] Loss: -1179.074951\n",
      "Train Epoch: 1571 [56320/101520 (55%)] Loss: -1174.120117\n",
      "Train Epoch: 1571 [67584/101520 (67%)] Loss: -1172.106201\n",
      "Train Epoch: 1571 [78848/101520 (78%)] Loss: -1178.124390\n",
      "Train Epoch: 1571 [90112/101520 (89%)] Loss: -1177.494751\n",
      "Train Epoch: 1571 [101376/101520 (100%)] Loss: -1177.829102\n",
      "    epoch          : 1571\n",
      "    loss           : -1177.0265640948885\n",
      "    ess            : 1.9659109487006412\n",
      "    log_marginal   : 1177.0588728554883\n",
      "    log_joint      : 1385.5079989792714\n",
      "    val_loss       : -1176.3570609714675\n",
      "    val_ess        : 1.9684321310209192\n",
      "    val_log_marginal: 1176.385009765625\n",
      "    val_log_joint  : 1384.9007037618885\n",
      "Train Epoch: 1572 [0/101520 (0%)] Loss: -1179.842651\n",
      "Train Epoch: 1572 [11264/101520 (11%)] Loss: -1170.285034\n",
      "Train Epoch: 1572 [22528/101520 (22%)] Loss: -1173.110474\n",
      "Train Epoch: 1572 [33792/101520 (33%)] Loss: -1173.919922\n",
      "Train Epoch: 1572 [45056/101520 (44%)] Loss: -1172.359985\n",
      "Train Epoch: 1572 [56320/101520 (55%)] Loss: -1169.197510\n",
      "Train Epoch: 1572 [67584/101520 (67%)] Loss: -1176.069580\n",
      "Train Epoch: 1572 [78848/101520 (78%)] Loss: -1171.623047\n",
      "Train Epoch: 1572 [90112/101520 (89%)] Loss: -1174.908081\n",
      "Train Epoch: 1572 [101376/101520 (100%)] Loss: -1173.467285\n",
      "    epoch          : 1572\n",
      "    loss           : -1177.0441581687735\n",
      "    ess            : 1.966647827445562\n",
      "    log_marginal   : 1177.0754670569645\n",
      "    log_joint      : 1385.5266867786197\n",
      "    val_loss       : -1176.1619819972825\n",
      "    val_ess        : 1.9691140185231748\n",
      "    val_log_marginal: 1176.1906844429348\n",
      "    val_log_joint  : 1384.4022376019022\n",
      "Train Epoch: 1573 [0/101520 (0%)] Loss: -1181.370972\n",
      "Train Epoch: 1573 [11264/101520 (11%)] Loss: -1175.240967\n",
      "Train Epoch: 1573 [22528/101520 (22%)] Loss: -1172.485352\n",
      "Train Epoch: 1573 [33792/101520 (33%)] Loss: -1178.364258\n",
      "Train Epoch: 1573 [45056/101520 (44%)] Loss: -1178.290527\n",
      "Train Epoch: 1573 [56320/101520 (55%)] Loss: -1178.224121\n",
      "Train Epoch: 1573 [67584/101520 (67%)] Loss: -1177.520508\n",
      "Train Epoch: 1573 [78848/101520 (78%)] Loss: -1176.094971\n",
      "Train Epoch: 1573 [90112/101520 (89%)] Loss: -1172.833740\n",
      "Train Epoch: 1573 [101376/101520 (100%)] Loss: -1174.513306\n",
      "    epoch          : 1573\n",
      "    loss           : -1177.0984168891332\n",
      "    ess            : 1.9668806162311803\n",
      "    log_marginal   : 1177.1297797581658\n",
      "    log_joint      : 1385.5722729860238\n",
      "    val_loss       : -1176.8733653192935\n",
      "    val_ess        : 1.9642919042836064\n",
      "    val_log_marginal: 1176.9071363366168\n",
      "    val_log_joint  : 1385.1907852836277\n",
      "Train Epoch: 1574 [0/101520 (0%)] Loss: -1172.913818\n",
      "Train Epoch: 1574 [11264/101520 (11%)] Loss: -1181.411499\n",
      "Train Epoch: 1574 [22528/101520 (22%)] Loss: -1176.230591\n",
      "Train Epoch: 1574 [33792/101520 (33%)] Loss: -1165.293945\n",
      "Train Epoch: 1574 [45056/101520 (44%)] Loss: -1184.580078\n",
      "Train Epoch: 1574 [56320/101520 (55%)] Loss: -1178.676758\n",
      "Train Epoch: 1574 [67584/101520 (67%)] Loss: -1166.691162\n",
      "Train Epoch: 1574 [78848/101520 (78%)] Loss: -1177.489624\n",
      "Train Epoch: 1574 [90112/101520 (89%)] Loss: -1183.518311\n",
      "Train Epoch: 1574 [101376/101520 (100%)] Loss: -1184.051880\n",
      "    epoch          : 1574\n",
      "    loss           : -1177.179926119857\n",
      "    ess            : 1.966623346410205\n",
      "    log_marginal   : 1177.2118619219143\n",
      "    log_joint      : 1385.607646386228\n",
      "    val_loss       : -1175.949027683424\n",
      "    val_ess        : 1.9642802580543186\n",
      "    val_log_marginal: 1175.983207370924\n",
      "    val_log_joint  : 1384.7870934527853\n",
      "Train Epoch: 1575 [0/101520 (0%)] Loss: -1174.378174\n",
      "Train Epoch: 1575 [11264/101520 (11%)] Loss: -1179.192505\n",
      "Train Epoch: 1575 [22528/101520 (22%)] Loss: -1174.638794\n",
      "Train Epoch: 1575 [33792/101520 (33%)] Loss: -1180.222656\n",
      "Train Epoch: 1575 [45056/101520 (44%)] Loss: -1175.651978\n",
      "Train Epoch: 1575 [56320/101520 (55%)] Loss: -1182.355347\n",
      "Train Epoch: 1575 [67584/101520 (67%)] Loss: -1177.581543\n",
      "Train Epoch: 1575 [78848/101520 (78%)] Loss: -1179.773682\n",
      "Train Epoch: 1575 [90112/101520 (89%)] Loss: -1179.550293\n",
      "Train Epoch: 1575 [101376/101520 (100%)] Loss: -1177.795166\n",
      "    epoch          : 1575\n",
      "    loss           : -1177.1504936793342\n",
      "    ess            : 1.9663024979021082\n",
      "    log_marginal   : 1177.1826730085977\n",
      "    log_joint      : 1385.60677839883\n",
      "    val_loss       : -1174.9768384850543\n",
      "    val_ess        : 1.9645728753960652\n",
      "    val_log_marginal: 1175.0139000934103\n",
      "    val_log_joint  : 1383.5651537024457\n",
      "Train Epoch: 1576 [0/101520 (0%)] Loss: -1176.419678\n",
      "Train Epoch: 1576 [11264/101520 (11%)] Loss: -1174.986328\n",
      "Train Epoch: 1576 [22528/101520 (22%)] Loss: -1173.921143\n",
      "Train Epoch: 1576 [33792/101520 (33%)] Loss: -1181.217285\n",
      "Train Epoch: 1576 [45056/101520 (44%)] Loss: -1183.897705\n",
      "Train Epoch: 1576 [56320/101520 (55%)] Loss: -1182.812500\n",
      "Train Epoch: 1576 [67584/101520 (67%)] Loss: -1175.946045\n",
      "Train Epoch: 1576 [78848/101520 (78%)] Loss: -1183.328369\n",
      "Train Epoch: 1576 [90112/101520 (89%)] Loss: -1178.667480\n",
      "Train Epoch: 1576 [101376/101520 (100%)] Loss: -1184.049438\n",
      "    epoch          : 1576\n",
      "    loss           : -1177.2299455038867\n",
      "    ess            : 1.966558354583817\n",
      "    log_marginal   : 1177.2616782643688\n",
      "    log_joint      : 1385.7230923906643\n",
      "    val_loss       : -1173.7558381453805\n",
      "    val_ess        : 1.9666581827661265\n",
      "    val_log_marginal: 1173.7907820991848\n",
      "    val_log_joint  : 1382.7939559273098\n",
      "Train Epoch: 1577 [0/101520 (0%)] Loss: -1178.878662\n",
      "Train Epoch: 1577 [11264/101520 (11%)] Loss: -1181.141846\n",
      "Train Epoch: 1577 [22528/101520 (22%)] Loss: -1181.656006\n",
      "Train Epoch: 1577 [33792/101520 (33%)] Loss: -1174.352539\n",
      "Train Epoch: 1577 [45056/101520 (44%)] Loss: -1178.930420\n",
      "Train Epoch: 1577 [56320/101520 (55%)] Loss: -1167.826904\n",
      "Train Epoch: 1577 [67584/101520 (67%)] Loss: -1174.230103\n",
      "Train Epoch: 1577 [78848/101520 (78%)] Loss: -1184.008057\n",
      "Train Epoch: 1577 [90112/101520 (89%)] Loss: -1185.721924\n",
      "Train Epoch: 1577 [101376/101520 (100%)] Loss: -1177.228271\n",
      "    epoch          : 1577\n",
      "    loss           : -1177.35643611122\n",
      "    ess            : 1.9662318205713627\n",
      "    log_marginal   : 1177.3893178048445\n",
      "    log_joint      : 1385.7086304324357\n",
      "    val_loss       : -1176.4065578294837\n",
      "    val_ess        : 1.9608279414798901\n",
      "    val_log_marginal: 1176.4437839673913\n",
      "    val_log_joint  : 1384.9986519191575\n",
      "Train Epoch: 1578 [0/101520 (0%)] Loss: -1175.413452\n",
      "Train Epoch: 1578 [11264/101520 (11%)] Loss: -1186.403564\n",
      "Train Epoch: 1578 [22528/101520 (22%)] Loss: -1175.413086\n",
      "Train Epoch: 1578 [33792/101520 (33%)] Loss: -1176.898682\n",
      "Train Epoch: 1578 [45056/101520 (44%)] Loss: -1183.162476\n",
      "Train Epoch: 1578 [56320/101520 (55%)] Loss: -1177.669678\n",
      "Train Epoch: 1578 [67584/101520 (67%)] Loss: -1180.081299\n",
      "Train Epoch: 1578 [78848/101520 (78%)] Loss: -1175.281738\n",
      "Train Epoch: 1578 [90112/101520 (89%)] Loss: -1162.696045\n",
      "Train Epoch: 1578 [101376/101520 (100%)] Loss: -1168.113037\n",
      "    epoch          : 1578\n",
      "    loss           : -1177.2086040554334\n",
      "    ess            : 1.9661775348174513\n",
      "    log_marginal   : 1177.240803014094\n",
      "    log_joint      : 1385.6800089313756\n",
      "    val_loss       : -1176.7770624575408\n",
      "    val_ess        : 1.9681916547858196\n",
      "    val_log_marginal: 1176.8079462466033\n",
      "    val_log_joint  : 1385.5180876358695\n",
      "Train Epoch: 1579 [0/101520 (0%)] Loss: -1177.990112\n",
      "Train Epoch: 1579 [11264/101520 (11%)] Loss: -1178.419312\n",
      "Train Epoch: 1579 [22528/101520 (22%)] Loss: -1176.804321\n",
      "Train Epoch: 1579 [33792/101520 (33%)] Loss: -1174.556396\n",
      "Train Epoch: 1579 [45056/101520 (44%)] Loss: -1172.461670\n",
      "Train Epoch: 1579 [56320/101520 (55%)] Loss: -1179.885132\n",
      "Train Epoch: 1579 [67584/101520 (67%)] Loss: -1179.672119\n",
      "Train Epoch: 1579 [78848/101520 (78%)] Loss: -1172.781128\n",
      "Train Epoch: 1579 [90112/101520 (89%)] Loss: -1180.535645\n",
      "Train Epoch: 1579 [101376/101520 (100%)] Loss: -1186.395874\n",
      "    epoch          : 1579\n",
      "    loss           : -1177.358628469496\n",
      "    ess            : 1.9670652121155705\n",
      "    log_marginal   : 1177.3901624823336\n",
      "    log_joint      : 1385.7795563510913\n",
      "    val_loss       : -1176.8325832201087\n",
      "    val_ess        : 1.9666047873704329\n",
      "    val_log_marginal: 1176.863673997962\n",
      "    val_log_joint  : 1385.5227263077445\n",
      "Train Epoch: 1580 [0/101520 (0%)] Loss: -1178.174072\n",
      "Train Epoch: 1580 [11264/101520 (11%)] Loss: -1182.255127\n",
      "Train Epoch: 1580 [22528/101520 (22%)] Loss: -1180.756836\n",
      "Train Epoch: 1580 [33792/101520 (33%)] Loss: -1182.375000\n",
      "Train Epoch: 1580 [45056/101520 (44%)] Loss: -1172.950073\n",
      "Train Epoch: 1580 [56320/101520 (55%)] Loss: -1172.608887\n",
      "Train Epoch: 1580 [67584/101520 (67%)] Loss: -1180.669922\n",
      "Train Epoch: 1580 [78848/101520 (78%)] Loss: -1180.509644\n",
      "Train Epoch: 1580 [90112/101520 (89%)] Loss: -1177.432617\n",
      "Train Epoch: 1580 [101376/101520 (100%)] Loss: -1182.506226\n",
      "    epoch          : 1580\n",
      "    loss           : -1177.339335839353\n",
      "    ess            : 1.967074182165328\n",
      "    log_marginal   : 1177.370623257891\n",
      "    log_joint      : 1385.820243183692\n",
      "    val_loss       : -1175.5515455163043\n",
      "    val_ess        : 1.9634864641272503\n",
      "    val_log_marginal: 1175.5866062330163\n",
      "    val_log_joint  : 1384.0104874320652\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1580.pth ...\n",
      "Train Epoch: 1581 [0/101520 (0%)] Loss: -1177.117920\n",
      "Train Epoch: 1581 [11264/101520 (11%)] Loss: -1175.239380\n",
      "Train Epoch: 1581 [22528/101520 (22%)] Loss: -1170.745239\n",
      "Train Epoch: 1581 [33792/101520 (33%)] Loss: -1168.667725\n",
      "Train Epoch: 1581 [45056/101520 (44%)] Loss: -1177.770752\n",
      "Train Epoch: 1581 [56320/101520 (55%)] Loss: -1185.202393\n",
      "Train Epoch: 1581 [67584/101520 (67%)] Loss: -1172.208130\n",
      "Train Epoch: 1581 [78848/101520 (78%)] Loss: -1177.556152\n",
      "Train Epoch: 1581 [90112/101520 (89%)] Loss: -1177.657227\n",
      "Train Epoch: 1581 [101376/101520 (100%)] Loss: -1164.824707\n",
      "    epoch          : 1581\n",
      "    loss           : -1177.3089157947943\n",
      "    ess            : 1.9660945435864243\n",
      "    log_marginal   : 1177.3418404277245\n",
      "    log_joint      : 1385.7763917242462\n",
      "    val_loss       : -1176.0978685461957\n",
      "    val_ess        : 1.964270679847054\n",
      "    val_log_marginal: 1176.1317987856658\n",
      "    val_log_joint  : 1384.6155528192935\n",
      "Train Epoch: 1582 [0/101520 (0%)] Loss: -1182.776367\n",
      "Train Epoch: 1582 [11264/101520 (11%)] Loss: -1181.719238\n",
      "Train Epoch: 1582 [22528/101520 (22%)] Loss: -1177.434937\n",
      "Train Epoch: 1582 [33792/101520 (33%)] Loss: -1170.574219\n",
      "Train Epoch: 1582 [45056/101520 (44%)] Loss: -1180.841431\n",
      "Train Epoch: 1582 [56320/101520 (55%)] Loss: -1177.425781\n",
      "Train Epoch: 1582 [67584/101520 (67%)] Loss: -1185.511230\n",
      "Train Epoch: 1582 [78848/101520 (78%)] Loss: -1178.025269\n",
      "Train Epoch: 1582 [90112/101520 (89%)] Loss: -1183.090210\n",
      "Train Epoch: 1582 [101376/101520 (100%)] Loss: -1175.539551\n",
      "    epoch          : 1582\n",
      "    loss           : -1177.4031927214196\n",
      "    ess            : 1.966286172818898\n",
      "    log_marginal   : 1177.4348506448257\n",
      "    log_joint      : 1385.795110807946\n",
      "    val_loss       : -1176.9759839928668\n",
      "    val_ess        : 1.9642050421756247\n",
      "    val_log_marginal: 1177.0117346722147\n",
      "    val_log_joint  : 1385.4014892578125\n",
      "Train Epoch: 1583 [0/101520 (0%)] Loss: -1185.738281\n",
      "Train Epoch: 1583 [11264/101520 (11%)] Loss: -1171.216309\n",
      "Train Epoch: 1583 [22528/101520 (22%)] Loss: -1182.805664\n",
      "Train Epoch: 1583 [33792/101520 (33%)] Loss: -1182.867798\n",
      "Train Epoch: 1583 [45056/101520 (44%)] Loss: -1174.181152\n",
      "Train Epoch: 1583 [56320/101520 (55%)] Loss: -1163.813477\n",
      "Train Epoch: 1583 [67584/101520 (67%)] Loss: -1180.540161\n",
      "Train Epoch: 1583 [78848/101520 (78%)] Loss: -1172.816650\n",
      "Train Epoch: 1583 [90112/101520 (89%)] Loss: -1179.920410\n",
      "Train Epoch: 1583 [101376/101520 (100%)] Loss: -1181.195923\n",
      "    epoch          : 1583\n",
      "    loss           : -1177.3945735758873\n",
      "    ess            : 1.9662798744949264\n",
      "    log_marginal   : 1177.4263909881438\n",
      "    log_joint      : 1385.851269899301\n",
      "    val_loss       : -1176.050484035326\n",
      "    val_ess        : 1.9655877766401872\n",
      "    val_log_marginal: 1176.0836234714675\n",
      "    val_log_joint  : 1384.3671662703805\n",
      "Train Epoch: 1584 [0/101520 (0%)] Loss: -1172.898193\n",
      "Train Epoch: 1584 [11264/101520 (11%)] Loss: -1177.228271\n",
      "Train Epoch: 1584 [22528/101520 (22%)] Loss: -1181.155884\n",
      "Train Epoch: 1584 [33792/101520 (33%)] Loss: -1177.700684\n",
      "Train Epoch: 1584 [45056/101520 (44%)] Loss: -1180.544678\n",
      "Train Epoch: 1584 [56320/101520 (55%)] Loss: -1179.735107\n",
      "Train Epoch: 1584 [67584/101520 (67%)] Loss: -1176.574829\n",
      "Train Epoch: 1584 [78848/101520 (78%)] Loss: -1175.109619\n",
      "Train Epoch: 1584 [90112/101520 (89%)] Loss: -1177.317139\n",
      "Train Epoch: 1584 [101376/101520 (100%)] Loss: -1181.428345\n",
      "    epoch          : 1584\n",
      "    loss           : -1177.4431428382145\n",
      "    ess            : 1.9670565128326416\n",
      "    log_marginal   : 1177.47427628867\n",
      "    log_joint      : 1385.8929474030308\n",
      "    val_loss       : -1176.5025634765625\n",
      "    val_ess        : 1.9675670654877373\n",
      "    val_log_marginal: 1176.531446373981\n",
      "    val_log_joint  : 1384.866163170856\n",
      "Train Epoch: 1585 [0/101520 (0%)] Loss: -1181.263428\n",
      "Train Epoch: 1585 [11264/101520 (11%)] Loss: -1185.375244\n",
      "Train Epoch: 1585 [22528/101520 (22%)] Loss: -1183.000122\n",
      "Train Epoch: 1585 [33792/101520 (33%)] Loss: -1173.007812\n",
      "Train Epoch: 1585 [45056/101520 (44%)] Loss: -1182.197876\n",
      "Train Epoch: 1585 [56320/101520 (55%)] Loss: -1173.465820\n",
      "Train Epoch: 1585 [67584/101520 (67%)] Loss: -1176.227905\n",
      "Train Epoch: 1585 [78848/101520 (78%)] Loss: -1179.069214\n",
      "Train Epoch: 1585 [90112/101520 (89%)] Loss: -1171.885498\n",
      "Train Epoch: 1585 [101376/101520 (100%)] Loss: -1173.618896\n",
      "    epoch          : 1585\n",
      "    loss           : -1177.4760214647456\n",
      "    ess            : 1.966013931748855\n",
      "    log_marginal   : 1177.508106327536\n",
      "    log_joint      : 1385.921413095752\n",
      "    val_loss       : -1177.031244692595\n",
      "    val_ess        : 1.968233144801596\n",
      "    val_log_marginal: 1177.0591244904892\n",
      "    val_log_joint  : 1385.1905995244565\n",
      "Train Epoch: 1586 [0/101520 (0%)] Loss: -1177.513062\n",
      "Train Epoch: 1586 [11264/101520 (11%)] Loss: -1183.486572\n",
      "Train Epoch: 1586 [22528/101520 (22%)] Loss: -1182.139404\n",
      "Train Epoch: 1586 [33792/101520 (33%)] Loss: -1176.017456\n",
      "Train Epoch: 1586 [45056/101520 (44%)] Loss: -1176.310791\n",
      "Train Epoch: 1586 [56320/101520 (55%)] Loss: -1182.489624\n",
      "Train Epoch: 1586 [67584/101520 (67%)] Loss: -1181.460449\n",
      "Train Epoch: 1586 [78848/101520 (78%)] Loss: -1178.239258\n",
      "Train Epoch: 1586 [90112/101520 (89%)] Loss: -1181.558105\n",
      "Train Epoch: 1586 [101376/101520 (100%)] Loss: -1185.814087\n",
      "    epoch          : 1586\n",
      "    loss           : -1177.5197336781564\n",
      "    ess            : 1.9662697135503568\n",
      "    log_marginal   : 1177.5511167900047\n",
      "    log_joint      : 1386.00083056886\n",
      "    val_loss       : -1174.81177288553\n",
      "    val_ess        : 1.9654955190161\n",
      "    val_log_marginal: 1174.8414731233017\n",
      "    val_log_joint  : 1383.2722804857337\n",
      "Train Epoch: 1587 [0/101520 (0%)] Loss: -1178.708496\n",
      "Train Epoch: 1587 [11264/101520 (11%)] Loss: -1185.386597\n",
      "Train Epoch: 1587 [22528/101520 (22%)] Loss: -1177.057251\n",
      "Train Epoch: 1587 [33792/101520 (33%)] Loss: -1181.076538\n",
      "Train Epoch: 1587 [45056/101520 (44%)] Loss: -1174.420166\n",
      "Train Epoch: 1587 [56320/101520 (55%)] Loss: -1176.946655\n",
      "Train Epoch: 1587 [67584/101520 (67%)] Loss: -1173.024170\n",
      "Train Epoch: 1587 [78848/101520 (78%)] Loss: -1174.509644\n",
      "Train Epoch: 1587 [90112/101520 (89%)] Loss: -1179.644409\n",
      "Train Epoch: 1587 [101376/101520 (100%)] Loss: -1164.840088\n",
      "    epoch          : 1587\n",
      "    loss           : -1177.5417412992697\n",
      "    ess            : 1.9651750883265355\n",
      "    log_marginal   : 1177.5759559516332\n",
      "    log_joint      : 1386.0032081795698\n",
      "    val_loss       : -1177.0555366847825\n",
      "    val_ess        : 1.9684457156969153\n",
      "    val_log_marginal: 1177.0828857421875\n",
      "    val_log_joint  : 1385.3855192764945\n",
      "Train Epoch: 1588 [0/101520 (0%)] Loss: -1176.944946\n",
      "Train Epoch: 1588 [11264/101520 (11%)] Loss: -1178.771973\n",
      "Train Epoch: 1588 [22528/101520 (22%)] Loss: -1177.047607\n",
      "Train Epoch: 1588 [33792/101520 (33%)] Loss: -1183.150146\n",
      "Train Epoch: 1588 [45056/101520 (44%)] Loss: -1184.151001\n",
      "Train Epoch: 1588 [56320/101520 (55%)] Loss: -1175.333984\n",
      "Train Epoch: 1588 [67584/101520 (67%)] Loss: -1179.915894\n",
      "Train Epoch: 1588 [78848/101520 (78%)] Loss: -1170.911499\n",
      "Train Epoch: 1588 [90112/101520 (89%)] Loss: -1182.329590\n",
      "Train Epoch: 1588 [101376/101520 (100%)] Loss: -1180.672485\n",
      "    epoch          : 1588\n",
      "    loss           : -1177.6938611514604\n",
      "    ess            : 1.9665920267153025\n",
      "    log_marginal   : 1177.7263864488457\n",
      "    log_joint      : 1386.1042989606235\n",
      "    val_loss       : -1178.0718516474185\n",
      "    val_ess        : 1.9672337355821028\n",
      "    val_log_marginal: 1178.105612049932\n",
      "    val_log_joint  : 1386.4005286175272\n",
      "Train Epoch: 1589 [0/101520 (0%)] Loss: -1178.238770\n",
      "Train Epoch: 1589 [11264/101520 (11%)] Loss: -1176.374146\n",
      "Train Epoch: 1589 [22528/101520 (22%)] Loss: -1172.428467\n",
      "Train Epoch: 1589 [33792/101520 (33%)] Loss: -1182.867432\n",
      "Train Epoch: 1589 [45056/101520 (44%)] Loss: -1173.469238\n",
      "Train Epoch: 1589 [56320/101520 (55%)] Loss: -1173.403442\n",
      "Train Epoch: 1589 [67584/101520 (67%)] Loss: -1177.184692\n",
      "Train Epoch: 1589 [78848/101520 (78%)] Loss: -1179.353760\n",
      "Train Epoch: 1589 [90112/101520 (89%)] Loss: -1169.700439\n",
      "Train Epoch: 1589 [101376/101520 (100%)] Loss: -1182.046631\n",
      "    epoch          : 1589\n",
      "    loss           : -1177.6099295304648\n",
      "    ess            : 1.9663543108120636\n",
      "    log_marginal   : 1177.641644501806\n",
      "    log_joint      : 1386.110442348461\n",
      "    val_loss       : -1175.6032396399457\n",
      "    val_ess        : 1.9671376736267754\n",
      "    val_log_marginal: 1175.6356625764267\n",
      "    val_log_joint  : 1384.2224227241848\n",
      "Train Epoch: 1590 [0/101520 (0%)] Loss: -1179.115479\n",
      "Train Epoch: 1590 [11264/101520 (11%)] Loss: -1180.842529\n",
      "Train Epoch: 1590 [22528/101520 (22%)] Loss: -1173.896118\n",
      "Train Epoch: 1590 [33792/101520 (33%)] Loss: -1176.841187\n",
      "Train Epoch: 1590 [45056/101520 (44%)] Loss: -1178.415039\n",
      "Train Epoch: 1590 [56320/101520 (55%)] Loss: -1178.662842\n",
      "Train Epoch: 1590 [67584/101520 (67%)] Loss: -1182.341309\n",
      "Train Epoch: 1590 [78848/101520 (78%)] Loss: -1177.850342\n",
      "Train Epoch: 1590 [90112/101520 (89%)] Loss: -1178.183228\n",
      "Train Epoch: 1590 [101376/101520 (100%)] Loss: -1164.234009\n",
      "    epoch          : 1590\n",
      "    loss           : -1177.6147718573336\n",
      "    ess            : 1.9659272743828933\n",
      "    log_marginal   : 1177.6469775635992\n",
      "    log_joint      : 1386.058057008676\n",
      "    val_loss       : -1175.5352358610733\n",
      "    val_ess        : 1.9670544966407444\n",
      "    val_log_marginal: 1175.5636463994565\n",
      "    val_log_joint  : 1384.0897588315217\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1590.pth ...\n",
      "Train Epoch: 1591 [0/101520 (0%)] Loss: -1175.437744\n",
      "Train Epoch: 1591 [11264/101520 (11%)] Loss: -1174.750000\n",
      "Train Epoch: 1591 [22528/101520 (22%)] Loss: -1169.451660\n",
      "Train Epoch: 1591 [33792/101520 (33%)] Loss: -1172.070190\n",
      "Train Epoch: 1591 [45056/101520 (44%)] Loss: -1180.618896\n",
      "Train Epoch: 1591 [56320/101520 (55%)] Loss: -1184.357666\n",
      "Train Epoch: 1591 [67584/101520 (67%)] Loss: -1173.732544\n",
      "Train Epoch: 1591 [78848/101520 (78%)] Loss: -1182.087891\n",
      "Train Epoch: 1591 [90112/101520 (89%)] Loss: -1177.076782\n",
      "Train Epoch: 1591 [101376/101520 (100%)] Loss: -1178.734253\n",
      "    epoch          : 1591\n",
      "    loss           : -1177.651119366363\n",
      "    ess            : 1.9671303752678722\n",
      "    log_marginal   : 1177.6816056601367\n",
      "    log_joint      : 1386.124631335388\n",
      "    val_loss       : -1176.5274817425272\n",
      "    val_ess        : 1.9658098117164944\n",
      "    val_log_marginal: 1176.5593845533288\n",
      "    val_log_joint  : 1384.9491815981658\n",
      "Train Epoch: 1592 [0/101520 (0%)] Loss: -1176.457275\n",
      "Train Epoch: 1592 [11264/101520 (11%)] Loss: -1181.556396\n",
      "Train Epoch: 1592 [22528/101520 (22%)] Loss: -1173.885498\n",
      "Train Epoch: 1592 [33792/101520 (33%)] Loss: -1181.677734\n",
      "Train Epoch: 1592 [45056/101520 (44%)] Loss: -1178.373535\n",
      "Train Epoch: 1592 [56320/101520 (55%)] Loss: -1176.325439\n",
      "Train Epoch: 1592 [67584/101520 (67%)] Loss: -1181.712402\n",
      "Train Epoch: 1592 [78848/101520 (78%)] Loss: -1185.658813\n",
      "Train Epoch: 1592 [90112/101520 (89%)] Loss: -1174.845947\n",
      "Train Epoch: 1592 [101376/101520 (100%)] Loss: -1171.159790\n",
      "    epoch          : 1592\n",
      "    loss           : -1177.6702598686793\n",
      "    ess            : 1.9662794018510599\n",
      "    log_marginal   : 1177.703091261974\n",
      "    log_joint      : 1386.140140399262\n",
      "    val_loss       : -1176.7100936226223\n",
      "    val_ess        : 1.96325625544009\n",
      "    val_log_marginal: 1176.7496284816575\n",
      "    val_log_joint  : 1385.184373938519\n",
      "Train Epoch: 1593 [0/101520 (0%)] Loss: -1178.157227\n",
      "Train Epoch: 1593 [11264/101520 (11%)] Loss: -1174.590942\n",
      "Train Epoch: 1593 [22528/101520 (22%)] Loss: -1177.931885\n",
      "Train Epoch: 1593 [33792/101520 (33%)] Loss: -1183.197510\n",
      "Train Epoch: 1593 [45056/101520 (44%)] Loss: -1173.612183\n",
      "Train Epoch: 1593 [56320/101520 (55%)] Loss: -1183.768188\n",
      "Train Epoch: 1593 [67584/101520 (67%)] Loss: -1178.423340\n",
      "Train Epoch: 1593 [78848/101520 (78%)] Loss: -1173.595459\n",
      "Train Epoch: 1593 [90112/101520 (89%)] Loss: -1176.179199\n",
      "Train Epoch: 1593 [101376/101520 (100%)] Loss: -1193.343140\n",
      "    epoch          : 1593\n",
      "    loss           : -1177.8200726533055\n",
      "    ess            : 1.9663198066117176\n",
      "    log_marginal   : 1177.8519017205167\n",
      "    log_joint      : 1386.276322407938\n",
      "    val_loss       : -1176.5069155485733\n",
      "    val_ess        : 1.9675437885781992\n",
      "    val_log_marginal: 1176.5370934527853\n",
      "    val_log_joint  : 1384.949802564538\n",
      "Train Epoch: 1594 [0/101520 (0%)] Loss: -1182.160645\n",
      "Train Epoch: 1594 [11264/101520 (11%)] Loss: -1174.301758\n",
      "Train Epoch: 1594 [22528/101520 (22%)] Loss: -1176.465698\n",
      "Train Epoch: 1594 [33792/101520 (33%)] Loss: -1170.698242\n",
      "Train Epoch: 1594 [45056/101520 (44%)] Loss: -1180.502075\n",
      "Train Epoch: 1594 [56320/101520 (55%)] Loss: -1178.446899\n",
      "Train Epoch: 1594 [67584/101520 (67%)] Loss: -1169.946777\n",
      "Train Epoch: 1594 [78848/101520 (78%)] Loss: -1176.319580\n",
      "Train Epoch: 1594 [90112/101520 (89%)] Loss: -1173.091553\n",
      "Train Epoch: 1594 [101376/101520 (100%)] Loss: -1182.237061\n",
      "    epoch          : 1594\n",
      "    loss           : -1177.8284807828204\n",
      "    ess            : 1.9663807441241776\n",
      "    log_marginal   : 1177.8603509490813\n",
      "    log_joint      : 1386.3321839912453\n",
      "    val_loss       : -1177.0327997622283\n",
      "    val_ess        : 1.9678411898405657\n",
      "    val_log_marginal: 1177.0595862347147\n",
      "    val_log_joint  : 1385.3639765200408\n",
      "Train Epoch: 1595 [0/101520 (0%)] Loss: -1179.529541\n",
      "Train Epoch: 1595 [11264/101520 (11%)] Loss: -1173.611938\n",
      "Train Epoch: 1595 [22528/101520 (22%)] Loss: -1176.163330\n",
      "Train Epoch: 1595 [33792/101520 (33%)] Loss: -1176.250000\n",
      "Train Epoch: 1595 [45056/101520 (44%)] Loss: -1178.328857\n",
      "Train Epoch: 1595 [56320/101520 (55%)] Loss: -1182.105957\n",
      "Train Epoch: 1595 [67584/101520 (67%)] Loss: -1182.887085\n",
      "Train Epoch: 1595 [78848/101520 (78%)] Loss: -1183.842773\n",
      "Train Epoch: 1595 [90112/101520 (89%)] Loss: -1176.651245\n",
      "Train Epoch: 1595 [101376/101520 (100%)] Loss: -1171.907349\n",
      "    epoch          : 1595\n",
      "    loss           : -1177.8128110032585\n",
      "    ess            : 1.9668953700281269\n",
      "    log_marginal   : 1177.8447345369425\n",
      "    log_joint      : 1386.3042409312186\n",
      "    val_loss       : -1176.889260996943\n",
      "    val_ess        : 1.9683761648509814\n",
      "    val_log_marginal: 1176.919189453125\n",
      "    val_log_joint  : 1385.1815610139267\n",
      "Train Epoch: 1596 [0/101520 (0%)] Loss: -1172.568237\n",
      "Train Epoch: 1596 [11264/101520 (11%)] Loss: -1179.791260\n",
      "Train Epoch: 1596 [22528/101520 (22%)] Loss: -1171.826416\n",
      "Train Epoch: 1596 [33792/101520 (33%)] Loss: -1181.366211\n",
      "Train Epoch: 1596 [45056/101520 (44%)] Loss: -1182.542969\n",
      "Train Epoch: 1596 [56320/101520 (55%)] Loss: -1186.685059\n",
      "Train Epoch: 1596 [67584/101520 (67%)] Loss: -1176.884155\n",
      "Train Epoch: 1596 [78848/101520 (78%)] Loss: -1176.674316\n",
      "Train Epoch: 1596 [90112/101520 (89%)] Loss: -1180.982666\n",
      "Train Epoch: 1596 [101376/101520 (100%)] Loss: -1179.338989\n",
      "    epoch          : 1596\n",
      "    loss           : -1177.8917144315326\n",
      "    ess            : 1.9662022087442217\n",
      "    log_marginal   : 1177.924098642627\n",
      "    log_joint      : 1386.2978687382224\n",
      "    val_loss       : -1176.7908988620925\n",
      "    val_ess        : 1.9636986462966255\n",
      "    val_log_marginal: 1176.8261400305707\n",
      "    val_log_joint  : 1385.1284975798233\n",
      "Train Epoch: 1597 [0/101520 (0%)] Loss: -1176.944092\n",
      "Train Epoch: 1597 [11264/101520 (11%)] Loss: -1178.454346\n",
      "Train Epoch: 1597 [22528/101520 (22%)] Loss: -1177.556152\n",
      "Train Epoch: 1597 [33792/101520 (33%)] Loss: -1184.263306\n",
      "Train Epoch: 1597 [45056/101520 (44%)] Loss: -1184.320801\n",
      "Train Epoch: 1597 [56320/101520 (55%)] Loss: -1178.022339\n",
      "Train Epoch: 1597 [67584/101520 (67%)] Loss: -1172.561279\n",
      "Train Epoch: 1597 [78848/101520 (78%)] Loss: -1179.148682\n",
      "Train Epoch: 1597 [90112/101520 (89%)] Loss: -1182.541992\n",
      "Train Epoch: 1597 [101376/101520 (100%)] Loss: -1187.303101\n",
      "    epoch          : 1597\n",
      "    loss           : -1177.960966330677\n",
      "    ess            : 1.9666758781701477\n",
      "    log_marginal   : 1177.992700931415\n",
      "    log_joint      : 1386.444496039769\n",
      "    val_loss       : -1176.9433806046195\n",
      "    val_ess        : 1.9700022158415422\n",
      "    val_log_marginal: 1176.9709313434103\n",
      "    val_log_joint  : 1385.5321469514267\n",
      "Train Epoch: 1598 [0/101520 (0%)] Loss: -1181.323730\n",
      "Train Epoch: 1598 [11264/101520 (11%)] Loss: -1177.918579\n",
      "Train Epoch: 1598 [22528/101520 (22%)] Loss: -1179.922729\n",
      "Train Epoch: 1598 [33792/101520 (33%)] Loss: -1183.392334\n",
      "Train Epoch: 1598 [45056/101520 (44%)] Loss: -1177.335327\n",
      "Train Epoch: 1598 [56320/101520 (55%)] Loss: -1181.526855\n",
      "Train Epoch: 1598 [67584/101520 (67%)] Loss: -1176.704102\n",
      "Train Epoch: 1598 [78848/101520 (78%)] Loss: -1178.202881\n",
      "Train Epoch: 1598 [90112/101520 (89%)] Loss: -1171.790894\n",
      "Train Epoch: 1598 [101376/101520 (100%)] Loss: -1180.028564\n",
      "    epoch          : 1598\n",
      "    loss           : -1177.9955514879082\n",
      "    ess            : 1.9661969695258978\n",
      "    log_marginal   : 1178.0273731940954\n",
      "    log_joint      : 1386.4235361377198\n",
      "    val_loss       : -1176.8309644616168\n",
      "    val_ess        : 1.9656411720358806\n",
      "    val_log_marginal: 1176.8673095703125\n",
      "    val_log_joint  : 1385.438428795856\n",
      "Train Epoch: 1599 [0/101520 (0%)] Loss: -1186.250244\n",
      "Train Epoch: 1599 [11264/101520 (11%)] Loss: -1180.340088\n",
      "Train Epoch: 1599 [22528/101520 (22%)] Loss: -1178.906006\n",
      "Train Epoch: 1599 [33792/101520 (33%)] Loss: -1175.594360\n",
      "Train Epoch: 1599 [45056/101520 (44%)] Loss: -1178.826538\n",
      "Train Epoch: 1599 [56320/101520 (55%)] Loss: -1178.291748\n",
      "Train Epoch: 1599 [67584/101520 (67%)] Loss: -1175.131714\n",
      "Train Epoch: 1599 [78848/101520 (78%)] Loss: -1177.199707\n",
      "Train Epoch: 1599 [90112/101520 (89%)] Loss: -1174.968750\n",
      "Train Epoch: 1599 [101376/101520 (100%)] Loss: -1166.831421\n",
      "    epoch          : 1599\n",
      "    loss           : -1177.891711977858\n",
      "    ess            : 1.966703970827649\n",
      "    log_marginal   : 1177.9238557288395\n",
      "    log_joint      : 1386.362570297778\n",
      "    val_loss       : -1176.0701904296875\n",
      "    val_ess        : 1.968468427658081\n",
      "    val_log_marginal: 1176.0981286090353\n",
      "    val_log_joint  : 1384.6822615913723\n",
      "Train Epoch: 1600 [0/101520 (0%)] Loss: -1180.866821\n",
      "Train Epoch: 1600 [11264/101520 (11%)] Loss: -1181.110840\n",
      "Train Epoch: 1600 [22528/101520 (22%)] Loss: -1178.342529\n",
      "Train Epoch: 1600 [33792/101520 (33%)] Loss: -1178.225586\n",
      "Train Epoch: 1600 [45056/101520 (44%)] Loss: -1178.994751\n",
      "Train Epoch: 1600 [56320/101520 (55%)] Loss: -1177.187012\n",
      "Train Epoch: 1600 [67584/101520 (67%)] Loss: -1175.182495\n",
      "Train Epoch: 1600 [78848/101520 (78%)] Loss: -1173.573486\n",
      "Train Epoch: 1600 [90112/101520 (89%)] Loss: -1166.936523\n",
      "Train Epoch: 1600 [101376/101520 (100%)] Loss: -1177.563599\n",
      "    epoch          : 1600\n",
      "    loss           : -1177.9705405690563\n",
      "    ess            : 1.9662478832743275\n",
      "    log_marginal   : 1178.002518083582\n",
      "    log_joint      : 1386.4595757105842\n",
      "    val_loss       : -1178.2096955672555\n",
      "    val_ess        : 1.9662855397100034\n",
      "    val_log_marginal: 1178.2432702105978\n",
      "    val_log_joint  : 1386.4630923063858\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1600.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1601 [0/101520 (0%)] Loss: -1176.314697\n",
      "Train Epoch: 1601 [11264/101520 (11%)] Loss: -1178.248169\n",
      "Train Epoch: 1601 [22528/101520 (22%)] Loss: -1178.764893\n",
      "Train Epoch: 1601 [33792/101520 (33%)] Loss: -1177.100098\n",
      "Train Epoch: 1601 [45056/101520 (44%)] Loss: -1180.886475\n",
      "Train Epoch: 1601 [56320/101520 (55%)] Loss: -1178.030884\n",
      "Train Epoch: 1601 [67584/101520 (67%)] Loss: -1178.656982\n",
      "Train Epoch: 1601 [78848/101520 (78%)] Loss: -1178.026001\n",
      "Train Epoch: 1601 [90112/101520 (89%)] Loss: -1179.307983\n",
      "Train Epoch: 1601 [101376/101520 (100%)] Loss: -1178.764893\n",
      "    epoch          : 1601\n",
      "    loss           : -1178.0142889741678\n",
      "    ess            : 1.9672175424182834\n",
      "    log_marginal   : 1178.0458855557083\n",
      "    log_joint      : 1386.4679488752356\n",
      "    val_loss       : -1176.3419667119565\n",
      "    val_ess        : 1.968267974646195\n",
      "    val_log_marginal: 1176.3696129840353\n",
      "    val_log_joint  : 1384.7534020465353\n",
      "Train Epoch: 1602 [0/101520 (0%)] Loss: -1174.437500\n",
      "Train Epoch: 1602 [11264/101520 (11%)] Loss: -1182.375977\n",
      "Train Epoch: 1602 [22528/101520 (22%)] Loss: -1176.270996\n",
      "Train Epoch: 1602 [33792/101520 (33%)] Loss: -1170.897827\n",
      "Train Epoch: 1602 [45056/101520 (44%)] Loss: -1177.016602\n",
      "Train Epoch: 1602 [56320/101520 (55%)] Loss: -1179.530884\n",
      "Train Epoch: 1602 [67584/101520 (67%)] Loss: -1177.778442\n",
      "Train Epoch: 1602 [78848/101520 (78%)] Loss: -1176.247192\n",
      "Train Epoch: 1602 [90112/101520 (89%)] Loss: -1163.103760\n",
      "Train Epoch: 1602 [101376/101520 (100%)] Loss: -1184.694702\n",
      "    epoch          : 1602\n",
      "    loss           : -1178.041061631399\n",
      "    ess            : 1.9658702354335307\n",
      "    log_marginal   : 1178.0739672483512\n",
      "    log_joint      : 1386.5548574169677\n",
      "    val_loss       : -1178.0186820652175\n",
      "    val_ess        : 1.965726551802262\n",
      "    val_log_marginal: 1178.05248492697\n",
      "    val_log_joint  : 1386.5659445057745\n",
      "Train Epoch: 1603 [0/101520 (0%)] Loss: -1182.743164\n",
      "Train Epoch: 1603 [11264/101520 (11%)] Loss: -1185.603516\n",
      "Train Epoch: 1603 [22528/101520 (22%)] Loss: -1177.115967\n",
      "Train Epoch: 1603 [33792/101520 (33%)] Loss: -1180.495117\n",
      "Train Epoch: 1603 [45056/101520 (44%)] Loss: -1185.163086\n",
      "Train Epoch: 1603 [56320/101520 (55%)] Loss: -1177.764648\n",
      "Train Epoch: 1603 [67584/101520 (67%)] Loss: -1178.284058\n",
      "Train Epoch: 1603 [78848/101520 (78%)] Loss: -1175.414307\n",
      "Train Epoch: 1603 [90112/101520 (89%)] Loss: -1182.152100\n",
      "Train Epoch: 1603 [101376/101520 (100%)] Loss: -1182.129517\n",
      "    epoch          : 1603\n",
      "    loss           : -1178.016617511385\n",
      "    ess            : 1.9663619396075531\n",
      "    log_marginal   : 1178.0485545402796\n",
      "    log_joint      : 1386.5302746643374\n",
      "    val_loss       : -1175.5889467985733\n",
      "    val_ess        : 1.9677185390306555\n",
      "    val_log_marginal: 1175.619283924932\n",
      "    val_log_joint  : 1384.0437117866848\n",
      "Train Epoch: 1604 [0/101520 (0%)] Loss: -1177.051880\n",
      "Train Epoch: 1604 [11264/101520 (11%)] Loss: -1177.349121\n",
      "Train Epoch: 1604 [22528/101520 (22%)] Loss: -1178.081543\n",
      "Train Epoch: 1604 [33792/101520 (33%)] Loss: -1181.934570\n",
      "Train Epoch: 1604 [45056/101520 (44%)] Loss: -1176.937012\n",
      "Train Epoch: 1604 [56320/101520 (55%)] Loss: -1188.020508\n",
      "Train Epoch: 1604 [67584/101520 (67%)] Loss: -1178.587036\n",
      "Train Epoch: 1604 [78848/101520 (78%)] Loss: -1180.703369\n",
      "Train Epoch: 1604 [90112/101520 (89%)] Loss: -1173.477905\n",
      "Train Epoch: 1604 [101376/101520 (100%)] Loss: -1180.638916\n",
      "    epoch          : 1604\n",
      "    loss           : -1178.0986444674545\n",
      "    ess            : 1.9663055278548045\n",
      "    log_marginal   : 1178.1306029660018\n",
      "    log_joint      : 1386.5534600492697\n",
      "    val_loss       : -1178.2919019616168\n",
      "    val_ess        : 1.967530888059865\n",
      "    val_log_marginal: 1178.320413340693\n",
      "    val_log_joint  : 1386.8703931725543\n",
      "Train Epoch: 1605 [0/101520 (0%)] Loss: -1183.642334\n",
      "Train Epoch: 1605 [11264/101520 (11%)] Loss: -1176.159912\n",
      "Train Epoch: 1605 [22528/101520 (22%)] Loss: -1179.777100\n",
      "Train Epoch: 1605 [33792/101520 (33%)] Loss: -1175.434082\n",
      "Train Epoch: 1605 [45056/101520 (44%)] Loss: -1167.121094\n",
      "Train Epoch: 1605 [56320/101520 (55%)] Loss: -1176.216309\n",
      "Train Epoch: 1605 [67584/101520 (67%)] Loss: -1172.987915\n",
      "Train Epoch: 1605 [78848/101520 (78%)] Loss: -1183.137451\n",
      "Train Epoch: 1605 [90112/101520 (89%)] Loss: -1177.202148\n",
      "Train Epoch: 1605 [101376/101520 (100%)] Loss: -1196.094849\n",
      "    epoch          : 1605\n",
      "    loss           : -1178.155929182043\n",
      "    ess            : 1.9658819910269887\n",
      "    log_marginal   : 1178.18848576378\n",
      "    log_joint      : 1386.6299607166693\n",
      "    val_loss       : -1176.2687139096467\n",
      "    val_ess        : 1.9651784585869831\n",
      "    val_log_marginal: 1176.3013119904892\n",
      "    val_log_joint  : 1384.552585767663\n",
      "Train Epoch: 1606 [0/101520 (0%)] Loss: -1178.466919\n",
      "Train Epoch: 1606 [11264/101520 (11%)] Loss: -1185.337769\n",
      "Train Epoch: 1606 [22528/101520 (22%)] Loss: -1173.650635\n",
      "Train Epoch: 1606 [33792/101520 (33%)] Loss: -1179.974365\n",
      "Train Epoch: 1606 [45056/101520 (44%)] Loss: -1176.774902\n",
      "Train Epoch: 1606 [56320/101520 (55%)] Loss: -1173.732422\n",
      "Train Epoch: 1606 [67584/101520 (67%)] Loss: -1181.289551\n",
      "Train Epoch: 1606 [78848/101520 (78%)] Loss: -1180.701904\n",
      "Train Epoch: 1606 [90112/101520 (89%)] Loss: -1175.890869\n",
      "Train Epoch: 1606 [101376/101520 (100%)] Loss: -1175.514282\n",
      "    epoch          : 1606\n",
      "    loss           : -1178.2165061145572\n",
      "    ess            : 1.9660259916554743\n",
      "    log_marginal   : 1178.2494681660255\n",
      "    log_joint      : 1386.5913036864008\n",
      "    val_loss       : -1176.6759192425272\n",
      "    val_ess        : 1.9663017988204956\n",
      "    val_log_marginal: 1176.7073178498642\n",
      "    val_log_joint  : 1385.1490160071332\n",
      "Train Epoch: 1607 [0/101520 (0%)] Loss: -1176.481934\n",
      "Train Epoch: 1607 [11264/101520 (11%)] Loss: -1177.463867\n",
      "Train Epoch: 1607 [22528/101520 (22%)] Loss: -1175.123047\n",
      "Train Epoch: 1607 [33792/101520 (33%)] Loss: -1177.305176\n",
      "Train Epoch: 1607 [45056/101520 (44%)] Loss: -1175.983276\n",
      "Train Epoch: 1607 [56320/101520 (55%)] Loss: -1181.850098\n",
      "Train Epoch: 1607 [67584/101520 (67%)] Loss: -1175.327881\n",
      "Train Epoch: 1607 [78848/101520 (78%)] Loss: -1175.589844\n",
      "Train Epoch: 1607 [90112/101520 (89%)] Loss: -1177.873535\n",
      "Train Epoch: 1607 [101376/101520 (100%)] Loss: -1164.117188\n",
      "    epoch          : 1607\n",
      "    loss           : -1178.0895394943468\n",
      "    ess            : 1.966797850239816\n",
      "    log_marginal   : 1178.121647667046\n",
      "    log_joint      : 1386.640461217219\n",
      "    val_loss       : -1177.278951893682\n",
      "    val_ess        : 1.967349446338156\n",
      "    val_log_marginal: 1177.309427012568\n",
      "    val_log_joint  : 1385.8246008831522\n",
      "Train Epoch: 1608 [0/101520 (0%)] Loss: -1176.331055\n",
      "Train Epoch: 1608 [11264/101520 (11%)] Loss: -1177.111206\n",
      "Train Epoch: 1608 [22528/101520 (22%)] Loss: -1175.133423\n",
      "Train Epoch: 1608 [33792/101520 (33%)] Loss: -1182.556519\n",
      "Train Epoch: 1608 [45056/101520 (44%)] Loss: -1180.661133\n",
      "Train Epoch: 1608 [56320/101520 (55%)] Loss: -1173.971802\n",
      "Train Epoch: 1608 [67584/101520 (67%)] Loss: -1175.966919\n",
      "Train Epoch: 1608 [78848/101520 (78%)] Loss: -1175.477539\n",
      "Train Epoch: 1608 [90112/101520 (89%)] Loss: -1173.294922\n",
      "Train Epoch: 1608 [101376/101520 (100%)] Loss: -1177.195557\n",
      "    epoch          : 1608\n",
      "    loss           : -1178.2904463724874\n",
      "    ess            : 1.9666363366285162\n",
      "    log_marginal   : 1178.3217362447\n",
      "    log_joint      : 1386.70041430296\n",
      "    val_loss       : -1176.9279519786005\n",
      "    val_ess        : 1.9628469425698984\n",
      "    val_log_marginal: 1176.9639361837635\n",
      "    val_log_joint  : 1385.3223505434783\n",
      "Train Epoch: 1609 [0/101520 (0%)] Loss: -1180.498291\n",
      "Train Epoch: 1609 [11264/101520 (11%)] Loss: -1173.245972\n",
      "Train Epoch: 1609 [22528/101520 (22%)] Loss: -1180.564697\n",
      "Train Epoch: 1609 [33792/101520 (33%)] Loss: -1176.769775\n",
      "Train Epoch: 1609 [45056/101520 (44%)] Loss: -1181.469116\n",
      "Train Epoch: 1609 [56320/101520 (55%)] Loss: -1177.468872\n",
      "Train Epoch: 1609 [67584/101520 (67%)] Loss: -1180.424072\n",
      "Train Epoch: 1609 [78848/101520 (78%)] Loss: -1174.994873\n",
      "Train Epoch: 1609 [90112/101520 (89%)] Loss: -1180.594238\n",
      "Train Epoch: 1609 [101376/101520 (100%)] Loss: -1189.043457\n",
      "    epoch          : 1609\n",
      "    loss           : -1178.3446701279836\n",
      "    ess            : 1.966156537209324\n",
      "    log_marginal   : 1178.3769647799545\n",
      "    log_joint      : 1386.7952629357726\n",
      "    val_loss       : -1175.88353961447\n",
      "    val_ess        : 1.964390490366065\n",
      "    val_log_marginal: 1175.9186905570652\n",
      "    val_log_joint  : 1384.568306300951\n",
      "Train Epoch: 1610 [0/101520 (0%)] Loss: -1176.707275\n",
      "Train Epoch: 1610 [11264/101520 (11%)] Loss: -1184.096924\n",
      "Train Epoch: 1610 [22528/101520 (22%)] Loss: -1178.110840\n",
      "Train Epoch: 1610 [33792/101520 (33%)] Loss: -1172.654907\n",
      "Train Epoch: 1610 [45056/101520 (44%)] Loss: -1179.507568\n",
      "Train Epoch: 1610 [56320/101520 (55%)] Loss: -1178.007812\n",
      "Train Epoch: 1610 [67584/101520 (67%)] Loss: -1175.796631\n",
      "Train Epoch: 1610 [78848/101520 (78%)] Loss: -1177.779785\n",
      "Train Epoch: 1610 [90112/101520 (89%)] Loss: -1171.787720\n",
      "Train Epoch: 1610 [101376/101520 (100%)] Loss: -1174.570923\n",
      "    epoch          : 1610\n",
      "    loss           : -1178.3026656721106\n",
      "    ess            : 1.9665193755423005\n",
      "    log_marginal   : 1178.334821691465\n",
      "    log_joint      : 1386.7885233050015\n",
      "    val_loss       : -1176.107618248981\n",
      "    val_ess        : 1.9681794021440588\n",
      "    val_log_marginal: 1176.1375679347825\n",
      "    val_log_joint  : 1384.632180918818\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1610.pth ...\n",
      "Train Epoch: 1611 [0/101520 (0%)] Loss: -1175.772339\n",
      "Train Epoch: 1611 [11264/101520 (11%)] Loss: -1183.357666\n",
      "Train Epoch: 1611 [22528/101520 (22%)] Loss: -1175.761353\n",
      "Train Epoch: 1611 [33792/101520 (33%)] Loss: -1182.618286\n",
      "Train Epoch: 1611 [45056/101520 (44%)] Loss: -1172.799561\n",
      "Train Epoch: 1611 [56320/101520 (55%)] Loss: -1180.356689\n",
      "Train Epoch: 1611 [67584/101520 (67%)] Loss: -1176.175537\n",
      "Train Epoch: 1611 [78848/101520 (78%)] Loss: -1173.611328\n",
      "Train Epoch: 1611 [90112/101520 (89%)] Loss: -1176.253418\n",
      "Train Epoch: 1611 [101376/101520 (100%)] Loss: -1176.679688\n",
      "    epoch          : 1611\n",
      "    loss           : -1178.3419379612908\n",
      "    ess            : 1.9662449329941716\n",
      "    log_marginal   : 1178.3738731499293\n",
      "    log_joint      : 1386.810659130614\n",
      "    val_loss       : -1178.173679517663\n",
      "    val_ess        : 1.9645736113838528\n",
      "    val_log_marginal: 1178.2064155910325\n",
      "    val_log_joint  : 1386.6908595872962\n",
      "Train Epoch: 1612 [0/101520 (0%)] Loss: -1175.290039\n",
      "Train Epoch: 1612 [11264/101520 (11%)] Loss: -1180.869629\n",
      "Train Epoch: 1612 [22528/101520 (22%)] Loss: -1183.823975\n",
      "Train Epoch: 1612 [33792/101520 (33%)] Loss: -1178.834717\n",
      "Train Epoch: 1612 [45056/101520 (44%)] Loss: -1173.287598\n",
      "Train Epoch: 1612 [56320/101520 (55%)] Loss: -1188.427002\n",
      "Train Epoch: 1612 [67584/101520 (67%)] Loss: -1181.093506\n",
      "Train Epoch: 1612 [78848/101520 (78%)] Loss: -1175.584473\n",
      "Train Epoch: 1612 [90112/101520 (89%)] Loss: -1171.801514\n",
      "Train Epoch: 1612 [101376/101520 (100%)] Loss: -1176.131592\n",
      "    epoch          : 1612\n",
      "    loss           : -1178.3382967081502\n",
      "    ess            : 1.9655805197193394\n",
      "    log_marginal   : 1178.3723831560144\n",
      "    log_joint      : 1386.859628341905\n",
      "    val_loss       : -1177.221340013587\n",
      "    val_ess        : 1.965048732964889\n",
      "    val_log_marginal: 1177.2548934273098\n",
      "    val_log_joint  : 1385.4749862007473\n",
      "Train Epoch: 1613 [0/101520 (0%)] Loss: -1169.185791\n",
      "Train Epoch: 1613 [11264/101520 (11%)] Loss: -1186.665527\n",
      "Train Epoch: 1613 [22528/101520 (22%)] Loss: -1176.916748\n",
      "Train Epoch: 1613 [33792/101520 (33%)] Loss: -1176.299561\n",
      "Train Epoch: 1613 [45056/101520 (44%)] Loss: -1175.733398\n",
      "Train Epoch: 1613 [56320/101520 (55%)] Loss: -1185.213989\n",
      "Train Epoch: 1613 [67584/101520 (67%)] Loss: -1174.257202\n",
      "Train Epoch: 1613 [78848/101520 (78%)] Loss: -1182.766357\n",
      "Train Epoch: 1613 [90112/101520 (89%)] Loss: -1171.976074\n",
      "Train Epoch: 1613 [101376/101520 (100%)] Loss: -1182.669556\n",
      "    epoch          : 1613\n",
      "    loss           : -1178.3921732686872\n",
      "    ess            : 1.9666016772763812\n",
      "    log_marginal   : 1178.4238949876335\n",
      "    log_joint      : 1386.8318944576397\n",
      "    val_loss       : -1175.7005562160325\n",
      "    val_ess        : 1.9682104069253672\n",
      "    val_log_marginal: 1175.7317743716033\n",
      "    val_log_joint  : 1384.1867569633152\n",
      "Train Epoch: 1614 [0/101520 (0%)] Loss: -1180.932495\n",
      "Train Epoch: 1614 [11264/101520 (11%)] Loss: -1179.636475\n",
      "Train Epoch: 1614 [22528/101520 (22%)] Loss: -1179.450806\n",
      "Train Epoch: 1614 [33792/101520 (33%)] Loss: -1182.203857\n",
      "Train Epoch: 1614 [45056/101520 (44%)] Loss: -1174.420410\n",
      "Train Epoch: 1614 [56320/101520 (55%)] Loss: -1175.102051\n",
      "Train Epoch: 1614 [67584/101520 (67%)] Loss: -1178.929443\n",
      "Train Epoch: 1614 [78848/101520 (78%)] Loss: -1182.128906\n",
      "Train Epoch: 1614 [90112/101520 (89%)] Loss: -1184.098145\n",
      "Train Epoch: 1614 [101376/101520 (100%)] Loss: -1163.475464\n",
      "    epoch          : 1614\n",
      "    loss           : -1178.371040382577\n",
      "    ess            : 1.966590303272458\n",
      "    log_marginal   : 1178.4027240695666\n",
      "    log_joint      : 1386.778664440366\n",
      "    val_loss       : -1176.4010009765625\n",
      "    val_ess        : 1.962089383083841\n",
      "    val_log_marginal: 1176.4405411430027\n",
      "    val_log_joint  : 1384.5076055112092\n",
      "Train Epoch: 1615 [0/101520 (0%)] Loss: -1180.142334\n",
      "Train Epoch: 1615 [11264/101520 (11%)] Loss: -1179.358521\n",
      "Train Epoch: 1615 [22528/101520 (22%)] Loss: -1182.802246\n",
      "Train Epoch: 1615 [33792/101520 (33%)] Loss: -1170.977905\n",
      "Train Epoch: 1615 [45056/101520 (44%)] Loss: -1182.801270\n",
      "Train Epoch: 1615 [56320/101520 (55%)] Loss: -1176.850342\n",
      "Train Epoch: 1615 [67584/101520 (67%)] Loss: -1180.617065\n",
      "Train Epoch: 1615 [78848/101520 (78%)] Loss: -1176.072510\n",
      "Train Epoch: 1615 [90112/101520 (89%)] Loss: -1173.705078\n",
      "Train Epoch: 1615 [101376/101520 (100%)] Loss: -1192.892090\n",
      "    epoch          : 1615\n",
      "    loss           : -1178.4933266184437\n",
      "    ess            : 1.9660033305086682\n",
      "    log_marginal   : 1178.5252949316896\n",
      "    log_joint      : 1386.9631335387876\n",
      "    val_loss       : -1177.5002865998642\n",
      "    val_ess        : 1.9648035505543584\n",
      "    val_log_marginal: 1177.534959876019\n",
      "    val_log_joint  : 1386.1598855723505\n",
      "Train Epoch: 1616 [0/101520 (0%)] Loss: -1174.185547\n",
      "Train Epoch: 1616 [11264/101520 (11%)] Loss: -1174.856934\n",
      "Train Epoch: 1616 [22528/101520 (22%)] Loss: -1182.605225\n",
      "Train Epoch: 1616 [33792/101520 (33%)] Loss: -1182.660400\n",
      "Train Epoch: 1616 [45056/101520 (44%)] Loss: -1178.841797\n",
      "Train Epoch: 1616 [56320/101520 (55%)] Loss: -1179.840820\n",
      "Train Epoch: 1616 [67584/101520 (67%)] Loss: -1170.616211\n",
      "Train Epoch: 1616 [78848/101520 (78%)] Loss: -1182.869263\n",
      "Train Epoch: 1616 [90112/101520 (89%)] Loss: -1178.119873\n",
      "Train Epoch: 1616 [101376/101520 (100%)] Loss: -1180.710327\n",
      "    epoch          : 1616\n",
      "    loss           : -1178.484235753965\n",
      "    ess            : 1.965765400747558\n",
      "    log_marginal   : 1178.516950597715\n",
      "    log_joint      : 1386.9377024281564\n",
      "    val_loss       : -1175.7792278787365\n",
      "    val_ess        : 1.9660737410835598\n",
      "    val_log_marginal: 1175.8115022078805\n",
      "    val_log_joint  : 1384.2706935716712\n",
      "Train Epoch: 1617 [0/101520 (0%)] Loss: -1174.021484\n",
      "Train Epoch: 1617 [11264/101520 (11%)] Loss: -1184.378906\n",
      "Train Epoch: 1617 [22528/101520 (22%)] Loss: -1177.748779\n",
      "Train Epoch: 1617 [33792/101520 (33%)] Loss: -1176.878174\n",
      "Train Epoch: 1617 [45056/101520 (44%)] Loss: -1176.657471\n",
      "Train Epoch: 1617 [56320/101520 (55%)] Loss: -1175.310181\n",
      "Train Epoch: 1617 [67584/101520 (67%)] Loss: -1183.385010\n",
      "Train Epoch: 1617 [78848/101520 (78%)] Loss: -1176.530273\n",
      "Train Epoch: 1617 [90112/101520 (89%)] Loss: -1179.205322\n",
      "Train Epoch: 1617 [101376/101520 (100%)] Loss: -1174.178345\n",
      "    epoch          : 1617\n",
      "    loss           : -1178.487707703557\n",
      "    ess            : 1.9660262300740534\n",
      "    log_marginal   : 1178.5211322726916\n",
      "    log_joint      : 1386.916543165044\n",
      "    val_loss       : -1177.2918595023777\n",
      "    val_ess        : 1.9626387668692546\n",
      "    val_log_marginal: 1177.3300675101902\n",
      "    val_log_joint  : 1385.8198029891305\n",
      "Train Epoch: 1618 [0/101520 (0%)] Loss: -1177.048340\n",
      "Train Epoch: 1618 [11264/101520 (11%)] Loss: -1180.789551\n",
      "Train Epoch: 1618 [22528/101520 (22%)] Loss: -1177.425049\n",
      "Train Epoch: 1618 [33792/101520 (33%)] Loss: -1178.955078\n",
      "Train Epoch: 1618 [45056/101520 (44%)] Loss: -1186.228638\n",
      "Train Epoch: 1618 [56320/101520 (55%)] Loss: -1177.235352\n",
      "Train Epoch: 1618 [67584/101520 (67%)] Loss: -1175.633789\n",
      "Train Epoch: 1618 [78848/101520 (78%)] Loss: -1174.157715\n",
      "Train Epoch: 1618 [90112/101520 (89%)] Loss: -1183.558960\n",
      "Train Epoch: 1618 [101376/101520 (100%)] Loss: -1183.242798\n",
      "    epoch          : 1618\n",
      "    loss           : -1178.5413468710742\n",
      "    ess            : 1.9666717310047628\n",
      "    log_marginal   : 1178.572695018059\n",
      "    log_joint      : 1387.0246907143137\n",
      "    val_loss       : -1176.111572265625\n",
      "    val_ess        : 1.9629896568215413\n",
      "    val_log_marginal: 1176.1517227836277\n",
      "    val_log_joint  : 1384.7418372112772\n",
      "Train Epoch: 1619 [0/101520 (0%)] Loss: -1181.379150\n",
      "Train Epoch: 1619 [11264/101520 (11%)] Loss: -1181.391235\n",
      "Train Epoch: 1619 [22528/101520 (22%)] Loss: -1179.886597\n",
      "Train Epoch: 1619 [33792/101520 (33%)] Loss: -1180.103760\n",
      "Train Epoch: 1619 [45056/101520 (44%)] Loss: -1178.973389\n",
      "Train Epoch: 1619 [56320/101520 (55%)] Loss: -1178.350220\n",
      "Train Epoch: 1619 [67584/101520 (67%)] Loss: -1181.156128\n",
      "Train Epoch: 1619 [78848/101520 (78%)] Loss: -1180.573486\n",
      "Train Epoch: 1619 [90112/101520 (89%)] Loss: -1189.110229\n",
      "Train Epoch: 1619 [101376/101520 (100%)] Loss: -1187.026001\n",
      "    epoch          : 1619\n",
      "    loss           : -1178.5911491048994\n",
      "    ess            : 1.966843313907259\n",
      "    log_marginal   : 1178.6224438844613\n",
      "    log_joint      : 1387.0625325111887\n",
      "    val_loss       : -1178.0228855298913\n",
      "    val_ess        : 1.9680625874063242\n",
      "    val_log_marginal: 1178.050632642663\n",
      "    val_log_joint  : 1386.550048828125\n",
      "Train Epoch: 1620 [0/101520 (0%)] Loss: -1180.309448\n",
      "Train Epoch: 1620 [11264/101520 (11%)] Loss: -1183.891602\n",
      "Train Epoch: 1620 [22528/101520 (22%)] Loss: -1178.694824\n",
      "Train Epoch: 1620 [33792/101520 (33%)] Loss: -1181.712891\n",
      "Train Epoch: 1620 [45056/101520 (44%)] Loss: -1183.298462\n",
      "Train Epoch: 1620 [56320/101520 (55%)] Loss: -1180.596191\n",
      "Train Epoch: 1620 [67584/101520 (67%)] Loss: -1173.885010\n",
      "Train Epoch: 1620 [78848/101520 (78%)] Loss: -1176.447144\n",
      "Train Epoch: 1620 [90112/101520 (89%)] Loss: -1176.875000\n",
      "Train Epoch: 1620 [101376/101520 (100%)] Loss: -1171.676880\n",
      "    epoch          : 1620\n",
      "    loss           : -1178.5541164072315\n",
      "    ess            : 1.9661820521905793\n",
      "    log_marginal   : 1178.5859006948806\n",
      "    log_joint      : 1387.0394146023084\n",
      "    val_loss       : -1178.7652004076087\n",
      "    val_ess        : 1.9688597606576008\n",
      "    val_log_marginal: 1178.7961054262908\n",
      "    val_log_joint  : 1387.3073412024457\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1620.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1621 [0/101520 (0%)] Loss: -1179.313965\n",
      "Train Epoch: 1621 [11264/101520 (11%)] Loss: -1178.651367\n",
      "Train Epoch: 1621 [22528/101520 (22%)] Loss: -1182.415527\n",
      "Train Epoch: 1621 [33792/101520 (33%)] Loss: -1175.273560\n",
      "Train Epoch: 1621 [45056/101520 (44%)] Loss: -1171.914551\n",
      "Train Epoch: 1621 [56320/101520 (55%)] Loss: -1178.731812\n",
      "Train Epoch: 1621 [67584/101520 (67%)] Loss: -1172.492798\n",
      "Train Epoch: 1621 [78848/101520 (78%)] Loss: -1174.695312\n",
      "Train Epoch: 1621 [90112/101520 (89%)] Loss: -1180.070923\n",
      "Train Epoch: 1621 [101376/101520 (100%)] Loss: -1178.156860\n",
      "    epoch          : 1621\n",
      "    loss           : -1178.573145880771\n",
      "    ess            : 1.9665084909554102\n",
      "    log_marginal   : 1178.6047982834093\n",
      "    log_joint      : 1387.0926164023242\n",
      "    val_loss       : -1176.99560546875\n",
      "    val_ess        : 1.9628169951231584\n",
      "    val_log_marginal: 1177.029493248981\n",
      "    val_log_joint  : 1385.381156589674\n",
      "Train Epoch: 1622 [0/101520 (0%)] Loss: -1177.201782\n",
      "Train Epoch: 1622 [11264/101520 (11%)] Loss: -1178.930908\n",
      "Train Epoch: 1622 [22528/101520 (22%)] Loss: -1169.498535\n",
      "Train Epoch: 1622 [33792/101520 (33%)] Loss: -1171.584229\n",
      "Train Epoch: 1622 [45056/101520 (44%)] Loss: -1179.639160\n",
      "Train Epoch: 1622 [56320/101520 (55%)] Loss: -1186.364258\n",
      "Train Epoch: 1622 [67584/101520 (67%)] Loss: -1171.158203\n",
      "Train Epoch: 1622 [78848/101520 (78%)] Loss: -1176.229248\n",
      "Train Epoch: 1622 [90112/101520 (89%)] Loss: -1183.099609\n",
      "Train Epoch: 1622 [101376/101520 (100%)] Loss: -1182.431885\n",
      "    epoch          : 1622\n",
      "    loss           : -1178.6977440915516\n",
      "    ess            : 1.965738766756489\n",
      "    log_marginal   : 1178.7300902706893\n",
      "    log_joint      : 1387.206997511974\n",
      "    val_loss       : -1175.658298658288\n",
      "    val_ess        : 1.9677642324696416\n",
      "    val_log_marginal: 1175.6884181810462\n",
      "    val_log_joint  : 1384.1301587975543\n",
      "Train Epoch: 1623 [0/101520 (0%)] Loss: -1182.053833\n",
      "Train Epoch: 1623 [11264/101520 (11%)] Loss: -1176.974121\n",
      "Train Epoch: 1623 [22528/101520 (22%)] Loss: -1182.267578\n",
      "Train Epoch: 1623 [33792/101520 (33%)] Loss: -1188.027344\n",
      "Train Epoch: 1623 [45056/101520 (44%)] Loss: -1181.931396\n",
      "Train Epoch: 1623 [56320/101520 (55%)] Loss: -1172.216919\n",
      "Train Epoch: 1623 [67584/101520 (67%)] Loss: -1177.159180\n",
      "Train Epoch: 1623 [78848/101520 (78%)] Loss: -1176.392822\n",
      "Train Epoch: 1623 [90112/101520 (89%)] Loss: -1183.604980\n",
      "Train Epoch: 1623 [101376/101520 (100%)] Loss: -1175.458984\n",
      "    epoch          : 1623\n",
      "    loss           : -1178.7448644590138\n",
      "    ess            : 1.9665824049082234\n",
      "    log_marginal   : 1178.7760064973304\n",
      "    log_joint      : 1387.2234168891332\n",
      "    val_loss       : -1178.4800494650135\n",
      "    val_ess        : 1.965141291203706\n",
      "    val_log_marginal: 1178.5171959918478\n",
      "    val_log_joint  : 1387.1539412788723\n",
      "Train Epoch: 1624 [0/101520 (0%)] Loss: -1179.616211\n",
      "Train Epoch: 1624 [11264/101520 (11%)] Loss: -1172.385742\n",
      "Train Epoch: 1624 [22528/101520 (22%)] Loss: -1180.186035\n",
      "Train Epoch: 1624 [33792/101520 (33%)] Loss: -1174.955078\n",
      "Train Epoch: 1624 [45056/101520 (44%)] Loss: -1180.369141\n",
      "Train Epoch: 1624 [56320/101520 (55%)] Loss: -1180.046021\n",
      "Train Epoch: 1624 [67584/101520 (67%)] Loss: -1175.438110\n",
      "Train Epoch: 1624 [78848/101520 (78%)] Loss: -1168.907715\n",
      "Train Epoch: 1624 [90112/101520 (89%)] Loss: -1171.854492\n",
      "Train Epoch: 1624 [101376/101520 (100%)] Loss: -1185.146118\n",
      "    epoch          : 1624\n",
      "    loss           : -1178.8000893137562\n",
      "    ess            : 1.9666546217760248\n",
      "    log_marginal   : 1178.8328299210898\n",
      "    log_joint      : 1387.2394258892116\n",
      "    val_loss       : -1177.8102708899457\n",
      "    val_ess        : 1.9659520543139914\n",
      "    val_log_marginal: 1177.8418234120245\n",
      "    val_log_joint  : 1386.2089259935462\n",
      "Train Epoch: 1625 [0/101520 (0%)] Loss: -1174.608643\n",
      "Train Epoch: 1625 [11264/101520 (11%)] Loss: -1175.801514\n",
      "Train Epoch: 1625 [22528/101520 (22%)] Loss: -1175.538208\n",
      "Train Epoch: 1625 [33792/101520 (33%)] Loss: -1182.191406\n",
      "Train Epoch: 1625 [45056/101520 (44%)] Loss: -1184.566162\n",
      "Train Epoch: 1625 [56320/101520 (55%)] Loss: -1175.471802\n",
      "Train Epoch: 1625 [67584/101520 (67%)] Loss: -1181.903320\n",
      "Train Epoch: 1625 [78848/101520 (78%)] Loss: -1180.345703\n",
      "Train Epoch: 1625 [90112/101520 (89%)] Loss: -1176.514404\n",
      "Train Epoch: 1625 [101376/101520 (100%)] Loss: -1177.245728\n",
      "    epoch          : 1625\n",
      "    loss           : -1178.9150519442917\n",
      "    ess            : 1.9664110214865986\n",
      "    log_marginal   : 1178.947114110592\n",
      "    log_joint      : 1387.341816504397\n",
      "    val_loss       : -1176.8033871858017\n",
      "    val_ess        : 1.9677387113156526\n",
      "    val_log_marginal: 1176.8314686650815\n",
      "    val_log_joint  : 1385.4472337805707\n",
      "Train Epoch: 1626 [0/101520 (0%)] Loss: -1176.338745\n",
      "Train Epoch: 1626 [11264/101520 (11%)] Loss: -1183.213257\n",
      "Train Epoch: 1626 [22528/101520 (22%)] Loss: -1175.415527\n",
      "Train Epoch: 1626 [33792/101520 (33%)] Loss: -1186.292969\n",
      "Train Epoch: 1626 [45056/101520 (44%)] Loss: -1174.758057\n",
      "Train Epoch: 1626 [56320/101520 (55%)] Loss: -1170.363647\n",
      "Train Epoch: 1626 [67584/101520 (67%)] Loss: -1184.156982\n",
      "Train Epoch: 1626 [78848/101520 (78%)] Loss: -1184.431396\n",
      "Train Epoch: 1626 [90112/101520 (89%)] Loss: -1171.196045\n",
      "Train Epoch: 1626 [101376/101520 (100%)] Loss: -1193.245850\n",
      "    epoch          : 1626\n",
      "    loss           : -1179.0006054442133\n",
      "    ess            : 1.9671254152029602\n",
      "    log_marginal   : 1179.031693501688\n",
      "    log_joint      : 1387.3775923072394\n",
      "    val_loss       : -1177.7597815472147\n",
      "    val_ess        : 1.9678897443025007\n",
      "    val_log_marginal: 1177.7888448963995\n",
      "    val_log_joint  : 1386.463920261549\n",
      "Train Epoch: 1627 [0/101520 (0%)] Loss: -1177.250244\n",
      "Train Epoch: 1627 [11264/101520 (11%)] Loss: -1173.447754\n",
      "Train Epoch: 1627 [22528/101520 (22%)] Loss: -1176.132568\n",
      "Train Epoch: 1627 [33792/101520 (33%)] Loss: -1172.878174\n",
      "Train Epoch: 1627 [45056/101520 (44%)] Loss: -1179.281128\n",
      "Train Epoch: 1627 [56320/101520 (55%)] Loss: -1179.846558\n",
      "Train Epoch: 1627 [67584/101520 (67%)] Loss: -1171.666870\n",
      "Train Epoch: 1627 [78848/101520 (78%)] Loss: -1180.962036\n",
      "Train Epoch: 1627 [90112/101520 (89%)] Loss: -1177.822632\n",
      "Train Epoch: 1627 [101376/101520 (100%)] Loss: -1170.613037\n",
      "    epoch          : 1627\n",
      "    loss           : -1178.7985772367697\n",
      "    ess            : 1.9666006403352747\n",
      "    log_marginal   : 1178.8302554029915\n",
      "    log_joint      : 1387.3088231685772\n",
      "    val_loss       : -1176.4927978515625\n",
      "    val_ess        : 1.9684414915416553\n",
      "    val_log_marginal: 1176.521096934443\n",
      "    val_log_joint  : 1384.9698857846467\n",
      "Train Epoch: 1628 [0/101520 (0%)] Loss: -1175.827393\n",
      "Train Epoch: 1628 [11264/101520 (11%)] Loss: -1178.815674\n",
      "Train Epoch: 1628 [22528/101520 (22%)] Loss: -1179.735352\n",
      "Train Epoch: 1628 [33792/101520 (33%)] Loss: -1185.802979\n",
      "Train Epoch: 1628 [45056/101520 (44%)] Loss: -1182.080811\n",
      "Train Epoch: 1628 [56320/101520 (55%)] Loss: -1184.637085\n",
      "Train Epoch: 1628 [67584/101520 (67%)] Loss: -1175.234375\n",
      "Train Epoch: 1628 [78848/101520 (78%)] Loss: -1183.996826\n",
      "Train Epoch: 1628 [90112/101520 (89%)] Loss: -1180.079346\n",
      "Train Epoch: 1628 [101376/101520 (100%)] Loss: -1185.877930\n",
      "    epoch          : 1628\n",
      "    loss           : -1178.9742008381752\n",
      "    ess            : 1.9662549453764107\n",
      "    log_marginal   : 1179.0064801546796\n",
      "    log_joint      : 1387.3774174829225\n",
      "    val_loss       : -1178.1439315132473\n",
      "    val_ess        : 1.9652449359064517\n",
      "    val_log_marginal: 1178.1794805112092\n",
      "    val_log_joint  : 1386.3247176460598\n",
      "Train Epoch: 1629 [0/101520 (0%)] Loss: -1174.202271\n",
      "Train Epoch: 1629 [11264/101520 (11%)] Loss: -1178.973389\n",
      "Train Epoch: 1629 [22528/101520 (22%)] Loss: -1177.449219\n",
      "Train Epoch: 1629 [33792/101520 (33%)] Loss: -1175.216675\n",
      "Train Epoch: 1629 [45056/101520 (44%)] Loss: -1174.698242\n",
      "Train Epoch: 1629 [56320/101520 (55%)] Loss: -1176.521973\n",
      "Train Epoch: 1629 [67584/101520 (67%)] Loss: -1181.501221\n",
      "Train Epoch: 1629 [78848/101520 (78%)] Loss: -1175.644653\n",
      "Train Epoch: 1629 [90112/101520 (89%)] Loss: -1180.618896\n",
      "Train Epoch: 1629 [101376/101520 (100%)] Loss: -1183.633057\n",
      "    epoch          : 1629\n",
      "    loss           : -1178.848638333268\n",
      "    ess            : 1.9663649953190405\n",
      "    log_marginal   : 1178.88108756674\n",
      "    log_joint      : 1387.3925364125314\n",
      "    val_loss       : -1177.9980256453805\n",
      "    val_ess        : 1.968387422354325\n",
      "    val_log_marginal: 1178.0288298233695\n",
      "    val_log_joint  : 1386.196920643682\n",
      "Train Epoch: 1630 [0/101520 (0%)] Loss: -1185.648682\n",
      "Train Epoch: 1630 [11264/101520 (11%)] Loss: -1179.880371\n",
      "Train Epoch: 1630 [22528/101520 (22%)] Loss: -1179.724854\n",
      "Train Epoch: 1630 [33792/101520 (33%)] Loss: -1180.568726\n",
      "Train Epoch: 1630 [45056/101520 (44%)] Loss: -1176.648926\n",
      "Train Epoch: 1630 [56320/101520 (55%)] Loss: -1175.478149\n",
      "Train Epoch: 1630 [67584/101520 (67%)] Loss: -1177.165283\n",
      "Train Epoch: 1630 [78848/101520 (78%)] Loss: -1173.848877\n",
      "Train Epoch: 1630 [90112/101520 (89%)] Loss: -1176.069092\n",
      "Train Epoch: 1630 [101376/101520 (100%)] Loss: -1188.039673\n",
      "    epoch          : 1630\n",
      "    loss           : -1178.9902417360238\n",
      "    ess            : 1.9664942283726217\n",
      "    log_marginal   : 1179.020980144865\n",
      "    log_joint      : 1387.49560546875\n",
      "    val_loss       : -1178.6511973505435\n",
      "    val_ess        : 1.9693274446155713\n",
      "    val_log_marginal: 1178.6801863960598\n",
      "    val_log_joint  : 1387.1986986243207\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1630.pth ...\n",
      "Train Epoch: 1631 [0/101520 (0%)] Loss: -1180.895508\n",
      "Train Epoch: 1631 [11264/101520 (11%)] Loss: -1179.990723\n",
      "Train Epoch: 1631 [22528/101520 (22%)] Loss: -1175.880493\n",
      "Train Epoch: 1631 [33792/101520 (33%)] Loss: -1183.514038\n",
      "Train Epoch: 1631 [45056/101520 (44%)] Loss: -1184.166992\n",
      "Train Epoch: 1631 [56320/101520 (55%)] Loss: -1168.493896\n",
      "Train Epoch: 1631 [67584/101520 (67%)] Loss: -1180.852783\n",
      "Train Epoch: 1631 [78848/101520 (78%)] Loss: -1175.968018\n",
      "Train Epoch: 1631 [90112/101520 (89%)] Loss: -1178.877441\n",
      "Train Epoch: 1631 [101376/101520 (100%)] Loss: -1177.456421\n",
      "    epoch          : 1631\n",
      "    loss           : -1178.9604817299387\n",
      "    ess            : 1.9662992906330818\n",
      "    log_marginal   : 1178.9930450592808\n",
      "    log_joint      : 1387.485406770179\n",
      "    val_loss       : -1178.5752696161685\n",
      "    val_ess        : 1.9621982522632764\n",
      "    val_log_marginal: 1178.6127664317255\n",
      "    val_log_joint  : 1386.760062839674\n",
      "Train Epoch: 1632 [0/101520 (0%)] Loss: -1178.600952\n",
      "Train Epoch: 1632 [11264/101520 (11%)] Loss: -1172.727539\n",
      "Train Epoch: 1632 [22528/101520 (22%)] Loss: -1174.766602\n",
      "Train Epoch: 1632 [33792/101520 (33%)] Loss: -1179.050049\n",
      "Train Epoch: 1632 [45056/101520 (44%)] Loss: -1188.639648\n",
      "Train Epoch: 1632 [56320/101520 (55%)] Loss: -1184.219482\n",
      "Train Epoch: 1632 [67584/101520 (67%)] Loss: -1175.813354\n",
      "Train Epoch: 1632 [78848/101520 (78%)] Loss: -1180.828735\n",
      "Train Epoch: 1632 [90112/101520 (89%)] Loss: -1177.408081\n",
      "Train Epoch: 1632 [101376/101520 (100%)] Loss: -1180.396362\n",
      "    epoch          : 1632\n",
      "    loss           : -1179.004328895454\n",
      "    ess            : 1.9659061359999768\n",
      "    log_marginal   : 1179.0368995858198\n",
      "    log_joint      : 1387.4925800879396\n",
      "    val_loss       : -1177.3970575747283\n",
      "    val_ess        : 1.967447094295336\n",
      "    val_log_marginal: 1177.4277237601902\n",
      "    val_log_joint  : 1386.0428997537365\n",
      "Train Epoch: 1633 [0/101520 (0%)] Loss: -1172.909546\n",
      "Train Epoch: 1633 [11264/101520 (11%)] Loss: -1188.221924\n",
      "Train Epoch: 1633 [22528/101520 (22%)] Loss: -1181.954346\n",
      "Train Epoch: 1633 [33792/101520 (33%)] Loss: -1176.484009\n",
      "Train Epoch: 1633 [45056/101520 (44%)] Loss: -1179.523682\n",
      "Train Epoch: 1633 [56320/101520 (55%)] Loss: -1179.201660\n",
      "Train Epoch: 1633 [67584/101520 (67%)] Loss: -1179.973877\n",
      "Train Epoch: 1633 [78848/101520 (78%)] Loss: -1174.382568\n",
      "Train Epoch: 1633 [90112/101520 (89%)] Loss: -1176.450195\n",
      "Train Epoch: 1633 [101376/101520 (100%)] Loss: -1181.307495\n",
      "    epoch          : 1633\n",
      "    loss           : -1179.0540881899733\n",
      "    ess            : 1.966640497571859\n",
      "    log_marginal   : 1179.0862846949592\n",
      "    log_joint      : 1387.499609865735\n",
      "    val_loss       : -1176.1412035071332\n",
      "    val_ess        : 1.9650295765503594\n",
      "    val_log_marginal: 1176.171047044837\n",
      "    val_log_joint  : 1384.739252505095\n",
      "Train Epoch: 1634 [0/101520 (0%)] Loss: -1184.333618\n",
      "Train Epoch: 1634 [11264/101520 (11%)] Loss: -1178.203613\n",
      "Train Epoch: 1634 [22528/101520 (22%)] Loss: -1180.690552\n",
      "Train Epoch: 1634 [33792/101520 (33%)] Loss: -1180.783936\n",
      "Train Epoch: 1634 [45056/101520 (44%)] Loss: -1181.866577\n",
      "Train Epoch: 1634 [56320/101520 (55%)] Loss: -1185.941406\n",
      "Train Epoch: 1634 [67584/101520 (67%)] Loss: -1169.304932\n",
      "Train Epoch: 1634 [78848/101520 (78%)] Loss: -1179.372803\n",
      "Train Epoch: 1634 [90112/101520 (89%)] Loss: -1178.321777\n",
      "Train Epoch: 1634 [101376/101520 (100%)] Loss: -1182.513428\n",
      "    epoch          : 1634\n",
      "    loss           : -1179.1272389780936\n",
      "    ess            : 1.9665405510657996\n",
      "    log_marginal   : 1179.158784032467\n",
      "    log_joint      : 1387.5442121496153\n",
      "    val_loss       : -1179.198629628057\n",
      "    val_ess        : 1.964764963025632\n",
      "    val_log_marginal: 1179.2392206606658\n",
      "    val_log_joint  : 1387.5296737007473\n",
      "Train Epoch: 1635 [0/101520 (0%)] Loss: -1179.123535\n",
      "Train Epoch: 1635 [11264/101520 (11%)] Loss: -1182.484131\n",
      "Train Epoch: 1635 [22528/101520 (22%)] Loss: -1181.290405\n",
      "Train Epoch: 1635 [33792/101520 (33%)] Loss: -1180.977051\n",
      "Train Epoch: 1635 [45056/101520 (44%)] Loss: -1173.628662\n",
      "Train Epoch: 1635 [56320/101520 (55%)] Loss: -1182.250732\n",
      "Train Epoch: 1635 [67584/101520 (67%)] Loss: -1177.075195\n",
      "Train Epoch: 1635 [78848/101520 (78%)] Loss: -1171.159424\n",
      "Train Epoch: 1635 [90112/101520 (89%)] Loss: -1180.374268\n",
      "Train Epoch: 1635 [101376/101520 (100%)] Loss: -1177.768921\n",
      "    epoch          : 1635\n",
      "    loss           : -1179.050350016685\n",
      "    ess            : 1.9657727821388435\n",
      "    log_marginal   : 1179.08277962076\n",
      "    log_joint      : 1387.5298808495604\n",
      "    val_loss       : -1178.2763300356658\n",
      "    val_ess        : 1.968446928521861\n",
      "    val_log_marginal: 1178.3038648522418\n",
      "    val_log_joint  : 1387.025395932405\n",
      "Train Epoch: 1636 [0/101520 (0%)] Loss: -1181.752930\n",
      "Train Epoch: 1636 [11264/101520 (11%)] Loss: -1172.040283\n",
      "Train Epoch: 1636 [22528/101520 (22%)] Loss: -1179.726807\n",
      "Train Epoch: 1636 [33792/101520 (33%)] Loss: -1173.221313\n",
      "Train Epoch: 1636 [45056/101520 (44%)] Loss: -1180.595947\n",
      "Train Epoch: 1636 [56320/101520 (55%)] Loss: -1173.280884\n",
      "Train Epoch: 1636 [67584/101520 (67%)] Loss: -1180.741089\n",
      "Train Epoch: 1636 [78848/101520 (78%)] Loss: -1185.866821\n",
      "Train Epoch: 1636 [90112/101520 (89%)] Loss: -1177.491943\n",
      "Train Epoch: 1636 [101376/101520 (100%)] Loss: -1172.063110\n",
      "    epoch          : 1636\n",
      "    loss           : -1179.1118906299073\n",
      "    ess            : 1.9669354644852068\n",
      "    log_marginal   : 1179.1435326044284\n",
      "    log_joint      : 1387.5370971066268\n",
      "    val_loss       : -1178.6065567680027\n",
      "    val_ess        : 1.9661339365917703\n",
      "    val_log_marginal: 1178.6393353006115\n",
      "    val_log_joint  : 1387.2321565047555\n",
      "Train Epoch: 1637 [0/101520 (0%)] Loss: -1175.126709\n",
      "Train Epoch: 1637 [11264/101520 (11%)] Loss: -1182.626221\n",
      "Train Epoch: 1637 [22528/101520 (22%)] Loss: -1183.719971\n",
      "Train Epoch: 1637 [33792/101520 (33%)] Loss: -1181.854004\n",
      "Train Epoch: 1637 [45056/101520 (44%)] Loss: -1182.147217\n",
      "Train Epoch: 1637 [56320/101520 (55%)] Loss: -1184.489502\n",
      "Train Epoch: 1637 [67584/101520 (67%)] Loss: -1179.093750\n",
      "Train Epoch: 1637 [78848/101520 (78%)] Loss: -1182.807373\n",
      "Train Epoch: 1637 [90112/101520 (89%)] Loss: -1178.000977\n",
      "Train Epoch: 1637 [101376/101520 (100%)] Loss: -1189.943359\n",
      "    epoch          : 1637\n",
      "    loss           : -1179.1743262209484\n",
      "    ess            : 1.9667843801891385\n",
      "    log_marginal   : 1179.205446789612\n",
      "    log_joint      : 1387.6405869680434\n",
      "    val_loss       : -1177.6993089758832\n",
      "    val_ess        : 1.9660256479097449\n",
      "    val_log_marginal: 1177.7322148862092\n",
      "    val_log_joint  : 1386.5027704653533\n",
      "Train Epoch: 1638 [0/101520 (0%)] Loss: -1174.223999\n",
      "Train Epoch: 1638 [11264/101520 (11%)] Loss: -1173.553223\n",
      "Train Epoch: 1638 [22528/101520 (22%)] Loss: -1176.917725\n",
      "Train Epoch: 1638 [33792/101520 (33%)] Loss: -1184.353027\n",
      "Train Epoch: 1638 [45056/101520 (44%)] Loss: -1182.781250\n",
      "Train Epoch: 1638 [56320/101520 (55%)] Loss: -1186.140747\n",
      "Train Epoch: 1638 [67584/101520 (67%)] Loss: -1176.081543\n",
      "Train Epoch: 1638 [78848/101520 (78%)] Loss: -1177.398193\n",
      "Train Epoch: 1638 [90112/101520 (89%)] Loss: -1183.677490\n",
      "Train Epoch: 1638 [101376/101520 (100%)] Loss: -1185.127808\n",
      "    epoch          : 1638\n",
      "    loss           : -1179.198192500589\n",
      "    ess            : 1.9664968503779503\n",
      "    log_marginal   : 1179.2298994974874\n",
      "    log_joint      : 1387.710431429609\n",
      "    val_loss       : -1178.1792576002038\n",
      "    val_ess        : 1.967221120129461\n",
      "    val_log_marginal: 1178.2109215777853\n",
      "    val_log_joint  : 1386.482522715693\n",
      "Train Epoch: 1639 [0/101520 (0%)] Loss: -1181.814575\n",
      "Train Epoch: 1639 [11264/101520 (11%)] Loss: -1179.064453\n",
      "Train Epoch: 1639 [22528/101520 (22%)] Loss: -1176.767212\n",
      "Train Epoch: 1639 [33792/101520 (33%)] Loss: -1181.171875\n",
      "Train Epoch: 1639 [45056/101520 (44%)] Loss: -1177.358398\n",
      "Train Epoch: 1639 [56320/101520 (55%)] Loss: -1181.146240\n",
      "Train Epoch: 1639 [67584/101520 (67%)] Loss: -1176.294678\n",
      "Train Epoch: 1639 [78848/101520 (78%)] Loss: -1176.266846\n",
      "Train Epoch: 1639 [90112/101520 (89%)] Loss: -1178.736572\n",
      "Train Epoch: 1639 [101376/101520 (100%)] Loss: -1184.423584\n",
      "    epoch          : 1639\n",
      "    loss           : -1179.3245291398398\n",
      "    ess            : 1.966716755574672\n",
      "    log_marginal   : 1179.3558864881045\n",
      "    log_joint      : 1387.805275768491\n",
      "    val_loss       : -1177.695503566576\n",
      "    val_ess        : 1.968846258909806\n",
      "    val_log_marginal: 1177.7235478940217\n",
      "    val_log_joint  : 1386.317335045856\n",
      "Train Epoch: 1640 [0/101520 (0%)] Loss: -1177.563477\n",
      "Train Epoch: 1640 [11264/101520 (11%)] Loss: -1186.373535\n",
      "Train Epoch: 1640 [22528/101520 (22%)] Loss: -1176.506348\n",
      "Train Epoch: 1640 [33792/101520 (33%)] Loss: -1187.004150\n",
      "Train Epoch: 1640 [45056/101520 (44%)] Loss: -1184.910645\n",
      "Train Epoch: 1640 [56320/101520 (55%)] Loss: -1177.149658\n",
      "Train Epoch: 1640 [67584/101520 (67%)] Loss: -1175.879028\n",
      "Train Epoch: 1640 [78848/101520 (78%)] Loss: -1175.777344\n",
      "Train Epoch: 1640 [90112/101520 (89%)] Loss: -1177.644531\n",
      "Train Epoch: 1640 [101376/101520 (100%)] Loss: -1172.400024\n",
      "    epoch          : 1640\n",
      "    loss           : -1179.280856185223\n",
      "    ess            : 1.966056208514688\n",
      "    log_marginal   : 1179.3138415466\n",
      "    log_joint      : 1387.748479948571\n",
      "    val_loss       : -1176.8636633831522\n",
      "    val_ess        : 1.9629048523695574\n",
      "    val_log_marginal: 1176.9006082286005\n",
      "    val_log_joint  : 1385.3524647588315\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1640.pth ...\n",
      "Train Epoch: 1641 [0/101520 (0%)] Loss: -1179.414551\n",
      "Train Epoch: 1641 [11264/101520 (11%)] Loss: -1179.471558\n",
      "Train Epoch: 1641 [22528/101520 (22%)] Loss: -1176.979248\n",
      "Train Epoch: 1641 [33792/101520 (33%)] Loss: -1175.469238\n",
      "Train Epoch: 1641 [45056/101520 (44%)] Loss: -1179.106689\n",
      "Train Epoch: 1641 [56320/101520 (55%)] Loss: -1174.149414\n",
      "Train Epoch: 1641 [67584/101520 (67%)] Loss: -1176.785156\n",
      "Train Epoch: 1641 [78848/101520 (78%)] Loss: -1178.678223\n",
      "Train Epoch: 1641 [90112/101520 (89%)] Loss: -1179.521240\n",
      "Train Epoch: 1641 [101376/101520 (100%)] Loss: -1181.224731\n",
      "    epoch          : 1641\n",
      "    loss           : -1179.3271803352702\n",
      "    ess            : 1.9659460315752268\n",
      "    log_marginal   : 1179.358990386503\n",
      "    log_joint      : 1387.868646822982\n",
      "    val_loss       : -1179.7042660920517\n",
      "    val_ess        : 1.970567884652511\n",
      "    val_log_marginal: 1179.7306438943615\n",
      "    val_log_joint  : 1388.1184878141983\n",
      "Train Epoch: 1642 [0/101520 (0%)] Loss: -1185.279663\n",
      "Train Epoch: 1642 [11264/101520 (11%)] Loss: -1180.523926\n",
      "Train Epoch: 1642 [22528/101520 (22%)] Loss: -1176.350342\n",
      "Train Epoch: 1642 [33792/101520 (33%)] Loss: -1176.444214\n",
      "Train Epoch: 1642 [45056/101520 (44%)] Loss: -1179.789062\n",
      "Train Epoch: 1642 [56320/101520 (55%)] Loss: -1178.276611\n",
      "Train Epoch: 1642 [67584/101520 (67%)] Loss: -1177.496460\n",
      "Train Epoch: 1642 [78848/101520 (78%)] Loss: -1172.830566\n",
      "Train Epoch: 1642 [90112/101520 (89%)] Loss: -1182.069580\n",
      "Train Epoch: 1642 [101376/101520 (100%)] Loss: -1182.496582\n",
      "    epoch          : 1642\n",
      "    loss           : -1179.3863672611103\n",
      "    ess            : 1.966361570597893\n",
      "    log_marginal   : 1179.418742270925\n",
      "    log_joint      : 1387.8986791869504\n",
      "    val_loss       : -1176.882616126019\n",
      "    val_ess        : 1.968785638394563\n",
      "    val_log_marginal: 1176.9105755349865\n",
      "    val_log_joint  : 1385.3568009086277\n",
      "Train Epoch: 1643 [0/101520 (0%)] Loss: -1180.441162\n",
      "Train Epoch: 1643 [11264/101520 (11%)] Loss: -1184.290161\n",
      "Train Epoch: 1643 [22528/101520 (22%)] Loss: -1174.023193\n",
      "Train Epoch: 1643 [33792/101520 (33%)] Loss: -1182.833618\n",
      "Train Epoch: 1643 [45056/101520 (44%)] Loss: -1174.766235\n",
      "Train Epoch: 1643 [56320/101520 (55%)] Loss: -1181.290039\n",
      "Train Epoch: 1643 [67584/101520 (67%)] Loss: -1180.186035\n",
      "Train Epoch: 1643 [78848/101520 (78%)] Loss: -1175.990845\n",
      "Train Epoch: 1643 [90112/101520 (89%)] Loss: -1180.322998\n",
      "Train Epoch: 1643 [101376/101520 (100%)] Loss: -1176.832031\n",
      "    epoch          : 1643\n",
      "    loss           : -1179.4080755339196\n",
      "    ess            : 1.965973416165491\n",
      "    log_marginal   : 1179.4397119876728\n",
      "    log_joint      : 1387.9198206609217\n",
      "    val_loss       : -1177.7199494735055\n",
      "    val_ess        : 1.9667614076448523\n",
      "    val_log_marginal: 1177.750583814538\n",
      "    val_log_joint  : 1385.9036228345788\n",
      "Train Epoch: 1644 [0/101520 (0%)] Loss: -1183.014648\n",
      "Train Epoch: 1644 [11264/101520 (11%)] Loss: -1176.161621\n",
      "Train Epoch: 1644 [22528/101520 (22%)] Loss: -1186.656982\n",
      "Train Epoch: 1644 [33792/101520 (33%)] Loss: -1181.123291\n",
      "Train Epoch: 1644 [45056/101520 (44%)] Loss: -1181.672119\n",
      "Train Epoch: 1644 [56320/101520 (55%)] Loss: -1185.669922\n",
      "Train Epoch: 1644 [67584/101520 (67%)] Loss: -1180.170288\n",
      "Train Epoch: 1644 [78848/101520 (78%)] Loss: -1180.438599\n",
      "Train Epoch: 1644 [90112/101520 (89%)] Loss: -1179.173584\n",
      "Train Epoch: 1644 [101376/101520 (100%)] Loss: -1186.375610\n",
      "    epoch          : 1644\n",
      "    loss           : -1179.576389638623\n",
      "    ess            : 1.9661990410119445\n",
      "    log_marginal   : 1179.607888686597\n",
      "    log_joint      : 1387.9931793979663\n",
      "    val_loss       : -1179.773931088655\n",
      "    val_ess        : 1.9663150725157366\n",
      "    val_log_marginal: 1179.8028246008832\n",
      "    val_log_joint  : 1388.3465045431385\n",
      "Train Epoch: 1645 [0/101520 (0%)] Loss: -1182.601929\n",
      "Train Epoch: 1645 [11264/101520 (11%)] Loss: -1177.307983\n",
      "Train Epoch: 1645 [22528/101520 (22%)] Loss: -1175.651855\n",
      "Train Epoch: 1645 [33792/101520 (33%)] Loss: -1187.826294\n",
      "Train Epoch: 1645 [45056/101520 (44%)] Loss: -1186.327759\n",
      "Train Epoch: 1645 [56320/101520 (55%)] Loss: -1181.093384\n",
      "Train Epoch: 1645 [67584/101520 (67%)] Loss: -1169.221436\n",
      "Train Epoch: 1645 [78848/101520 (78%)] Loss: -1175.785156\n",
      "Train Epoch: 1645 [90112/101520 (89%)] Loss: -1178.095337\n",
      "Train Epoch: 1645 [101376/101520 (100%)] Loss: -1174.058594\n",
      "    epoch          : 1645\n",
      "    loss           : -1179.4916820430276\n",
      "    ess            : 1.9661705326195338\n",
      "    log_marginal   : 1179.524239238183\n",
      "    log_joint      : 1387.9929585672503\n",
      "    val_loss       : -1178.9328825577445\n",
      "    val_ess        : 1.9685313494309136\n",
      "    val_log_marginal: 1178.961133873981\n",
      "    val_log_joint  : 1387.3083814538043\n",
      "Train Epoch: 1646 [0/101520 (0%)] Loss: -1192.652954\n",
      "Train Epoch: 1646 [11264/101520 (11%)] Loss: -1177.947021\n",
      "Train Epoch: 1646 [22528/101520 (22%)] Loss: -1175.974609\n",
      "Train Epoch: 1646 [33792/101520 (33%)] Loss: -1178.631592\n",
      "Train Epoch: 1646 [45056/101520 (44%)] Loss: -1178.484253\n",
      "Train Epoch: 1646 [56320/101520 (55%)] Loss: -1176.128784\n",
      "Train Epoch: 1646 [67584/101520 (67%)] Loss: -1179.550903\n",
      "Train Epoch: 1646 [78848/101520 (78%)] Loss: -1181.536377\n",
      "Train Epoch: 1646 [90112/101520 (89%)] Loss: -1171.889648\n",
      "Train Epoch: 1646 [101376/101520 (100%)] Loss: -1177.633423\n",
      "    epoch          : 1646\n",
      "    loss           : -1179.607345197668\n",
      "    ess            : 1.9657805966372466\n",
      "    log_marginal   : 1179.6407967572236\n",
      "    log_joint      : 1388.0273118522298\n",
      "    val_loss       : -1178.3599216627038\n",
      "    val_ess        : 1.966822131820347\n",
      "    val_log_marginal: 1178.3912459663723\n",
      "    val_log_joint  : 1386.993795643682\n",
      "Train Epoch: 1647 [0/101520 (0%)] Loss: -1183.298096\n",
      "Train Epoch: 1647 [11264/101520 (11%)] Loss: -1176.468018\n",
      "Train Epoch: 1647 [22528/101520 (22%)] Loss: -1183.740112\n",
      "Train Epoch: 1647 [33792/101520 (33%)] Loss: -1173.789917\n",
      "Train Epoch: 1647 [45056/101520 (44%)] Loss: -1176.273682\n",
      "Train Epoch: 1647 [56320/101520 (55%)] Loss: -1183.147583\n",
      "Train Epoch: 1647 [67584/101520 (67%)] Loss: -1181.267456\n",
      "Train Epoch: 1647 [78848/101520 (78%)] Loss: -1170.942017\n",
      "Train Epoch: 1647 [90112/101520 (89%)] Loss: -1182.614502\n",
      "Train Epoch: 1647 [101376/101520 (100%)] Loss: -1182.643066\n",
      "    epoch          : 1647\n",
      "    loss           : -1179.5356469849246\n",
      "    ess            : 1.965551105575945\n",
      "    log_marginal   : 1179.569480090884\n",
      "    log_joint      : 1388.080117383794\n",
      "    val_loss       : -1177.260933254076\n",
      "    val_ess        : 1.9695198587749316\n",
      "    val_log_marginal: 1177.28832477072\n",
      "    val_log_joint  : 1385.763178286345\n",
      "Train Epoch: 1648 [0/101520 (0%)] Loss: -1176.635864\n",
      "Train Epoch: 1648 [11264/101520 (11%)] Loss: -1187.392456\n",
      "Train Epoch: 1648 [22528/101520 (22%)] Loss: -1184.832031\n",
      "Train Epoch: 1648 [33792/101520 (33%)] Loss: -1176.754639\n",
      "Train Epoch: 1648 [45056/101520 (44%)] Loss: -1179.377075\n",
      "Train Epoch: 1648 [56320/101520 (55%)] Loss: -1182.045410\n",
      "Train Epoch: 1648 [67584/101520 (67%)] Loss: -1180.945312\n",
      "Train Epoch: 1648 [78848/101520 (78%)] Loss: -1183.549072\n",
      "Train Epoch: 1648 [90112/101520 (89%)] Loss: -1178.472168\n",
      "Train Epoch: 1648 [101376/101520 (100%)] Loss: -1172.679321\n",
      "    epoch          : 1648\n",
      "    loss           : -1179.6078205971262\n",
      "    ess            : 1.9666717974983867\n",
      "    log_marginal   : 1179.6391024948964\n",
      "    log_joint      : 1388.0346176684202\n",
      "    val_loss       : -1178.1549178413723\n",
      "    val_ess        : 1.9643338296724402\n",
      "    val_log_marginal: 1178.186571204144\n",
      "    val_log_joint  : 1386.7742866847825\n",
      "Train Epoch: 1649 [0/101520 (0%)] Loss: -1183.443970\n",
      "Train Epoch: 1649 [11264/101520 (11%)] Loss: -1175.053223\n",
      "Train Epoch: 1649 [22528/101520 (22%)] Loss: -1184.794678\n",
      "Train Epoch: 1649 [33792/101520 (33%)] Loss: -1177.365967\n",
      "Train Epoch: 1649 [45056/101520 (44%)] Loss: -1182.175049\n",
      "Train Epoch: 1649 [56320/101520 (55%)] Loss: -1177.780396\n",
      "Train Epoch: 1649 [67584/101520 (67%)] Loss: -1178.324219\n",
      "Train Epoch: 1649 [78848/101520 (78%)] Loss: -1175.464355\n",
      "Train Epoch: 1649 [90112/101520 (89%)] Loss: -1176.557861\n",
      "Train Epoch: 1649 [101376/101520 (100%)] Loss: -1170.609619\n",
      "    epoch          : 1649\n",
      "    loss           : -1179.628738786707\n",
      "    ess            : 1.967155607501466\n",
      "    log_marginal   : 1179.660569694174\n",
      "    log_joint      : 1388.1046676252356\n",
      "    val_loss       : -1179.2177946671195\n",
      "    val_ess        : 1.9638045767079229\n",
      "    val_log_marginal: 1179.2529296875\n",
      "    val_log_joint  : 1388.1458209493885\n",
      "Train Epoch: 1650 [0/101520 (0%)] Loss: -1181.714722\n",
      "Train Epoch: 1650 [11264/101520 (11%)] Loss: -1186.162964\n",
      "Train Epoch: 1650 [22528/101520 (22%)] Loss: -1179.932739\n",
      "Train Epoch: 1650 [33792/101520 (33%)] Loss: -1171.834473\n",
      "Train Epoch: 1650 [45056/101520 (44%)] Loss: -1181.089966\n",
      "Train Epoch: 1650 [56320/101520 (55%)] Loss: -1180.981201\n",
      "Train Epoch: 1650 [67584/101520 (67%)] Loss: -1182.778198\n",
      "Train Epoch: 1650 [78848/101520 (78%)] Loss: -1176.580078\n",
      "Train Epoch: 1650 [90112/101520 (89%)] Loss: -1177.104980\n",
      "Train Epoch: 1650 [101376/101520 (100%)] Loss: -1182.699585\n",
      "    epoch          : 1650\n",
      "    loss           : -1179.7448607785018\n",
      "    ess            : 1.9666943046914873\n",
      "    log_marginal   : 1179.7765413983982\n",
      "    log_joint      : 1388.203602853133\n",
      "    val_loss       : -1178.729975161345\n",
      "    val_ess        : 1.9652393227038176\n",
      "    val_log_marginal: 1178.7643671450408\n",
      "    val_log_joint  : 1387.344333814538\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1650.pth ...\n",
      "Train Epoch: 1651 [0/101520 (0%)] Loss: -1176.119629\n",
      "Train Epoch: 1651 [11264/101520 (11%)] Loss: -1177.325195\n",
      "Train Epoch: 1651 [22528/101520 (22%)] Loss: -1180.067139\n",
      "Train Epoch: 1651 [33792/101520 (33%)] Loss: -1184.137451\n",
      "Train Epoch: 1651 [45056/101520 (44%)] Loss: -1174.510498\n",
      "Train Epoch: 1651 [56320/101520 (55%)] Loss: -1179.426025\n",
      "Train Epoch: 1651 [67584/101520 (67%)] Loss: -1178.631592\n",
      "Train Epoch: 1651 [78848/101520 (78%)] Loss: -1186.563232\n",
      "Train Epoch: 1651 [90112/101520 (89%)] Loss: -1180.732910\n",
      "Train Epoch: 1651 [101376/101520 (100%)] Loss: -1201.321899\n",
      "    epoch          : 1651\n",
      "    loss           : -1179.7643766930355\n",
      "    ess            : 1.9661677434815834\n",
      "    log_marginal   : 1179.7972964186165\n",
      "    log_joint      : 1388.2241100522142\n",
      "    val_loss       : -1178.0173923658288\n",
      "    val_ess        : 1.966322271720223\n",
      "    val_log_marginal: 1178.0500275985055\n",
      "    val_log_joint  : 1386.7118026069973\n",
      "Train Epoch: 1652 [0/101520 (0%)] Loss: -1173.265381\n",
      "Train Epoch: 1652 [11264/101520 (11%)] Loss: -1184.669678\n",
      "Train Epoch: 1652 [22528/101520 (22%)] Loss: -1179.993164\n",
      "Train Epoch: 1652 [33792/101520 (33%)] Loss: -1178.256348\n",
      "Train Epoch: 1652 [45056/101520 (44%)] Loss: -1175.699707\n",
      "Train Epoch: 1652 [56320/101520 (55%)] Loss: -1178.947266\n",
      "Train Epoch: 1652 [67584/101520 (67%)] Loss: -1178.303223\n",
      "Train Epoch: 1652 [78848/101520 (78%)] Loss: -1177.482910\n",
      "Train Epoch: 1652 [90112/101520 (89%)] Loss: -1174.194092\n",
      "Train Epoch: 1652 [101376/101520 (100%)] Loss: -1180.596802\n",
      "    epoch          : 1652\n",
      "    loss           : -1179.6415138436323\n",
      "    ess            : 1.9655335704285894\n",
      "    log_marginal   : 1179.6745047257773\n",
      "    log_joint      : 1388.1474652314305\n",
      "    val_loss       : -1176.860356869905\n",
      "    val_ess        : 1.9663086611291636\n",
      "    val_log_marginal: 1176.891118588655\n",
      "    val_log_joint  : 1385.318651282269\n",
      "Train Epoch: 1653 [0/101520 (0%)] Loss: -1174.724609\n",
      "Train Epoch: 1653 [11264/101520 (11%)] Loss: -1180.708008\n",
      "Train Epoch: 1653 [22528/101520 (22%)] Loss: -1174.951660\n",
      "Train Epoch: 1653 [33792/101520 (33%)] Loss: -1178.281982\n",
      "Train Epoch: 1653 [45056/101520 (44%)] Loss: -1180.769043\n",
      "Train Epoch: 1653 [56320/101520 (55%)] Loss: -1191.851685\n",
      "Train Epoch: 1653 [67584/101520 (67%)] Loss: -1176.039185\n",
      "Train Epoch: 1653 [78848/101520 (78%)] Loss: -1178.648071\n",
      "Train Epoch: 1653 [90112/101520 (89%)] Loss: -1177.760742\n",
      "Train Epoch: 1653 [101376/101520 (100%)] Loss: -1186.885986\n",
      "    epoch          : 1653\n",
      "    loss           : -1179.8157020453832\n",
      "    ess            : 1.9654716193376474\n",
      "    log_marginal   : 1179.8487358668342\n",
      "    log_joint      : 1388.2565832090138\n",
      "    val_loss       : -1178.141797936481\n",
      "    val_ess        : 1.9653061680171802\n",
      "    val_log_marginal: 1178.1726658033288\n",
      "    val_log_joint  : 1386.5939569887908\n",
      "Train Epoch: 1654 [0/101520 (0%)] Loss: -1191.508789\n",
      "Train Epoch: 1654 [11264/101520 (11%)] Loss: -1180.547241\n",
      "Train Epoch: 1654 [22528/101520 (22%)] Loss: -1182.346680\n",
      "Train Epoch: 1654 [33792/101520 (33%)] Loss: -1177.694092\n",
      "Train Epoch: 1654 [45056/101520 (44%)] Loss: -1172.980713\n",
      "Train Epoch: 1654 [56320/101520 (55%)] Loss: -1183.321777\n",
      "Train Epoch: 1654 [67584/101520 (67%)] Loss: -1182.519531\n",
      "Train Epoch: 1654 [78848/101520 (78%)] Loss: -1179.400146\n",
      "Train Epoch: 1654 [90112/101520 (89%)] Loss: -1174.891113\n",
      "Train Epoch: 1654 [101376/101520 (100%)] Loss: -1188.005615\n",
      "    epoch          : 1654\n",
      "    loss           : -1179.858632150008\n",
      "    ess            : 1.966828313904192\n",
      "    log_marginal   : 1179.889516552489\n",
      "    log_joint      : 1388.367686209367\n",
      "    val_loss       : -1179.8532927139945\n",
      "    val_ess        : 1.96842534645744\n",
      "    val_log_marginal: 1179.8818147078805\n",
      "    val_log_joint  : 1388.6709674337635\n",
      "Train Epoch: 1655 [0/101520 (0%)] Loss: -1187.805664\n",
      "Train Epoch: 1655 [11264/101520 (11%)] Loss: -1171.641113\n",
      "Train Epoch: 1655 [22528/101520 (22%)] Loss: -1177.110352\n",
      "Train Epoch: 1655 [33792/101520 (33%)] Loss: -1179.939453\n",
      "Train Epoch: 1655 [45056/101520 (44%)] Loss: -1179.550659\n",
      "Train Epoch: 1655 [56320/101520 (55%)] Loss: -1178.450195\n",
      "Train Epoch: 1655 [67584/101520 (67%)] Loss: -1179.442383\n",
      "Train Epoch: 1655 [78848/101520 (78%)] Loss: -1180.860229\n",
      "Train Epoch: 1655 [90112/101520 (89%)] Loss: -1180.989258\n",
      "Train Epoch: 1655 [101376/101520 (100%)] Loss: -1179.398071\n",
      "    epoch          : 1655\n",
      "    loss           : -1179.8398051046247\n",
      "    ess            : 1.9665866874570224\n",
      "    log_marginal   : 1179.871386350699\n",
      "    log_joint      : 1388.3373569507694\n",
      "    val_loss       : -1180.0258523692255\n",
      "    val_ess        : 1.9652221565661223\n",
      "    val_log_marginal: 1180.0613854449728\n",
      "    val_log_joint  : 1388.5464928668478\n",
      "Train Epoch: 1656 [0/101520 (0%)] Loss: -1178.440430\n",
      "Train Epoch: 1656 [11264/101520 (11%)] Loss: -1182.756714\n",
      "Train Epoch: 1656 [22528/101520 (22%)] Loss: -1179.035889\n",
      "Train Epoch: 1656 [33792/101520 (33%)] Loss: -1171.975342\n",
      "Train Epoch: 1656 [45056/101520 (44%)] Loss: -1182.931396\n",
      "Train Epoch: 1656 [56320/101520 (55%)] Loss: -1174.400635\n",
      "Train Epoch: 1656 [67584/101520 (67%)] Loss: -1167.226807\n",
      "Train Epoch: 1656 [78848/101520 (78%)] Loss: -1187.320068\n",
      "Train Epoch: 1656 [90112/101520 (89%)] Loss: -1178.254395\n",
      "Train Epoch: 1656 [101376/101520 (100%)] Loss: -1178.833740\n",
      "    epoch          : 1656\n",
      "    loss           : -1179.9745836114164\n",
      "    ess            : 1.9667970756789548\n",
      "    log_marginal   : 1180.0066494582286\n",
      "    log_joint      : 1388.358537683535\n",
      "    val_loss       : -1179.9559326171875\n",
      "    val_ess        : 1.9672116663144983\n",
      "    val_log_marginal: 1179.986572265625\n",
      "    val_log_joint  : 1388.4739459493885\n",
      "Train Epoch: 1657 [0/101520 (0%)] Loss: -1182.740479\n",
      "Train Epoch: 1657 [11264/101520 (11%)] Loss: -1181.722778\n",
      "Train Epoch: 1657 [22528/101520 (22%)] Loss: -1175.539673\n",
      "Train Epoch: 1657 [33792/101520 (33%)] Loss: -1182.943359\n",
      "Train Epoch: 1657 [45056/101520 (44%)] Loss: -1180.893799\n",
      "Train Epoch: 1657 [56320/101520 (55%)] Loss: -1176.324219\n",
      "Train Epoch: 1657 [67584/101520 (67%)] Loss: -1185.180664\n",
      "Train Epoch: 1657 [78848/101520 (78%)] Loss: -1176.649658\n",
      "Train Epoch: 1657 [90112/101520 (89%)] Loss: -1181.125000\n",
      "Train Epoch: 1657 [101376/101520 (100%)] Loss: -1159.992798\n",
      "    epoch          : 1657\n",
      "    loss           : -1179.7844084926587\n",
      "    ess            : 1.9659270102055229\n",
      "    log_marginal   : 1179.817298774144\n",
      "    log_joint      : 1388.268734419166\n",
      "    val_loss       : -1177.8254235309103\n",
      "    val_ess        : 1.9672776149666829\n",
      "    val_log_marginal: 1177.8560154127038\n",
      "    val_log_joint  : 1386.2411260190217\n",
      "Train Epoch: 1658 [0/101520 (0%)] Loss: -1186.951172\n",
      "Train Epoch: 1658 [11264/101520 (11%)] Loss: -1182.312988\n",
      "Train Epoch: 1658 [22528/101520 (22%)] Loss: -1178.809570\n",
      "Train Epoch: 1658 [33792/101520 (33%)] Loss: -1180.552002\n",
      "Train Epoch: 1658 [45056/101520 (44%)] Loss: -1177.366211\n",
      "Train Epoch: 1658 [56320/101520 (55%)] Loss: -1179.469604\n",
      "Train Epoch: 1658 [67584/101520 (67%)] Loss: -1181.278442\n",
      "Train Epoch: 1658 [78848/101520 (78%)] Loss: -1178.198486\n",
      "Train Epoch: 1658 [90112/101520 (89%)] Loss: -1182.007812\n",
      "Train Epoch: 1658 [101376/101520 (100%)] Loss: -1179.880005\n",
      "    epoch          : 1658\n",
      "    loss           : -1179.8851269285883\n",
      "    ess            : 1.9661019483403346\n",
      "    log_marginal   : 1179.9172044303548\n",
      "    log_joint      : 1388.3735228878768\n",
      "    val_loss       : -1179.7634648862092\n",
      "    val_ess        : 1.9664677847986636\n",
      "    val_log_marginal: 1179.7945238196332\n",
      "    val_log_joint  : 1387.7991359544837\n",
      "Train Epoch: 1659 [0/101520 (0%)] Loss: -1173.258179\n",
      "Train Epoch: 1659 [11264/101520 (11%)] Loss: -1182.776489\n",
      "Train Epoch: 1659 [22528/101520 (22%)] Loss: -1168.031982\n",
      "Train Epoch: 1659 [33792/101520 (33%)] Loss: -1174.429810\n",
      "Train Epoch: 1659 [45056/101520 (44%)] Loss: -1188.093384\n",
      "Train Epoch: 1659 [56320/101520 (55%)] Loss: -1175.374512\n",
      "Train Epoch: 1659 [67584/101520 (67%)] Loss: -1181.904663\n",
      "Train Epoch: 1659 [78848/101520 (78%)] Loss: -1180.073120\n",
      "Train Epoch: 1659 [90112/101520 (89%)] Loss: -1182.912720\n",
      "Train Epoch: 1659 [101376/101520 (100%)] Loss: -1176.921509\n",
      "    epoch          : 1659\n",
      "    loss           : -1179.9364927665672\n",
      "    ess            : 1.9661568900448594\n",
      "    log_marginal   : 1179.9683985111103\n",
      "    log_joint      : 1388.4305990401224\n",
      "    val_loss       : -1177.6983695652175\n",
      "    val_ess        : 1.964279672373896\n",
      "    val_log_marginal: 1177.7337274966033\n",
      "    val_log_joint  : 1386.282369862432\n",
      "Train Epoch: 1660 [0/101520 (0%)] Loss: -1185.848755\n",
      "Train Epoch: 1660 [11264/101520 (11%)] Loss: -1181.571289\n",
      "Train Epoch: 1660 [22528/101520 (22%)] Loss: -1178.155884\n",
      "Train Epoch: 1660 [33792/101520 (33%)] Loss: -1191.859741\n",
      "Train Epoch: 1660 [45056/101520 (44%)] Loss: -1183.330200\n",
      "Train Epoch: 1660 [56320/101520 (55%)] Loss: -1177.904053\n",
      "Train Epoch: 1660 [67584/101520 (67%)] Loss: -1175.813110\n",
      "Train Epoch: 1660 [78848/101520 (78%)] Loss: -1184.812988\n",
      "Train Epoch: 1660 [90112/101520 (89%)] Loss: -1174.156006\n",
      "Train Epoch: 1660 [101376/101520 (100%)] Loss: -1165.827026\n",
      "    epoch          : 1660\n",
      "    loss           : -1179.9703602239715\n",
      "    ess            : 1.9662204016393154\n",
      "    log_marginal   : 1180.002475144276\n",
      "    log_joint      : 1388.408470575534\n",
      "    val_loss       : -1177.8104194972825\n",
      "    val_ess        : 1.9654469075410261\n",
      "    val_log_marginal: 1177.8431714928668\n",
      "    val_log_joint  : 1386.1775273862092\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1660.pth ...\n",
      "Train Epoch: 1661 [0/101520 (0%)] Loss: -1188.892456\n",
      "Train Epoch: 1661 [11264/101520 (11%)] Loss: -1181.145752\n",
      "Train Epoch: 1661 [22528/101520 (22%)] Loss: -1183.685547\n",
      "Train Epoch: 1661 [33792/101520 (33%)] Loss: -1184.645996\n",
      "Train Epoch: 1661 [45056/101520 (44%)] Loss: -1175.081543\n",
      "Train Epoch: 1661 [56320/101520 (55%)] Loss: -1177.168701\n",
      "Train Epoch: 1661 [67584/101520 (67%)] Loss: -1192.693359\n",
      "Train Epoch: 1661 [78848/101520 (78%)] Loss: -1187.557617\n",
      "Train Epoch: 1661 [90112/101520 (89%)] Loss: -1176.917969\n",
      "Train Epoch: 1661 [101376/101520 (100%)] Loss: -1178.044556\n",
      "    epoch          : 1661\n",
      "    loss           : -1180.0601702359454\n",
      "    ess            : 1.9667905125785712\n",
      "    log_marginal   : 1180.091820184909\n",
      "    log_joint      : 1388.4744627679413\n",
      "    val_loss       : -1178.935451341712\n",
      "    val_ess        : 1.9675700560860012\n",
      "    val_log_marginal: 1178.9675558338995\n",
      "    val_log_joint  : 1387.2020741338315\n",
      "Train Epoch: 1662 [0/101520 (0%)] Loss: -1179.008301\n",
      "Train Epoch: 1662 [11264/101520 (11%)] Loss: -1182.125000\n",
      "Train Epoch: 1662 [22528/101520 (22%)] Loss: -1172.943115\n",
      "Train Epoch: 1662 [33792/101520 (33%)] Loss: -1182.555542\n",
      "Train Epoch: 1662 [45056/101520 (44%)] Loss: -1172.106445\n",
      "Train Epoch: 1662 [56320/101520 (55%)] Loss: -1175.232178\n",
      "Train Epoch: 1662 [67584/101520 (67%)] Loss: -1180.722534\n",
      "Train Epoch: 1662 [78848/101520 (78%)] Loss: -1179.976807\n",
      "Train Epoch: 1662 [90112/101520 (89%)] Loss: -1179.079224\n",
      "Train Epoch: 1662 [101376/101520 (100%)] Loss: -1192.380737\n",
      "    epoch          : 1662\n",
      "    loss           : -1180.0747806414886\n",
      "    ess            : 1.9654248455661025\n",
      "    log_marginal   : 1180.1087143481077\n",
      "    log_joint      : 1388.5242576407427\n",
      "    val_loss       : -1180.3138586956522\n",
      "    val_ess        : 1.9661975321562395\n",
      "    val_log_marginal: 1180.3458729619565\n",
      "    val_log_joint  : 1388.6414423403533\n",
      "Train Epoch: 1663 [0/101520 (0%)] Loss: -1176.779785\n",
      "Train Epoch: 1663 [11264/101520 (11%)] Loss: -1177.917603\n",
      "Train Epoch: 1663 [22528/101520 (22%)] Loss: -1180.488037\n",
      "Train Epoch: 1663 [33792/101520 (33%)] Loss: -1187.455566\n",
      "Train Epoch: 1663 [45056/101520 (44%)] Loss: -1178.806885\n",
      "Train Epoch: 1663 [56320/101520 (55%)] Loss: -1178.699463\n",
      "Train Epoch: 1663 [67584/101520 (67%)] Loss: -1182.658691\n",
      "Train Epoch: 1663 [78848/101520 (78%)] Loss: -1178.571167\n",
      "Train Epoch: 1663 [90112/101520 (89%)] Loss: -1177.588135\n",
      "Train Epoch: 1663 [101376/101520 (100%)] Loss: -1169.607666\n",
      "    epoch          : 1663\n",
      "    loss           : -1179.9964237692368\n",
      "    ess            : 1.965969076108693\n",
      "    log_marginal   : 1180.0290650027482\n",
      "    log_joint      : 1388.5449120603016\n",
      "    val_loss       : -1179.0891431725543\n",
      "    val_ess        : 1.9669487890989885\n",
      "    val_log_marginal: 1179.122261379076\n",
      "    val_log_joint  : 1387.4870446246603\n",
      "Train Epoch: 1664 [0/101520 (0%)] Loss: -1180.569702\n",
      "Train Epoch: 1664 [11264/101520 (11%)] Loss: -1182.460571\n",
      "Train Epoch: 1664 [22528/101520 (22%)] Loss: -1172.362549\n",
      "Train Epoch: 1664 [33792/101520 (33%)] Loss: -1175.946533\n",
      "Train Epoch: 1664 [45056/101520 (44%)] Loss: -1181.156006\n",
      "Train Epoch: 1664 [56320/101520 (55%)] Loss: -1185.984497\n",
      "Train Epoch: 1664 [67584/101520 (67%)] Loss: -1175.054565\n",
      "Train Epoch: 1664 [78848/101520 (78%)] Loss: -1182.702148\n",
      "Train Epoch: 1664 [90112/101520 (89%)] Loss: -1181.364746\n",
      "Train Epoch: 1664 [101376/101520 (100%)] Loss: -1178.295410\n",
      "    epoch          : 1664\n",
      "    loss           : -1180.123855974207\n",
      "    ess            : 1.9665966806699282\n",
      "    log_marginal   : 1180.1558788817133\n",
      "    log_joint      : 1388.5568485739243\n",
      "    val_loss       : -1178.2633534307065\n",
      "    val_ess        : 1.9677580750506858\n",
      "    val_log_marginal: 1178.2952774711277\n",
      "    val_log_joint  : 1386.422315514606\n",
      "Train Epoch: 1665 [0/101520 (0%)] Loss: -1187.856445\n",
      "Train Epoch: 1665 [11264/101520 (11%)] Loss: -1176.903320\n",
      "Train Epoch: 1665 [22528/101520 (22%)] Loss: -1176.071777\n",
      "Train Epoch: 1665 [33792/101520 (33%)] Loss: -1173.976074\n",
      "Train Epoch: 1665 [45056/101520 (44%)] Loss: -1173.843750\n",
      "Train Epoch: 1665 [56320/101520 (55%)] Loss: -1182.956787\n",
      "Train Epoch: 1665 [67584/101520 (67%)] Loss: -1178.435791\n",
      "Train Epoch: 1665 [78848/101520 (78%)] Loss: -1192.545166\n",
      "Train Epoch: 1665 [90112/101520 (89%)] Loss: -1191.147217\n",
      "Train Epoch: 1665 [101376/101520 (100%)] Loss: -1180.247437\n",
      "    epoch          : 1665\n",
      "    loss           : -1180.1081064502198\n",
      "    ess            : 1.9663107628798364\n",
      "    log_marginal   : 1180.141510776539\n",
      "    log_joint      : 1388.5935598402168\n",
      "    val_loss       : -1178.5970565132473\n",
      "    val_ess        : 1.9584899570630945\n",
      "    val_log_marginal: 1178.6379288383152\n",
      "    val_log_joint  : 1387.2744034476902\n",
      "Train Epoch: 1666 [0/101520 (0%)] Loss: -1180.984741\n",
      "Train Epoch: 1666 [11264/101520 (11%)] Loss: -1176.332520\n",
      "Train Epoch: 1666 [22528/101520 (22%)] Loss: -1185.063721\n",
      "Train Epoch: 1666 [33792/101520 (33%)] Loss: -1184.199951\n",
      "Train Epoch: 1666 [45056/101520 (44%)] Loss: -1173.593750\n",
      "Train Epoch: 1666 [56320/101520 (55%)] Loss: -1179.318848\n",
      "Train Epoch: 1666 [67584/101520 (67%)] Loss: -1182.339844\n",
      "Train Epoch: 1666 [78848/101520 (78%)] Loss: -1170.752930\n",
      "Train Epoch: 1666 [90112/101520 (89%)] Loss: -1180.619507\n",
      "Train Epoch: 1666 [101376/101520 (100%)] Loss: -1184.194458\n",
      "    epoch          : 1666\n",
      "    loss           : -1180.2132924142195\n",
      "    ess            : 1.9660579870693649\n",
      "    log_marginal   : 1180.245828139722\n",
      "    log_joint      : 1388.6163004966238\n",
      "    val_loss       : -1179.2818497367527\n",
      "    val_ess        : 1.9679220811180447\n",
      "    val_log_marginal: 1179.3097826086957\n",
      "    val_log_joint  : 1387.606440005095\n",
      "Train Epoch: 1667 [0/101520 (0%)] Loss: -1183.938721\n",
      "Train Epoch: 1667 [11264/101520 (11%)] Loss: -1179.325439\n",
      "Train Epoch: 1667 [22528/101520 (22%)] Loss: -1190.634888\n",
      "Train Epoch: 1667 [33792/101520 (33%)] Loss: -1177.020386\n",
      "Train Epoch: 1667 [45056/101520 (44%)] Loss: -1174.649658\n",
      "Train Epoch: 1667 [56320/101520 (55%)] Loss: -1178.587402\n",
      "Train Epoch: 1667 [67584/101520 (67%)] Loss: -1179.372070\n",
      "Train Epoch: 1667 [78848/101520 (78%)] Loss: -1179.477417\n",
      "Train Epoch: 1667 [90112/101520 (89%)] Loss: -1170.940063\n",
      "Train Epoch: 1667 [101376/101520 (100%)] Loss: -1179.325317\n",
      "    epoch          : 1667\n",
      "    loss           : -1180.070611848304\n",
      "    ess            : 1.9663016826064144\n",
      "    log_marginal   : 1180.1029966728172\n",
      "    log_joint      : 1388.558245941622\n",
      "    val_loss       : -1178.6044656504755\n",
      "    val_ess        : 1.9664242993230405\n",
      "    val_log_marginal: 1178.6347337805707\n",
      "    val_log_joint  : 1387.0444070567255\n",
      "Train Epoch: 1668 [0/101520 (0%)] Loss: -1179.981689\n",
      "Train Epoch: 1668 [11264/101520 (11%)] Loss: -1183.958130\n",
      "Train Epoch: 1668 [22528/101520 (22%)] Loss: -1175.099121\n",
      "Train Epoch: 1668 [33792/101520 (33%)] Loss: -1179.668701\n",
      "Train Epoch: 1668 [45056/101520 (44%)] Loss: -1173.854004\n",
      "Train Epoch: 1668 [56320/101520 (55%)] Loss: -1175.362549\n",
      "Train Epoch: 1668 [67584/101520 (67%)] Loss: -1181.107544\n",
      "Train Epoch: 1668 [78848/101520 (78%)] Loss: -1183.046265\n",
      "Train Epoch: 1668 [90112/101520 (89%)] Loss: -1164.273682\n",
      "Train Epoch: 1668 [101376/101520 (100%)] Loss: -1185.349243\n",
      "    epoch          : 1668\n",
      "    loss           : -1180.1277916683025\n",
      "    ess            : 1.9662349080320578\n",
      "    log_marginal   : 1180.159632390468\n",
      "    log_joint      : 1388.5747119385992\n",
      "    val_loss       : -1178.3104831861413\n",
      "    val_ess        : 1.9617574733236562\n",
      "    val_log_marginal: 1178.3464567764945\n",
      "    val_log_joint  : 1386.6599864130435\n",
      "Train Epoch: 1669 [0/101520 (0%)] Loss: -1183.535767\n",
      "Train Epoch: 1669 [11264/101520 (11%)] Loss: -1170.404785\n",
      "Train Epoch: 1669 [22528/101520 (22%)] Loss: -1178.364502\n",
      "Train Epoch: 1669 [33792/101520 (33%)] Loss: -1178.991699\n",
      "Train Epoch: 1669 [45056/101520 (44%)] Loss: -1179.931885\n",
      "Train Epoch: 1669 [56320/101520 (55%)] Loss: -1178.640625\n",
      "Train Epoch: 1669 [67584/101520 (67%)] Loss: -1179.120605\n",
      "Train Epoch: 1669 [78848/101520 (78%)] Loss: -1178.906860\n",
      "Train Epoch: 1669 [90112/101520 (89%)] Loss: -1171.986816\n",
      "Train Epoch: 1669 [101376/101520 (100%)] Loss: -1180.348755\n",
      "    epoch          : 1669\n",
      "    loss           : -1180.1366132419912\n",
      "    ess            : 1.9661166871612394\n",
      "    log_marginal   : 1180.1688974658448\n",
      "    log_joint      : 1388.5727747624842\n",
      "    val_loss       : -1177.7795250934103\n",
      "    val_ess        : 1.9679129123687744\n",
      "    val_log_marginal: 1177.810844089674\n",
      "    val_log_joint  : 1386.3599481997283\n",
      "Train Epoch: 1670 [0/101520 (0%)] Loss: -1182.662354\n",
      "Train Epoch: 1670 [11264/101520 (11%)] Loss: -1182.355469\n",
      "Train Epoch: 1670 [22528/101520 (22%)] Loss: -1186.725342\n",
      "Train Epoch: 1670 [33792/101520 (33%)] Loss: -1179.541260\n",
      "Train Epoch: 1670 [45056/101520 (44%)] Loss: -1184.951416\n",
      "Train Epoch: 1670 [56320/101520 (55%)] Loss: -1177.774414\n",
      "Train Epoch: 1670 [67584/101520 (67%)] Loss: -1179.138184\n",
      "Train Epoch: 1670 [78848/101520 (78%)] Loss: -1177.152344\n",
      "Train Epoch: 1670 [90112/101520 (89%)] Loss: -1185.721680\n",
      "Train Epoch: 1670 [101376/101520 (100%)] Loss: -1170.388062\n",
      "    epoch          : 1670\n",
      "    loss           : -1180.1643912923994\n",
      "    ess            : 1.965956651385705\n",
      "    log_marginal   : 1180.1967804108433\n",
      "    log_joint      : 1388.6332817652717\n",
      "    val_loss       : -1178.7252728006115\n",
      "    val_ess        : 1.9649905277335125\n",
      "    val_log_marginal: 1178.7577806555707\n",
      "    val_log_joint  : 1387.2561618970788\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1670.pth ...\n",
      "Train Epoch: 1671 [0/101520 (0%)] Loss: -1177.398438\n",
      "Train Epoch: 1671 [11264/101520 (11%)] Loss: -1176.574463\n",
      "Train Epoch: 1671 [22528/101520 (22%)] Loss: -1181.137451\n",
      "Train Epoch: 1671 [33792/101520 (33%)] Loss: -1179.012207\n",
      "Train Epoch: 1671 [45056/101520 (44%)] Loss: -1176.868408\n",
      "Train Epoch: 1671 [56320/101520 (55%)] Loss: -1182.230225\n",
      "Train Epoch: 1671 [67584/101520 (67%)] Loss: -1180.351318\n",
      "Train Epoch: 1671 [78848/101520 (78%)] Loss: -1177.803101\n",
      "Train Epoch: 1671 [90112/101520 (89%)] Loss: -1182.872803\n",
      "Train Epoch: 1671 [101376/101520 (100%)] Loss: -1183.615723\n",
      "    epoch          : 1671\n",
      "    loss           : -1180.2501545815012\n",
      "    ess            : 1.9672362331169932\n",
      "    log_marginal   : 1180.2815677508636\n",
      "    log_joint      : 1388.723989208739\n",
      "    val_loss       : -1179.9200757897418\n",
      "    val_ess        : 1.9712022594783618\n",
      "    val_log_marginal: 1179.943656589674\n",
      "    val_log_joint  : 1387.9022641389267\n",
      "Train Epoch: 1672 [0/101520 (0%)] Loss: -1183.312256\n",
      "Train Epoch: 1672 [11264/101520 (11%)] Loss: -1179.742676\n",
      "Train Epoch: 1672 [22528/101520 (22%)] Loss: -1182.629395\n",
      "Train Epoch: 1672 [33792/101520 (33%)] Loss: -1178.686279\n",
      "Train Epoch: 1672 [45056/101520 (44%)] Loss: -1187.894287\n",
      "Train Epoch: 1672 [56320/101520 (55%)] Loss: -1182.551758\n",
      "Train Epoch: 1672 [67584/101520 (67%)] Loss: -1179.209473\n",
      "Train Epoch: 1672 [78848/101520 (78%)] Loss: -1171.023682\n",
      "Train Epoch: 1672 [90112/101520 (89%)] Loss: -1185.779785\n",
      "Train Epoch: 1672 [101376/101520 (100%)] Loss: -1188.791016\n",
      "    epoch          : 1672\n",
      "    loss           : -1180.354406922307\n",
      "    ess            : 1.9655354927532638\n",
      "    log_marginal   : 1180.387698993012\n",
      "    log_joint      : 1388.8503307553392\n",
      "    val_loss       : -1178.6727454144022\n",
      "    val_ess        : 1.965447239253832\n",
      "    val_log_marginal: 1178.702344811481\n",
      "    val_log_joint  : 1387.1962625254755\n",
      "Train Epoch: 1673 [0/101520 (0%)] Loss: -1182.092041\n",
      "Train Epoch: 1673 [11264/101520 (11%)] Loss: -1185.284668\n",
      "Train Epoch: 1673 [22528/101520 (22%)] Loss: -1185.323364\n",
      "Train Epoch: 1673 [33792/101520 (33%)] Loss: -1180.270874\n",
      "Train Epoch: 1673 [45056/101520 (44%)] Loss: -1179.363770\n",
      "Train Epoch: 1673 [56320/101520 (55%)] Loss: -1176.630493\n",
      "Train Epoch: 1673 [67584/101520 (67%)] Loss: -1176.648438\n",
      "Train Epoch: 1673 [78848/101520 (78%)] Loss: -1177.000122\n",
      "Train Epoch: 1673 [90112/101520 (89%)] Loss: -1183.839844\n",
      "Train Epoch: 1673 [101376/101520 (100%)] Loss: -1164.561890\n",
      "    epoch          : 1673\n",
      "    loss           : -1180.3274207953832\n",
      "    ess            : 1.9658783764096361\n",
      "    log_marginal   : 1180.3601601758794\n",
      "    log_joint      : 1388.7767597754396\n",
      "    val_loss       : -1180.776706861413\n",
      "    val_ess        : 1.9671071664146755\n",
      "    val_log_marginal: 1180.8074951171875\n",
      "    val_log_joint  : 1389.2523724099865\n",
      "Train Epoch: 1674 [0/101520 (0%)] Loss: -1185.285767\n",
      "Train Epoch: 1674 [11264/101520 (11%)] Loss: -1186.938232\n",
      "Train Epoch: 1674 [22528/101520 (22%)] Loss: -1184.553955\n",
      "Train Epoch: 1674 [33792/101520 (33%)] Loss: -1183.482178\n",
      "Train Epoch: 1674 [45056/101520 (44%)] Loss: -1177.809082\n",
      "Train Epoch: 1674 [56320/101520 (55%)] Loss: -1177.687500\n",
      "Train Epoch: 1674 [67584/101520 (67%)] Loss: -1176.311035\n",
      "Train Epoch: 1674 [78848/101520 (78%)] Loss: -1180.776611\n",
      "Train Epoch: 1674 [90112/101520 (89%)] Loss: -1184.005127\n",
      "Train Epoch: 1674 [101376/101520 (100%)] Loss: -1178.305908\n",
      "    epoch          : 1674\n",
      "    loss           : -1180.3769347224404\n",
      "    ess            : 1.9662352249250938\n",
      "    log_marginal   : 1180.4092416427843\n",
      "    log_joint      : 1388.852068570391\n",
      "    val_loss       : -1178.5444760529892\n",
      "    val_ess        : 1.965573937996574\n",
      "    val_log_marginal: 1178.5774828040082\n",
      "    val_log_joint  : 1386.93822711447\n",
      "Train Epoch: 1675 [0/101520 (0%)] Loss: -1177.798096\n",
      "Train Epoch: 1675 [11264/101520 (11%)] Loss: -1181.398315\n",
      "Train Epoch: 1675 [22528/101520 (22%)] Loss: -1177.066650\n",
      "Train Epoch: 1675 [33792/101520 (33%)] Loss: -1178.424438\n",
      "Train Epoch: 1675 [45056/101520 (44%)] Loss: -1183.545898\n",
      "Train Epoch: 1675 [56320/101520 (55%)] Loss: -1173.689453\n",
      "Train Epoch: 1675 [67584/101520 (67%)] Loss: -1174.807861\n",
      "Train Epoch: 1675 [78848/101520 (78%)] Loss: -1185.288086\n",
      "Train Epoch: 1675 [90112/101520 (89%)] Loss: -1174.796997\n",
      "Train Epoch: 1675 [101376/101520 (100%)] Loss: -1186.677124\n",
      "    epoch          : 1675\n",
      "    loss           : -1180.383562711016\n",
      "    ess            : 1.9654684737699115\n",
      "    log_marginal   : 1180.4169627434046\n",
      "    log_joint      : 1388.9173620789495\n",
      "    val_loss       : -1178.9883184018342\n",
      "    val_ess        : 1.963734999946926\n",
      "    val_log_marginal: 1179.0265370244565\n",
      "    val_log_joint  : 1387.6624596637228\n",
      "Train Epoch: 1676 [0/101520 (0%)] Loss: -1186.255615\n",
      "Train Epoch: 1676 [11264/101520 (11%)] Loss: -1174.479004\n",
      "Train Epoch: 1676 [22528/101520 (22%)] Loss: -1179.459717\n",
      "Train Epoch: 1676 [33792/101520 (33%)] Loss: -1186.435669\n",
      "Train Epoch: 1676 [45056/101520 (44%)] Loss: -1187.600830\n",
      "Train Epoch: 1676 [56320/101520 (55%)] Loss: -1184.023926\n",
      "Train Epoch: 1676 [67584/101520 (67%)] Loss: -1187.961060\n",
      "Train Epoch: 1676 [78848/101520 (78%)] Loss: -1181.418457\n",
      "Train Epoch: 1676 [90112/101520 (89%)] Loss: -1181.249146\n",
      "Train Epoch: 1676 [101376/101520 (100%)] Loss: -1182.266235\n",
      "    epoch          : 1676\n",
      "    loss           : -1180.4497377021828\n",
      "    ess            : 1.9656010087411009\n",
      "    log_marginal   : 1180.4828432936165\n",
      "    log_joint      : 1388.92858764035\n",
      "    val_loss       : -1180.3897174337635\n",
      "    val_ess        : 1.9678824725358381\n",
      "    val_log_marginal: 1180.4204526154892\n",
      "    val_log_joint  : 1388.7113673997962\n",
      "Train Epoch: 1677 [0/101520 (0%)] Loss: -1181.932007\n",
      "Train Epoch: 1677 [11264/101520 (11%)] Loss: -1177.652100\n",
      "Train Epoch: 1677 [22528/101520 (22%)] Loss: -1181.089233\n",
      "Train Epoch: 1677 [33792/101520 (33%)] Loss: -1180.669434\n",
      "Train Epoch: 1677 [45056/101520 (44%)] Loss: -1178.347290\n",
      "Train Epoch: 1677 [56320/101520 (55%)] Loss: -1176.711304\n",
      "Train Epoch: 1677 [67584/101520 (67%)] Loss: -1178.938477\n",
      "Train Epoch: 1677 [78848/101520 (78%)] Loss: -1185.915527\n",
      "Train Epoch: 1677 [90112/101520 (89%)] Loss: -1177.496338\n",
      "Train Epoch: 1677 [101376/101520 (100%)] Loss: -1173.896240\n",
      "    epoch          : 1677\n",
      "    loss           : -1180.460354752277\n",
      "    ess            : 1.9654636748472052\n",
      "    log_marginal   : 1180.4942577879633\n",
      "    log_joint      : 1388.9313443437893\n",
      "    val_loss       : -1180.8613387398098\n",
      "    val_ess        : 1.9661976099014282\n",
      "    val_log_marginal: 1180.893013332201\n",
      "    val_log_joint  : 1389.301656971807\n",
      "Train Epoch: 1678 [0/101520 (0%)] Loss: -1184.299316\n",
      "Train Epoch: 1678 [11264/101520 (11%)] Loss: -1180.870605\n",
      "Train Epoch: 1678 [22528/101520 (22%)] Loss: -1176.812988\n",
      "Train Epoch: 1678 [33792/101520 (33%)] Loss: -1175.640747\n",
      "Train Epoch: 1678 [45056/101520 (44%)] Loss: -1184.877686\n",
      "Train Epoch: 1678 [56320/101520 (55%)] Loss: -1181.154785\n",
      "Train Epoch: 1678 [67584/101520 (67%)] Loss: -1180.115479\n",
      "Train Epoch: 1678 [78848/101520 (78%)] Loss: -1175.610596\n",
      "Train Epoch: 1678 [90112/101520 (89%)] Loss: -1174.533936\n",
      "Train Epoch: 1678 [101376/101520 (100%)] Loss: -1181.318237\n",
      "    epoch          : 1678\n",
      "    loss           : -1180.516418150322\n",
      "    ess            : 1.965445574803568\n",
      "    log_marginal   : 1180.5492691730135\n",
      "    log_joint      : 1388.960880452065\n",
      "    val_loss       : -1177.7816215183425\n",
      "    val_ess        : 1.9621518383855405\n",
      "    val_log_marginal: 1177.8175526494565\n",
      "    val_log_joint  : 1386.3951044497283\n",
      "Train Epoch: 1679 [0/101520 (0%)] Loss: -1186.270020\n",
      "Train Epoch: 1679 [11264/101520 (11%)] Loss: -1181.715210\n",
      "Train Epoch: 1679 [22528/101520 (22%)] Loss: -1182.114014\n",
      "Train Epoch: 1679 [33792/101520 (33%)] Loss: -1168.653809\n",
      "Train Epoch: 1679 [45056/101520 (44%)] Loss: -1177.941895\n",
      "Train Epoch: 1679 [56320/101520 (55%)] Loss: -1177.407837\n",
      "Train Epoch: 1679 [67584/101520 (67%)] Loss: -1172.124023\n",
      "Train Epoch: 1679 [78848/101520 (78%)] Loss: -1175.019043\n",
      "Train Epoch: 1679 [90112/101520 (89%)] Loss: -1173.510620\n",
      "Train Epoch: 1679 [101376/101520 (100%)] Loss: -1171.039917\n",
      "    epoch          : 1679\n",
      "    loss           : -1180.4748841865578\n",
      "    ess            : 1.9664392896633052\n",
      "    log_marginal   : 1180.5064396690484\n",
      "    log_joint      : 1389.0063415220634\n",
      "    val_loss       : -1179.2667660920517\n",
      "    val_ess        : 1.969025907309159\n",
      "    val_log_marginal: 1179.2955163043478\n",
      "    val_log_joint  : 1387.3947647758152\n",
      "Train Epoch: 1680 [0/101520 (0%)] Loss: -1185.642822\n",
      "Train Epoch: 1680 [11264/101520 (11%)] Loss: -1179.572632\n",
      "Train Epoch: 1680 [22528/101520 (22%)] Loss: -1190.883301\n",
      "Train Epoch: 1680 [33792/101520 (33%)] Loss: -1179.501709\n",
      "Train Epoch: 1680 [45056/101520 (44%)] Loss: -1178.085938\n",
      "Train Epoch: 1680 [56320/101520 (55%)] Loss: -1178.205322\n",
      "Train Epoch: 1680 [67584/101520 (67%)] Loss: -1182.113892\n",
      "Train Epoch: 1680 [78848/101520 (78%)] Loss: -1181.584717\n",
      "Train Epoch: 1680 [90112/101520 (89%)] Loss: -1181.395996\n",
      "Train Epoch: 1680 [101376/101520 (100%)] Loss: -1188.926392\n",
      "    epoch          : 1680\n",
      "    loss           : -1180.5775508401382\n",
      "    ess            : 1.9663036486611294\n",
      "    log_marginal   : 1180.610822054609\n",
      "    log_joint      : 1389.095400096184\n",
      "    val_loss       : -1179.3521834663723\n",
      "    val_ess        : 1.9664070243420808\n",
      "    val_log_marginal: 1179.3854131283967\n",
      "    val_log_joint  : 1387.6784986413043\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1680.pth ...\n",
      "Train Epoch: 1681 [0/101520 (0%)] Loss: -1185.913696\n",
      "Train Epoch: 1681 [11264/101520 (11%)] Loss: -1175.892578\n",
      "Train Epoch: 1681 [22528/101520 (22%)] Loss: -1180.071655\n",
      "Train Epoch: 1681 [33792/101520 (33%)] Loss: -1176.559082\n",
      "Train Epoch: 1681 [45056/101520 (44%)] Loss: -1173.440186\n",
      "Train Epoch: 1681 [56320/101520 (55%)] Loss: -1179.493774\n",
      "Train Epoch: 1681 [67584/101520 (67%)] Loss: -1184.314331\n",
      "Train Epoch: 1681 [78848/101520 (78%)] Loss: -1182.477295\n",
      "Train Epoch: 1681 [90112/101520 (89%)] Loss: -1195.768555\n",
      "Train Epoch: 1681 [101376/101520 (100%)] Loss: -1176.018433\n",
      "    epoch          : 1681\n",
      "    loss           : -1180.5476632429727\n",
      "    ess            : 1.9655236473035573\n",
      "    log_marginal   : 1180.580716080402\n",
      "    log_joint      : 1389.0431245583386\n",
      "    val_loss       : -1178.3740234375\n",
      "    val_ess        : 1.970054206640824\n",
      "    val_log_marginal: 1178.4036971382473\n",
      "    val_log_joint  : 1387.0440992272418\n",
      "Train Epoch: 1682 [0/101520 (0%)] Loss: -1181.131104\n",
      "Train Epoch: 1682 [11264/101520 (11%)] Loss: -1183.443848\n",
      "Train Epoch: 1682 [22528/101520 (22%)] Loss: -1183.890015\n",
      "Train Epoch: 1682 [33792/101520 (33%)] Loss: -1176.814453\n",
      "Train Epoch: 1682 [45056/101520 (44%)] Loss: -1177.102539\n",
      "Train Epoch: 1682 [56320/101520 (55%)] Loss: -1184.859741\n",
      "Train Epoch: 1682 [67584/101520 (67%)] Loss: -1183.598145\n",
      "Train Epoch: 1682 [78848/101520 (78%)] Loss: -1178.476807\n",
      "Train Epoch: 1682 [90112/101520 (89%)] Loss: -1187.937012\n",
      "Train Epoch: 1682 [101376/101520 (100%)] Loss: -1175.120728\n",
      "    epoch          : 1682\n",
      "    loss           : -1180.6135364321608\n",
      "    ess            : 1.966442916261491\n",
      "    log_marginal   : 1180.645693064934\n",
      "    log_joint      : 1389.107051370132\n",
      "    val_loss       : -1180.0299178413723\n",
      "    val_ess        : 1.9629760773285576\n",
      "    val_log_marginal: 1180.0658595872962\n",
      "    val_log_joint  : 1388.3788797129755\n",
      "Train Epoch: 1683 [0/101520 (0%)] Loss: -1177.348877\n",
      "Train Epoch: 1683 [11264/101520 (11%)] Loss: -1183.441040\n",
      "Train Epoch: 1683 [22528/101520 (22%)] Loss: -1187.107056\n",
      "Train Epoch: 1683 [33792/101520 (33%)] Loss: -1174.933838\n",
      "Train Epoch: 1683 [45056/101520 (44%)] Loss: -1181.577515\n",
      "Train Epoch: 1683 [56320/101520 (55%)] Loss: -1183.005127\n",
      "Train Epoch: 1683 [67584/101520 (67%)] Loss: -1179.239868\n",
      "Train Epoch: 1683 [78848/101520 (78%)] Loss: -1182.387329\n",
      "Train Epoch: 1683 [90112/101520 (89%)] Loss: -1179.247070\n",
      "Train Epoch: 1683 [101376/101520 (100%)] Loss: -1169.303955\n",
      "    epoch          : 1683\n",
      "    loss           : -1180.5765221370525\n",
      "    ess            : 1.966462005322902\n",
      "    log_marginal   : 1180.6073390634815\n",
      "    log_joint      : 1389.103957899851\n",
      "    val_loss       : -1180.2582264775815\n",
      "    val_ess        : 1.966400991315427\n",
      "    val_log_marginal: 1180.289794921875\n",
      "    val_log_joint  : 1388.5438126273777\n",
      "Train Epoch: 1684 [0/101520 (0%)] Loss: -1183.459106\n",
      "Train Epoch: 1684 [11264/101520 (11%)] Loss: -1183.694702\n",
      "Train Epoch: 1684 [22528/101520 (22%)] Loss: -1185.949585\n",
      "Train Epoch: 1684 [33792/101520 (33%)] Loss: -1183.307373\n",
      "Train Epoch: 1684 [45056/101520 (44%)] Loss: -1180.998291\n",
      "Train Epoch: 1684 [56320/101520 (55%)] Loss: -1182.360474\n",
      "Train Epoch: 1684 [67584/101520 (67%)] Loss: -1180.052490\n",
      "Train Epoch: 1684 [78848/101520 (78%)] Loss: -1181.152222\n",
      "Train Epoch: 1684 [90112/101520 (89%)] Loss: -1178.013550\n",
      "Train Epoch: 1684 [101376/101520 (100%)] Loss: -1180.272095\n",
      "    epoch          : 1684\n",
      "    loss           : -1180.8051874362045\n",
      "    ess            : 1.9662165270378842\n",
      "    log_marginal   : 1180.8376158134422\n",
      "    log_joint      : 1389.2442841649654\n",
      "    val_loss       : -1180.114841627038\n",
      "    val_ess        : 1.9659212931342747\n",
      "    val_log_marginal: 1180.1452795940897\n",
      "    val_log_joint  : 1389.0827318274457\n",
      "Train Epoch: 1685 [0/101520 (0%)] Loss: -1184.883423\n",
      "Train Epoch: 1685 [11264/101520 (11%)] Loss: -1185.572632\n",
      "Train Epoch: 1685 [22528/101520 (22%)] Loss: -1176.575195\n",
      "Train Epoch: 1685 [33792/101520 (33%)] Loss: -1178.744385\n",
      "Train Epoch: 1685 [45056/101520 (44%)] Loss: -1177.127075\n",
      "Train Epoch: 1685 [56320/101520 (55%)] Loss: -1183.368164\n",
      "Train Epoch: 1685 [67584/101520 (67%)] Loss: -1183.773438\n",
      "Train Epoch: 1685 [78848/101520 (78%)] Loss: -1182.054077\n",
      "Train Epoch: 1685 [90112/101520 (89%)] Loss: -1186.646729\n",
      "Train Epoch: 1685 [101376/101520 (100%)] Loss: -1179.181274\n",
      "    epoch          : 1685\n",
      "    loss           : -1180.8052696343043\n",
      "    ess            : 1.965929984447345\n",
      "    log_marginal   : 1180.8383157241285\n",
      "    log_joint      : 1389.2118619219143\n",
      "    val_loss       : -1180.8131952700408\n",
      "    val_ess        : 1.9673923772314321\n",
      "    val_log_marginal: 1180.8428424337635\n",
      "    val_log_joint  : 1389.384760317595\n",
      "Train Epoch: 1686 [0/101520 (0%)] Loss: -1184.025269\n",
      "Train Epoch: 1686 [11264/101520 (11%)] Loss: -1179.048096\n",
      "Train Epoch: 1686 [22528/101520 (22%)] Loss: -1180.799561\n",
      "Train Epoch: 1686 [33792/101520 (33%)] Loss: -1182.650635\n",
      "Train Epoch: 1686 [45056/101520 (44%)] Loss: -1183.692627\n",
      "Train Epoch: 1686 [56320/101520 (55%)] Loss: -1183.938843\n",
      "Train Epoch: 1686 [67584/101520 (67%)] Loss: -1187.896729\n",
      "Train Epoch: 1686 [78848/101520 (78%)] Loss: -1184.806396\n",
      "Train Epoch: 1686 [90112/101520 (89%)] Loss: -1177.001221\n",
      "Train Epoch: 1686 [101376/101520 (100%)] Loss: -1190.979126\n",
      "    epoch          : 1686\n",
      "    loss           : -1180.7770615774184\n",
      "    ess            : 1.9659331563729137\n",
      "    log_marginal   : 1180.8102665426743\n",
      "    log_joint      : 1389.2401994101367\n",
      "    val_loss       : -1180.3452254585598\n",
      "    val_ess        : 1.9660238234893135\n",
      "    val_log_marginal: 1180.3754776664402\n",
      "    val_log_joint  : 1388.509080969769\n",
      "Train Epoch: 1687 [0/101520 (0%)] Loss: -1177.729736\n",
      "Train Epoch: 1687 [11264/101520 (11%)] Loss: -1176.539673\n",
      "Train Epoch: 1687 [22528/101520 (22%)] Loss: -1178.280151\n",
      "Train Epoch: 1687 [33792/101520 (33%)] Loss: -1179.803711\n",
      "Train Epoch: 1687 [45056/101520 (44%)] Loss: -1181.287354\n",
      "Train Epoch: 1687 [56320/101520 (55%)] Loss: -1174.558838\n",
      "Train Epoch: 1687 [67584/101520 (67%)] Loss: -1186.483887\n",
      "Train Epoch: 1687 [78848/101520 (78%)] Loss: -1179.829834\n",
      "Train Epoch: 1687 [90112/101520 (89%)] Loss: -1183.703613\n",
      "Train Epoch: 1687 [101376/101520 (100%)] Loss: -1174.291748\n",
      "    epoch          : 1687\n",
      "    loss           : -1180.7367538375472\n",
      "    ess            : 1.9657860874530657\n",
      "    log_marginal   : 1180.7695484257224\n",
      "    log_joint      : 1389.205820974992\n",
      "    val_loss       : -1180.485988451087\n",
      "    val_ess        : 1.95841558083244\n",
      "    val_log_marginal: 1180.5348696501358\n",
      "    val_log_joint  : 1388.9339121942935\n",
      "Train Epoch: 1688 [0/101520 (0%)] Loss: -1177.473877\n",
      "Train Epoch: 1688 [11264/101520 (11%)] Loss: -1172.598389\n",
      "Train Epoch: 1688 [22528/101520 (22%)] Loss: -1180.069946\n",
      "Train Epoch: 1688 [33792/101520 (33%)] Loss: -1184.255493\n",
      "Train Epoch: 1688 [45056/101520 (44%)] Loss: -1175.318359\n",
      "Train Epoch: 1688 [56320/101520 (55%)] Loss: -1191.489258\n",
      "Train Epoch: 1688 [67584/101520 (67%)] Loss: -1179.860596\n",
      "Train Epoch: 1688 [78848/101520 (78%)] Loss: -1180.394531\n",
      "Train Epoch: 1688 [90112/101520 (89%)] Loss: -1182.672729\n",
      "Train Epoch: 1688 [101376/101520 (100%)] Loss: -1183.721558\n",
      "    epoch          : 1688\n",
      "    loss           : -1180.769322687657\n",
      "    ess            : 1.9655546291389656\n",
      "    log_marginal   : 1180.803078502866\n",
      "    log_joint      : 1389.2737141518137\n",
      "    val_loss       : -1180.5822541610055\n",
      "    val_ess        : 1.963117205578348\n",
      "    val_log_marginal: 1180.6173148777175\n",
      "    val_log_joint  : 1388.7264032778533\n",
      "Train Epoch: 1689 [0/101520 (0%)] Loss: -1181.667114\n",
      "Train Epoch: 1689 [11264/101520 (11%)] Loss: -1176.302490\n",
      "Train Epoch: 1689 [22528/101520 (22%)] Loss: -1185.531494\n",
      "Train Epoch: 1689 [33792/101520 (33%)] Loss: -1171.716431\n",
      "Train Epoch: 1689 [45056/101520 (44%)] Loss: -1188.213379\n",
      "Train Epoch: 1689 [56320/101520 (55%)] Loss: -1178.148438\n",
      "Train Epoch: 1689 [67584/101520 (67%)] Loss: -1180.085083\n",
      "Train Epoch: 1689 [78848/101520 (78%)] Loss: -1181.152100\n",
      "Train Epoch: 1689 [90112/101520 (89%)] Loss: -1185.920532\n",
      "Train Epoch: 1689 [101376/101520 (100%)] Loss: -1179.107666\n",
      "    epoch          : 1689\n",
      "    loss           : -1180.8734167664495\n",
      "    ess            : 1.9655203328060744\n",
      "    log_marginal   : 1180.9068057573022\n",
      "    log_joint      : 1389.3332991863615\n",
      "    val_loss       : -1180.7968537703805\n",
      "    val_ess        : 1.9685189931289009\n",
      "    val_log_marginal: 1180.82642132303\n",
      "    val_log_joint  : 1389.3550706946332\n",
      "Train Epoch: 1690 [0/101520 (0%)] Loss: -1177.968506\n",
      "Train Epoch: 1690 [11264/101520 (11%)] Loss: -1181.414307\n",
      "Train Epoch: 1690 [22528/101520 (22%)] Loss: -1175.345215\n",
      "Train Epoch: 1690 [33792/101520 (33%)] Loss: -1176.798584\n",
      "Train Epoch: 1690 [45056/101520 (44%)] Loss: -1177.794434\n",
      "Train Epoch: 1690 [56320/101520 (55%)] Loss: -1177.789307\n",
      "Train Epoch: 1690 [67584/101520 (67%)] Loss: -1177.581055\n",
      "Train Epoch: 1690 [78848/101520 (78%)] Loss: -1175.692139\n",
      "Train Epoch: 1690 [90112/101520 (89%)] Loss: -1176.823486\n",
      "Train Epoch: 1690 [101376/101520 (100%)] Loss: -1177.267334\n",
      "    epoch          : 1690\n",
      "    loss           : -1180.8290770257538\n",
      "    ess            : 1.9662698309625213\n",
      "    log_marginal   : 1180.861741569174\n",
      "    log_joint      : 1389.3159921924073\n",
      "    val_loss       : -1180.8886134935462\n",
      "    val_ess        : 1.9673577495243237\n",
      "    val_log_marginal: 1180.918064283288\n",
      "    val_log_joint  : 1389.0847433338995\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1690.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1691 [0/101520 (0%)] Loss: -1183.459473\n",
      "Train Epoch: 1691 [11264/101520 (11%)] Loss: -1180.661133\n",
      "Train Epoch: 1691 [22528/101520 (22%)] Loss: -1178.267822\n",
      "Train Epoch: 1691 [33792/101520 (33%)] Loss: -1186.052734\n",
      "Train Epoch: 1691 [45056/101520 (44%)] Loss: -1176.192383\n",
      "Train Epoch: 1691 [56320/101520 (55%)] Loss: -1176.690186\n",
      "Train Epoch: 1691 [67584/101520 (67%)] Loss: -1189.436768\n",
      "Train Epoch: 1691 [78848/101520 (78%)] Loss: -1180.679932\n",
      "Train Epoch: 1691 [90112/101520 (89%)] Loss: -1182.747681\n",
      "Train Epoch: 1691 [101376/101520 (100%)] Loss: -1179.962524\n",
      "    epoch          : 1691\n",
      "    loss           : -1180.8752803323257\n",
      "    ess            : 1.9668185321529905\n",
      "    log_marginal   : 1180.906939482569\n",
      "    log_joint      : 1389.359255383362\n",
      "    val_loss       : -1180.5833740234375\n",
      "    val_ess        : 1.9670772915301116\n",
      "    val_log_marginal: 1180.616752292799\n",
      "    val_log_joint  : 1388.916843580163\n",
      "Train Epoch: 1692 [0/101520 (0%)] Loss: -1182.308838\n",
      "Train Epoch: 1692 [11264/101520 (11%)] Loss: -1180.934814\n",
      "Train Epoch: 1692 [22528/101520 (22%)] Loss: -1185.647217\n",
      "Train Epoch: 1692 [33792/101520 (33%)] Loss: -1186.235107\n",
      "Train Epoch: 1692 [45056/101520 (44%)] Loss: -1187.974731\n",
      "Train Epoch: 1692 [56320/101520 (55%)] Loss: -1183.284668\n",
      "Train Epoch: 1692 [67584/101520 (67%)] Loss: -1176.022949\n",
      "Train Epoch: 1692 [78848/101520 (78%)] Loss: -1180.757324\n",
      "Train Epoch: 1692 [90112/101520 (89%)] Loss: -1178.429443\n",
      "Train Epoch: 1692 [101376/101520 (100%)] Loss: -1183.986572\n",
      "    epoch          : 1692\n",
      "    loss           : -1180.8553055806376\n",
      "    ess            : 1.9655531069741177\n",
      "    log_marginal   : 1180.888577408527\n",
      "    log_joint      : 1389.3842571009343\n",
      "    val_loss       : -1181.0890847911005\n",
      "    val_ess        : 1.9639662296875664\n",
      "    val_log_marginal: 1181.1244798743207\n",
      "    val_log_joint  : 1389.5214525305707\n",
      "Train Epoch: 1693 [0/101520 (0%)] Loss: -1179.061890\n",
      "Train Epoch: 1693 [11264/101520 (11%)] Loss: -1180.427490\n",
      "Train Epoch: 1693 [22528/101520 (22%)] Loss: -1176.432617\n",
      "Train Epoch: 1693 [33792/101520 (33%)] Loss: -1184.807373\n",
      "Train Epoch: 1693 [45056/101520 (44%)] Loss: -1185.574097\n",
      "Train Epoch: 1693 [56320/101520 (55%)] Loss: -1182.634033\n",
      "Train Epoch: 1693 [67584/101520 (67%)] Loss: -1181.623413\n",
      "Train Epoch: 1693 [78848/101520 (78%)] Loss: -1178.853394\n",
      "Train Epoch: 1693 [90112/101520 (89%)] Loss: -1185.450684\n",
      "Train Epoch: 1693 [101376/101520 (100%)] Loss: -1187.780884\n",
      "    epoch          : 1693\n",
      "    loss           : -1180.976954474521\n",
      "    ess            : 1.966662251170556\n",
      "    log_marginal   : 1181.0091982127435\n",
      "    log_joint      : 1389.4730347293107\n",
      "    val_loss       : -1179.5731997282608\n",
      "    val_ess        : 1.9676337345786716\n",
      "    val_log_marginal: 1179.605320142663\n",
      "    val_log_joint  : 1388.0124617866848\n",
      "Train Epoch: 1694 [0/101520 (0%)] Loss: -1185.436523\n",
      "Train Epoch: 1694 [11264/101520 (11%)] Loss: -1180.891113\n",
      "Train Epoch: 1694 [22528/101520 (22%)] Loss: -1171.288086\n",
      "Train Epoch: 1694 [33792/101520 (33%)] Loss: -1183.796387\n",
      "Train Epoch: 1694 [45056/101520 (44%)] Loss: -1179.880127\n",
      "Train Epoch: 1694 [56320/101520 (55%)] Loss: -1180.936401\n",
      "Train Epoch: 1694 [67584/101520 (67%)] Loss: -1177.570801\n",
      "Train Epoch: 1694 [78848/101520 (78%)] Loss: -1184.308472\n",
      "Train Epoch: 1694 [90112/101520 (89%)] Loss: -1181.225708\n",
      "Train Epoch: 1694 [101376/101520 (100%)] Loss: -1171.062622\n",
      "    epoch          : 1694\n",
      "    loss           : -1180.9722010933574\n",
      "    ess            : 1.9666253382237113\n",
      "    log_marginal   : 1181.0036946205637\n",
      "    log_joint      : 1389.388128386071\n",
      "    val_loss       : -1179.8277163298233\n",
      "    val_ess        : 1.9648601853329202\n",
      "    val_log_marginal: 1179.859566066576\n",
      "    val_log_joint  : 1388.1586754840353\n",
      "Train Epoch: 1695 [0/101520 (0%)] Loss: -1177.667480\n",
      "Train Epoch: 1695 [11264/101520 (11%)] Loss: -1182.597168\n",
      "Train Epoch: 1695 [22528/101520 (22%)] Loss: -1180.566162\n",
      "Train Epoch: 1695 [33792/101520 (33%)] Loss: -1183.270264\n",
      "Train Epoch: 1695 [45056/101520 (44%)] Loss: -1180.499390\n",
      "Train Epoch: 1695 [56320/101520 (55%)] Loss: -1177.692871\n",
      "Train Epoch: 1695 [67584/101520 (67%)] Loss: -1177.893066\n",
      "Train Epoch: 1695 [78848/101520 (78%)] Loss: -1180.313232\n",
      "Train Epoch: 1695 [90112/101520 (89%)] Loss: -1182.274414\n",
      "Train Epoch: 1695 [101376/101520 (100%)] Loss: -1172.425049\n",
      "    epoch          : 1695\n",
      "    loss           : -1180.951724565209\n",
      "    ess            : 1.966464427847359\n",
      "    log_marginal   : 1180.9830070763976\n",
      "    log_joint      : 1389.4272822854507\n",
      "    val_loss       : -1181.6635476817255\n",
      "    val_ess        : 1.9613757755445398\n",
      "    val_log_marginal: 1181.702291737432\n",
      "    val_log_joint  : 1390.1222029976223\n",
      "Train Epoch: 1696 [0/101520 (0%)] Loss: -1184.634033\n",
      "Train Epoch: 1696 [11264/101520 (11%)] Loss: -1191.440918\n",
      "Train Epoch: 1696 [22528/101520 (22%)] Loss: -1179.721436\n",
      "Train Epoch: 1696 [33792/101520 (33%)] Loss: -1182.458008\n",
      "Train Epoch: 1696 [45056/101520 (44%)] Loss: -1184.566406\n",
      "Train Epoch: 1696 [56320/101520 (55%)] Loss: -1178.710693\n",
      "Train Epoch: 1696 [67584/101520 (67%)] Loss: -1180.390625\n",
      "Train Epoch: 1696 [78848/101520 (78%)] Loss: -1179.974121\n",
      "Train Epoch: 1696 [90112/101520 (89%)] Loss: -1177.657593\n",
      "Train Epoch: 1696 [101376/101520 (100%)] Loss: -1182.045532\n",
      "    epoch          : 1696\n",
      "    loss           : -1181.0491268598853\n",
      "    ess            : 1.9661001925492407\n",
      "    log_marginal   : 1181.0806344957207\n",
      "    log_joint      : 1389.4987056866364\n",
      "    val_loss       : -1180.193895422894\n",
      "    val_ess        : 1.9674362255179363\n",
      "    val_log_marginal: 1180.2264563519022\n",
      "    val_log_joint  : 1388.9051407523777\n",
      "Train Epoch: 1697 [0/101520 (0%)] Loss: -1187.255371\n",
      "Train Epoch: 1697 [11264/101520 (11%)] Loss: -1177.715454\n",
      "Train Epoch: 1697 [22528/101520 (22%)] Loss: -1185.775879\n",
      "Train Epoch: 1697 [33792/101520 (33%)] Loss: -1182.622803\n",
      "Train Epoch: 1697 [45056/101520 (44%)] Loss: -1185.089600\n",
      "Train Epoch: 1697 [56320/101520 (55%)] Loss: -1178.831543\n",
      "Train Epoch: 1697 [67584/101520 (67%)] Loss: -1179.882446\n",
      "Train Epoch: 1697 [78848/101520 (78%)] Loss: -1181.269165\n",
      "Train Epoch: 1697 [90112/101520 (89%)] Loss: -1185.203857\n",
      "Train Epoch: 1697 [101376/101520 (100%)] Loss: -1187.935547\n",
      "    epoch          : 1697\n",
      "    loss           : -1181.1136904002435\n",
      "    ess            : 1.966354268280106\n",
      "    log_marginal   : 1181.1460034547738\n",
      "    log_joint      : 1389.536437681572\n",
      "    val_loss       : -1179.848781419837\n",
      "    val_ess        : 1.9679577402446582\n",
      "    val_log_marginal: 1179.8791397758152\n",
      "    val_log_joint  : 1388.1805366847825\n",
      "Train Epoch: 1698 [0/101520 (0%)] Loss: -1172.484863\n",
      "Train Epoch: 1698 [11264/101520 (11%)] Loss: -1181.012939\n",
      "Train Epoch: 1698 [22528/101520 (22%)] Loss: -1180.785400\n",
      "Train Epoch: 1698 [33792/101520 (33%)] Loss: -1179.146118\n",
      "Train Epoch: 1698 [45056/101520 (44%)] Loss: -1176.006836\n",
      "Train Epoch: 1698 [56320/101520 (55%)] Loss: -1183.877441\n",
      "Train Epoch: 1698 [67584/101520 (67%)] Loss: -1182.203003\n",
      "Train Epoch: 1698 [78848/101520 (78%)] Loss: -1185.684448\n",
      "Train Epoch: 1698 [90112/101520 (89%)] Loss: -1176.324463\n",
      "Train Epoch: 1698 [101376/101520 (100%)] Loss: -1170.419678\n",
      "    epoch          : 1698\n",
      "    loss           : -1180.9806313059437\n",
      "    ess            : 1.966869914951037\n",
      "    log_marginal   : 1181.012303337979\n",
      "    log_joint      : 1389.5098729732647\n",
      "    val_loss       : -1178.7651632557745\n",
      "    val_ess        : 1.966326532156571\n",
      "    val_log_marginal: 1178.7961319633152\n",
      "    val_log_joint  : 1387.11596148947\n",
      "Train Epoch: 1699 [0/101520 (0%)] Loss: -1184.364258\n",
      "Train Epoch: 1699 [11264/101520 (11%)] Loss: -1180.412354\n",
      "Train Epoch: 1699 [22528/101520 (22%)] Loss: -1179.421143\n",
      "Train Epoch: 1699 [33792/101520 (33%)] Loss: -1174.762817\n",
      "Train Epoch: 1699 [45056/101520 (44%)] Loss: -1178.031616\n",
      "Train Epoch: 1699 [56320/101520 (55%)] Loss: -1180.272705\n",
      "Train Epoch: 1699 [67584/101520 (67%)] Loss: -1176.726807\n",
      "Train Epoch: 1699 [78848/101520 (78%)] Loss: -1181.742188\n",
      "Train Epoch: 1699 [90112/101520 (89%)] Loss: -1184.619141\n",
      "Train Epoch: 1699 [101376/101520 (100%)] Loss: -1184.658325\n",
      "    epoch          : 1699\n",
      "    loss           : -1181.1496060625393\n",
      "    ess            : 1.9662317133429061\n",
      "    log_marginal   : 1181.181565787924\n",
      "    log_joint      : 1389.6054245838568\n",
      "    val_loss       : -1179.6369469684103\n",
      "    val_ess        : 1.966151413710221\n",
      "    val_log_marginal: 1179.6689453125\n",
      "    val_log_joint  : 1388.10326617697\n",
      "Train Epoch: 1700 [0/101520 (0%)] Loss: -1186.252563\n",
      "Train Epoch: 1700 [11264/101520 (11%)] Loss: -1180.666260\n",
      "Train Epoch: 1700 [22528/101520 (22%)] Loss: -1188.614746\n",
      "Train Epoch: 1700 [33792/101520 (33%)] Loss: -1182.006104\n",
      "Train Epoch: 1700 [45056/101520 (44%)] Loss: -1183.136108\n",
      "Train Epoch: 1700 [56320/101520 (55%)] Loss: -1174.593628\n",
      "Train Epoch: 1700 [67584/101520 (67%)] Loss: -1179.421875\n",
      "Train Epoch: 1700 [78848/101520 (78%)] Loss: -1188.182129\n",
      "Train Epoch: 1700 [90112/101520 (89%)] Loss: -1179.051270\n",
      "Train Epoch: 1700 [101376/101520 (100%)] Loss: -1177.548462\n",
      "    epoch          : 1700\n",
      "    loss           : -1181.0785114537532\n",
      "    ess            : 1.9658587811580255\n",
      "    log_marginal   : 1181.1107760482098\n",
      "    log_joint      : 1389.580495249686\n",
      "    val_loss       : -1178.5725469174592\n",
      "    val_ess        : 1.9658899721891985\n",
      "    val_log_marginal: 1178.6042215098505\n",
      "    val_log_joint  : 1387.1436236837635\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1700.pth ...\n",
      "Train Epoch: 1701 [0/101520 (0%)] Loss: -1176.674072\n",
      "Train Epoch: 1701 [11264/101520 (11%)] Loss: -1179.120361\n",
      "Train Epoch: 1701 [22528/101520 (22%)] Loss: -1190.420776\n",
      "Train Epoch: 1701 [33792/101520 (33%)] Loss: -1181.897339\n",
      "Train Epoch: 1701 [45056/101520 (44%)] Loss: -1186.534912\n",
      "Train Epoch: 1701 [56320/101520 (55%)] Loss: -1183.222168\n",
      "Train Epoch: 1701 [67584/101520 (67%)] Loss: -1185.337646\n",
      "Train Epoch: 1701 [78848/101520 (78%)] Loss: -1178.998291\n",
      "Train Epoch: 1701 [90112/101520 (89%)] Loss: -1179.749268\n",
      "Train Epoch: 1701 [101376/101520 (100%)] Loss: -1176.124756\n",
      "    epoch          : 1701\n",
      "    loss           : -1181.1452986367383\n",
      "    ess            : 1.9666903468232657\n",
      "    log_marginal   : 1181.176975576123\n",
      "    log_joint      : 1389.5757062902403\n",
      "    val_loss       : -1182.2858302904212\n",
      "    val_ess        : 1.9675710201263428\n",
      "    val_log_marginal: 1182.3173987347147\n",
      "    val_log_joint  : 1390.6495361328125\n",
      "Train Epoch: 1702 [0/101520 (0%)] Loss: -1187.156494\n",
      "Train Epoch: 1702 [11264/101520 (11%)] Loss: -1178.027832\n",
      "Train Epoch: 1702 [22528/101520 (22%)] Loss: -1183.589233\n",
      "Train Epoch: 1702 [33792/101520 (33%)] Loss: -1181.826416\n",
      "Train Epoch: 1702 [45056/101520 (44%)] Loss: -1177.678345\n",
      "Train Epoch: 1702 [56320/101520 (55%)] Loss: -1180.770996\n",
      "Train Epoch: 1702 [67584/101520 (67%)] Loss: -1184.431641\n",
      "Train Epoch: 1702 [78848/101520 (78%)] Loss: -1185.638550\n",
      "Train Epoch: 1702 [90112/101520 (89%)] Loss: -1182.006348\n",
      "Train Epoch: 1702 [101376/101520 (100%)] Loss: -1175.140625\n",
      "    epoch          : 1702\n",
      "    loss           : -1181.1534528109296\n",
      "    ess            : 1.966268656241834\n",
      "    log_marginal   : 1181.1859063383322\n",
      "    log_joint      : 1389.6256569713803\n",
      "    val_loss       : -1180.5506538722825\n",
      "    val_ess        : 1.9676305936730427\n",
      "    val_log_marginal: 1180.583198879076\n",
      "    val_log_joint  : 1389.0218134341033\n",
      "Train Epoch: 1703 [0/101520 (0%)] Loss: -1184.390625\n",
      "Train Epoch: 1703 [11264/101520 (11%)] Loss: -1179.500244\n",
      "Train Epoch: 1703 [22528/101520 (22%)] Loss: -1179.581421\n",
      "Train Epoch: 1703 [33792/101520 (33%)] Loss: -1190.213135\n",
      "Train Epoch: 1703 [45056/101520 (44%)] Loss: -1179.266113\n",
      "Train Epoch: 1703 [56320/101520 (55%)] Loss: -1173.984375\n",
      "Train Epoch: 1703 [67584/101520 (67%)] Loss: -1184.919067\n",
      "Train Epoch: 1703 [78848/101520 (78%)] Loss: -1181.177612\n",
      "Train Epoch: 1703 [90112/101520 (89%)] Loss: -1186.623413\n",
      "Train Epoch: 1703 [101376/101520 (100%)] Loss: -1184.702759\n",
      "    epoch          : 1703\n",
      "    loss           : -1181.1634122762248\n",
      "    ess            : 1.9663893649326496\n",
      "    log_marginal   : 1181.1954897779915\n",
      "    log_joint      : 1389.7362238438286\n",
      "    val_loss       : -1180.1035315472147\n",
      "    val_ess        : 1.961932140847911\n",
      "    val_log_marginal: 1180.1404180112092\n",
      "    val_log_joint  : 1388.599312160326\n",
      "Train Epoch: 1704 [0/101520 (0%)] Loss: -1182.775635\n",
      "Train Epoch: 1704 [11264/101520 (11%)] Loss: -1184.035645\n",
      "Train Epoch: 1704 [22528/101520 (22%)] Loss: -1186.845947\n",
      "Train Epoch: 1704 [33792/101520 (33%)] Loss: -1180.376099\n",
      "Train Epoch: 1704 [45056/101520 (44%)] Loss: -1178.813599\n",
      "Train Epoch: 1704 [56320/101520 (55%)] Loss: -1186.276001\n",
      "Train Epoch: 1704 [67584/101520 (67%)] Loss: -1180.954956\n",
      "Train Epoch: 1704 [78848/101520 (78%)] Loss: -1172.268311\n",
      "Train Epoch: 1704 [90112/101520 (89%)] Loss: -1183.061523\n",
      "Train Epoch: 1704 [101376/101520 (100%)] Loss: -1179.092407\n",
      "    epoch          : 1704\n",
      "    loss           : -1181.2718413846576\n",
      "    ess            : 1.9663216013405191\n",
      "    log_marginal   : 1181.303208547621\n",
      "    log_joint      : 1389.7322452104272\n",
      "    val_loss       : -1180.3362559442935\n",
      "    val_ess        : 1.9703779324241306\n",
      "    val_log_marginal: 1180.3628035835598\n",
      "    val_log_joint  : 1388.6512928838315\n",
      "Train Epoch: 1705 [0/101520 (0%)] Loss: -1174.687134\n",
      "Train Epoch: 1705 [11264/101520 (11%)] Loss: -1181.090698\n",
      "Train Epoch: 1705 [22528/101520 (22%)] Loss: -1182.852539\n",
      "Train Epoch: 1705 [33792/101520 (33%)] Loss: -1182.082520\n",
      "Train Epoch: 1705 [45056/101520 (44%)] Loss: -1185.262573\n",
      "Train Epoch: 1705 [56320/101520 (55%)] Loss: -1176.182983\n",
      "Train Epoch: 1705 [67584/101520 (67%)] Loss: -1179.163452\n",
      "Train Epoch: 1705 [78848/101520 (78%)] Loss: -1175.617676\n",
      "Train Epoch: 1705 [90112/101520 (89%)] Loss: -1173.490234\n",
      "Train Epoch: 1705 [101376/101520 (100%)] Loss: -1177.188599\n",
      "    epoch          : 1705\n",
      "    loss           : -1181.263044347715\n",
      "    ess            : 1.966895866034618\n",
      "    log_marginal   : 1181.2941814786825\n",
      "    log_joint      : 1389.7841569910097\n",
      "    val_loss       : -1179.2850447944973\n",
      "    val_ess        : 1.9668705307919045\n",
      "    val_log_marginal: 1179.3177012567935\n",
      "    val_log_joint  : 1387.8368769106658\n",
      "Train Epoch: 1706 [0/101520 (0%)] Loss: -1182.839966\n",
      "Train Epoch: 1706 [11264/101520 (11%)] Loss: -1189.027710\n",
      "Train Epoch: 1706 [22528/101520 (22%)] Loss: -1179.808105\n",
      "Train Epoch: 1706 [33792/101520 (33%)] Loss: -1185.956543\n",
      "Train Epoch: 1706 [45056/101520 (44%)] Loss: -1170.877808\n",
      "Train Epoch: 1706 [56320/101520 (55%)] Loss: -1195.837158\n",
      "Train Epoch: 1706 [67584/101520 (67%)] Loss: -1175.405151\n",
      "Train Epoch: 1706 [78848/101520 (78%)] Loss: -1177.926636\n",
      "Train Epoch: 1706 [90112/101520 (89%)] Loss: -1180.294922\n",
      "Train Epoch: 1706 [101376/101520 (100%)] Loss: -1176.051514\n",
      "    epoch          : 1706\n",
      "    loss           : -1181.3521642637013\n",
      "    ess            : 1.9660920689453432\n",
      "    log_marginal   : 1181.384491426861\n",
      "    log_joint      : 1389.8042213018216\n",
      "    val_loss       : -1181.5826893682065\n",
      "    val_ess        : 1.9677766613338306\n",
      "    val_log_marginal: 1181.612400220788\n",
      "    val_log_joint  : 1389.987787661345\n",
      "Train Epoch: 1707 [0/101520 (0%)] Loss: -1190.294189\n",
      "Train Epoch: 1707 [11264/101520 (11%)] Loss: -1181.002686\n",
      "Train Epoch: 1707 [22528/101520 (22%)] Loss: -1169.542969\n",
      "Train Epoch: 1707 [33792/101520 (33%)] Loss: -1178.746338\n",
      "Train Epoch: 1707 [45056/101520 (44%)] Loss: -1177.676025\n",
      "Train Epoch: 1707 [56320/101520 (55%)] Loss: -1186.175537\n",
      "Train Epoch: 1707 [67584/101520 (67%)] Loss: -1183.520508\n",
      "Train Epoch: 1707 [78848/101520 (78%)] Loss: -1184.632812\n",
      "Train Epoch: 1707 [90112/101520 (89%)] Loss: -1176.240845\n",
      "Train Epoch: 1707 [101376/101520 (100%)] Loss: -1193.147095\n",
      "    epoch          : 1707\n",
      "    loss           : -1181.3948318251414\n",
      "    ess            : 1.967128076745038\n",
      "    log_marginal   : 1181.4270013397063\n",
      "    log_joint      : 1389.8730750922582\n",
      "    val_loss       : -1179.147121263587\n",
      "    val_ess        : 1.9660369054130886\n",
      "    val_log_marginal: 1179.1776441491168\n",
      "    val_log_joint  : 1387.4558689283288\n",
      "Train Epoch: 1708 [0/101520 (0%)] Loss: -1188.634644\n",
      "Train Epoch: 1708 [11264/101520 (11%)] Loss: -1184.551880\n",
      "Train Epoch: 1708 [22528/101520 (22%)] Loss: -1186.848877\n",
      "Train Epoch: 1708 [33792/101520 (33%)] Loss: -1180.495972\n",
      "Train Epoch: 1708 [45056/101520 (44%)] Loss: -1180.080811\n",
      "Train Epoch: 1708 [56320/101520 (55%)] Loss: -1182.717651\n",
      "Train Epoch: 1708 [67584/101520 (67%)] Loss: -1178.284424\n",
      "Train Epoch: 1708 [78848/101520 (78%)] Loss: -1185.390137\n",
      "Train Epoch: 1708 [90112/101520 (89%)] Loss: -1176.250732\n",
      "Train Epoch: 1708 [101376/101520 (100%)] Loss: -1195.370728\n",
      "    epoch          : 1708\n",
      "    loss           : -1181.5621350159\n",
      "    ess            : 1.9670906486223692\n",
      "    log_marginal   : 1181.593707060694\n",
      "    log_joint      : 1389.9895853780622\n",
      "    val_loss       : -1180.5237028702445\n",
      "    val_ess        : 1.9661771007206128\n",
      "    val_log_marginal: 1180.5547299592392\n",
      "    val_log_joint  : 1389.2312330163043\n",
      "Train Epoch: 1709 [0/101520 (0%)] Loss: -1182.624268\n",
      "Train Epoch: 1709 [11264/101520 (11%)] Loss: -1186.351562\n",
      "Train Epoch: 1709 [22528/101520 (22%)] Loss: -1182.044189\n",
      "Train Epoch: 1709 [33792/101520 (33%)] Loss: -1178.290039\n",
      "Train Epoch: 1709 [45056/101520 (44%)] Loss: -1176.105103\n",
      "Train Epoch: 1709 [56320/101520 (55%)] Loss: -1181.112183\n",
      "Train Epoch: 1709 [67584/101520 (67%)] Loss: -1179.728760\n",
      "Train Epoch: 1709 [78848/101520 (78%)] Loss: -1182.548340\n",
      "Train Epoch: 1709 [90112/101520 (89%)] Loss: -1179.922119\n",
      "Train Epoch: 1709 [101376/101520 (100%)] Loss: -1178.600952\n",
      "    epoch          : 1709\n",
      "    loss           : -1181.4829162904366\n",
      "    ess            : 1.9665632487541467\n",
      "    log_marginal   : 1181.515058201162\n",
      "    log_joint      : 1389.93213810753\n",
      "    val_loss       : -1179.6929135529892\n",
      "    val_ess        : 1.9668837371079817\n",
      "    val_log_marginal: 1179.7239034901495\n",
      "    val_log_joint  : 1387.8963251528533\n",
      "Train Epoch: 1710 [0/101520 (0%)] Loss: -1173.878540\n",
      "Train Epoch: 1710 [11264/101520 (11%)] Loss: -1179.606934\n",
      "Train Epoch: 1710 [22528/101520 (22%)] Loss: -1186.699585\n",
      "Train Epoch: 1710 [33792/101520 (33%)] Loss: -1176.471924\n",
      "Train Epoch: 1710 [45056/101520 (44%)] Loss: -1181.334717\n",
      "Train Epoch: 1710 [56320/101520 (55%)] Loss: -1175.701416\n",
      "Train Epoch: 1710 [67584/101520 (67%)] Loss: -1172.892212\n",
      "Train Epoch: 1710 [78848/101520 (78%)] Loss: -1181.963135\n",
      "Train Epoch: 1710 [90112/101520 (89%)] Loss: -1178.843872\n",
      "Train Epoch: 1710 [101376/101520 (100%)] Loss: -1190.814453\n",
      "    epoch          : 1710\n",
      "    loss           : -1181.5664890615185\n",
      "    ess            : 1.966294501295042\n",
      "    log_marginal   : 1181.598857323728\n",
      "    log_joint      : 1389.9655252581265\n",
      "    val_loss       : -1180.7086553158967\n",
      "    val_ess        : 1.9671718504117883\n",
      "    val_log_marginal: 1180.7390720533288\n",
      "    val_log_joint  : 1389.2970607591712\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1710.pth ...\n",
      "Train Epoch: 1711 [0/101520 (0%)] Loss: -1182.572021\n",
      "Train Epoch: 1711 [11264/101520 (11%)] Loss: -1181.682007\n",
      "Train Epoch: 1711 [22528/101520 (22%)] Loss: -1184.878784\n",
      "Train Epoch: 1711 [33792/101520 (33%)] Loss: -1188.859985\n",
      "Train Epoch: 1711 [45056/101520 (44%)] Loss: -1184.408203\n",
      "Train Epoch: 1711 [56320/101520 (55%)] Loss: -1189.844360\n",
      "Train Epoch: 1711 [67584/101520 (67%)] Loss: -1177.092529\n",
      "Train Epoch: 1711 [78848/101520 (78%)] Loss: -1181.185791\n",
      "Train Epoch: 1711 [90112/101520 (89%)] Loss: -1179.054688\n",
      "Train Epoch: 1711 [101376/101520 (100%)] Loss: -1187.020386\n",
      "    epoch          : 1711\n",
      "    loss           : -1181.52175366579\n",
      "    ess            : 1.9657695754688589\n",
      "    log_marginal   : 1181.5538477298603\n",
      "    log_joint      : 1390.030927341787\n",
      "    val_loss       : -1180.8584090523098\n",
      "    val_ess        : 1.9649504267651101\n",
      "    val_log_marginal: 1180.889791737432\n",
      "    val_log_joint  : 1389.5260062839675\n",
      "Train Epoch: 1712 [0/101520 (0%)] Loss: -1187.062134\n",
      "Train Epoch: 1712 [11264/101520 (11%)] Loss: -1178.906982\n",
      "Train Epoch: 1712 [22528/101520 (22%)] Loss: -1188.359009\n",
      "Train Epoch: 1712 [33792/101520 (33%)] Loss: -1183.729980\n",
      "Train Epoch: 1712 [45056/101520 (44%)] Loss: -1179.545532\n",
      "Train Epoch: 1712 [56320/101520 (55%)] Loss: -1178.986816\n",
      "Train Epoch: 1712 [67584/101520 (67%)] Loss: -1178.937866\n",
      "Train Epoch: 1712 [78848/101520 (78%)] Loss: -1184.270996\n",
      "Train Epoch: 1712 [90112/101520 (89%)] Loss: -1189.959717\n",
      "Train Epoch: 1712 [101376/101520 (100%)] Loss: -1180.927368\n",
      "    epoch          : 1712\n",
      "    loss           : -1181.4831690189228\n",
      "    ess            : 1.9660554303595768\n",
      "    log_marginal   : 1181.51566609905\n",
      "    log_joint      : 1389.9996405366678\n",
      "    val_loss       : -1180.1609948199728\n",
      "    val_ess        : 1.9687486835148023\n",
      "    val_log_marginal: 1180.1898777173913\n",
      "    val_log_joint  : 1388.9131283967392\n",
      "Train Epoch: 1713 [0/101520 (0%)] Loss: -1180.652832\n",
      "Train Epoch: 1713 [11264/101520 (11%)] Loss: -1180.653198\n",
      "Train Epoch: 1713 [22528/101520 (22%)] Loss: -1184.031006\n",
      "Train Epoch: 1713 [33792/101520 (33%)] Loss: -1181.887207\n",
      "Train Epoch: 1713 [45056/101520 (44%)] Loss: -1176.211914\n",
      "Train Epoch: 1713 [56320/101520 (55%)] Loss: -1179.172363\n",
      "Train Epoch: 1713 [67584/101520 (67%)] Loss: -1188.741211\n",
      "Train Epoch: 1713 [78848/101520 (78%)] Loss: -1177.726318\n",
      "Train Epoch: 1713 [90112/101520 (89%)] Loss: -1172.847900\n",
      "Train Epoch: 1713 [101376/101520 (100%)] Loss: -1188.088257\n",
      "    epoch          : 1713\n",
      "    loss           : -1181.5262966443545\n",
      "    ess            : 1.9660409203725844\n",
      "    log_marginal   : 1181.558536702065\n",
      "    log_joint      : 1389.9832561243718\n",
      "    val_loss       : -1180.7975171959918\n",
      "    val_ess        : 1.9683093920997952\n",
      "    val_log_marginal: 1180.8289741847825\n",
      "    val_log_joint  : 1389.1636591372283\n",
      "Train Epoch: 1714 [0/101520 (0%)] Loss: -1181.795654\n",
      "Train Epoch: 1714 [11264/101520 (11%)] Loss: -1176.353394\n",
      "Train Epoch: 1714 [22528/101520 (22%)] Loss: -1182.407593\n",
      "Train Epoch: 1714 [33792/101520 (33%)] Loss: -1185.602295\n",
      "Train Epoch: 1714 [45056/101520 (44%)] Loss: -1189.353271\n",
      "Train Epoch: 1714 [56320/101520 (55%)] Loss: -1177.339111\n",
      "Train Epoch: 1714 [67584/101520 (67%)] Loss: -1182.530029\n",
      "Train Epoch: 1714 [78848/101520 (78%)] Loss: -1184.781860\n",
      "Train Epoch: 1714 [90112/101520 (89%)] Loss: -1179.451416\n",
      "Train Epoch: 1714 [101376/101520 (100%)] Loss: -1172.658447\n",
      "    epoch          : 1714\n",
      "    loss           : -1181.528346689502\n",
      "    ess            : 1.9663102207471377\n",
      "    log_marginal   : 1181.5602610219064\n",
      "    log_joint      : 1390.0038927547896\n",
      "    val_loss       : -1182.0721860139267\n",
      "    val_ess        : 1.9651451991951985\n",
      "    val_log_marginal: 1182.1028893512228\n",
      "    val_log_joint  : 1390.5596605383832\n",
      "Train Epoch: 1715 [0/101520 (0%)] Loss: -1172.486572\n",
      "Train Epoch: 1715 [11264/101520 (11%)] Loss: -1179.963257\n",
      "Train Epoch: 1715 [22528/101520 (22%)] Loss: -1175.890137\n",
      "Train Epoch: 1715 [33792/101520 (33%)] Loss: -1184.374634\n",
      "Train Epoch: 1715 [45056/101520 (44%)] Loss: -1187.356934\n",
      "Train Epoch: 1715 [56320/101520 (55%)] Loss: -1183.105225\n",
      "Train Epoch: 1715 [67584/101520 (67%)] Loss: -1181.042480\n",
      "Train Epoch: 1715 [78848/101520 (78%)] Loss: -1183.119385\n",
      "Train Epoch: 1715 [90112/101520 (89%)] Loss: -1179.542725\n",
      "Train Epoch: 1715 [101376/101520 (100%)] Loss: -1178.432251\n",
      "    epoch          : 1715\n",
      "    loss           : -1181.5876691808653\n",
      "    ess            : 1.9652759238104125\n",
      "    log_marginal   : 1181.621177788356\n",
      "    log_joint      : 1390.07178654503\n",
      "    val_loss       : -1180.8867612092392\n",
      "    val_ess        : 1.9634105951889702\n",
      "    val_log_marginal: 1180.9219068444293\n",
      "    val_log_joint  : 1389.6692266049592\n",
      "Train Epoch: 1716 [0/101520 (0%)] Loss: -1184.354736\n",
      "Train Epoch: 1716 [11264/101520 (11%)] Loss: -1181.377930\n",
      "Train Epoch: 1716 [22528/101520 (22%)] Loss: -1179.838623\n",
      "Train Epoch: 1716 [33792/101520 (33%)] Loss: -1185.860840\n",
      "Train Epoch: 1716 [45056/101520 (44%)] Loss: -1185.282471\n",
      "Train Epoch: 1716 [56320/101520 (55%)] Loss: -1184.562134\n",
      "Train Epoch: 1716 [67584/101520 (67%)] Loss: -1182.966553\n",
      "Train Epoch: 1716 [78848/101520 (78%)] Loss: -1186.931763\n",
      "Train Epoch: 1716 [90112/101520 (89%)] Loss: -1171.912598\n",
      "Train Epoch: 1716 [101376/101520 (100%)] Loss: -1172.644653\n",
      "    epoch          : 1716\n",
      "    loss           : -1181.6646286854193\n",
      "    ess            : 1.966900764997281\n",
      "    log_marginal   : 1181.6958118227858\n",
      "    log_joint      : 1390.1103006487515\n",
      "    val_loss       : -1180.5656101392663\n",
      "    val_ess        : 1.968176007270813\n",
      "    val_log_marginal: 1180.5923223080842\n",
      "    val_log_joint  : 1389.0178434952445\n",
      "Train Epoch: 1717 [0/101520 (0%)] Loss: -1182.348633\n",
      "Train Epoch: 1717 [11264/101520 (11%)] Loss: -1183.251709\n",
      "Train Epoch: 1717 [22528/101520 (22%)] Loss: -1180.485107\n",
      "Train Epoch: 1717 [33792/101520 (33%)] Loss: -1184.018799\n",
      "Train Epoch: 1717 [45056/101520 (44%)] Loss: -1185.924683\n",
      "Train Epoch: 1717 [56320/101520 (55%)] Loss: -1178.910645\n",
      "Train Epoch: 1717 [67584/101520 (67%)] Loss: -1188.703613\n",
      "Train Epoch: 1717 [78848/101520 (78%)] Loss: -1184.323364\n",
      "Train Epoch: 1717 [90112/101520 (89%)] Loss: -1183.417969\n",
      "Train Epoch: 1717 [101376/101520 (100%)] Loss: -1176.727051\n",
      "    epoch          : 1717\n",
      "    loss           : -1181.6619713558025\n",
      "    ess            : 1.9657684402849207\n",
      "    log_marginal   : 1181.694617496663\n",
      "    log_joint      : 1390.1306618541928\n",
      "    val_loss       : -1179.9483961022418\n",
      "    val_ess        : 1.9668544116227522\n",
      "    val_log_marginal: 1179.9824431046195\n",
      "    val_log_joint  : 1388.2847422724185\n",
      "Train Epoch: 1718 [0/101520 (0%)] Loss: -1178.868774\n",
      "Train Epoch: 1718 [11264/101520 (11%)] Loss: -1176.860718\n",
      "Train Epoch: 1718 [22528/101520 (22%)] Loss: -1179.367676\n",
      "Train Epoch: 1718 [33792/101520 (33%)] Loss: -1172.219360\n",
      "Train Epoch: 1718 [45056/101520 (44%)] Loss: -1181.928955\n",
      "Train Epoch: 1718 [56320/101520 (55%)] Loss: -1184.033325\n",
      "Train Epoch: 1718 [67584/101520 (67%)] Loss: -1183.140381\n",
      "Train Epoch: 1718 [78848/101520 (78%)] Loss: -1184.065186\n",
      "Train Epoch: 1718 [90112/101520 (89%)] Loss: -1182.723511\n",
      "Train Epoch: 1718 [101376/101520 (100%)] Loss: -1186.867798\n",
      "    epoch          : 1718\n",
      "    loss           : -1181.6688281495367\n",
      "    ess            : 1.9656631383464565\n",
      "    log_marginal   : 1181.702214073296\n",
      "    log_joint      : 1390.1865381595478\n",
      "    val_loss       : -1179.990382982337\n",
      "    val_ess        : 1.9685731348784075\n",
      "    val_log_marginal: 1180.020406971807\n",
      "    val_log_joint  : 1388.5049199643342\n",
      "Train Epoch: 1719 [0/101520 (0%)] Loss: -1182.256836\n",
      "Train Epoch: 1719 [11264/101520 (11%)] Loss: -1181.598877\n",
      "Train Epoch: 1719 [22528/101520 (22%)] Loss: -1178.521851\n",
      "Train Epoch: 1719 [33792/101520 (33%)] Loss: -1179.486938\n",
      "Train Epoch: 1719 [45056/101520 (44%)] Loss: -1175.592163\n",
      "Train Epoch: 1719 [56320/101520 (55%)] Loss: -1183.506714\n",
      "Train Epoch: 1719 [67584/101520 (67%)] Loss: -1182.373779\n",
      "Train Epoch: 1719 [78848/101520 (78%)] Loss: -1182.419922\n",
      "Train Epoch: 1719 [90112/101520 (89%)] Loss: -1185.729126\n",
      "Train Epoch: 1719 [101376/101520 (100%)] Loss: -1179.664917\n",
      "    epoch          : 1719\n",
      "    loss           : -1181.669802258362\n",
      "    ess            : 1.9660939002156856\n",
      "    log_marginal   : 1181.7019938559988\n",
      "    log_joint      : 1390.1858474501414\n",
      "    val_loss       : -1178.0256666100543\n",
      "    val_ess        : 1.9656975321147754\n",
      "    val_log_marginal: 1178.0685769786005\n",
      "    val_log_joint  : 1386.8310865319293\n",
      "Train Epoch: 1720 [0/101520 (0%)] Loss: -1180.425415\n",
      "Train Epoch: 1720 [11264/101520 (11%)] Loss: -1183.740356\n",
      "Train Epoch: 1720 [22528/101520 (22%)] Loss: -1185.174194\n",
      "Train Epoch: 1720 [33792/101520 (33%)] Loss: -1174.114502\n",
      "Train Epoch: 1720 [45056/101520 (44%)] Loss: -1181.945190\n",
      "Train Epoch: 1720 [56320/101520 (55%)] Loss: -1183.453003\n",
      "Train Epoch: 1720 [67584/101520 (67%)] Loss: -1184.951904\n",
      "Train Epoch: 1720 [78848/101520 (78%)] Loss: -1174.727539\n",
      "Train Epoch: 1720 [90112/101520 (89%)] Loss: -1176.731201\n",
      "Train Epoch: 1720 [101376/101520 (100%)] Loss: -1175.247314\n",
      "    epoch          : 1720\n",
      "    loss           : -1181.781638294009\n",
      "    ess            : 1.9667068833681807\n",
      "    log_marginal   : 1181.8135152078753\n",
      "    log_joint      : 1390.2363293518374\n",
      "    val_loss       : -1181.098096764606\n",
      "    val_ess        : 1.9642936302267986\n",
      "    val_log_marginal: 1181.1394839079483\n",
      "    val_log_joint  : 1389.1328443444293\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1720.pth ...\n",
      "Train Epoch: 1721 [0/101520 (0%)] Loss: -1177.328613\n",
      "Train Epoch: 1721 [11264/101520 (11%)] Loss: -1180.862061\n",
      "Train Epoch: 1721 [22528/101520 (22%)] Loss: -1183.260498\n",
      "Train Epoch: 1721 [33792/101520 (33%)] Loss: -1180.510254\n",
      "Train Epoch: 1721 [45056/101520 (44%)] Loss: -1187.335205\n",
      "Train Epoch: 1721 [56320/101520 (55%)] Loss: -1182.384033\n",
      "Train Epoch: 1721 [67584/101520 (67%)] Loss: -1186.795898\n",
      "Train Epoch: 1721 [78848/101520 (78%)] Loss: -1177.960449\n",
      "Train Epoch: 1721 [90112/101520 (89%)] Loss: -1178.930664\n",
      "Train Epoch: 1721 [101376/101520 (100%)] Loss: -1192.332764\n",
      "    epoch          : 1721\n",
      "    loss           : -1181.8931780484454\n",
      "    ess            : 1.9666715926261404\n",
      "    log_marginal   : 1181.9250279718908\n",
      "    log_joint      : 1390.3862580725895\n",
      "    val_loss       : -1180.1835619055707\n",
      "    val_ess        : 1.9653949426568074\n",
      "    val_log_marginal: 1180.217868970788\n",
      "    val_log_joint  : 1388.8863153872283\n",
      "Train Epoch: 1722 [0/101520 (0%)] Loss: -1177.276855\n",
      "Train Epoch: 1722 [11264/101520 (11%)] Loss: -1174.932373\n",
      "Train Epoch: 1722 [22528/101520 (22%)] Loss: -1181.108398\n",
      "Train Epoch: 1722 [33792/101520 (33%)] Loss: -1181.200684\n",
      "Train Epoch: 1722 [45056/101520 (44%)] Loss: -1182.013916\n",
      "Train Epoch: 1722 [56320/101520 (55%)] Loss: -1184.430420\n",
      "Train Epoch: 1722 [67584/101520 (67%)] Loss: -1184.951416\n",
      "Train Epoch: 1722 [78848/101520 (78%)] Loss: -1181.325439\n",
      "Train Epoch: 1722 [90112/101520 (89%)] Loss: -1184.890259\n",
      "Train Epoch: 1722 [101376/101520 (100%)] Loss: -1179.486328\n",
      "    epoch          : 1722\n",
      "    loss           : -1181.8615734924624\n",
      "    ess            : 1.9660157222843648\n",
      "    log_marginal   : 1181.894322687657\n",
      "    log_joint      : 1390.3405234178706\n",
      "    val_loss       : -1182.4010062839675\n",
      "    val_ess        : 1.966236596522124\n",
      "    val_log_marginal: 1182.4353239639945\n",
      "    val_log_joint  : 1390.8325938349185\n",
      "Train Epoch: 1723 [0/101520 (0%)] Loss: -1182.929688\n",
      "Train Epoch: 1723 [11264/101520 (11%)] Loss: -1174.971313\n",
      "Train Epoch: 1723 [22528/101520 (22%)] Loss: -1184.388550\n",
      "Train Epoch: 1723 [33792/101520 (33%)] Loss: -1182.949707\n",
      "Train Epoch: 1723 [45056/101520 (44%)] Loss: -1175.225220\n",
      "Train Epoch: 1723 [56320/101520 (55%)] Loss: -1177.263062\n",
      "Train Epoch: 1723 [67584/101520 (67%)] Loss: -1178.112793\n",
      "Train Epoch: 1723 [78848/101520 (78%)] Loss: -1185.137695\n",
      "Train Epoch: 1723 [90112/101520 (89%)] Loss: -1186.821533\n",
      "Train Epoch: 1723 [101376/101520 (100%)] Loss: -1180.725098\n",
      "    epoch          : 1723\n",
      "    loss           : -1181.8242193634187\n",
      "    ess            : 1.966321821188807\n",
      "    log_marginal   : 1181.8569434084484\n",
      "    log_joint      : 1390.3219300849953\n",
      "    val_loss       : -1180.1103197180707\n",
      "    val_ess        : 1.9646822110466335\n",
      "    val_log_marginal: 1180.147455630095\n",
      "    val_log_joint  : 1388.721684994905\n",
      "Train Epoch: 1724 [0/101520 (0%)] Loss: -1181.591797\n",
      "Train Epoch: 1724 [11264/101520 (11%)] Loss: -1185.180908\n",
      "Train Epoch: 1724 [22528/101520 (22%)] Loss: -1180.378662\n",
      "Train Epoch: 1724 [33792/101520 (33%)] Loss: -1183.948120\n",
      "Train Epoch: 1724 [45056/101520 (44%)] Loss: -1184.140381\n",
      "Train Epoch: 1724 [56320/101520 (55%)] Loss: -1184.620117\n",
      "Train Epoch: 1724 [67584/101520 (67%)] Loss: -1184.097778\n",
      "Train Epoch: 1724 [78848/101520 (78%)] Loss: -1174.999023\n",
      "Train Epoch: 1724 [90112/101520 (89%)] Loss: -1178.653809\n",
      "Train Epoch: 1724 [101376/101520 (100%)] Loss: -1178.090698\n",
      "    epoch          : 1724\n",
      "    loss           : -1181.871761762916\n",
      "    ess            : 1.9659491280215469\n",
      "    log_marginal   : 1181.90380859375\n",
      "    log_joint      : 1390.4043569900282\n",
      "    val_loss       : -1181.0231296705163\n",
      "    val_ess        : 1.9679778036863909\n",
      "    val_log_marginal: 1181.053854237432\n",
      "    val_log_joint  : 1389.4710109544837\n",
      "Train Epoch: 1725 [0/101520 (0%)] Loss: -1182.574463\n",
      "Train Epoch: 1725 [11264/101520 (11%)] Loss: -1183.104492\n",
      "Train Epoch: 1725 [22528/101520 (22%)] Loss: -1180.292480\n",
      "Train Epoch: 1725 [33792/101520 (33%)] Loss: -1177.917236\n",
      "Train Epoch: 1725 [45056/101520 (44%)] Loss: -1177.285767\n",
      "Train Epoch: 1725 [56320/101520 (55%)] Loss: -1188.622925\n",
      "Train Epoch: 1725 [67584/101520 (67%)] Loss: -1178.626953\n",
      "Train Epoch: 1725 [78848/101520 (78%)] Loss: -1184.048096\n",
      "Train Epoch: 1725 [90112/101520 (89%)] Loss: -1182.447876\n",
      "Train Epoch: 1725 [101376/101520 (100%)] Loss: -1186.934204\n",
      "    epoch          : 1725\n",
      "    loss           : -1181.982746373469\n",
      "    ess            : 1.966860113431461\n",
      "    log_marginal   : 1182.014398776107\n",
      "    log_joint      : 1390.4169382066582\n",
      "    val_loss       : -1180.0565291694973\n",
      "    val_ess        : 1.9667860943338145\n",
      "    val_log_marginal: 1180.0885062839675\n",
      "    val_log_joint  : 1388.4478600543478\n",
      "Train Epoch: 1726 [0/101520 (0%)] Loss: -1184.565430\n",
      "Train Epoch: 1726 [11264/101520 (11%)] Loss: -1182.099609\n",
      "Train Epoch: 1726 [22528/101520 (22%)] Loss: -1185.789429\n",
      "Train Epoch: 1726 [33792/101520 (33%)] Loss: -1177.760498\n",
      "Train Epoch: 1726 [45056/101520 (44%)] Loss: -1176.390747\n",
      "Train Epoch: 1726 [56320/101520 (55%)] Loss: -1181.551270\n",
      "Train Epoch: 1726 [67584/101520 (67%)] Loss: -1179.699463\n",
      "Train Epoch: 1726 [78848/101520 (78%)] Loss: -1182.169434\n",
      "Train Epoch: 1726 [90112/101520 (89%)] Loss: -1182.393555\n",
      "Train Epoch: 1726 [101376/101520 (100%)] Loss: -1182.786011\n",
      "    epoch          : 1726\n",
      "    loss           : -1181.9192415937107\n",
      "    ess            : 1.9664056642570686\n",
      "    log_marginal   : 1181.9517932680983\n",
      "    log_joint      : 1390.4691524026382\n",
      "    val_loss       : -1180.5899658203125\n",
      "    val_ess        : 1.9655987749929014\n",
      "    val_log_marginal: 1180.6214015794837\n",
      "    val_log_joint  : 1389.0919083305027\n",
      "Train Epoch: 1727 [0/101520 (0%)] Loss: -1184.532837\n",
      "Train Epoch: 1727 [11264/101520 (11%)] Loss: -1187.942261\n",
      "Train Epoch: 1727 [22528/101520 (22%)] Loss: -1186.697876\n",
      "Train Epoch: 1727 [33792/101520 (33%)] Loss: -1187.470337\n",
      "Train Epoch: 1727 [45056/101520 (44%)] Loss: -1182.653320\n",
      "Train Epoch: 1727 [56320/101520 (55%)] Loss: -1180.205566\n",
      "Train Epoch: 1727 [67584/101520 (67%)] Loss: -1185.079590\n",
      "Train Epoch: 1727 [78848/101520 (78%)] Loss: -1183.465820\n",
      "Train Epoch: 1727 [90112/101520 (89%)] Loss: -1184.250244\n",
      "Train Epoch: 1727 [101376/101520 (100%)] Loss: -1178.525269\n",
      "    epoch          : 1727\n",
      "    loss           : -1182.009442966787\n",
      "    ess            : 1.9669923063498647\n",
      "    log_marginal   : 1182.0412468838333\n",
      "    log_joint      : 1390.532830779876\n",
      "    val_loss       : -1181.6761952275815\n",
      "    val_ess        : 1.9675464941107708\n",
      "    val_log_marginal: 1181.708013119905\n",
      "    val_log_joint  : 1390.2268278702445\n",
      "Train Epoch: 1728 [0/101520 (0%)] Loss: -1187.034790\n",
      "Train Epoch: 1728 [11264/101520 (11%)] Loss: -1182.838623\n",
      "Train Epoch: 1728 [22528/101520 (22%)] Loss: -1186.922241\n",
      "Train Epoch: 1728 [33792/101520 (33%)] Loss: -1177.815796\n",
      "Train Epoch: 1728 [45056/101520 (44%)] Loss: -1179.927734\n",
      "Train Epoch: 1728 [56320/101520 (55%)] Loss: -1180.824463\n",
      "Train Epoch: 1728 [67584/101520 (67%)] Loss: -1183.713501\n",
      "Train Epoch: 1728 [78848/101520 (78%)] Loss: -1184.569336\n",
      "Train Epoch: 1728 [90112/101520 (89%)] Loss: -1171.969727\n",
      "Train Epoch: 1728 [101376/101520 (100%)] Loss: -1179.745728\n",
      "    epoch          : 1728\n",
      "    loss           : -1182.0661259176743\n",
      "    ess            : 1.9662500452156642\n",
      "    log_marginal   : 1182.0981936047424\n",
      "    log_joint      : 1390.528857053824\n",
      "    val_loss       : -1180.8514935037365\n",
      "    val_ess        : 1.9674137934394504\n",
      "    val_log_marginal: 1180.885057532269\n",
      "    val_log_joint  : 1389.2913924507473\n",
      "Train Epoch: 1729 [0/101520 (0%)] Loss: -1176.235962\n",
      "Train Epoch: 1729 [11264/101520 (11%)] Loss: -1177.454102\n",
      "Train Epoch: 1729 [22528/101520 (22%)] Loss: -1182.471313\n",
      "Train Epoch: 1729 [33792/101520 (33%)] Loss: -1190.199707\n",
      "Train Epoch: 1729 [45056/101520 (44%)] Loss: -1187.036621\n",
      "Train Epoch: 1729 [56320/101520 (55%)] Loss: -1176.986084\n",
      "Train Epoch: 1729 [67584/101520 (67%)] Loss: -1182.449707\n",
      "Train Epoch: 1729 [78848/101520 (78%)] Loss: -1187.203125\n",
      "Train Epoch: 1729 [90112/101520 (89%)] Loss: -1185.681396\n",
      "Train Epoch: 1729 [101376/101520 (100%)] Loss: -1186.754517\n",
      "    epoch          : 1729\n",
      "    loss           : -1182.0942014761306\n",
      "    ess            : 1.9656009218800607\n",
      "    log_marginal   : 1182.126247080127\n",
      "    log_joint      : 1390.5700088577653\n",
      "    val_loss       : -1180.730367909307\n",
      "    val_ess        : 1.9658958600914997\n",
      "    val_log_marginal: 1180.7633215862772\n",
      "    val_log_joint  : 1389.4448560631793\n",
      "Train Epoch: 1730 [0/101520 (0%)] Loss: -1187.042969\n",
      "Train Epoch: 1730 [11264/101520 (11%)] Loss: -1176.967041\n",
      "Train Epoch: 1730 [22528/101520 (22%)] Loss: -1181.711304\n",
      "Train Epoch: 1730 [33792/101520 (33%)] Loss: -1187.744629\n",
      "Train Epoch: 1730 [45056/101520 (44%)] Loss: -1182.617676\n",
      "Train Epoch: 1730 [56320/101520 (55%)] Loss: -1177.979126\n",
      "Train Epoch: 1730 [67584/101520 (67%)] Loss: -1173.789307\n",
      "Train Epoch: 1730 [78848/101520 (78%)] Loss: -1179.657227\n",
      "Train Epoch: 1730 [90112/101520 (89%)] Loss: -1183.102173\n",
      "Train Epoch: 1730 [101376/101520 (100%)] Loss: -1167.397095\n",
      "    epoch          : 1730\n",
      "    loss           : -1182.025110906093\n",
      "    ess            : 1.96662878870365\n",
      "    log_marginal   : 1182.0568424397377\n",
      "    log_joint      : 1390.570661535215\n",
      "    val_loss       : -1181.1044497282608\n",
      "    val_ess        : 1.964312133581742\n",
      "    val_log_marginal: 1181.1393140709918\n",
      "    val_log_joint  : 1389.9455354110055\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1730.pth ...\n",
      "Train Epoch: 1731 [0/101520 (0%)] Loss: -1185.602173\n",
      "Train Epoch: 1731 [11264/101520 (11%)] Loss: -1182.836670\n",
      "Train Epoch: 1731 [22528/101520 (22%)] Loss: -1183.920776\n",
      "Train Epoch: 1731 [33792/101520 (33%)] Loss: -1181.803101\n",
      "Train Epoch: 1731 [45056/101520 (44%)] Loss: -1176.406738\n",
      "Train Epoch: 1731 [56320/101520 (55%)] Loss: -1171.372314\n",
      "Train Epoch: 1731 [67584/101520 (67%)] Loss: -1179.677490\n",
      "Train Epoch: 1731 [78848/101520 (78%)] Loss: -1180.490234\n",
      "Train Epoch: 1731 [90112/101520 (89%)] Loss: -1181.435547\n",
      "Train Epoch: 1731 [101376/101520 (100%)] Loss: -1181.363281\n",
      "    epoch          : 1731\n",
      "    loss           : -1182.1958731646514\n",
      "    ess            : 1.9669905182105214\n",
      "    log_marginal   : 1182.2278568133636\n",
      "    log_joint      : 1390.636716909744\n",
      "    val_loss       : -1178.5932829483695\n",
      "    val_ess        : 1.9678218105564946\n",
      "    val_log_marginal: 1178.6233175526495\n",
      "    val_log_joint  : 1387.0716446586277\n",
      "Train Epoch: 1732 [0/101520 (0%)] Loss: -1188.355225\n",
      "Train Epoch: 1732 [11264/101520 (11%)] Loss: -1177.084961\n",
      "Train Epoch: 1732 [22528/101520 (22%)] Loss: -1181.269897\n",
      "Train Epoch: 1732 [33792/101520 (33%)] Loss: -1180.929321\n",
      "Train Epoch: 1732 [45056/101520 (44%)] Loss: -1176.220215\n",
      "Train Epoch: 1732 [56320/101520 (55%)] Loss: -1187.418335\n",
      "Train Epoch: 1732 [67584/101520 (67%)] Loss: -1184.032593\n",
      "Train Epoch: 1732 [78848/101520 (78%)] Loss: -1183.383911\n",
      "Train Epoch: 1732 [90112/101520 (89%)] Loss: -1185.723267\n",
      "Train Epoch: 1732 [101376/101520 (100%)] Loss: -1189.295044\n",
      "    epoch          : 1732\n",
      "    loss           : -1182.2577327555748\n",
      "    ess            : 1.9671119751043655\n",
      "    log_marginal   : 1182.2896802125863\n",
      "    log_joint      : 1390.649742241481\n",
      "    val_loss       : -1179.2587200662365\n",
      "    val_ess        : 1.9693299013635386\n",
      "    val_log_marginal: 1179.2883990743885\n",
      "    val_log_joint  : 1387.4731763756793\n",
      "Train Epoch: 1733 [0/101520 (0%)] Loss: -1178.709717\n",
      "Train Epoch: 1733 [11264/101520 (11%)] Loss: -1186.393433\n",
      "Train Epoch: 1733 [22528/101520 (22%)] Loss: -1184.146851\n",
      "Train Epoch: 1733 [33792/101520 (33%)] Loss: -1182.945190\n",
      "Train Epoch: 1733 [45056/101520 (44%)] Loss: -1179.887817\n",
      "Train Epoch: 1733 [56320/101520 (55%)] Loss: -1175.137939\n",
      "Train Epoch: 1733 [67584/101520 (67%)] Loss: -1193.145874\n",
      "Train Epoch: 1733 [78848/101520 (78%)] Loss: -1179.434082\n",
      "Train Epoch: 1733 [90112/101520 (89%)] Loss: -1175.171387\n",
      "Train Epoch: 1733 [101376/101520 (100%)] Loss: -1184.380371\n",
      "    epoch          : 1733\n",
      "    loss           : -1182.1894825690954\n",
      "    ess            : 1.96479943769062\n",
      "    log_marginal   : 1182.2231727485082\n",
      "    log_joint      : 1390.649504848461\n",
      "    val_loss       : -1182.3936396059783\n",
      "    val_ess        : 1.9693495605302893\n",
      "    val_log_marginal: 1182.4238652768342\n",
      "    val_log_joint  : 1390.847120202106\n",
      "Train Epoch: 1734 [0/101520 (0%)] Loss: -1178.715942\n",
      "Train Epoch: 1734 [11264/101520 (11%)] Loss: -1182.709351\n",
      "Train Epoch: 1734 [22528/101520 (22%)] Loss: -1181.250366\n",
      "Train Epoch: 1734 [33792/101520 (33%)] Loss: -1177.645264\n",
      "Train Epoch: 1734 [45056/101520 (44%)] Loss: -1186.491211\n",
      "Train Epoch: 1734 [56320/101520 (55%)] Loss: -1180.229614\n",
      "Train Epoch: 1734 [67584/101520 (67%)] Loss: -1184.795898\n",
      "Train Epoch: 1734 [78848/101520 (78%)] Loss: -1186.271729\n",
      "Train Epoch: 1734 [90112/101520 (89%)] Loss: -1182.063232\n",
      "Train Epoch: 1734 [101376/101520 (100%)] Loss: -1184.199585\n",
      "    epoch          : 1734\n",
      "    loss           : -1182.2335008784155\n",
      "    ess            : 1.9659962402516273\n",
      "    log_marginal   : 1182.2660255623823\n",
      "    log_joint      : 1390.6541245043577\n",
      "    val_loss       : -1179.6502367102582\n",
      "    val_ess        : 1.9657559861307559\n",
      "    val_log_marginal: 1179.6833867612092\n",
      "    val_log_joint  : 1388.2390561311142\n",
      "Train Epoch: 1735 [0/101520 (0%)] Loss: -1179.454468\n",
      "Train Epoch: 1735 [11264/101520 (11%)] Loss: -1186.688599\n",
      "Train Epoch: 1735 [22528/101520 (22%)] Loss: -1180.030151\n",
      "Train Epoch: 1735 [33792/101520 (33%)] Loss: -1185.895264\n",
      "Train Epoch: 1735 [45056/101520 (44%)] Loss: -1182.539429\n",
      "Train Epoch: 1735 [56320/101520 (55%)] Loss: -1192.932129\n",
      "Train Epoch: 1735 [67584/101520 (67%)] Loss: -1181.414429\n",
      "Train Epoch: 1735 [78848/101520 (78%)] Loss: -1176.839722\n",
      "Train Epoch: 1735 [90112/101520 (89%)] Loss: -1177.041748\n",
      "Train Epoch: 1735 [101376/101520 (100%)] Loss: -1182.154541\n",
      "    epoch          : 1735\n",
      "    loss           : -1182.1998057916535\n",
      "    ess            : 1.9662372281203917\n",
      "    log_marginal   : 1182.2323421305748\n",
      "    log_joint      : 1390.7082298700534\n",
      "    val_loss       : -1180.8409742272418\n",
      "    val_ess        : 1.9665636187014373\n",
      "    val_log_marginal: 1180.87353515625\n",
      "    val_log_joint  : 1388.9528490149457\n",
      "Train Epoch: 1736 [0/101520 (0%)] Loss: -1177.465088\n",
      "Train Epoch: 1736 [11264/101520 (11%)] Loss: -1174.935425\n",
      "Train Epoch: 1736 [22528/101520 (22%)] Loss: -1182.127319\n",
      "Train Epoch: 1736 [33792/101520 (33%)] Loss: -1180.548340\n",
      "Train Epoch: 1736 [45056/101520 (44%)] Loss: -1177.154785\n",
      "Train Epoch: 1736 [56320/101520 (55%)] Loss: -1182.557007\n",
      "Train Epoch: 1736 [67584/101520 (67%)] Loss: -1190.599121\n",
      "Train Epoch: 1736 [78848/101520 (78%)] Loss: -1192.791382\n",
      "Train Epoch: 1736 [90112/101520 (89%)] Loss: -1189.452637\n",
      "Train Epoch: 1736 [101376/101520 (100%)] Loss: -1178.174683\n",
      "    epoch          : 1736\n",
      "    loss           : -1182.290292404405\n",
      "    ess            : 1.9662903367574491\n",
      "    log_marginal   : 1182.3220619700062\n",
      "    log_joint      : 1390.7474046256673\n",
      "    val_loss       : -1180.5149085003397\n",
      "    val_ess        : 1.9669841787089473\n",
      "    val_log_marginal: 1180.5469970703125\n",
      "    val_log_joint  : 1389.046530018682\n",
      "Train Epoch: 1737 [0/101520 (0%)] Loss: -1191.007812\n",
      "Train Epoch: 1737 [11264/101520 (11%)] Loss: -1182.340576\n",
      "Train Epoch: 1737 [22528/101520 (22%)] Loss: -1179.467529\n",
      "Train Epoch: 1737 [33792/101520 (33%)] Loss: -1184.631958\n",
      "Train Epoch: 1737 [45056/101520 (44%)] Loss: -1185.999023\n",
      "Train Epoch: 1737 [56320/101520 (55%)] Loss: -1187.370728\n",
      "Train Epoch: 1737 [67584/101520 (67%)] Loss: -1183.305664\n",
      "Train Epoch: 1737 [78848/101520 (78%)] Loss: -1181.122559\n",
      "Train Epoch: 1737 [90112/101520 (89%)] Loss: -1182.280518\n",
      "Train Epoch: 1737 [101376/101520 (100%)] Loss: -1183.833618\n",
      "    epoch          : 1737\n",
      "    loss           : -1182.3550700278738\n",
      "    ess            : 1.9659241348055738\n",
      "    log_marginal   : 1182.3877088077104\n",
      "    log_joint      : 1390.8127128562735\n",
      "    val_loss       : -1179.5577445652175\n",
      "    val_ess        : 1.9646851549977842\n",
      "    val_log_marginal: 1179.6022312330163\n",
      "    val_log_joint  : 1388.0880233101223\n",
      "Train Epoch: 1738 [0/101520 (0%)] Loss: -1175.033569\n",
      "Train Epoch: 1738 [11264/101520 (11%)] Loss: -1181.820679\n",
      "Train Epoch: 1738 [22528/101520 (22%)] Loss: -1182.898560\n",
      "Train Epoch: 1738 [33792/101520 (33%)] Loss: -1190.563599\n",
      "Train Epoch: 1738 [45056/101520 (44%)] Loss: -1184.404541\n",
      "Train Epoch: 1738 [56320/101520 (55%)] Loss: -1182.924194\n",
      "Train Epoch: 1738 [67584/101520 (67%)] Loss: -1183.044189\n",
      "Train Epoch: 1738 [78848/101520 (78%)] Loss: -1177.834229\n",
      "Train Epoch: 1738 [90112/101520 (89%)] Loss: -1197.638184\n",
      "Train Epoch: 1738 [101376/101520 (100%)] Loss: -1188.086792\n",
      "    epoch          : 1738\n",
      "    loss           : -1182.3248321686558\n",
      "    ess            : 1.9660094191680602\n",
      "    log_marginal   : 1182.357580750432\n",
      "    log_joint      : 1390.8827744680434\n",
      "    val_loss       : -1180.865770422894\n",
      "    val_ess        : 1.9673966065697048\n",
      "    val_log_marginal: 1180.8977687669837\n",
      "    val_log_joint  : 1389.466159986413\n",
      "Train Epoch: 1739 [0/101520 (0%)] Loss: -1179.600342\n",
      "Train Epoch: 1739 [11264/101520 (11%)] Loss: -1184.545776\n",
      "Train Epoch: 1739 [22528/101520 (22%)] Loss: -1185.575439\n",
      "Train Epoch: 1739 [33792/101520 (33%)] Loss: -1187.033936\n",
      "Train Epoch: 1739 [45056/101520 (44%)] Loss: -1184.145996\n",
      "Train Epoch: 1739 [56320/101520 (55%)] Loss: -1183.711060\n",
      "Train Epoch: 1739 [67584/101520 (67%)] Loss: -1187.713867\n",
      "Train Epoch: 1739 [78848/101520 (78%)] Loss: -1180.817017\n",
      "Train Epoch: 1739 [90112/101520 (89%)] Loss: -1182.361572\n",
      "Train Epoch: 1739 [101376/101520 (100%)] Loss: -1187.254517\n",
      "    epoch          : 1739\n",
      "    loss           : -1182.4298003690326\n",
      "    ess            : 1.9662077528747481\n",
      "    log_marginal   : 1182.4622496025047\n",
      "    log_joint      : 1390.8893944821766\n",
      "    val_loss       : -1181.0693677819293\n",
      "    val_ess        : 1.9658082049825918\n",
      "    val_log_marginal: 1181.1014298148777\n",
      "    val_log_joint  : 1389.6437829059103\n",
      "Train Epoch: 1740 [0/101520 (0%)] Loss: -1179.421387\n",
      "Train Epoch: 1740 [11264/101520 (11%)] Loss: -1178.059448\n",
      "Train Epoch: 1740 [22528/101520 (22%)] Loss: -1182.328369\n",
      "Train Epoch: 1740 [33792/101520 (33%)] Loss: -1173.197021\n",
      "Train Epoch: 1740 [45056/101520 (44%)] Loss: -1189.370483\n",
      "Train Epoch: 1740 [56320/101520 (55%)] Loss: -1184.918579\n",
      "Train Epoch: 1740 [67584/101520 (67%)] Loss: -1187.025391\n",
      "Train Epoch: 1740 [78848/101520 (78%)] Loss: -1178.271851\n",
      "Train Epoch: 1740 [90112/101520 (89%)] Loss: -1182.402954\n",
      "Train Epoch: 1740 [101376/101520 (100%)] Loss: -1184.690063\n",
      "    epoch          : 1740\n",
      "    loss           : -1182.443375323885\n",
      "    ess            : 1.9669957113026375\n",
      "    log_marginal   : 1182.4751160588096\n",
      "    log_joint      : 1390.8536677528266\n",
      "    val_loss       : -1182.5335374915082\n",
      "    val_ess        : 1.9648864321086719\n",
      "    val_log_marginal: 1182.567775560462\n",
      "    val_log_joint  : 1391.0686300526495\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1740.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1741 [0/101520 (0%)] Loss: -1182.885254\n",
      "Train Epoch: 1741 [11264/101520 (11%)] Loss: -1176.274170\n",
      "Train Epoch: 1741 [22528/101520 (22%)] Loss: -1186.996582\n",
      "Train Epoch: 1741 [33792/101520 (33%)] Loss: -1182.605225\n",
      "Train Epoch: 1741 [45056/101520 (44%)] Loss: -1181.591553\n",
      "Train Epoch: 1741 [56320/101520 (55%)] Loss: -1185.447510\n",
      "Train Epoch: 1741 [67584/101520 (67%)] Loss: -1180.104736\n",
      "Train Epoch: 1741 [78848/101520 (78%)] Loss: -1178.629639\n",
      "Train Epoch: 1741 [90112/101520 (89%)] Loss: -1181.204102\n",
      "Train Epoch: 1741 [101376/101520 (100%)] Loss: -1190.714478\n",
      "    epoch          : 1741\n",
      "    loss           : -1182.400324989204\n",
      "    ess            : 1.966271653247239\n",
      "    log_marginal   : 1182.432157736927\n",
      "    log_joint      : 1390.8649896209563\n",
      "    val_loss       : -1180.0638321586277\n",
      "    val_ess        : 1.963916327642358\n",
      "    val_log_marginal: 1180.1019711701767\n",
      "    val_log_joint  : 1388.7826936141305\n",
      "Train Epoch: 1742 [0/101520 (0%)] Loss: -1183.605225\n",
      "Train Epoch: 1742 [11264/101520 (11%)] Loss: -1186.596680\n",
      "Train Epoch: 1742 [22528/101520 (22%)] Loss: -1185.816406\n",
      "Train Epoch: 1742 [33792/101520 (33%)] Loss: -1175.661255\n",
      "Train Epoch: 1742 [45056/101520 (44%)] Loss: -1186.667480\n",
      "Train Epoch: 1742 [56320/101520 (55%)] Loss: -1186.814331\n",
      "Train Epoch: 1742 [67584/101520 (67%)] Loss: -1185.072998\n",
      "Train Epoch: 1742 [78848/101520 (78%)] Loss: -1185.240967\n",
      "Train Epoch: 1742 [90112/101520 (89%)] Loss: -1176.000977\n",
      "Train Epoch: 1742 [101376/101520 (100%)] Loss: -1196.707886\n",
      "    epoch          : 1742\n",
      "    loss           : -1182.4068008499528\n",
      "    ess            : 1.9662311801958323\n",
      "    log_marginal   : 1182.439661687343\n",
      "    log_joint      : 1390.939756153816\n",
      "    val_loss       : -1181.525687839674\n",
      "    val_ess        : 1.9658281647640725\n",
      "    val_log_marginal: 1181.5570068359375\n",
      "    val_log_joint  : 1390.1356625764267\n",
      "Train Epoch: 1743 [0/101520 (0%)] Loss: -1185.992676\n",
      "Train Epoch: 1743 [11264/101520 (11%)] Loss: -1176.867676\n",
      "Train Epoch: 1743 [22528/101520 (22%)] Loss: -1186.863037\n",
      "Train Epoch: 1743 [33792/101520 (33%)] Loss: -1176.601929\n",
      "Train Epoch: 1743 [45056/101520 (44%)] Loss: -1180.435547\n",
      "Train Epoch: 1743 [56320/101520 (55%)] Loss: -1181.685303\n",
      "Train Epoch: 1743 [67584/101520 (67%)] Loss: -1172.789062\n",
      "Train Epoch: 1743 [78848/101520 (78%)] Loss: -1182.149414\n",
      "Train Epoch: 1743 [90112/101520 (89%)] Loss: -1185.130127\n",
      "Train Epoch: 1743 [101376/101520 (100%)] Loss: -1174.764160\n",
      "    epoch          : 1743\n",
      "    loss           : -1182.360855792635\n",
      "    ess            : 1.9664953102418525\n",
      "    log_marginal   : 1182.392193511503\n",
      "    log_joint      : 1390.8255756320666\n",
      "    val_loss       : -1179.417135487432\n",
      "    val_ess        : 1.9651884151541668\n",
      "    val_log_marginal: 1179.4534752887228\n",
      "    val_log_joint  : 1387.7619045091712\n",
      "Train Epoch: 1744 [0/101520 (0%)] Loss: -1176.744507\n",
      "Train Epoch: 1744 [11264/101520 (11%)] Loss: -1176.234985\n",
      "Train Epoch: 1744 [22528/101520 (22%)] Loss: -1180.355957\n",
      "Train Epoch: 1744 [33792/101520 (33%)] Loss: -1192.478271\n",
      "Train Epoch: 1744 [45056/101520 (44%)] Loss: -1183.997437\n",
      "Train Epoch: 1744 [56320/101520 (55%)] Loss: -1178.140869\n",
      "Train Epoch: 1744 [67584/101520 (67%)] Loss: -1180.221680\n",
      "Train Epoch: 1744 [78848/101520 (78%)] Loss: -1181.208740\n",
      "Train Epoch: 1744 [90112/101520 (89%)] Loss: -1185.563477\n",
      "Train Epoch: 1744 [101376/101520 (100%)] Loss: -1175.929688\n",
      "    epoch          : 1744\n",
      "    loss           : -1182.379098863458\n",
      "    ess            : 1.965865725847944\n",
      "    log_marginal   : 1182.4114432023398\n",
      "    log_joint      : 1390.823167350424\n",
      "    val_loss       : -1181.7840470023777\n",
      "    val_ess        : 1.9636438048404197\n",
      "    val_log_marginal: 1181.81787109375\n",
      "    val_log_joint  : 1390.4896611752717\n",
      "Train Epoch: 1745 [0/101520 (0%)] Loss: -1186.573975\n",
      "Train Epoch: 1745 [11264/101520 (11%)] Loss: -1180.889282\n",
      "Train Epoch: 1745 [22528/101520 (22%)] Loss: -1186.802246\n",
      "Train Epoch: 1745 [33792/101520 (33%)] Loss: -1174.641968\n",
      "Train Epoch: 1745 [45056/101520 (44%)] Loss: -1182.729492\n",
      "Train Epoch: 1745 [56320/101520 (55%)] Loss: -1179.320557\n",
      "Train Epoch: 1745 [67584/101520 (67%)] Loss: -1181.571045\n",
      "Train Epoch: 1745 [78848/101520 (78%)] Loss: -1181.081299\n",
      "Train Epoch: 1745 [90112/101520 (89%)] Loss: -1178.863770\n",
      "Train Epoch: 1745 [101376/101520 (100%)] Loss: -1184.810547\n",
      "    epoch          : 1745\n",
      "    loss           : -1182.3366944586212\n",
      "    ess            : 1.9659973394930663\n",
      "    log_marginal   : 1182.36963810753\n",
      "    log_joint      : 1390.8846006153815\n",
      "    val_loss       : -1181.2122378141983\n",
      "    val_ess        : 1.964907609898111\n",
      "    val_log_marginal: 1181.2457753057065\n",
      "    val_log_joint  : 1389.778760827106\n",
      "Train Epoch: 1746 [0/101520 (0%)] Loss: -1173.597900\n",
      "Train Epoch: 1746 [11264/101520 (11%)] Loss: -1178.663574\n",
      "Train Epoch: 1746 [22528/101520 (22%)] Loss: -1181.201904\n",
      "Train Epoch: 1746 [33792/101520 (33%)] Loss: -1186.343750\n",
      "Train Epoch: 1746 [45056/101520 (44%)] Loss: -1186.658447\n",
      "Train Epoch: 1746 [56320/101520 (55%)] Loss: -1187.809082\n",
      "Train Epoch: 1746 [67584/101520 (67%)] Loss: -1180.915527\n",
      "Train Epoch: 1746 [78848/101520 (78%)] Loss: -1185.906982\n",
      "Train Epoch: 1746 [90112/101520 (89%)] Loss: -1181.132446\n",
      "Train Epoch: 1746 [101376/101520 (100%)] Loss: -1186.582275\n",
      "    epoch          : 1746\n",
      "    loss           : -1182.4126596115343\n",
      "    ess            : 1.9665023364014362\n",
      "    log_marginal   : 1182.445137675683\n",
      "    log_joint      : 1390.9190778109296\n",
      "    val_loss       : -1181.2472985309103\n",
      "    val_ess        : 1.9665755396303923\n",
      "    val_log_marginal: 1181.2794295601223\n",
      "    val_log_joint  : 1389.7677001953125\n",
      "Train Epoch: 1747 [0/101520 (0%)] Loss: -1182.690308\n",
      "Train Epoch: 1747 [11264/101520 (11%)] Loss: -1179.332031\n",
      "Train Epoch: 1747 [22528/101520 (22%)] Loss: -1182.111328\n",
      "Train Epoch: 1747 [33792/101520 (33%)] Loss: -1188.281860\n",
      "Train Epoch: 1747 [45056/101520 (44%)] Loss: -1179.366577\n",
      "Train Epoch: 1747 [56320/101520 (55%)] Loss: -1182.622925\n",
      "Train Epoch: 1747 [67584/101520 (67%)] Loss: -1186.829102\n",
      "Train Epoch: 1747 [78848/101520 (78%)] Loss: -1189.326904\n",
      "Train Epoch: 1747 [90112/101520 (89%)] Loss: -1184.576172\n",
      "Train Epoch: 1747 [101376/101520 (100%)] Loss: -1163.279053\n",
      "    epoch          : 1747\n",
      "    loss           : -1182.3625218377042\n",
      "    ess            : 1.9660493093519356\n",
      "    log_marginal   : 1182.395317039298\n",
      "    log_joint      : 1390.8810403335035\n",
      "    val_loss       : -1181.0690705672555\n",
      "    val_ess        : 1.9675868542298027\n",
      "    val_log_marginal: 1181.098537279212\n",
      "    val_log_joint  : 1389.4019616168478\n",
      "Train Epoch: 1748 [0/101520 (0%)] Loss: -1185.014526\n",
      "Train Epoch: 1748 [11264/101520 (11%)] Loss: -1181.849731\n",
      "Train Epoch: 1748 [22528/101520 (22%)] Loss: -1183.559082\n",
      "Train Epoch: 1748 [33792/101520 (33%)] Loss: -1176.873413\n",
      "Train Epoch: 1748 [45056/101520 (44%)] Loss: -1177.228027\n",
      "Train Epoch: 1748 [56320/101520 (55%)] Loss: -1183.897217\n",
      "Train Epoch: 1748 [67584/101520 (67%)] Loss: -1184.201904\n",
      "Train Epoch: 1748 [78848/101520 (78%)] Loss: -1181.431152\n",
      "Train Epoch: 1748 [90112/101520 (89%)] Loss: -1182.348022\n",
      "Train Epoch: 1748 [101376/101520 (100%)] Loss: -1174.925049\n",
      "    epoch          : 1748\n",
      "    loss           : -1182.450489140036\n",
      "    ess            : 1.9660546935383398\n",
      "    log_marginal   : 1182.482979472558\n",
      "    log_joint      : 1390.9694971439228\n",
      "    val_loss       : -1181.4083305027175\n",
      "    val_ess        : 1.9666464639746624\n",
      "    val_log_marginal: 1181.4405676800272\n",
      "    val_log_joint  : 1390.0142716117527\n",
      "Train Epoch: 1749 [0/101520 (0%)] Loss: -1182.291626\n",
      "Train Epoch: 1749 [11264/101520 (11%)] Loss: -1184.519287\n",
      "Train Epoch: 1749 [22528/101520 (22%)] Loss: -1183.018066\n",
      "Train Epoch: 1749 [33792/101520 (33%)] Loss: -1193.780518\n",
      "Train Epoch: 1749 [45056/101520 (44%)] Loss: -1178.914307\n",
      "Train Epoch: 1749 [56320/101520 (55%)] Loss: -1185.693115\n",
      "Train Epoch: 1749 [67584/101520 (67%)] Loss: -1178.938965\n",
      "Train Epoch: 1749 [78848/101520 (78%)] Loss: -1187.674316\n",
      "Train Epoch: 1749 [90112/101520 (89%)] Loss: -1184.314453\n",
      "Train Epoch: 1749 [101376/101520 (100%)] Loss: -1187.617432\n",
      "    epoch          : 1749\n",
      "    loss           : -1182.505034326908\n",
      "    ess            : 1.9659282298543346\n",
      "    log_marginal   : 1182.5384503081816\n",
      "    log_joint      : 1391.0569142097204\n",
      "    val_loss       : -1181.650443699049\n",
      "    val_ess        : 1.9677731006041816\n",
      "    val_log_marginal: 1181.6821979025135\n",
      "    val_log_joint  : 1390.236572265625\n",
      "Train Epoch: 1750 [0/101520 (0%)] Loss: -1191.081543\n",
      "Train Epoch: 1750 [11264/101520 (11%)] Loss: -1186.301392\n",
      "Train Epoch: 1750 [22528/101520 (22%)] Loss: -1185.695679\n",
      "Train Epoch: 1750 [33792/101520 (33%)] Loss: -1186.564697\n",
      "Train Epoch: 1750 [45056/101520 (44%)] Loss: -1177.162354\n",
      "Train Epoch: 1750 [56320/101520 (55%)] Loss: -1185.698486\n",
      "Train Epoch: 1750 [67584/101520 (67%)] Loss: -1182.399292\n",
      "Train Epoch: 1750 [78848/101520 (78%)] Loss: -1181.620728\n",
      "Train Epoch: 1750 [90112/101520 (89%)] Loss: -1182.777466\n",
      "Train Epoch: 1750 [101376/101520 (100%)] Loss: -1197.380981\n",
      "    epoch          : 1750\n",
      "    loss           : -1182.635934800958\n",
      "    ess            : 1.9667452196380002\n",
      "    log_marginal   : 1182.6676307563207\n",
      "    log_joint      : 1391.1234523447315\n",
      "    val_loss       : -1180.578708814538\n",
      "    val_ess        : 1.967525461445684\n",
      "    val_log_marginal: 1180.6095819887908\n",
      "    val_log_joint  : 1388.945413340693\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1750.pth ...\n",
      "Train Epoch: 1751 [0/101520 (0%)] Loss: -1180.868896\n",
      "Train Epoch: 1751 [11264/101520 (11%)] Loss: -1183.145996\n",
      "Train Epoch: 1751 [22528/101520 (22%)] Loss: -1187.195435\n",
      "Train Epoch: 1751 [33792/101520 (33%)] Loss: -1187.290283\n",
      "Train Epoch: 1751 [45056/101520 (44%)] Loss: -1181.714844\n",
      "Train Epoch: 1751 [56320/101520 (55%)] Loss: -1181.664551\n",
      "Train Epoch: 1751 [67584/101520 (67%)] Loss: -1182.634399\n",
      "Train Epoch: 1751 [78848/101520 (78%)] Loss: -1176.898926\n",
      "Train Epoch: 1751 [90112/101520 (89%)] Loss: -1178.231201\n",
      "Train Epoch: 1751 [101376/101520 (100%)] Loss: -1186.909790\n",
      "    epoch          : 1751\n",
      "    loss           : -1182.6690882390467\n",
      "    ess            : 1.9661789713193423\n",
      "    log_marginal   : 1182.7019914023242\n",
      "    log_joint      : 1391.1867522426587\n",
      "    val_loss       : -1179.8400242017663\n",
      "    val_ess        : 1.9625567405120186\n",
      "    val_log_marginal: 1179.8729778787365\n",
      "    val_log_joint  : 1388.76244586447\n",
      "Train Epoch: 1752 [0/101520 (0%)] Loss: -1180.317627\n",
      "Train Epoch: 1752 [11264/101520 (11%)] Loss: -1179.913940\n",
      "Train Epoch: 1752 [22528/101520 (22%)] Loss: -1176.554932\n",
      "Train Epoch: 1752 [33792/101520 (33%)] Loss: -1180.548340\n",
      "Train Epoch: 1752 [45056/101520 (44%)] Loss: -1181.544312\n",
      "Train Epoch: 1752 [56320/101520 (55%)] Loss: -1184.716675\n",
      "Train Epoch: 1752 [67584/101520 (67%)] Loss: -1182.074829\n",
      "Train Epoch: 1752 [78848/101520 (78%)] Loss: -1184.022461\n",
      "Train Epoch: 1752 [90112/101520 (89%)] Loss: -1180.354004\n",
      "Train Epoch: 1752 [101376/101520 (100%)] Loss: -1174.877441\n",
      "    epoch          : 1752\n",
      "    loss           : -1182.6988580598304\n",
      "    ess            : 1.9651798009872437\n",
      "    log_marginal   : 1182.731887587351\n",
      "    log_joint      : 1391.118716752709\n",
      "    val_loss       : -1181.4494841202445\n",
      "    val_ess        : 1.9687020674995754\n",
      "    val_log_marginal: 1181.4773798403533\n",
      "    val_log_joint  : 1389.8274031929348\n",
      "Train Epoch: 1753 [0/101520 (0%)] Loss: -1180.314697\n",
      "Train Epoch: 1753 [11264/101520 (11%)] Loss: -1179.019531\n",
      "Train Epoch: 1753 [22528/101520 (22%)] Loss: -1179.349121\n",
      "Train Epoch: 1753 [33792/101520 (33%)] Loss: -1177.482666\n",
      "Train Epoch: 1753 [45056/101520 (44%)] Loss: -1187.941284\n",
      "Train Epoch: 1753 [56320/101520 (55%)] Loss: -1181.157837\n",
      "Train Epoch: 1753 [67584/101520 (67%)] Loss: -1183.087646\n",
      "Train Epoch: 1753 [78848/101520 (78%)] Loss: -1181.077515\n",
      "Train Epoch: 1753 [90112/101520 (89%)] Loss: -1175.369629\n",
      "Train Epoch: 1753 [101376/101520 (100%)] Loss: -1182.689331\n",
      "    epoch          : 1753\n",
      "    loss           : -1182.6209324208935\n",
      "    ess            : 1.966125526619916\n",
      "    log_marginal   : 1182.6534429962312\n",
      "    log_joint      : 1391.136397932043\n",
      "    val_loss       : -1181.9389436141305\n",
      "    val_ess        : 1.9648296107416567\n",
      "    val_log_marginal: 1181.9857655400815\n",
      "    val_log_joint  : 1390.370064113451\n",
      "Train Epoch: 1754 [0/101520 (0%)] Loss: -1182.552979\n",
      "Train Epoch: 1754 [11264/101520 (11%)] Loss: -1186.305664\n",
      "Train Epoch: 1754 [22528/101520 (22%)] Loss: -1184.433716\n",
      "Train Epoch: 1754 [33792/101520 (33%)] Loss: -1192.925171\n",
      "Train Epoch: 1754 [45056/101520 (44%)] Loss: -1176.562744\n",
      "Train Epoch: 1754 [56320/101520 (55%)] Loss: -1183.730591\n",
      "Train Epoch: 1754 [67584/101520 (67%)] Loss: -1181.227051\n",
      "Train Epoch: 1754 [78848/101520 (78%)] Loss: -1179.322998\n",
      "Train Epoch: 1754 [90112/101520 (89%)] Loss: -1182.361572\n",
      "Train Epoch: 1754 [101376/101520 (100%)] Loss: -1187.061279\n",
      "    epoch          : 1754\n",
      "    loss           : -1182.7109976150282\n",
      "    ess            : 1.9655548663594615\n",
      "    log_marginal   : 1182.7442504269393\n",
      "    log_joint      : 1391.2535762307632\n",
      "    val_loss       : -1181.8439092221467\n",
      "    val_ess        : 1.9641681909561157\n",
      "    val_log_marginal: 1181.8768841287365\n",
      "    val_log_joint  : 1389.9303562330163\n",
      "Train Epoch: 1755 [0/101520 (0%)] Loss: -1179.457397\n",
      "Train Epoch: 1755 [11264/101520 (11%)] Loss: -1171.495850\n",
      "Train Epoch: 1755 [22528/101520 (22%)] Loss: -1188.807129\n",
      "Train Epoch: 1755 [33792/101520 (33%)] Loss: -1182.802246\n",
      "Train Epoch: 1755 [45056/101520 (44%)] Loss: -1184.856445\n",
      "Train Epoch: 1755 [56320/101520 (55%)] Loss: -1179.828125\n",
      "Train Epoch: 1755 [67584/101520 (67%)] Loss: -1187.496338\n",
      "Train Epoch: 1755 [78848/101520 (78%)] Loss: -1180.707031\n",
      "Train Epoch: 1755 [90112/101520 (89%)] Loss: -1175.250977\n",
      "Train Epoch: 1755 [101376/101520 (100%)] Loss: -1189.471436\n",
      "    epoch          : 1755\n",
      "    loss           : -1182.74109684163\n",
      "    ess            : 1.9665516411239778\n",
      "    log_marginal   : 1182.7729167075613\n",
      "    log_joint      : 1391.2323654404836\n",
      "    val_loss       : -1180.8600702700408\n",
      "    val_ess        : 1.9681491385335508\n",
      "    val_log_marginal: 1180.892583432405\n",
      "    val_log_joint  : 1389.4115627122962\n",
      "Train Epoch: 1756 [0/101520 (0%)] Loss: -1175.466675\n",
      "Train Epoch: 1756 [11264/101520 (11%)] Loss: -1180.723999\n",
      "Train Epoch: 1756 [22528/101520 (22%)] Loss: -1182.665894\n",
      "Train Epoch: 1756 [33792/101520 (33%)] Loss: -1185.948975\n",
      "Train Epoch: 1756 [45056/101520 (44%)] Loss: -1181.258545\n",
      "Train Epoch: 1756 [56320/101520 (55%)] Loss: -1192.361938\n",
      "Train Epoch: 1756 [67584/101520 (67%)] Loss: -1180.648315\n",
      "Train Epoch: 1756 [78848/101520 (78%)] Loss: -1180.726807\n",
      "Train Epoch: 1756 [90112/101520 (89%)] Loss: -1186.589600\n",
      "Train Epoch: 1756 [101376/101520 (100%)] Loss: -1184.843750\n",
      "    epoch          : 1756\n",
      "    loss           : -1182.7515672846655\n",
      "    ess            : 1.9654599715716874\n",
      "    log_marginal   : 1182.7845624607412\n",
      "    log_joint      : 1391.2843680070273\n",
      "    val_loss       : -1180.2078326681385\n",
      "    val_ess        : 1.9667949728343799\n",
      "    val_log_marginal: 1180.238918138587\n",
      "    val_log_joint  : 1389.1872293223505\n",
      "Train Epoch: 1757 [0/101520 (0%)] Loss: -1177.200928\n",
      "Train Epoch: 1757 [11264/101520 (11%)] Loss: -1189.119019\n",
      "Train Epoch: 1757 [22528/101520 (22%)] Loss: -1182.116089\n",
      "Train Epoch: 1757 [33792/101520 (33%)] Loss: -1184.152100\n",
      "Train Epoch: 1757 [45056/101520 (44%)] Loss: -1182.179932\n",
      "Train Epoch: 1757 [56320/101520 (55%)] Loss: -1184.615967\n",
      "Train Epoch: 1757 [67584/101520 (67%)] Loss: -1183.690674\n",
      "Train Epoch: 1757 [78848/101520 (78%)] Loss: -1178.417725\n",
      "Train Epoch: 1757 [90112/101520 (89%)] Loss: -1182.213623\n",
      "Train Epoch: 1757 [101376/101520 (100%)] Loss: -1170.670288\n",
      "    epoch          : 1757\n",
      "    loss           : -1182.8141967160018\n",
      "    ess            : 1.9660378143415977\n",
      "    log_marginal   : 1182.845866907781\n",
      "    log_joint      : 1391.276506433535\n",
      "    val_loss       : -1180.4048541525135\n",
      "    val_ess        : 1.9688170474508535\n",
      "    val_log_marginal: 1180.4345384680707\n",
      "    val_log_joint  : 1388.8359162703805\n",
      "Train Epoch: 1758 [0/101520 (0%)] Loss: -1178.823730\n",
      "Train Epoch: 1758 [11264/101520 (11%)] Loss: -1176.695801\n",
      "Train Epoch: 1758 [22528/101520 (22%)] Loss: -1179.559814\n",
      "Train Epoch: 1758 [33792/101520 (33%)] Loss: -1181.079956\n",
      "Train Epoch: 1758 [45056/101520 (44%)] Loss: -1189.979004\n",
      "Train Epoch: 1758 [56320/101520 (55%)] Loss: -1179.441650\n",
      "Train Epoch: 1758 [67584/101520 (67%)] Loss: -1181.444092\n",
      "Train Epoch: 1758 [78848/101520 (78%)] Loss: -1187.475586\n",
      "Train Epoch: 1758 [90112/101520 (89%)] Loss: -1183.365967\n",
      "Train Epoch: 1758 [101376/101520 (100%)] Loss: -1190.627686\n",
      "    epoch          : 1758\n",
      "    loss           : -1182.840681066465\n",
      "    ess            : 1.9658611839141078\n",
      "    log_marginal   : 1182.8736161275126\n",
      "    log_joint      : 1391.3498522887876\n",
      "    val_loss       : -1182.1221499235733\n",
      "    val_ess        : 1.963477725568025\n",
      "    val_log_marginal: 1182.159912109375\n",
      "    val_log_joint  : 1390.76488727072\n",
      "Train Epoch: 1759 [0/101520 (0%)] Loss: -1185.383179\n",
      "Train Epoch: 1759 [11264/101520 (11%)] Loss: -1181.045654\n",
      "Train Epoch: 1759 [22528/101520 (22%)] Loss: -1180.475830\n",
      "Train Epoch: 1759 [33792/101520 (33%)] Loss: -1179.436768\n",
      "Train Epoch: 1759 [45056/101520 (44%)] Loss: -1184.658813\n",
      "Train Epoch: 1759 [56320/101520 (55%)] Loss: -1191.318726\n",
      "Train Epoch: 1759 [67584/101520 (67%)] Loss: -1182.061279\n",
      "Train Epoch: 1759 [78848/101520 (78%)] Loss: -1180.354492\n",
      "Train Epoch: 1759 [90112/101520 (89%)] Loss: -1189.843750\n",
      "Train Epoch: 1759 [101376/101520 (100%)] Loss: -1197.160400\n",
      "    epoch          : 1759\n",
      "    loss           : -1182.9392936361494\n",
      "    ess            : 1.9664340594306065\n",
      "    log_marginal   : 1182.9709656681846\n",
      "    log_joint      : 1391.396484375\n",
      "    val_loss       : -1181.1973080842392\n",
      "    val_ess        : 1.9679889160653818\n",
      "    val_log_marginal: 1181.2279105808425\n",
      "    val_log_joint  : 1389.6457731827445\n",
      "Train Epoch: 1760 [0/101520 (0%)] Loss: -1182.985352\n",
      "Train Epoch: 1760 [11264/101520 (11%)] Loss: -1185.080078\n",
      "Train Epoch: 1760 [22528/101520 (22%)] Loss: -1177.907227\n",
      "Train Epoch: 1760 [33792/101520 (33%)] Loss: -1186.239624\n",
      "Train Epoch: 1760 [45056/101520 (44%)] Loss: -1183.919678\n",
      "Train Epoch: 1760 [56320/101520 (55%)] Loss: -1176.917969\n",
      "Train Epoch: 1760 [67584/101520 (67%)] Loss: -1181.029297\n",
      "Train Epoch: 1760 [78848/101520 (78%)] Loss: -1183.197266\n",
      "Train Epoch: 1760 [90112/101520 (89%)] Loss: -1187.908203\n",
      "Train Epoch: 1760 [101376/101520 (100%)] Loss: -1177.240601\n",
      "    epoch          : 1760\n",
      "    loss           : -1182.876285112084\n",
      "    ess            : 1.9662269881023235\n",
      "    log_marginal   : 1182.9083454381282\n",
      "    log_joint      : 1391.3182195155464\n",
      "    val_loss       : -1179.3873291015625\n",
      "    val_ess        : 1.9705066058946692\n",
      "    val_log_marginal: 1179.4148108440897\n",
      "    val_log_joint  : 1387.7024615743885\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1760.pth ...\n",
      "Train Epoch: 1761 [0/101520 (0%)] Loss: -1187.399902\n",
      "Train Epoch: 1761 [11264/101520 (11%)] Loss: -1189.121216\n",
      "Train Epoch: 1761 [22528/101520 (22%)] Loss: -1182.027100\n",
      "Train Epoch: 1761 [33792/101520 (33%)] Loss: -1186.281982\n",
      "Train Epoch: 1761 [45056/101520 (44%)] Loss: -1192.748413\n",
      "Train Epoch: 1761 [56320/101520 (55%)] Loss: -1185.821045\n",
      "Train Epoch: 1761 [67584/101520 (67%)] Loss: -1176.194702\n",
      "Train Epoch: 1761 [78848/101520 (78%)] Loss: -1177.221069\n",
      "Train Epoch: 1761 [90112/101520 (89%)] Loss: -1182.045898\n",
      "Train Epoch: 1761 [101376/101520 (100%)] Loss: -1188.723267\n",
      "    epoch          : 1761\n",
      "    loss           : -1182.9267890968515\n",
      "    ess            : 1.9660222979646231\n",
      "    log_marginal   : 1182.9598995956344\n",
      "    log_joint      : 1391.368613084956\n",
      "    val_loss       : -1181.308981190557\n",
      "    val_ess        : 1.9653928487197212\n",
      "    val_log_marginal: 1181.3427309782608\n",
      "    val_log_joint  : 1389.900003184443\n",
      "Train Epoch: 1762 [0/101520 (0%)] Loss: -1186.131836\n",
      "Train Epoch: 1762 [11264/101520 (11%)] Loss: -1173.791992\n",
      "Train Epoch: 1762 [22528/101520 (22%)] Loss: -1184.005249\n",
      "Train Epoch: 1762 [33792/101520 (33%)] Loss: -1184.115234\n",
      "Train Epoch: 1762 [45056/101520 (44%)] Loss: -1184.823975\n",
      "Train Epoch: 1762 [56320/101520 (55%)] Loss: -1179.220947\n",
      "Train Epoch: 1762 [67584/101520 (67%)] Loss: -1177.846558\n",
      "Train Epoch: 1762 [78848/101520 (78%)] Loss: -1182.351074\n",
      "Train Epoch: 1762 [90112/101520 (89%)] Loss: -1188.503052\n",
      "Train Epoch: 1762 [101376/101520 (100%)] Loss: -1190.780884\n",
      "    epoch          : 1762\n",
      "    loss           : -1182.9396899046012\n",
      "    ess            : 1.9668873434689775\n",
      "    log_marginal   : 1182.970728275165\n",
      "    log_joint      : 1391.4526753641253\n",
      "    val_loss       : -1181.757908033288\n",
      "    val_ess        : 1.9648896766745525\n",
      "    val_log_marginal: 1181.796094811481\n",
      "    val_log_joint  : 1390.1168849779212\n",
      "Train Epoch: 1763 [0/101520 (0%)] Loss: -1183.270752\n",
      "Train Epoch: 1763 [11264/101520 (11%)] Loss: -1185.531738\n",
      "Train Epoch: 1763 [22528/101520 (22%)] Loss: -1180.039795\n",
      "Train Epoch: 1763 [33792/101520 (33%)] Loss: -1191.587891\n",
      "Train Epoch: 1763 [45056/101520 (44%)] Loss: -1183.769043\n",
      "Train Epoch: 1763 [56320/101520 (55%)] Loss: -1181.398926\n",
      "Train Epoch: 1763 [67584/101520 (67%)] Loss: -1188.254150\n",
      "Train Epoch: 1763 [78848/101520 (78%)] Loss: -1182.639038\n",
      "Train Epoch: 1763 [90112/101520 (89%)] Loss: -1185.922974\n",
      "Train Epoch: 1763 [101376/101520 (100%)] Loss: -1179.265991\n",
      "    epoch          : 1763\n",
      "    loss           : -1182.8452927479193\n",
      "    ess            : 1.9662029827060412\n",
      "    log_marginal   : 1182.8779922562028\n",
      "    log_joint      : 1391.4016775773398\n",
      "    val_loss       : -1182.3328804347825\n",
      "    val_ess        : 1.9647055553353352\n",
      "    val_log_marginal: 1182.368848717731\n",
      "    val_log_joint  : 1390.9900326936142\n",
      "Train Epoch: 1764 [0/101520 (0%)] Loss: -1180.859985\n",
      "Train Epoch: 1764 [11264/101520 (11%)] Loss: -1180.750977\n",
      "Train Epoch: 1764 [22528/101520 (22%)] Loss: -1176.669922\n",
      "Train Epoch: 1764 [33792/101520 (33%)] Loss: -1191.967407\n",
      "Train Epoch: 1764 [45056/101520 (44%)] Loss: -1187.336792\n",
      "Train Epoch: 1764 [56320/101520 (55%)] Loss: -1181.028320\n",
      "Train Epoch: 1764 [67584/101520 (67%)] Loss: -1182.824829\n",
      "Train Epoch: 1764 [78848/101520 (78%)] Loss: -1182.441650\n",
      "Train Epoch: 1764 [90112/101520 (89%)] Loss: -1178.867920\n",
      "Train Epoch: 1764 [101376/101520 (100%)] Loss: -1162.402954\n",
      "    epoch          : 1764\n",
      "    loss           : -1182.8853814973304\n",
      "    ess            : 1.965840676921097\n",
      "    log_marginal   : 1182.917608673249\n",
      "    log_joint      : 1391.3743951692054\n",
      "    val_loss       : -1180.2257982336957\n",
      "    val_ess        : 1.9658942067104836\n",
      "    val_log_marginal: 1180.2590013586957\n",
      "    val_log_joint  : 1388.7634065047555\n",
      "Train Epoch: 1765 [0/101520 (0%)] Loss: -1181.650024\n",
      "Train Epoch: 1765 [11264/101520 (11%)] Loss: -1189.206909\n",
      "Train Epoch: 1765 [22528/101520 (22%)] Loss: -1177.579712\n",
      "Train Epoch: 1765 [33792/101520 (33%)] Loss: -1175.807861\n",
      "Train Epoch: 1765 [45056/101520 (44%)] Loss: -1177.614502\n",
      "Train Epoch: 1765 [56320/101520 (55%)] Loss: -1186.111816\n",
      "Train Epoch: 1765 [67584/101520 (67%)] Loss: -1183.056396\n",
      "Train Epoch: 1765 [78848/101520 (78%)] Loss: -1182.352783\n",
      "Train Epoch: 1765 [90112/101520 (89%)] Loss: -1182.380981\n",
      "Train Epoch: 1765 [101376/101520 (100%)] Loss: -1194.302124\n",
      "    epoch          : 1765\n",
      "    loss           : -1183.0825753523477\n",
      "    ess            : 1.9667022276164299\n",
      "    log_marginal   : 1183.1139431287295\n",
      "    log_joint      : 1391.58233243856\n",
      "    val_loss       : -1180.985160495924\n",
      "    val_ess        : 1.9680016766423765\n",
      "    val_log_marginal: 1181.0167130180027\n",
      "    val_log_joint  : 1389.3204239555027\n",
      "Train Epoch: 1766 [0/101520 (0%)] Loss: -1187.686890\n",
      "Train Epoch: 1766 [11264/101520 (11%)] Loss: -1180.290039\n",
      "Train Epoch: 1766 [22528/101520 (22%)] Loss: -1188.063232\n",
      "Train Epoch: 1766 [33792/101520 (33%)] Loss: -1183.928467\n",
      "Train Epoch: 1766 [45056/101520 (44%)] Loss: -1182.625854\n",
      "Train Epoch: 1766 [56320/101520 (55%)] Loss: -1182.198853\n",
      "Train Epoch: 1766 [67584/101520 (67%)] Loss: -1187.164062\n",
      "Train Epoch: 1766 [78848/101520 (78%)] Loss: -1184.903198\n",
      "Train Epoch: 1766 [90112/101520 (89%)] Loss: -1183.926514\n",
      "Train Epoch: 1766 [101376/101520 (100%)] Loss: -1188.492432\n",
      "    epoch          : 1766\n",
      "    loss           : -1183.0363131575848\n",
      "    ess            : 1.9659886503938455\n",
      "    log_marginal   : 1183.0689464166535\n",
      "    log_joint      : 1391.5480687127042\n",
      "    val_loss       : -1182.1830630095108\n",
      "    val_ess        : 1.9683936367864194\n",
      "    val_log_marginal: 1182.2132409137228\n",
      "    val_log_joint  : 1390.3197127632473\n",
      "Train Epoch: 1767 [0/101520 (0%)] Loss: -1181.096313\n",
      "Train Epoch: 1767 [11264/101520 (11%)] Loss: -1183.665527\n",
      "Train Epoch: 1767 [22528/101520 (22%)] Loss: -1188.770386\n",
      "Train Epoch: 1767 [33792/101520 (33%)] Loss: -1181.605103\n",
      "Train Epoch: 1767 [45056/101520 (44%)] Loss: -1184.836304\n",
      "Train Epoch: 1767 [56320/101520 (55%)] Loss: -1182.976562\n",
      "Train Epoch: 1767 [67584/101520 (67%)] Loss: -1183.772217\n",
      "Train Epoch: 1767 [78848/101520 (78%)] Loss: -1178.556396\n",
      "Train Epoch: 1767 [90112/101520 (89%)] Loss: -1184.960449\n",
      "Train Epoch: 1767 [101376/101520 (100%)] Loss: -1185.071411\n",
      "    epoch          : 1767\n",
      "    loss           : -1183.1663572991913\n",
      "    ess            : 1.9661731234746962\n",
      "    log_marginal   : 1183.1989163346027\n",
      "    log_joint      : 1391.6082248400205\n",
      "    val_loss       : -1180.8626337466033\n",
      "    val_ess        : 1.962043663729792\n",
      "    val_log_marginal: 1180.9019403872283\n",
      "    val_log_joint  : 1389.1909551205842\n",
      "Train Epoch: 1768 [0/101520 (0%)] Loss: -1189.114990\n",
      "Train Epoch: 1768 [11264/101520 (11%)] Loss: -1182.843628\n",
      "Train Epoch: 1768 [22528/101520 (22%)] Loss: -1180.091797\n",
      "Train Epoch: 1768 [33792/101520 (33%)] Loss: -1176.400146\n",
      "Train Epoch: 1768 [45056/101520 (44%)] Loss: -1179.784180\n",
      "Train Epoch: 1768 [56320/101520 (55%)] Loss: -1180.395752\n",
      "Train Epoch: 1768 [67584/101520 (67%)] Loss: -1181.964722\n",
      "Train Epoch: 1768 [78848/101520 (78%)] Loss: -1194.459351\n",
      "Train Epoch: 1768 [90112/101520 (89%)] Loss: -1179.608032\n",
      "Train Epoch: 1768 [101376/101520 (100%)] Loss: -1180.843750\n",
      "    epoch          : 1768\n",
      "    loss           : -1183.057447883951\n",
      "    ess            : 1.9664284092697066\n",
      "    log_marginal   : 1183.089152427175\n",
      "    log_joint      : 1391.6356237976995\n",
      "    val_loss       : -1183.1266346807065\n",
      "    val_ess        : 1.958081110664036\n",
      "    val_log_marginal: 1183.1711266559103\n",
      "    val_log_joint  : 1391.48413616678\n",
      "Train Epoch: 1769 [0/101520 (0%)] Loss: -1186.389160\n",
      "Train Epoch: 1769 [11264/101520 (11%)] Loss: -1184.402588\n",
      "Train Epoch: 1769 [22528/101520 (22%)] Loss: -1184.879639\n",
      "Train Epoch: 1769 [33792/101520 (33%)] Loss: -1177.750977\n",
      "Train Epoch: 1769 [45056/101520 (44%)] Loss: -1186.174805\n",
      "Train Epoch: 1769 [56320/101520 (55%)] Loss: -1183.553223\n",
      "Train Epoch: 1769 [67584/101520 (67%)] Loss: -1185.253296\n",
      "Train Epoch: 1769 [78848/101520 (78%)] Loss: -1182.161499\n",
      "Train Epoch: 1769 [90112/101520 (89%)] Loss: -1178.218506\n",
      "Train Epoch: 1769 [101376/101520 (100%)] Loss: -1176.679199\n",
      "    epoch          : 1769\n",
      "    loss           : -1183.0836721449043\n",
      "    ess            : 1.9654700714140083\n",
      "    log_marginal   : 1183.1165378896435\n",
      "    log_joint      : 1391.5925618080637\n",
      "    val_loss       : -1181.8463081691575\n",
      "    val_ess        : 1.9640296127485193\n",
      "    val_log_marginal: 1181.88257366678\n",
      "    val_log_joint  : 1390.5064325747283\n",
      "Train Epoch: 1770 [0/101520 (0%)] Loss: -1183.627197\n",
      "Train Epoch: 1770 [11264/101520 (11%)] Loss: -1179.491455\n",
      "Train Epoch: 1770 [22528/101520 (22%)] Loss: -1180.996582\n",
      "Train Epoch: 1770 [33792/101520 (33%)] Loss: -1187.141357\n",
      "Train Epoch: 1770 [45056/101520 (44%)] Loss: -1179.269775\n",
      "Train Epoch: 1770 [56320/101520 (55%)] Loss: -1182.399658\n",
      "Train Epoch: 1770 [67584/101520 (67%)] Loss: -1184.859497\n",
      "Train Epoch: 1770 [78848/101520 (78%)] Loss: -1186.326416\n",
      "Train Epoch: 1770 [90112/101520 (89%)] Loss: -1179.116577\n",
      "Train Epoch: 1770 [101376/101520 (100%)] Loss: -1178.791138\n",
      "    epoch          : 1770\n",
      "    loss           : -1183.0407487878847\n",
      "    ess            : 1.9654623389843122\n",
      "    log_marginal   : 1183.0746591845948\n",
      "    log_joint      : 1391.5593145169205\n",
      "    val_loss       : -1181.3923180621603\n",
      "    val_ess        : 1.9671335168506787\n",
      "    val_log_marginal: 1181.4268161939538\n",
      "    val_log_joint  : 1389.7923159391983\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1770.pth ...\n",
      "Train Epoch: 1771 [0/101520 (0%)] Loss: -1182.243164\n",
      "Train Epoch: 1771 [11264/101520 (11%)] Loss: -1182.686523\n",
      "Train Epoch: 1771 [22528/101520 (22%)] Loss: -1179.908325\n",
      "Train Epoch: 1771 [33792/101520 (33%)] Loss: -1189.135254\n",
      "Train Epoch: 1771 [45056/101520 (44%)] Loss: -1185.095093\n",
      "Train Epoch: 1771 [56320/101520 (55%)] Loss: -1186.934692\n",
      "Train Epoch: 1771 [67584/101520 (67%)] Loss: -1177.555420\n",
      "Train Epoch: 1771 [78848/101520 (78%)] Loss: -1186.469238\n",
      "Train Epoch: 1771 [90112/101520 (89%)] Loss: -1186.660645\n",
      "Train Epoch: 1771 [101376/101520 (100%)] Loss: -1191.045898\n",
      "    epoch          : 1771\n",
      "    loss           : -1183.2798176674387\n",
      "    ess            : 1.9664156898182241\n",
      "    log_marginal   : 1183.3126576485945\n",
      "    log_joint      : 1391.733423587665\n",
      "    val_loss       : -1182.7471446161685\n",
      "    val_ess        : 1.9635397973267927\n",
      "    val_log_marginal: 1182.7843389096467\n",
      "    val_log_joint  : 1391.2586669921875\n",
      "Train Epoch: 1772 [0/101520 (0%)] Loss: -1185.936523\n",
      "Train Epoch: 1772 [11264/101520 (11%)] Loss: -1190.688843\n",
      "Train Epoch: 1772 [22528/101520 (22%)] Loss: -1187.865601\n",
      "Train Epoch: 1772 [33792/101520 (33%)] Loss: -1177.181641\n",
      "Train Epoch: 1772 [45056/101520 (44%)] Loss: -1188.157959\n",
      "Train Epoch: 1772 [56320/101520 (55%)] Loss: -1177.901367\n",
      "Train Epoch: 1772 [67584/101520 (67%)] Loss: -1179.659180\n",
      "Train Epoch: 1772 [78848/101520 (78%)] Loss: -1183.211182\n",
      "Train Epoch: 1772 [90112/101520 (89%)] Loss: -1180.484375\n",
      "Train Epoch: 1772 [101376/101520 (100%)] Loss: -1190.362183\n",
      "    epoch          : 1772\n",
      "    loss           : -1183.2701820871937\n",
      "    ess            : 1.9668254169387434\n",
      "    log_marginal   : 1183.301606298092\n",
      "    log_joint      : 1391.7502551821608\n",
      "    val_loss       : -1181.1484640370245\n",
      "    val_ess        : 1.9642810977023581\n",
      "    val_log_marginal: 1181.1885827105978\n",
      "    val_log_joint  : 1389.900687839674\n",
      "Train Epoch: 1773 [0/101520 (0%)] Loss: -1186.518555\n",
      "Train Epoch: 1773 [11264/101520 (11%)] Loss: -1184.385498\n",
      "Train Epoch: 1773 [22528/101520 (22%)] Loss: -1180.349243\n",
      "Train Epoch: 1773 [33792/101520 (33%)] Loss: -1188.821045\n",
      "Train Epoch: 1773 [45056/101520 (44%)] Loss: -1189.452393\n",
      "Train Epoch: 1773 [56320/101520 (55%)] Loss: -1177.571533\n",
      "Train Epoch: 1773 [67584/101520 (67%)] Loss: -1181.380127\n",
      "Train Epoch: 1773 [78848/101520 (78%)] Loss: -1188.041992\n",
      "Train Epoch: 1773 [90112/101520 (89%)] Loss: -1183.870117\n",
      "Train Epoch: 1773 [101376/101520 (100%)] Loss: -1187.307129\n",
      "    epoch          : 1773\n",
      "    loss           : -1183.3281826613536\n",
      "    ess            : 1.9664739831608145\n",
      "    log_marginal   : 1183.3596320224167\n",
      "    log_joint      : 1391.7383290966552\n",
      "    val_loss       : -1180.879203464674\n",
      "    val_ess        : 1.9671610179154768\n",
      "    val_log_marginal: 1180.912650730299\n",
      "    val_log_joint  : 1389.414741847826\n",
      "Train Epoch: 1774 [0/101520 (0%)] Loss: -1187.106445\n",
      "Train Epoch: 1774 [11264/101520 (11%)] Loss: -1187.367432\n",
      "Train Epoch: 1774 [22528/101520 (22%)] Loss: -1183.777588\n",
      "Train Epoch: 1774 [33792/101520 (33%)] Loss: -1179.223633\n",
      "Train Epoch: 1774 [45056/101520 (44%)] Loss: -1185.193726\n",
      "Train Epoch: 1774 [56320/101520 (55%)] Loss: -1178.496094\n",
      "Train Epoch: 1774 [67584/101520 (67%)] Loss: -1185.483154\n",
      "Train Epoch: 1774 [78848/101520 (78%)] Loss: -1180.693359\n",
      "Train Epoch: 1774 [90112/101520 (89%)] Loss: -1175.720093\n",
      "Train Epoch: 1774 [101376/101520 (100%)] Loss: -1179.449585\n",
      "    epoch          : 1774\n",
      "    loss           : -1183.2910168518374\n",
      "    ess            : 1.9664141772380426\n",
      "    log_marginal   : 1183.3233587370446\n",
      "    log_joint      : 1391.7347577732412\n",
      "    val_loss       : -1180.673828125\n",
      "    val_ess        : 1.9640086267305457\n",
      "    val_log_marginal: 1180.7095098080842\n",
      "    val_log_joint  : 1389.2707891049592\n",
      "Train Epoch: 1775 [0/101520 (0%)] Loss: -1184.269287\n",
      "Train Epoch: 1775 [11264/101520 (11%)] Loss: -1184.308350\n",
      "Train Epoch: 1775 [22528/101520 (22%)] Loss: -1190.188477\n",
      "Train Epoch: 1775 [33792/101520 (33%)] Loss: -1182.992920\n",
      "Train Epoch: 1775 [45056/101520 (44%)] Loss: -1179.269043\n",
      "Train Epoch: 1775 [56320/101520 (55%)] Loss: -1185.135864\n",
      "Train Epoch: 1775 [67584/101520 (67%)] Loss: -1183.287354\n",
      "Train Epoch: 1775 [78848/101520 (78%)] Loss: -1181.515015\n",
      "Train Epoch: 1775 [90112/101520 (89%)] Loss: -1177.717651\n",
      "Train Epoch: 1775 [101376/101520 (100%)] Loss: -1174.588135\n",
      "    epoch          : 1775\n",
      "    loss           : -1183.2850924544598\n",
      "    ess            : 1.965995958702049\n",
      "    log_marginal   : 1183.318865445391\n",
      "    log_joint      : 1391.7782534498665\n",
      "    val_loss       : -1180.5356763756793\n",
      "    val_ess        : 1.9659780419391135\n",
      "    val_log_marginal: 1180.5640338400135\n",
      "    val_log_joint  : 1388.9364385190217\n",
      "Train Epoch: 1776 [0/101520 (0%)] Loss: -1186.194092\n",
      "Train Epoch: 1776 [11264/101520 (11%)] Loss: -1177.226196\n",
      "Train Epoch: 1776 [22528/101520 (22%)] Loss: -1187.064575\n",
      "Train Epoch: 1776 [33792/101520 (33%)] Loss: -1183.476685\n",
      "Train Epoch: 1776 [45056/101520 (44%)] Loss: -1181.116821\n",
      "Train Epoch: 1776 [56320/101520 (55%)] Loss: -1192.279663\n",
      "Train Epoch: 1776 [67584/101520 (67%)] Loss: -1189.156982\n",
      "Train Epoch: 1776 [78848/101520 (78%)] Loss: -1177.937622\n",
      "Train Epoch: 1776 [90112/101520 (89%)] Loss: -1180.399414\n",
      "Train Epoch: 1776 [101376/101520 (100%)] Loss: -1195.266968\n",
      "    epoch          : 1776\n",
      "    loss           : -1183.4145765448336\n",
      "    ess            : 1.9662654381900577\n",
      "    log_marginal   : 1183.4470717847048\n",
      "    log_joint      : 1391.8324293587077\n",
      "    val_loss       : -1180.1426418138587\n",
      "    val_ess        : 1.9658861367598823\n",
      "    val_log_marginal: 1180.1744437839675\n",
      "    val_log_joint  : 1388.6326532778533\n",
      "Train Epoch: 1777 [0/101520 (0%)] Loss: -1187.259155\n",
      "Train Epoch: 1777 [11264/101520 (11%)] Loss: -1180.555786\n",
      "Train Epoch: 1777 [22528/101520 (22%)] Loss: -1184.387207\n",
      "Train Epoch: 1777 [33792/101520 (33%)] Loss: -1184.384033\n",
      "Train Epoch: 1777 [45056/101520 (44%)] Loss: -1176.357422\n",
      "Train Epoch: 1777 [56320/101520 (55%)] Loss: -1182.452271\n",
      "Train Epoch: 1777 [67584/101520 (67%)] Loss: -1186.227783\n",
      "Train Epoch: 1777 [78848/101520 (78%)] Loss: -1184.238770\n",
      "Train Epoch: 1777 [90112/101520 (89%)] Loss: -1180.638916\n",
      "Train Epoch: 1777 [101376/101520 (100%)] Loss: -1173.284790\n",
      "    epoch          : 1777\n",
      "    loss           : -1183.2198541535804\n",
      "    ess            : 1.9662537149448491\n",
      "    log_marginal   : 1183.2515678980842\n",
      "    log_joint      : 1391.71694286864\n",
      "    val_loss       : -1182.9522598930027\n",
      "    val_ess        : 1.9674231073130732\n",
      "    val_log_marginal: 1182.982565174932\n",
      "    val_log_joint  : 1391.2641442340353\n",
      "Train Epoch: 1778 [0/101520 (0%)] Loss: -1182.236206\n",
      "Train Epoch: 1778 [11264/101520 (11%)] Loss: -1183.730103\n",
      "Train Epoch: 1778 [22528/101520 (22%)] Loss: -1179.526123\n",
      "Train Epoch: 1778 [33792/101520 (33%)] Loss: -1179.716919\n",
      "Train Epoch: 1778 [45056/101520 (44%)] Loss: -1183.434814\n",
      "Train Epoch: 1778 [56320/101520 (55%)] Loss: -1180.044922\n",
      "Train Epoch: 1778 [67584/101520 (67%)] Loss: -1182.379761\n",
      "Train Epoch: 1778 [78848/101520 (78%)] Loss: -1187.426636\n",
      "Train Epoch: 1778 [90112/101520 (89%)] Loss: -1181.644653\n",
      "Train Epoch: 1778 [101376/101520 (100%)] Loss: -1176.928345\n",
      "    epoch          : 1778\n",
      "    loss           : -1183.284986946451\n",
      "    ess            : 1.966464154684364\n",
      "    log_marginal   : 1183.3178582119583\n",
      "    log_joint      : 1391.7653692044205\n",
      "    val_loss       : -1182.9416663128397\n",
      "    val_ess        : 1.9684771040211553\n",
      "    val_log_marginal: 1182.9722475798233\n",
      "    val_log_joint  : 1391.2495966372283\n",
      "Train Epoch: 1779 [0/101520 (0%)] Loss: -1186.233521\n",
      "Train Epoch: 1779 [11264/101520 (11%)] Loss: -1186.197266\n",
      "Train Epoch: 1779 [22528/101520 (22%)] Loss: -1183.118408\n",
      "Train Epoch: 1779 [33792/101520 (33%)] Loss: -1180.706543\n",
      "Train Epoch: 1779 [45056/101520 (44%)] Loss: -1186.288574\n",
      "Train Epoch: 1779 [56320/101520 (55%)] Loss: -1178.952271\n",
      "Train Epoch: 1779 [67584/101520 (67%)] Loss: -1186.983887\n",
      "Train Epoch: 1779 [78848/101520 (78%)] Loss: -1178.981445\n",
      "Train Epoch: 1779 [90112/101520 (89%)] Loss: -1180.104980\n",
      "Train Epoch: 1779 [101376/101520 (100%)] Loss: -1171.786133\n",
      "    epoch          : 1779\n",
      "    loss           : -1183.3137752973853\n",
      "    ess            : 1.965645434269354\n",
      "    log_marginal   : 1183.3470268824592\n",
      "    log_joint      : 1391.7592190689777\n",
      "    val_loss       : -1182.0514234459918\n",
      "    val_ess        : 1.965407412985097\n",
      "    val_log_marginal: 1182.0856349779212\n",
      "    val_log_joint  : 1390.3952265200408\n",
      "Train Epoch: 1780 [0/101520 (0%)] Loss: -1180.379272\n",
      "Train Epoch: 1780 [11264/101520 (11%)] Loss: -1182.968750\n",
      "Train Epoch: 1780 [22528/101520 (22%)] Loss: -1182.815430\n",
      "Train Epoch: 1780 [33792/101520 (33%)] Loss: -1177.324463\n",
      "Train Epoch: 1780 [45056/101520 (44%)] Loss: -1187.068970\n",
      "Train Epoch: 1780 [56320/101520 (55%)] Loss: -1178.028809\n",
      "Train Epoch: 1780 [67584/101520 (67%)] Loss: -1185.949219\n",
      "Train Epoch: 1780 [78848/101520 (78%)] Loss: -1187.377930\n",
      "Train Epoch: 1780 [90112/101520 (89%)] Loss: -1183.590576\n",
      "Train Epoch: 1780 [101376/101520 (100%)] Loss: -1174.708740\n",
      "    epoch          : 1780\n",
      "    loss           : -1183.335472528659\n",
      "    ess            : 1.965834429515666\n",
      "    log_marginal   : 1183.3682984011857\n",
      "    log_joint      : 1391.8057824523005\n",
      "    val_loss       : -1180.4844599184783\n",
      "    val_ess        : 1.9675619913184124\n",
      "    val_log_marginal: 1180.5158426036005\n",
      "    val_log_joint  : 1388.9608950407608\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1780.pth ...\n",
      "Train Epoch: 1781 [0/101520 (0%)] Loss: -1189.433838\n",
      "Train Epoch: 1781 [11264/101520 (11%)] Loss: -1190.768799\n",
      "Train Epoch: 1781 [22528/101520 (22%)] Loss: -1180.345337\n",
      "Train Epoch: 1781 [33792/101520 (33%)] Loss: -1179.314941\n",
      "Train Epoch: 1781 [45056/101520 (44%)] Loss: -1177.633301\n",
      "Train Epoch: 1781 [56320/101520 (55%)] Loss: -1185.811768\n",
      "Train Epoch: 1781 [67584/101520 (67%)] Loss: -1187.499390\n",
      "Train Epoch: 1781 [78848/101520 (78%)] Loss: -1186.070923\n",
      "Train Epoch: 1781 [90112/101520 (89%)] Loss: -1185.538086\n",
      "Train Epoch: 1781 [101376/101520 (100%)] Loss: -1171.962524\n",
      "    epoch          : 1781\n",
      "    loss           : -1183.3805140202967\n",
      "    ess            : 1.9661615434004434\n",
      "    log_marginal   : 1183.4122934005968\n",
      "    log_joint      : 1391.8460300771435\n",
      "    val_loss       : -1183.1991762907608\n",
      "    val_ess        : 1.9649302078329998\n",
      "    val_log_marginal: 1183.2340884001358\n",
      "    val_log_joint  : 1391.6004585597825\n",
      "Train Epoch: 1782 [0/101520 (0%)] Loss: -1185.781982\n",
      "Train Epoch: 1782 [11264/101520 (11%)] Loss: -1185.003906\n",
      "Train Epoch: 1782 [22528/101520 (22%)] Loss: -1183.432983\n",
      "Train Epoch: 1782 [33792/101520 (33%)] Loss: -1179.415527\n",
      "Train Epoch: 1782 [45056/101520 (44%)] Loss: -1178.262939\n",
      "Train Epoch: 1782 [56320/101520 (55%)] Loss: -1186.385498\n",
      "Train Epoch: 1782 [67584/101520 (67%)] Loss: -1182.686646\n",
      "Train Epoch: 1782 [78848/101520 (78%)] Loss: -1179.662109\n",
      "Train Epoch: 1782 [90112/101520 (89%)] Loss: -1179.393188\n",
      "Train Epoch: 1782 [101376/101520 (100%)] Loss: -1179.782959\n",
      "    epoch          : 1782\n",
      "    loss           : -1183.3882308269865\n",
      "    ess            : 1.9657673488310234\n",
      "    log_marginal   : 1183.421258514251\n",
      "    log_joint      : 1391.8902716708542\n",
      "    val_loss       : -1181.9752197265625\n",
      "    val_ess        : 1.9669785655063132\n",
      "    val_log_marginal: 1182.0047023607337\n",
      "    val_log_joint  : 1390.3563073199728\n",
      "Train Epoch: 1783 [0/101520 (0%)] Loss: -1188.333862\n",
      "Train Epoch: 1783 [11264/101520 (11%)] Loss: -1184.764404\n",
      "Train Epoch: 1783 [22528/101520 (22%)] Loss: -1185.682739\n",
      "Train Epoch: 1783 [33792/101520 (33%)] Loss: -1188.410645\n",
      "Train Epoch: 1783 [45056/101520 (44%)] Loss: -1182.441772\n",
      "Train Epoch: 1783 [56320/101520 (55%)] Loss: -1178.249756\n",
      "Train Epoch: 1783 [67584/101520 (67%)] Loss: -1182.636597\n",
      "Train Epoch: 1783 [78848/101520 (78%)] Loss: -1180.231812\n",
      "Train Epoch: 1783 [90112/101520 (89%)] Loss: -1193.630127\n",
      "Train Epoch: 1783 [101376/101520 (100%)] Loss: -1182.116089\n",
      "    epoch          : 1783\n",
      "    loss           : -1183.4610902412453\n",
      "    ess            : 1.9668224391026712\n",
      "    log_marginal   : 1183.492226758794\n",
      "    log_joint      : 1391.9502302773633\n",
      "    val_loss       : -1181.813083814538\n",
      "    val_ess        : 1.9684681322263635\n",
      "    val_log_marginal: 1181.8433731742527\n",
      "    val_log_joint  : 1390.1951426630435\n",
      "Train Epoch: 1784 [0/101520 (0%)] Loss: -1186.838745\n",
      "Train Epoch: 1784 [11264/101520 (11%)] Loss: -1181.245239\n",
      "Train Epoch: 1784 [22528/101520 (22%)] Loss: -1186.045532\n",
      "Train Epoch: 1784 [33792/101520 (33%)] Loss: -1188.087158\n",
      "Train Epoch: 1784 [45056/101520 (44%)] Loss: -1181.497437\n",
      "Train Epoch: 1784 [56320/101520 (55%)] Loss: -1185.866577\n",
      "Train Epoch: 1784 [67584/101520 (67%)] Loss: -1185.040771\n",
      "Train Epoch: 1784 [78848/101520 (78%)] Loss: -1181.083008\n",
      "Train Epoch: 1784 [90112/101520 (89%)] Loss: -1182.347168\n",
      "Train Epoch: 1784 [101376/101520 (100%)] Loss: -1187.580566\n",
      "    epoch          : 1784\n",
      "    loss           : -1183.5050097901617\n",
      "    ess            : 1.9662182612634784\n",
      "    log_marginal   : 1183.537688442211\n",
      "    log_joint      : 1392.0290840187265\n",
      "    val_loss       : -1183.2613896908967\n",
      "    val_ess        : 1.9689745540204255\n",
      "    val_log_marginal: 1183.2907874065897\n",
      "    val_log_joint  : 1391.45873492697\n",
      "Train Epoch: 1785 [0/101520 (0%)] Loss: -1188.561035\n",
      "Train Epoch: 1785 [11264/101520 (11%)] Loss: -1177.271362\n",
      "Train Epoch: 1785 [22528/101520 (22%)] Loss: -1194.218018\n",
      "Train Epoch: 1785 [33792/101520 (33%)] Loss: -1183.458252\n",
      "Train Epoch: 1785 [45056/101520 (44%)] Loss: -1183.394165\n",
      "Train Epoch: 1785 [56320/101520 (55%)] Loss: -1180.628296\n",
      "Train Epoch: 1785 [67584/101520 (67%)] Loss: -1178.853394\n",
      "Train Epoch: 1785 [78848/101520 (78%)] Loss: -1173.779541\n",
      "Train Epoch: 1785 [90112/101520 (89%)] Loss: -1184.954102\n",
      "Train Epoch: 1785 [101376/101520 (100%)] Loss: -1192.048706\n",
      "    epoch          : 1785\n",
      "    loss           : -1183.586577295658\n",
      "    ess            : 1.9666348545994592\n",
      "    log_marginal   : 1183.6184946951555\n",
      "    log_joint      : 1392.043923842847\n",
      "    val_loss       : -1182.1235510784647\n",
      "    val_ess        : 1.9636680665223494\n",
      "    val_log_marginal: 1182.1568497367527\n",
      "    val_log_joint  : 1390.4922458814538\n",
      "Train Epoch: 1786 [0/101520 (0%)] Loss: -1188.503052\n",
      "Train Epoch: 1786 [11264/101520 (11%)] Loss: -1188.802002\n",
      "Train Epoch: 1786 [22528/101520 (22%)] Loss: -1183.217041\n",
      "Train Epoch: 1786 [33792/101520 (33%)] Loss: -1188.302734\n",
      "Train Epoch: 1786 [45056/101520 (44%)] Loss: -1177.968018\n",
      "Train Epoch: 1786 [56320/101520 (55%)] Loss: -1184.526123\n",
      "Train Epoch: 1786 [67584/101520 (67%)] Loss: -1183.664795\n",
      "Train Epoch: 1786 [78848/101520 (78%)] Loss: -1178.317627\n",
      "Train Epoch: 1786 [90112/101520 (89%)] Loss: -1183.408203\n",
      "Train Epoch: 1786 [101376/101520 (100%)] Loss: -1179.824829\n",
      "    epoch          : 1786\n",
      "    loss           : -1183.577071760168\n",
      "    ess            : 1.966276383280155\n",
      "    log_marginal   : 1183.6089443801036\n",
      "    log_joint      : 1392.0577159479035\n",
      "    val_loss       : -1183.1575980808425\n",
      "    val_ess        : 1.9635268190632695\n",
      "    val_log_marginal: 1183.1947923743207\n",
      "    val_log_joint  : 1391.5046652088995\n",
      "Train Epoch: 1787 [0/101520 (0%)] Loss: -1175.370850\n",
      "Train Epoch: 1787 [11264/101520 (11%)] Loss: -1182.544678\n",
      "Train Epoch: 1787 [22528/101520 (22%)] Loss: -1184.469971\n",
      "Train Epoch: 1787 [33792/101520 (33%)] Loss: -1176.238525\n",
      "Train Epoch: 1787 [45056/101520 (44%)] Loss: -1187.466797\n",
      "Train Epoch: 1787 [56320/101520 (55%)] Loss: -1185.804443\n",
      "Train Epoch: 1787 [67584/101520 (67%)] Loss: -1184.518555\n",
      "Train Epoch: 1787 [78848/101520 (78%)] Loss: -1176.226318\n",
      "Train Epoch: 1787 [90112/101520 (89%)] Loss: -1180.142334\n",
      "Train Epoch: 1787 [101376/101520 (100%)] Loss: -1185.927368\n",
      "    epoch          : 1787\n",
      "    loss           : -1183.5511775184516\n",
      "    ess            : 1.9660105471634985\n",
      "    log_marginal   : 1183.584499033252\n",
      "    log_joint      : 1392.0141736514604\n",
      "    val_loss       : -1182.4123747452445\n",
      "    val_ess        : 1.9688099933707195\n",
      "    val_log_marginal: 1182.4423032014267\n",
      "    val_log_joint  : 1390.725193189538\n",
      "Train Epoch: 1788 [0/101520 (0%)] Loss: -1184.817139\n",
      "Train Epoch: 1788 [11264/101520 (11%)] Loss: -1188.740967\n",
      "Train Epoch: 1788 [22528/101520 (22%)] Loss: -1177.021973\n",
      "Train Epoch: 1788 [33792/101520 (33%)] Loss: -1177.334229\n",
      "Train Epoch: 1788 [45056/101520 (44%)] Loss: -1182.364258\n",
      "Train Epoch: 1788 [56320/101520 (55%)] Loss: -1183.883667\n",
      "Train Epoch: 1788 [67584/101520 (67%)] Loss: -1183.966797\n",
      "Train Epoch: 1788 [78848/101520 (78%)] Loss: -1184.899902\n",
      "Train Epoch: 1788 [90112/101520 (89%)] Loss: -1174.368164\n",
      "Train Epoch: 1788 [101376/101520 (100%)] Loss: -1192.058838\n",
      "    epoch          : 1788\n",
      "    loss           : -1183.6491785097362\n",
      "    ess            : 1.966093761837063\n",
      "    log_marginal   : 1183.6818498007617\n",
      "    log_joint      : 1392.102781362869\n",
      "    val_loss       : -1181.4089143172555\n",
      "    val_ess        : 1.9645921095557835\n",
      "    val_log_marginal: 1181.4471117102582\n",
      "    val_log_joint  : 1389.970220151155\n",
      "Train Epoch: 1789 [0/101520 (0%)] Loss: -1178.334717\n",
      "Train Epoch: 1789 [11264/101520 (11%)] Loss: -1189.500488\n",
      "Train Epoch: 1789 [22528/101520 (22%)] Loss: -1181.815430\n",
      "Train Epoch: 1789 [33792/101520 (33%)] Loss: -1181.013428\n",
      "Train Epoch: 1789 [45056/101520 (44%)] Loss: -1188.408447\n",
      "Train Epoch: 1789 [56320/101520 (55%)] Loss: -1180.054565\n",
      "Train Epoch: 1789 [67584/101520 (67%)] Loss: -1185.292969\n",
      "Train Epoch: 1789 [78848/101520 (78%)] Loss: -1185.213135\n",
      "Train Epoch: 1789 [90112/101520 (89%)] Loss: -1178.786377\n",
      "Train Epoch: 1789 [101376/101520 (100%)] Loss: -1181.025879\n",
      "    epoch          : 1789\n",
      "    loss           : -1183.6416199650596\n",
      "    ess            : 1.9655911377326927\n",
      "    log_marginal   : 1183.6753800741992\n",
      "    log_joint      : 1392.1019894393844\n",
      "    val_loss       : -1182.865229067595\n",
      "    val_ess        : 1.965737399847611\n",
      "    val_log_marginal: 1182.897609544837\n",
      "    val_log_joint  : 1391.1790453040082\n",
      "Train Epoch: 1790 [0/101520 (0%)] Loss: -1187.062744\n",
      "Train Epoch: 1790 [11264/101520 (11%)] Loss: -1190.631836\n",
      "Train Epoch: 1790 [22528/101520 (22%)] Loss: -1182.712891\n",
      "Train Epoch: 1790 [33792/101520 (33%)] Loss: -1179.691406\n",
      "Train Epoch: 1790 [45056/101520 (44%)] Loss: -1188.347168\n",
      "Train Epoch: 1790 [56320/101520 (55%)] Loss: -1186.793335\n",
      "Train Epoch: 1790 [67584/101520 (67%)] Loss: -1190.568481\n",
      "Train Epoch: 1790 [78848/101520 (78%)] Loss: -1187.552246\n",
      "Train Epoch: 1790 [90112/101520 (89%)] Loss: -1190.451050\n",
      "Train Epoch: 1790 [101376/101520 (100%)] Loss: -1202.119751\n",
      "    epoch          : 1790\n",
      "    loss           : -1183.6853659164965\n",
      "    ess            : 1.9662480210539084\n",
      "    log_marginal   : 1183.717023226484\n",
      "    log_joint      : 1392.1278720261464\n",
      "    val_loss       : -1180.46850055197\n",
      "    val_ess        : 1.9675926695699277\n",
      "    val_log_marginal: 1180.5015869140625\n",
      "    val_log_joint  : 1388.577827785326\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1790.pth ...\n",
      "Train Epoch: 1791 [0/101520 (0%)] Loss: -1179.658813\n",
      "Train Epoch: 1791 [11264/101520 (11%)] Loss: -1185.620850\n",
      "Train Epoch: 1791 [22528/101520 (22%)] Loss: -1183.199219\n",
      "Train Epoch: 1791 [33792/101520 (33%)] Loss: -1188.144775\n",
      "Train Epoch: 1791 [45056/101520 (44%)] Loss: -1188.767090\n",
      "Train Epoch: 1791 [56320/101520 (55%)] Loss: -1186.604370\n",
      "Train Epoch: 1791 [67584/101520 (67%)] Loss: -1180.109619\n",
      "Train Epoch: 1791 [78848/101520 (78%)] Loss: -1189.076050\n",
      "Train Epoch: 1791 [90112/101520 (89%)] Loss: -1179.476196\n",
      "Train Epoch: 1791 [101376/101520 (100%)] Loss: -1180.966553\n",
      "    epoch          : 1791\n",
      "    loss           : -1183.6028512925957\n",
      "    ess            : 1.9659057615989417\n",
      "    log_marginal   : 1183.6355262641332\n",
      "    log_joint      : 1392.112188751374\n",
      "    val_loss       : -1181.495552394701\n",
      "    val_ess        : 1.9655722535174827\n",
      "    val_log_marginal: 1181.5280655570652\n",
      "    val_log_joint  : 1389.8940111243207\n",
      "Train Epoch: 1792 [0/101520 (0%)] Loss: -1178.262695\n",
      "Train Epoch: 1792 [11264/101520 (11%)] Loss: -1184.131592\n",
      "Train Epoch: 1792 [22528/101520 (22%)] Loss: -1188.733154\n",
      "Train Epoch: 1792 [33792/101520 (33%)] Loss: -1188.157227\n",
      "Train Epoch: 1792 [45056/101520 (44%)] Loss: -1192.634766\n",
      "Train Epoch: 1792 [56320/101520 (55%)] Loss: -1176.122314\n",
      "Train Epoch: 1792 [67584/101520 (67%)] Loss: -1189.873413\n",
      "Train Epoch: 1792 [78848/101520 (78%)] Loss: -1180.316406\n",
      "Train Epoch: 1792 [90112/101520 (89%)] Loss: -1183.614746\n",
      "Train Epoch: 1792 [101376/101520 (100%)] Loss: -1181.989746\n",
      "    epoch          : 1792\n",
      "    loss           : -1183.7201369395807\n",
      "    ess            : 1.9669172859671127\n",
      "    log_marginal   : 1183.7511642686086\n",
      "    log_joint      : 1392.1170169696136\n",
      "    val_loss       : -1181.4219811480978\n",
      "    val_ess        : 1.9654909579650215\n",
      "    val_log_marginal: 1181.4530135444973\n",
      "    val_log_joint  : 1389.7683795431385\n",
      "Train Epoch: 1793 [0/101520 (0%)] Loss: -1176.198486\n",
      "Train Epoch: 1793 [11264/101520 (11%)] Loss: -1187.084473\n",
      "Train Epoch: 1793 [22528/101520 (22%)] Loss: -1185.464355\n",
      "Train Epoch: 1793 [33792/101520 (33%)] Loss: -1178.110962\n",
      "Train Epoch: 1793 [45056/101520 (44%)] Loss: -1182.832397\n",
      "Train Epoch: 1793 [56320/101520 (55%)] Loss: -1183.321045\n",
      "Train Epoch: 1793 [67584/101520 (67%)] Loss: -1187.348022\n",
      "Train Epoch: 1793 [78848/101520 (78%)] Loss: -1184.421875\n",
      "Train Epoch: 1793 [90112/101520 (89%)] Loss: -1179.236572\n",
      "Train Epoch: 1793 [101376/101520 (100%)] Loss: -1183.635010\n",
      "    epoch          : 1793\n",
      "    loss           : -1183.6734410578283\n",
      "    ess            : 1.9655016570834059\n",
      "    log_marginal   : 1183.7071459592887\n",
      "    log_joint      : 1392.1770866048994\n",
      "    val_loss       : -1182.7693561056385\n",
      "    val_ess        : 1.9685222065967063\n",
      "    val_log_marginal: 1182.7998896059783\n",
      "    val_log_joint  : 1391.2176036005435\n",
      "Train Epoch: 1794 [0/101520 (0%)] Loss: -1187.543701\n",
      "Train Epoch: 1794 [11264/101520 (11%)] Loss: -1186.570801\n",
      "Train Epoch: 1794 [22528/101520 (22%)] Loss: -1182.463379\n",
      "Train Epoch: 1794 [33792/101520 (33%)] Loss: -1188.816162\n",
      "Train Epoch: 1794 [45056/101520 (44%)] Loss: -1184.777588\n",
      "Train Epoch: 1794 [56320/101520 (55%)] Loss: -1188.711670\n",
      "Train Epoch: 1794 [67584/101520 (67%)] Loss: -1177.290161\n",
      "Train Epoch: 1794 [78848/101520 (78%)] Loss: -1177.072632\n",
      "Train Epoch: 1794 [90112/101520 (89%)] Loss: -1182.667480\n",
      "Train Epoch: 1794 [101376/101520 (100%)] Loss: -1182.796631\n",
      "    epoch          : 1794\n",
      "    loss           : -1183.7826449140232\n",
      "    ess            : 1.9665789065049522\n",
      "    log_marginal   : 1183.8146764093908\n",
      "    log_joint      : 1392.2158282869425\n",
      "    val_loss       : -1181.4777619735055\n",
      "    val_ess        : 1.9642973308977874\n",
      "    val_log_marginal: 1181.509908924932\n",
      "    val_log_joint  : 1389.9133619225543\n",
      "Train Epoch: 1795 [0/101520 (0%)] Loss: -1181.752686\n",
      "Train Epoch: 1795 [11264/101520 (11%)] Loss: -1185.006958\n",
      "Train Epoch: 1795 [22528/101520 (22%)] Loss: -1186.172119\n",
      "Train Epoch: 1795 [33792/101520 (33%)] Loss: -1192.769653\n",
      "Train Epoch: 1795 [45056/101520 (44%)] Loss: -1183.980225\n",
      "Train Epoch: 1795 [56320/101520 (55%)] Loss: -1179.685059\n",
      "Train Epoch: 1795 [67584/101520 (67%)] Loss: -1181.760132\n",
      "Train Epoch: 1795 [78848/101520 (78%)] Loss: -1180.583740\n",
      "Train Epoch: 1795 [90112/101520 (89%)] Loss: -1181.326660\n",
      "Train Epoch: 1795 [101376/101520 (100%)] Loss: -1187.318970\n",
      "    epoch          : 1795\n",
      "    loss           : -1183.756850046129\n",
      "    ess            : 1.9669598137313997\n",
      "    log_marginal   : 1183.7894489537532\n",
      "    log_joint      : 1392.2499441789023\n",
      "    val_loss       : -1182.773580799932\n",
      "    val_ess        : 1.9652911735617595\n",
      "    val_log_marginal: 1182.808301842731\n",
      "    val_log_joint  : 1391.409763502038\n",
      "Train Epoch: 1796 [0/101520 (0%)] Loss: -1185.665894\n",
      "Train Epoch: 1796 [11264/101520 (11%)] Loss: -1186.141724\n",
      "Train Epoch: 1796 [22528/101520 (22%)] Loss: -1184.003418\n",
      "Train Epoch: 1796 [33792/101520 (33%)] Loss: -1185.004517\n",
      "Train Epoch: 1796 [45056/101520 (44%)] Loss: -1185.907227\n",
      "Train Epoch: 1796 [56320/101520 (55%)] Loss: -1185.598389\n",
      "Train Epoch: 1796 [67584/101520 (67%)] Loss: -1184.065063\n",
      "Train Epoch: 1796 [78848/101520 (78%)] Loss: -1188.661133\n",
      "Train Epoch: 1796 [90112/101520 (89%)] Loss: -1192.433228\n",
      "Train Epoch: 1796 [101376/101520 (100%)] Loss: -1181.457886\n",
      "    epoch          : 1796\n",
      "    loss           : -1183.7483032839982\n",
      "    ess            : 1.9663907187667924\n",
      "    log_marginal   : 1183.7808034312186\n",
      "    log_joint      : 1392.2493375078518\n",
      "    val_loss       : -1184.218744692595\n",
      "    val_ess        : 1.9659156540165776\n",
      "    val_log_marginal: 1184.254150390625\n",
      "    val_log_joint  : 1392.5571872877038\n",
      "Train Epoch: 1797 [0/101520 (0%)] Loss: -1187.577759\n",
      "Train Epoch: 1797 [11264/101520 (11%)] Loss: -1179.617065\n",
      "Train Epoch: 1797 [22528/101520 (22%)] Loss: -1184.778931\n",
      "Train Epoch: 1797 [33792/101520 (33%)] Loss: -1192.199463\n",
      "Train Epoch: 1797 [45056/101520 (44%)] Loss: -1183.573853\n",
      "Train Epoch: 1797 [56320/101520 (55%)] Loss: -1195.761108\n",
      "Train Epoch: 1797 [67584/101520 (67%)] Loss: -1184.202637\n",
      "Train Epoch: 1797 [78848/101520 (78%)] Loss: -1186.567261\n",
      "Train Epoch: 1797 [90112/101520 (89%)] Loss: -1177.022461\n",
      "Train Epoch: 1797 [101376/101520 (100%)] Loss: -1183.797363\n",
      "    epoch          : 1797\n",
      "    loss           : -1183.7841625117776\n",
      "    ess            : 1.9664345626255975\n",
      "    log_marginal   : 1183.8160063010364\n",
      "    log_joint      : 1392.3063259412295\n",
      "    val_loss       : -1183.18237835428\n",
      "    val_ess        : 1.9652089502500452\n",
      "    val_log_marginal: 1183.2157619310462\n",
      "    val_log_joint  : 1391.4338750424592\n",
      "Train Epoch: 1798 [0/101520 (0%)] Loss: -1178.656982\n",
      "Train Epoch: 1798 [11264/101520 (11%)] Loss: -1178.044189\n",
      "Train Epoch: 1798 [22528/101520 (22%)] Loss: -1180.666748\n",
      "Train Epoch: 1798 [33792/101520 (33%)] Loss: -1185.014648\n",
      "Train Epoch: 1798 [45056/101520 (44%)] Loss: -1187.906250\n",
      "Train Epoch: 1798 [56320/101520 (55%)] Loss: -1183.980835\n",
      "Train Epoch: 1798 [67584/101520 (67%)] Loss: -1178.447998\n",
      "Train Epoch: 1798 [78848/101520 (78%)] Loss: -1186.063232\n",
      "Train Epoch: 1798 [90112/101520 (89%)] Loss: -1190.983643\n",
      "Train Epoch: 1798 [101376/101520 (100%)] Loss: -1175.018188\n",
      "    epoch          : 1798\n",
      "    loss           : -1183.844033399419\n",
      "    ess            : 1.966552560053878\n",
      "    log_marginal   : 1183.8756465432632\n",
      "    log_joint      : 1392.2592626217022\n",
      "    val_loss       : -1182.9285304857337\n",
      "    val_ess        : 1.9668287453444109\n",
      "    val_log_marginal: 1182.9616115404212\n",
      "    val_log_joint  : 1391.3352156929348\n",
      "Train Epoch: 1799 [0/101520 (0%)] Loss: -1180.189453\n",
      "Train Epoch: 1799 [11264/101520 (11%)] Loss: -1178.240967\n",
      "Train Epoch: 1799 [22528/101520 (22%)] Loss: -1182.380005\n",
      "Train Epoch: 1799 [33792/101520 (33%)] Loss: -1185.148926\n",
      "Train Epoch: 1799 [45056/101520 (44%)] Loss: -1183.741211\n",
      "Train Epoch: 1799 [56320/101520 (55%)] Loss: -1181.874023\n",
      "Train Epoch: 1799 [67584/101520 (67%)] Loss: -1178.886475\n",
      "Train Epoch: 1799 [78848/101520 (78%)] Loss: -1185.607422\n",
      "Train Epoch: 1799 [90112/101520 (89%)] Loss: -1181.282593\n",
      "Train Epoch: 1799 [101376/101520 (100%)] Loss: -1177.117188\n",
      "    epoch          : 1799\n",
      "    loss           : -1183.7481585171954\n",
      "    ess            : 1.9663982163721592\n",
      "    log_marginal   : 1183.7803924407192\n",
      "    log_joint      : 1392.3170037197708\n",
      "    val_loss       : -1183.2239034901495\n",
      "    val_ess        : 1.9624569001405134\n",
      "    val_log_marginal: 1183.2676524286685\n",
      "    val_log_joint  : 1391.8429591966712\n",
      "Train Epoch: 1800 [0/101520 (0%)] Loss: -1189.582764\n",
      "Train Epoch: 1800 [11264/101520 (11%)] Loss: -1185.013062\n",
      "Train Epoch: 1800 [22528/101520 (22%)] Loss: -1188.002441\n",
      "Train Epoch: 1800 [33792/101520 (33%)] Loss: -1188.134888\n",
      "Train Epoch: 1800 [45056/101520 (44%)] Loss: -1185.187744\n",
      "Train Epoch: 1800 [56320/101520 (55%)] Loss: -1176.683228\n",
      "Train Epoch: 1800 [67584/101520 (67%)] Loss: -1182.416748\n",
      "Train Epoch: 1800 [78848/101520 (78%)] Loss: -1183.170410\n",
      "Train Epoch: 1800 [90112/101520 (89%)] Loss: -1189.440063\n",
      "Train Epoch: 1800 [101376/101520 (100%)] Loss: -1186.234375\n",
      "    epoch          : 1800\n",
      "    loss           : -1183.9189391783134\n",
      "    ess            : 1.9665186033776059\n",
      "    log_marginal   : 1183.9520742138427\n",
      "    log_joint      : 1392.3629095182946\n",
      "    val_loss       : -1184.497606360394\n",
      "    val_ess        : 1.9657828496850056\n",
      "    val_log_marginal: 1184.5293764860733\n",
      "    val_log_joint  : 1392.9957699983017\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1800.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 1801 [0/101520 (0%)] Loss: -1186.020020\n",
      "Train Epoch: 1801 [11264/101520 (11%)] Loss: -1182.095215\n",
      "Train Epoch: 1801 [22528/101520 (22%)] Loss: -1183.950195\n",
      "Train Epoch: 1801 [33792/101520 (33%)] Loss: -1187.442505\n",
      "Train Epoch: 1801 [45056/101520 (44%)] Loss: -1181.309326\n",
      "Train Epoch: 1801 [56320/101520 (55%)] Loss: -1184.534668\n",
      "Train Epoch: 1801 [67584/101520 (67%)] Loss: -1184.401611\n",
      "Train Epoch: 1801 [78848/101520 (78%)] Loss: -1176.170898\n",
      "Train Epoch: 1801 [90112/101520 (89%)] Loss: -1182.730957\n",
      "Train Epoch: 1801 [101376/101520 (100%)] Loss: -1179.071411\n",
      "    epoch          : 1801\n",
      "    loss           : -1183.85531355508\n",
      "    ess            : 1.966086915989018\n",
      "    log_marginal   : 1183.888254750314\n",
      "    log_joint      : 1392.319243311283\n",
      "    val_loss       : -1183.255519701087\n",
      "    val_ess        : 1.9615889062052188\n",
      "    val_log_marginal: 1183.2963124150815\n",
      "    val_log_joint  : 1391.9580502717392\n",
      "Train Epoch: 1802 [0/101520 (0%)] Loss: -1177.791626\n",
      "Train Epoch: 1802 [11264/101520 (11%)] Loss: -1182.363892\n",
      "Train Epoch: 1802 [22528/101520 (22%)] Loss: -1172.841064\n",
      "Train Epoch: 1802 [33792/101520 (33%)] Loss: -1184.169434\n",
      "Train Epoch: 1802 [45056/101520 (44%)] Loss: -1186.396729\n",
      "Train Epoch: 1802 [56320/101520 (55%)] Loss: -1181.850830\n",
      "Train Epoch: 1802 [67584/101520 (67%)] Loss: -1180.363770\n",
      "Train Epoch: 1802 [78848/101520 (78%)] Loss: -1184.815186\n",
      "Train Epoch: 1802 [90112/101520 (89%)] Loss: -1184.587402\n",
      "Train Epoch: 1802 [101376/101520 (100%)] Loss: -1174.878052\n",
      "    epoch          : 1802\n",
      "    loss           : -1183.8605447893765\n",
      "    ess            : 1.9661358499047745\n",
      "    log_marginal   : 1183.8927664445273\n",
      "    log_joint      : 1392.4160223726053\n",
      "    val_loss       : -1184.0360638162365\n",
      "    val_ess        : 1.9681028283160666\n",
      "    val_log_marginal: 1184.065233313519\n",
      "    val_log_joint  : 1392.5271739130435\n",
      "Train Epoch: 1803 [0/101520 (0%)] Loss: -1184.738159\n",
      "Train Epoch: 1803 [11264/101520 (11%)] Loss: -1179.125366\n",
      "Train Epoch: 1803 [22528/101520 (22%)] Loss: -1180.797607\n",
      "Train Epoch: 1803 [33792/101520 (33%)] Loss: -1182.063477\n",
      "Train Epoch: 1803 [45056/101520 (44%)] Loss: -1180.044189\n",
      "Train Epoch: 1803 [56320/101520 (55%)] Loss: -1185.256958\n",
      "Train Epoch: 1803 [67584/101520 (67%)] Loss: -1181.933594\n",
      "Train Epoch: 1803 [78848/101520 (78%)] Loss: -1187.670898\n",
      "Train Epoch: 1803 [90112/101520 (89%)] Loss: -1186.356812\n",
      "Train Epoch: 1803 [101376/101520 (100%)] Loss: -1181.428955\n",
      "    epoch          : 1803\n",
      "    loss           : -1183.9346868620446\n",
      "    ess            : 1.9662501422604124\n",
      "    log_marginal   : 1183.967324415044\n",
      "    log_joint      : 1392.419398015468\n",
      "    val_loss       : -1184.2688784391983\n",
      "    val_ess        : 1.9688299116880998\n",
      "    val_log_marginal: 1184.2973208220108\n",
      "    val_log_joint  : 1392.7400008491848\n",
      "Train Epoch: 1804 [0/101520 (0%)] Loss: -1182.171509\n",
      "Train Epoch: 1804 [11264/101520 (11%)] Loss: -1181.535278\n",
      "Train Epoch: 1804 [22528/101520 (22%)] Loss: -1186.732910\n",
      "Train Epoch: 1804 [33792/101520 (33%)] Loss: -1183.409058\n",
      "Train Epoch: 1804 [45056/101520 (44%)] Loss: -1170.824951\n",
      "Train Epoch: 1804 [56320/101520 (55%)] Loss: -1182.580811\n",
      "Train Epoch: 1804 [67584/101520 (67%)] Loss: -1185.856934\n",
      "Train Epoch: 1804 [78848/101520 (78%)] Loss: -1184.696045\n",
      "Train Epoch: 1804 [90112/101520 (89%)] Loss: -1181.261108\n",
      "Train Epoch: 1804 [101376/101520 (100%)] Loss: -1198.325806\n",
      "    epoch          : 1804\n",
      "    loss           : -1184.030002919873\n",
      "    ess            : 1.9657727491915526\n",
      "    log_marginal   : 1184.0642948629868\n",
      "    log_joint      : 1392.5033995661904\n",
      "    val_loss       : -1183.048000169837\n",
      "    val_ess        : 1.9666784068812495\n",
      "    val_log_marginal: 1183.079584536345\n",
      "    val_log_joint  : 1391.5851838485055\n",
      "Train Epoch: 1805 [0/101520 (0%)] Loss: -1181.549927\n",
      "Train Epoch: 1805 [11264/101520 (11%)] Loss: -1193.999756\n",
      "Train Epoch: 1805 [22528/101520 (22%)] Loss: -1183.276123\n",
      "Train Epoch: 1805 [33792/101520 (33%)] Loss: -1182.068481\n",
      "Train Epoch: 1805 [45056/101520 (44%)] Loss: -1180.320679\n",
      "Train Epoch: 1805 [56320/101520 (55%)] Loss: -1178.299561\n",
      "Train Epoch: 1805 [67584/101520 (67%)] Loss: -1176.736084\n",
      "Train Epoch: 1805 [78848/101520 (78%)] Loss: -1179.312988\n",
      "Train Epoch: 1805 [90112/101520 (89%)] Loss: -1189.793213\n",
      "Train Epoch: 1805 [101376/101520 (100%)] Loss: -1186.963379\n",
      "    epoch          : 1805\n",
      "    loss           : -1184.0142472616992\n",
      "    ess            : 1.966638436868562\n",
      "    log_marginal   : 1184.0462646484375\n",
      "    log_joint      : 1392.5377442633087\n",
      "    val_loss       : -1182.346186098845\n",
      "    val_ess        : 1.966315025868623\n",
      "    val_log_marginal: 1182.3798721976902\n",
      "    val_log_joint  : 1390.5952785326087\n",
      "Train Epoch: 1806 [0/101520 (0%)] Loss: -1179.813110\n",
      "Train Epoch: 1806 [11264/101520 (11%)] Loss: -1184.832764\n",
      "Train Epoch: 1806 [22528/101520 (22%)] Loss: -1180.359131\n",
      "Train Epoch: 1806 [33792/101520 (33%)] Loss: -1185.280518\n",
      "Train Epoch: 1806 [45056/101520 (44%)] Loss: -1185.289673\n",
      "Train Epoch: 1806 [56320/101520 (55%)] Loss: -1182.993164\n",
      "Train Epoch: 1806 [67584/101520 (67%)] Loss: -1188.192383\n",
      "Train Epoch: 1806 [78848/101520 (78%)] Loss: -1189.418335\n",
      "Train Epoch: 1806 [90112/101520 (89%)] Loss: -1179.240356\n",
      "Train Epoch: 1806 [101376/101520 (100%)] Loss: -1194.636353\n",
      "    epoch          : 1806\n",
      "    loss           : -1184.1108999587782\n",
      "    ess            : 1.966936098271279\n",
      "    log_marginal   : 1184.1427149173603\n",
      "    log_joint      : 1392.5639869268216\n",
      "    val_loss       : -1182.4984130859375\n",
      "    val_ess        : 1.9674201115317966\n",
      "    val_log_marginal: 1182.530910326087\n",
      "    val_log_joint  : 1391.0037947944973\n",
      "Train Epoch: 1807 [0/101520 (0%)] Loss: -1186.708130\n",
      "Train Epoch: 1807 [11264/101520 (11%)] Loss: -1185.524414\n",
      "Train Epoch: 1807 [22528/101520 (22%)] Loss: -1192.735718\n",
      "Train Epoch: 1807 [33792/101520 (33%)] Loss: -1181.808228\n",
      "Train Epoch: 1807 [45056/101520 (44%)] Loss: -1187.376953\n",
      "Train Epoch: 1807 [56320/101520 (55%)] Loss: -1183.883545\n",
      "Train Epoch: 1807 [67584/101520 (67%)] Loss: -1187.721924\n",
      "Train Epoch: 1807 [78848/101520 (78%)] Loss: -1182.916382\n",
      "Train Epoch: 1807 [90112/101520 (89%)] Loss: -1183.700073\n",
      "Train Epoch: 1807 [101376/101520 (100%)] Loss: -1181.447876\n",
      "    epoch          : 1807\n",
      "    loss           : -1184.0337318918812\n",
      "    ess            : 1.9664155688118097\n",
      "    log_marginal   : 1184.0662252914965\n",
      "    log_joint      : 1392.5578785038474\n",
      "    val_loss       : -1183.0169146993885\n",
      "    val_ess        : 1.965425247731416\n",
      "    val_log_marginal: 1183.0478727921195\n",
      "    val_log_joint  : 1391.9027258831522\n",
      "Train Epoch: 1808 [0/101520 (0%)] Loss: -1181.314209\n",
      "Train Epoch: 1808 [11264/101520 (11%)] Loss: -1183.279785\n",
      "Train Epoch: 1808 [22528/101520 (22%)] Loss: -1189.852661\n",
      "Train Epoch: 1808 [33792/101520 (33%)] Loss: -1181.988525\n",
      "Train Epoch: 1808 [45056/101520 (44%)] Loss: -1184.929932\n",
      "Train Epoch: 1808 [56320/101520 (55%)] Loss: -1193.564453\n",
      "Train Epoch: 1808 [67584/101520 (67%)] Loss: -1188.258911\n",
      "Train Epoch: 1808 [78848/101520 (78%)] Loss: -1185.015869\n",
      "Train Epoch: 1808 [90112/101520 (89%)] Loss: -1187.965576\n",
      "Train Epoch: 1808 [101376/101520 (100%)] Loss: -1162.882080\n",
      "    epoch          : 1808\n",
      "    loss           : -1183.970792070705\n",
      "    ess            : 1.965537555852727\n",
      "    log_marginal   : 1184.004369381085\n",
      "    log_joint      : 1392.4946669382066\n",
      "    val_loss       : -1183.40942913553\n",
      "    val_ess        : 1.9611835687056831\n",
      "    val_log_marginal: 1183.4445057744565\n",
      "    val_log_joint  : 1391.9396601137908\n",
      "Train Epoch: 1809 [0/101520 (0%)] Loss: -1183.237793\n",
      "Train Epoch: 1809 [11264/101520 (11%)] Loss: -1190.328613\n",
      "Train Epoch: 1809 [22528/101520 (22%)] Loss: -1191.065308\n",
      "Train Epoch: 1809 [33792/101520 (33%)] Loss: -1187.789673\n",
      "Train Epoch: 1809 [45056/101520 (44%)] Loss: -1188.051025\n",
      "Train Epoch: 1809 [56320/101520 (55%)] Loss: -1183.025146\n",
      "Train Epoch: 1809 [67584/101520 (67%)] Loss: -1187.708252\n",
      "Train Epoch: 1809 [78848/101520 (78%)] Loss: -1184.938965\n",
      "Train Epoch: 1809 [90112/101520 (89%)] Loss: -1193.036011\n",
      "Train Epoch: 1809 [101376/101520 (100%)] Loss: -1189.210693\n",
      "    epoch          : 1809\n",
      "    loss           : -1184.1861498655387\n",
      "    ess            : 1.9665337279813373\n",
      "    log_marginal   : 1184.2176961467494\n",
      "    log_joint      : 1392.6224984787218\n",
      "    val_loss       : -1183.2565174932065\n",
      "    val_ess        : 1.9682715250098186\n",
      "    val_log_marginal: 1183.2856869904892\n",
      "    val_log_joint  : 1391.8801959493885\n",
      "Train Epoch: 1810 [0/101520 (0%)] Loss: -1188.158569\n",
      "Train Epoch: 1810 [11264/101520 (11%)] Loss: -1184.352783\n",
      "Train Epoch: 1810 [22528/101520 (22%)] Loss: -1180.427979\n",
      "Train Epoch: 1810 [33792/101520 (33%)] Loss: -1186.833008\n",
      "Train Epoch: 1810 [45056/101520 (44%)] Loss: -1189.504395\n",
      "Train Epoch: 1810 [56320/101520 (55%)] Loss: -1180.852905\n",
      "Train Epoch: 1810 [67584/101520 (67%)] Loss: -1181.662842\n",
      "Train Epoch: 1810 [78848/101520 (78%)] Loss: -1182.801270\n",
      "Train Epoch: 1810 [90112/101520 (89%)] Loss: -1187.837524\n",
      "Train Epoch: 1810 [101376/101520 (100%)] Loss: -1191.480103\n",
      "    epoch          : 1810\n",
      "    loss           : -1184.1666566474953\n",
      "    ess            : 1.967228129880512\n",
      "    log_marginal   : 1184.1979741235475\n",
      "    log_joint      : 1392.667100762602\n",
      "    val_loss       : -1182.7977188773777\n",
      "    val_ess        : 1.9678523074025693\n",
      "    val_log_marginal: 1182.8292130180027\n",
      "    val_log_joint  : 1391.0887822690217\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1810.pth ...\n",
      "Train Epoch: 1811 [0/101520 (0%)] Loss: -1180.234619\n",
      "Train Epoch: 1811 [11264/101520 (11%)] Loss: -1180.060303\n",
      "Train Epoch: 1811 [22528/101520 (22%)] Loss: -1185.142090\n",
      "Train Epoch: 1811 [33792/101520 (33%)] Loss: -1183.151855\n",
      "Train Epoch: 1811 [45056/101520 (44%)] Loss: -1190.636230\n",
      "Train Epoch: 1811 [56320/101520 (55%)] Loss: -1186.496216\n",
      "Train Epoch: 1811 [67584/101520 (67%)] Loss: -1187.568115\n",
      "Train Epoch: 1811 [78848/101520 (78%)] Loss: -1187.321289\n",
      "Train Epoch: 1811 [90112/101520 (89%)] Loss: -1179.926025\n",
      "Train Epoch: 1811 [101376/101520 (100%)] Loss: -1180.118652\n",
      "    epoch          : 1811\n",
      "    loss           : -1184.190957227544\n",
      "    ess            : 1.9657833863742387\n",
      "    log_marginal   : 1184.2239260266174\n",
      "    log_joint      : 1392.6541803254554\n",
      "    val_loss       : -1182.7035867442255\n",
      "    val_ess        : 1.9686209999996682\n",
      "    val_log_marginal: 1182.7333347486413\n",
      "    val_log_joint  : 1391.1266346807065\n",
      "Train Epoch: 1812 [0/101520 (0%)] Loss: -1182.446289\n",
      "Train Epoch: 1812 [11264/101520 (11%)] Loss: -1187.798340\n",
      "Train Epoch: 1812 [22528/101520 (22%)] Loss: -1172.853516\n",
      "Train Epoch: 1812 [33792/101520 (33%)] Loss: -1183.817017\n",
      "Train Epoch: 1812 [45056/101520 (44%)] Loss: -1183.743286\n",
      "Train Epoch: 1812 [56320/101520 (55%)] Loss: -1181.362549\n",
      "Train Epoch: 1812 [67584/101520 (67%)] Loss: -1189.085205\n",
      "Train Epoch: 1812 [78848/101520 (78%)] Loss: -1189.389648\n",
      "Train Epoch: 1812 [90112/101520 (89%)] Loss: -1183.296631\n",
      "Train Epoch: 1812 [101376/101520 (100%)] Loss: -1184.314697\n",
      "    epoch          : 1812\n",
      "    loss           : -1184.2180826005026\n",
      "    ess            : 1.965811916332149\n",
      "    log_marginal   : 1184.2507440768295\n",
      "    log_joint      : 1392.6860185939463\n",
      "    val_loss       : -1182.9885572350543\n",
      "    val_ess        : 1.9678263560585354\n",
      "    val_log_marginal: 1183.018989894701\n",
      "    val_log_joint  : 1391.3451458474865\n",
      "Train Epoch: 1813 [0/101520 (0%)] Loss: -1183.294556\n",
      "Train Epoch: 1813 [11264/101520 (11%)] Loss: -1183.582397\n",
      "Train Epoch: 1813 [22528/101520 (22%)] Loss: -1185.444824\n",
      "Train Epoch: 1813 [33792/101520 (33%)] Loss: -1189.184326\n",
      "Train Epoch: 1813 [45056/101520 (44%)] Loss: -1183.636475\n",
      "Train Epoch: 1813 [56320/101520 (55%)] Loss: -1187.673096\n",
      "Train Epoch: 1813 [67584/101520 (67%)] Loss: -1187.423584\n",
      "Train Epoch: 1813 [78848/101520 (78%)] Loss: -1190.181396\n",
      "Train Epoch: 1813 [90112/101520 (89%)] Loss: -1185.673584\n",
      "Train Epoch: 1813 [101376/101520 (100%)] Loss: -1193.037109\n",
      "    epoch          : 1813\n",
      "    loss           : -1184.2620450887248\n",
      "    ess            : 1.9654818132295082\n",
      "    log_marginal   : 1184.2956996898556\n",
      "    log_joint      : 1392.7620420216315\n",
      "    val_loss       : -1183.5588272758152\n",
      "    val_ess        : 1.9683021358821704\n",
      "    val_log_marginal: 1183.5892440132473\n",
      "    val_log_joint  : 1391.9236211362092\n",
      "Train Epoch: 1814 [0/101520 (0%)] Loss: -1182.708740\n",
      "Train Epoch: 1814 [11264/101520 (11%)] Loss: -1181.704346\n",
      "Train Epoch: 1814 [22528/101520 (22%)] Loss: -1180.613159\n",
      "Train Epoch: 1814 [33792/101520 (33%)] Loss: -1181.973755\n",
      "Train Epoch: 1814 [45056/101520 (44%)] Loss: -1187.174683\n",
      "Train Epoch: 1814 [56320/101520 (55%)] Loss: -1184.865479\n",
      "Train Epoch: 1814 [67584/101520 (67%)] Loss: -1192.535156\n",
      "Train Epoch: 1814 [78848/101520 (78%)] Loss: -1183.493530\n",
      "Train Epoch: 1814 [90112/101520 (89%)] Loss: -1181.318359\n",
      "Train Epoch: 1814 [101376/101520 (100%)] Loss: -1195.911255\n",
      "    epoch          : 1814\n",
      "    loss           : -1184.2737730400047\n",
      "    ess            : 1.9651644247860165\n",
      "    log_marginal   : 1184.3076809830402\n",
      "    log_joint      : 1392.7637638877984\n",
      "    val_loss       : -1183.8601180366848\n",
      "    val_ess        : 1.9663897441781086\n",
      "    val_log_marginal: 1183.8903490149457\n",
      "    val_log_joint  : 1391.9848845108695\n",
      "Train Epoch: 1815 [0/101520 (0%)] Loss: -1184.046631\n",
      "Train Epoch: 1815 [11264/101520 (11%)] Loss: -1181.411133\n",
      "Train Epoch: 1815 [22528/101520 (22%)] Loss: -1181.710205\n",
      "Train Epoch: 1815 [33792/101520 (33%)] Loss: -1179.041382\n",
      "Train Epoch: 1815 [45056/101520 (44%)] Loss: -1190.594727\n",
      "Train Epoch: 1815 [56320/101520 (55%)] Loss: -1181.104126\n",
      "Train Epoch: 1815 [67584/101520 (67%)] Loss: -1188.650024\n",
      "Train Epoch: 1815 [78848/101520 (78%)] Loss: -1190.238037\n",
      "Train Epoch: 1815 [90112/101520 (89%)] Loss: -1178.305786\n",
      "Train Epoch: 1815 [101376/101520 (100%)] Loss: -1182.064697\n",
      "    epoch          : 1815\n",
      "    loss           : -1184.2925043920775\n",
      "    ess            : 1.9663527509075913\n",
      "    log_marginal   : 1184.3252634019707\n",
      "    log_joint      : 1392.7634804883794\n",
      "    val_loss       : -1182.980903957201\n",
      "    val_ess        : 1.966235435527304\n",
      "    val_log_marginal: 1183.0130456012228\n",
      "    val_log_joint  : 1391.3772184952445\n",
      "Train Epoch: 1816 [0/101520 (0%)] Loss: -1185.391846\n",
      "Train Epoch: 1816 [11264/101520 (11%)] Loss: -1189.518066\n",
      "Train Epoch: 1816 [22528/101520 (22%)] Loss: -1182.765015\n",
      "Train Epoch: 1816 [33792/101520 (33%)] Loss: -1184.209839\n",
      "Train Epoch: 1816 [45056/101520 (44%)] Loss: -1179.056396\n",
      "Train Epoch: 1816 [56320/101520 (55%)] Loss: -1183.382812\n",
      "Train Epoch: 1816 [67584/101520 (67%)] Loss: -1188.053467\n",
      "Train Epoch: 1816 [78848/101520 (78%)] Loss: -1178.268188\n",
      "Train Epoch: 1816 [90112/101520 (89%)] Loss: -1178.826416\n",
      "Train Epoch: 1816 [101376/101520 (100%)] Loss: -1197.066772\n",
      "    epoch          : 1816\n",
      "    loss           : -1184.307080446176\n",
      "    ess            : 1.9661767309035487\n",
      "    log_marginal   : 1184.3398879161432\n",
      "    log_joint      : 1392.777470114243\n",
      "    val_loss       : -1183.243217136549\n",
      "    val_ess        : 1.9677151130593342\n",
      "    val_log_marginal: 1183.2734693444293\n",
      "    val_log_joint  : 1391.828714121943\n",
      "Train Epoch: 1817 [0/101520 (0%)] Loss: -1187.192627\n",
      "Train Epoch: 1817 [11264/101520 (11%)] Loss: -1188.850342\n",
      "Train Epoch: 1817 [22528/101520 (22%)] Loss: -1186.187988\n",
      "Train Epoch: 1817 [33792/101520 (33%)] Loss: -1188.854370\n",
      "Train Epoch: 1817 [45056/101520 (44%)] Loss: -1187.503906\n",
      "Train Epoch: 1817 [56320/101520 (55%)] Loss: -1184.642822\n",
      "Train Epoch: 1817 [67584/101520 (67%)] Loss: -1189.067871\n",
      "Train Epoch: 1817 [78848/101520 (78%)] Loss: -1175.376953\n",
      "Train Epoch: 1817 [90112/101520 (89%)] Loss: -1187.699951\n",
      "Train Epoch: 1817 [101376/101520 (100%)] Loss: -1179.534058\n",
      "    epoch          : 1817\n",
      "    loss           : -1184.322215938089\n",
      "    ess            : 1.9661108099635523\n",
      "    log_marginal   : 1184.3552080470713\n",
      "    log_joint      : 1392.7925270885678\n",
      "    val_loss       : -1183.470803965693\n",
      "    val_ess        : 1.96805827513985\n",
      "    val_log_marginal: 1183.5022184952445\n",
      "    val_log_joint  : 1392.0470554517663\n",
      "Train Epoch: 1818 [0/101520 (0%)] Loss: -1184.511963\n",
      "Train Epoch: 1818 [11264/101520 (11%)] Loss: -1180.396606\n",
      "Train Epoch: 1818 [22528/101520 (22%)] Loss: -1187.621216\n",
      "Train Epoch: 1818 [33792/101520 (33%)] Loss: -1195.595459\n",
      "Train Epoch: 1818 [45056/101520 (44%)] Loss: -1177.407715\n",
      "Train Epoch: 1818 [56320/101520 (55%)] Loss: -1185.228394\n",
      "Train Epoch: 1818 [67584/101520 (67%)] Loss: -1184.271973\n",
      "Train Epoch: 1818 [78848/101520 (78%)] Loss: -1183.990967\n",
      "Train Epoch: 1818 [90112/101520 (89%)] Loss: -1183.817871\n",
      "Train Epoch: 1818 [101376/101520 (100%)] Loss: -1186.505981\n",
      "    epoch          : 1818\n",
      "    loss           : -1184.320820410647\n",
      "    ess            : 1.9663213407573987\n",
      "    log_marginal   : 1184.3533536824748\n",
      "    log_joint      : 1392.7915664749528\n",
      "    val_loss       : -1185.1678095278533\n",
      "    val_ess        : 1.9646437841912974\n",
      "    val_log_marginal: 1185.20044476053\n",
      "    val_log_joint  : 1393.4793117357337\n",
      "Train Epoch: 1819 [0/101520 (0%)] Loss: -1190.432251\n",
      "Train Epoch: 1819 [11264/101520 (11%)] Loss: -1183.142334\n",
      "Train Epoch: 1819 [22528/101520 (22%)] Loss: -1188.976318\n",
      "Train Epoch: 1819 [33792/101520 (33%)] Loss: -1184.532349\n",
      "Train Epoch: 1819 [45056/101520 (44%)] Loss: -1183.094116\n",
      "Train Epoch: 1819 [56320/101520 (55%)] Loss: -1183.434204\n",
      "Train Epoch: 1819 [67584/101520 (67%)] Loss: -1182.228760\n",
      "Train Epoch: 1819 [78848/101520 (78%)] Loss: -1182.410156\n",
      "Train Epoch: 1819 [90112/101520 (89%)] Loss: -1184.422363\n",
      "Train Epoch: 1819 [101376/101520 (100%)] Loss: -1178.809814\n",
      "    epoch          : 1819\n",
      "    loss           : -1184.2782841207993\n",
      "    ess            : 1.96662259940526\n",
      "    log_marginal   : 1184.3107762935772\n",
      "    log_joint      : 1392.7887778737438\n",
      "    val_loss       : -1182.8721499235733\n",
      "    val_ess        : 1.9677577381548674\n",
      "    val_log_marginal: 1182.9037183678668\n",
      "    val_log_joint  : 1391.2880116338315\n",
      "Train Epoch: 1820 [0/101520 (0%)] Loss: -1186.045898\n",
      "Train Epoch: 1820 [11264/101520 (11%)] Loss: -1191.651123\n",
      "Train Epoch: 1820 [22528/101520 (22%)] Loss: -1189.693604\n",
      "Train Epoch: 1820 [33792/101520 (33%)] Loss: -1188.786499\n",
      "Train Epoch: 1820 [45056/101520 (44%)] Loss: -1179.267334\n",
      "Train Epoch: 1820 [56320/101520 (55%)] Loss: -1195.556396\n",
      "Train Epoch: 1820 [67584/101520 (67%)] Loss: -1190.449097\n",
      "Train Epoch: 1820 [78848/101520 (78%)] Loss: -1188.669189\n",
      "Train Epoch: 1820 [90112/101520 (89%)] Loss: -1180.742920\n",
      "Train Epoch: 1820 [101376/101520 (100%)] Loss: -1181.264282\n",
      "    epoch          : 1820\n",
      "    loss           : -1184.3681064011464\n",
      "    ess            : 1.9665131964276183\n",
      "    log_marginal   : 1184.3993987270337\n",
      "    log_joint      : 1392.867811346773\n",
      "    val_loss       : -1182.8268299932065\n",
      "    val_ess        : 1.9676108515780906\n",
      "    val_log_marginal: 1182.8578464673913\n",
      "    val_log_joint  : 1391.4211531929348\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1820.pth ...\n",
      "Train Epoch: 1821 [0/101520 (0%)] Loss: -1194.502441\n",
      "Train Epoch: 1821 [11264/101520 (11%)] Loss: -1185.589600\n",
      "Train Epoch: 1821 [22528/101520 (22%)] Loss: -1180.371094\n",
      "Train Epoch: 1821 [33792/101520 (33%)] Loss: -1185.186523\n",
      "Train Epoch: 1821 [45056/101520 (44%)] Loss: -1182.187378\n",
      "Train Epoch: 1821 [56320/101520 (55%)] Loss: -1184.097046\n",
      "Train Epoch: 1821 [67584/101520 (67%)] Loss: -1186.251953\n",
      "Train Epoch: 1821 [78848/101520 (78%)] Loss: -1187.982422\n",
      "Train Epoch: 1821 [90112/101520 (89%)] Loss: -1175.368652\n",
      "Train Epoch: 1821 [101376/101520 (100%)] Loss: -1183.101807\n",
      "    epoch          : 1821\n",
      "    loss           : -1184.3742491755654\n",
      "    ess            : 1.966100127852742\n",
      "    log_marginal   : 1184.4065260383952\n",
      "    log_joint      : 1392.8723524850816\n",
      "    val_loss       : -1183.384468410326\n",
      "    val_ess        : 1.9652282569719397\n",
      "    val_log_marginal: 1183.4180005944293\n",
      "    val_log_joint  : 1392.0064378821332\n",
      "Train Epoch: 1822 [0/101520 (0%)] Loss: -1176.124268\n",
      "Train Epoch: 1822 [11264/101520 (11%)] Loss: -1189.113281\n",
      "Train Epoch: 1822 [22528/101520 (22%)] Loss: -1183.048828\n",
      "Train Epoch: 1822 [33792/101520 (33%)] Loss: -1185.098389\n",
      "Train Epoch: 1822 [45056/101520 (44%)] Loss: -1183.202026\n",
      "Train Epoch: 1822 [56320/101520 (55%)] Loss: -1183.359863\n",
      "Train Epoch: 1822 [67584/101520 (67%)] Loss: -1189.718506\n",
      "Train Epoch: 1822 [78848/101520 (78%)] Loss: -1175.197998\n",
      "Train Epoch: 1822 [90112/101520 (89%)] Loss: -1186.762451\n",
      "Train Epoch: 1822 [101376/101520 (100%)] Loss: -1181.017334\n",
      "    epoch          : 1822\n",
      "    loss           : -1184.375723220595\n",
      "    ess            : 1.9650505397787046\n",
      "    log_marginal   : 1184.4098685566505\n",
      "    log_joint      : 1392.8565078812028\n",
      "    val_loss       : -1181.828130307405\n",
      "    val_ess        : 1.965452966482743\n",
      "    val_log_marginal: 1181.8582710597825\n",
      "    val_log_joint  : 1390.646532141644\n",
      "Train Epoch: 1823 [0/101520 (0%)] Loss: -1178.462769\n",
      "Train Epoch: 1823 [11264/101520 (11%)] Loss: -1182.040771\n",
      "Train Epoch: 1823 [22528/101520 (22%)] Loss: -1185.221191\n",
      "Train Epoch: 1823 [33792/101520 (33%)] Loss: -1177.098145\n",
      "Train Epoch: 1823 [45056/101520 (44%)] Loss: -1176.298218\n",
      "Train Epoch: 1823 [56320/101520 (55%)] Loss: -1181.114990\n",
      "Train Epoch: 1823 [67584/101520 (67%)] Loss: -1192.021362\n",
      "Train Epoch: 1823 [78848/101520 (78%)] Loss: -1192.160034\n",
      "Train Epoch: 1823 [90112/101520 (89%)] Loss: -1192.927002\n",
      "Train Epoch: 1823 [101376/101520 (100%)] Loss: -1184.584839\n",
      "    epoch          : 1823\n",
      "    loss           : -1184.4266707070508\n",
      "    ess            : 1.966029215697667\n",
      "    log_marginal   : 1184.459510074788\n",
      "    log_joint      : 1392.8843386856156\n",
      "    val_loss       : -1182.5230341372283\n",
      "    val_ess        : 1.966562965641851\n",
      "    val_log_marginal: 1182.5561417289402\n",
      "    val_log_joint  : 1390.9880795686142\n",
      "Train Epoch: 1824 [0/101520 (0%)] Loss: -1188.589600\n",
      "Train Epoch: 1824 [11264/101520 (11%)] Loss: -1182.521851\n",
      "Train Epoch: 1824 [22528/101520 (22%)] Loss: -1183.178467\n",
      "Train Epoch: 1824 [33792/101520 (33%)] Loss: -1188.354004\n",
      "Train Epoch: 1824 [45056/101520 (44%)] Loss: -1181.374512\n",
      "Train Epoch: 1824 [56320/101520 (55%)] Loss: -1185.764404\n",
      "Train Epoch: 1824 [67584/101520 (67%)] Loss: -1184.605469\n",
      "Train Epoch: 1824 [78848/101520 (78%)] Loss: -1183.710205\n",
      "Train Epoch: 1824 [90112/101520 (89%)] Loss: -1181.513062\n",
      "Train Epoch: 1824 [101376/101520 (100%)] Loss: -1186.127563\n",
      "    epoch          : 1824\n",
      "    loss           : -1184.438296830834\n",
      "    ess            : 1.9654820672231703\n",
      "    log_marginal   : 1184.4715895149577\n",
      "    log_joint      : 1392.92209399046\n",
      "    val_loss       : -1182.62963336447\n",
      "    val_ess        : 1.9674051689065022\n",
      "    val_log_marginal: 1182.6637971297555\n",
      "    val_log_joint  : 1390.9520369819973\n",
      "Train Epoch: 1825 [0/101520 (0%)] Loss: -1183.281006\n",
      "Train Epoch: 1825 [11264/101520 (11%)] Loss: -1177.447021\n",
      "Train Epoch: 1825 [22528/101520 (22%)] Loss: -1185.371826\n",
      "Train Epoch: 1825 [33792/101520 (33%)] Loss: -1181.183350\n",
      "Train Epoch: 1825 [45056/101520 (44%)] Loss: -1184.108887\n",
      "Train Epoch: 1825 [56320/101520 (55%)] Loss: -1187.211670\n",
      "Train Epoch: 1825 [67584/101520 (67%)] Loss: -1179.292480\n",
      "Train Epoch: 1825 [78848/101520 (78%)] Loss: -1188.419189\n",
      "Train Epoch: 1825 [90112/101520 (89%)] Loss: -1184.779541\n",
      "Train Epoch: 1825 [101376/101520 (100%)] Loss: -1177.771851\n",
      "    epoch          : 1825\n",
      "    loss           : -1184.4316492128612\n",
      "    ess            : 1.966104584123621\n",
      "    log_marginal   : 1184.4642904463724\n",
      "    log_joint      : 1392.877364728918\n",
      "    val_loss       : -1182.4866412618885\n",
      "    val_ess        : 1.9660471211309019\n",
      "    val_log_marginal: 1182.518851902174\n",
      "    val_log_joint  : 1390.8113005264945\n",
      "Train Epoch: 1826 [0/101520 (0%)] Loss: -1180.827148\n",
      "Train Epoch: 1826 [11264/101520 (11%)] Loss: -1190.363770\n",
      "Train Epoch: 1826 [22528/101520 (22%)] Loss: -1177.017578\n",
      "Train Epoch: 1826 [33792/101520 (33%)] Loss: -1183.492188\n",
      "Train Epoch: 1826 [45056/101520 (44%)] Loss: -1191.685791\n",
      "Train Epoch: 1826 [56320/101520 (55%)] Loss: -1183.977417\n",
      "Train Epoch: 1826 [67584/101520 (67%)] Loss: -1184.303101\n",
      "Train Epoch: 1826 [78848/101520 (78%)] Loss: -1187.325684\n",
      "Train Epoch: 1826 [90112/101520 (89%)] Loss: -1180.842651\n",
      "Train Epoch: 1826 [101376/101520 (100%)] Loss: -1198.563110\n",
      "    epoch          : 1826\n",
      "    loss           : -1184.496706555237\n",
      "    ess            : 1.9673064743454134\n",
      "    log_marginal   : 1184.5277933858747\n",
      "    log_joint      : 1392.9586537423445\n",
      "    val_loss       : -1183.4961680536685\n",
      "    val_ess        : 1.9662640042926953\n",
      "    val_log_marginal: 1183.528861667799\n",
      "    val_log_joint  : 1392.2203952955163\n",
      "Train Epoch: 1827 [0/101520 (0%)] Loss: -1182.593262\n",
      "Train Epoch: 1827 [11264/101520 (11%)] Loss: -1179.014771\n",
      "Train Epoch: 1827 [22528/101520 (22%)] Loss: -1188.666504\n",
      "Train Epoch: 1827 [33792/101520 (33%)] Loss: -1182.805176\n",
      "Train Epoch: 1827 [45056/101520 (44%)] Loss: -1185.650391\n",
      "Train Epoch: 1827 [56320/101520 (55%)] Loss: -1192.028320\n",
      "Train Epoch: 1827 [67584/101520 (67%)] Loss: -1184.343506\n",
      "Train Epoch: 1827 [78848/101520 (78%)] Loss: -1178.030273\n",
      "Train Epoch: 1827 [90112/101520 (89%)] Loss: -1187.287842\n",
      "Train Epoch: 1827 [101376/101520 (100%)] Loss: -1188.685669\n",
      "    epoch          : 1827\n",
      "    loss           : -1184.4673195076948\n",
      "    ess            : 1.9665822533506845\n",
      "    log_marginal   : 1184.4987247026147\n",
      "    log_joint      : 1392.9485176124765\n",
      "    val_loss       : -1184.0021548063858\n",
      "    val_ess        : 1.9679011261981467\n",
      "    val_log_marginal: 1184.0310695482337\n",
      "    val_log_joint  : 1392.5146696671195\n",
      "Train Epoch: 1828 [0/101520 (0%)] Loss: -1180.606567\n",
      "Train Epoch: 1828 [11264/101520 (11%)] Loss: -1186.814331\n",
      "Train Epoch: 1828 [22528/101520 (22%)] Loss: -1192.397217\n",
      "Train Epoch: 1828 [33792/101520 (33%)] Loss: -1184.700195\n",
      "Train Epoch: 1828 [45056/101520 (44%)] Loss: -1193.679565\n",
      "Train Epoch: 1828 [56320/101520 (55%)] Loss: -1182.528320\n",
      "Train Epoch: 1828 [67584/101520 (67%)] Loss: -1181.061890\n",
      "Train Epoch: 1828 [78848/101520 (78%)] Loss: -1184.217529\n",
      "Train Epoch: 1828 [90112/101520 (89%)] Loss: -1179.275879\n",
      "Train Epoch: 1828 [101376/101520 (100%)] Loss: -1189.716187\n",
      "    epoch          : 1828\n",
      "    loss           : -1184.4522490381596\n",
      "    ess            : 1.9662079667326193\n",
      "    log_marginal   : 1184.485098220595\n",
      "    log_joint      : 1392.9579323620053\n",
      "    val_loss       : -1183.1134988536005\n",
      "    val_ess        : 1.9674297519352124\n",
      "    val_log_marginal: 1183.1458103345788\n",
      "    val_log_joint  : 1391.6348240064538\n",
      "Train Epoch: 1829 [0/101520 (0%)] Loss: -1190.105713\n",
      "Train Epoch: 1829 [11264/101520 (11%)] Loss: -1174.691895\n",
      "Train Epoch: 1829 [22528/101520 (22%)] Loss: -1181.837891\n",
      "Train Epoch: 1829 [33792/101520 (33%)] Loss: -1174.375122\n",
      "Train Epoch: 1829 [45056/101520 (44%)] Loss: -1182.614502\n",
      "Train Epoch: 1829 [56320/101520 (55%)] Loss: -1185.264404\n",
      "Train Epoch: 1829 [67584/101520 (67%)] Loss: -1179.950562\n",
      "Train Epoch: 1829 [78848/101520 (78%)] Loss: -1184.098389\n",
      "Train Epoch: 1829 [90112/101520 (89%)] Loss: -1186.101807\n",
      "Train Epoch: 1829 [101376/101520 (100%)] Loss: -1184.948364\n",
      "    epoch          : 1829\n",
      "    loss           : -1184.538727573414\n",
      "    ess            : 1.966070198533523\n",
      "    log_marginal   : 1184.5718687431297\n",
      "    log_joint      : 1392.975735611652\n",
      "    val_loss       : -1182.5951776919158\n",
      "    val_ess        : 1.9657809941664985\n",
      "    val_log_marginal: 1182.6274148692255\n",
      "    val_log_joint  : 1391.281149159307\n",
      "Train Epoch: 1830 [0/101520 (0%)] Loss: -1191.301636\n",
      "Train Epoch: 1830 [11264/101520 (11%)] Loss: -1183.543945\n",
      "Train Epoch: 1830 [22528/101520 (22%)] Loss: -1182.522949\n",
      "Train Epoch: 1830 [33792/101520 (33%)] Loss: -1188.379150\n",
      "Train Epoch: 1830 [45056/101520 (44%)] Loss: -1181.993042\n",
      "Train Epoch: 1830 [56320/101520 (55%)] Loss: -1184.521729\n",
      "Train Epoch: 1830 [67584/101520 (67%)] Loss: -1185.874390\n",
      "Train Epoch: 1830 [78848/101520 (78%)] Loss: -1182.885742\n",
      "Train Epoch: 1830 [90112/101520 (89%)] Loss: -1181.851807\n",
      "Train Epoch: 1830 [101376/101520 (100%)] Loss: -1170.100586\n",
      "    epoch          : 1830\n",
      "    loss           : -1184.5458622457993\n",
      "    ess            : 1.9663936265149908\n",
      "    log_marginal   : 1184.5774361308495\n",
      "    log_joint      : 1392.996304152599\n",
      "    val_loss       : -1182.3199569038723\n",
      "    val_ess        : 1.96647194157476\n",
      "    val_log_marginal: 1182.349800441576\n",
      "    val_log_joint  : 1390.6976742951767\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1830.pth ...\n",
      "Train Epoch: 1831 [0/101520 (0%)] Loss: -1191.753906\n",
      "Train Epoch: 1831 [11264/101520 (11%)] Loss: -1183.431396\n",
      "Train Epoch: 1831 [22528/101520 (22%)] Loss: -1182.662598\n",
      "Train Epoch: 1831 [33792/101520 (33%)] Loss: -1184.328369\n",
      "Train Epoch: 1831 [45056/101520 (44%)] Loss: -1189.975830\n",
      "Train Epoch: 1831 [56320/101520 (55%)] Loss: -1181.486938\n",
      "Train Epoch: 1831 [67584/101520 (67%)] Loss: -1189.897461\n",
      "Train Epoch: 1831 [78848/101520 (78%)] Loss: -1175.739502\n",
      "Train Epoch: 1831 [90112/101520 (89%)] Loss: -1184.273438\n",
      "Train Epoch: 1831 [101376/101520 (100%)] Loss: -1194.622559\n",
      "    epoch          : 1831\n",
      "    loss           : -1184.6600151637092\n",
      "    ess            : 1.9660249463277846\n",
      "    log_marginal   : 1184.692453969064\n",
      "    log_joint      : 1393.0758259068782\n",
      "    val_loss       : -1184.1970533288043\n",
      "    val_ess        : 1.9663638498472131\n",
      "    val_log_marginal: 1184.2288234544837\n",
      "    val_log_joint  : 1392.3682012143342\n",
      "Train Epoch: 1832 [0/101520 (0%)] Loss: -1193.177368\n",
      "Train Epoch: 1832 [11264/101520 (11%)] Loss: -1190.738037\n",
      "Train Epoch: 1832 [22528/101520 (22%)] Loss: -1175.364990\n",
      "Train Epoch: 1832 [33792/101520 (33%)] Loss: -1176.008179\n",
      "Train Epoch: 1832 [45056/101520 (44%)] Loss: -1180.044434\n",
      "Train Epoch: 1832 [56320/101520 (55%)] Loss: -1182.449219\n",
      "Train Epoch: 1832 [67584/101520 (67%)] Loss: -1180.707275\n",
      "Train Epoch: 1832 [78848/101520 (78%)] Loss: -1180.508911\n",
      "Train Epoch: 1832 [90112/101520 (89%)] Loss: -1187.110596\n",
      "Train Epoch: 1832 [101376/101520 (100%)] Loss: -1183.399536\n",
      "    epoch          : 1832\n",
      "    loss           : -1184.585618522299\n",
      "    ess            : 1.9651514537370385\n",
      "    log_marginal   : 1184.6200294686323\n",
      "    log_joint      : 1393.077736705991\n",
      "    val_loss       : -1185.400199558424\n",
      "    val_ess        : 1.966695769973423\n",
      "    val_log_marginal: 1185.432808254076\n",
      "    val_log_joint  : 1393.4458113960598\n",
      "Train Epoch: 1833 [0/101520 (0%)] Loss: -1185.158691\n",
      "Train Epoch: 1833 [11264/101520 (11%)] Loss: -1196.580811\n",
      "Train Epoch: 1833 [22528/101520 (22%)] Loss: -1177.558594\n",
      "Train Epoch: 1833 [33792/101520 (33%)] Loss: -1176.890625\n",
      "Train Epoch: 1833 [45056/101520 (44%)] Loss: -1187.185913\n",
      "Train Epoch: 1833 [56320/101520 (55%)] Loss: -1181.939697\n",
      "Train Epoch: 1833 [67584/101520 (67%)] Loss: -1185.730713\n",
      "Train Epoch: 1833 [78848/101520 (78%)] Loss: -1184.150635\n",
      "Train Epoch: 1833 [90112/101520 (89%)] Loss: -1186.446533\n",
      "Train Epoch: 1833 [101376/101520 (100%)] Loss: -1163.594849\n",
      "    epoch          : 1833\n",
      "    loss           : -1184.49714330932\n",
      "    ess            : 1.965870436711527\n",
      "    log_marginal   : 1184.5313438530543\n",
      "    log_joint      : 1392.9578360552764\n",
      "    val_loss       : -1182.2344652258832\n",
      "    val_ess        : 1.965881700101106\n",
      "    val_log_marginal: 1182.2678912618885\n",
      "    val_log_joint  : 1390.9868854025135\n",
      "Train Epoch: 1834 [0/101520 (0%)] Loss: -1189.007812\n",
      "Train Epoch: 1834 [11264/101520 (11%)] Loss: -1188.631470\n",
      "Train Epoch: 1834 [22528/101520 (22%)] Loss: -1185.769531\n",
      "Train Epoch: 1834 [33792/101520 (33%)] Loss: -1192.250366\n",
      "Train Epoch: 1834 [45056/101520 (44%)] Loss: -1184.931396\n",
      "Train Epoch: 1834 [56320/101520 (55%)] Loss: -1179.810791\n",
      "Train Epoch: 1834 [67584/101520 (67%)] Loss: -1191.000000\n",
      "Train Epoch: 1834 [78848/101520 (78%)] Loss: -1185.895020\n",
      "Train Epoch: 1834 [90112/101520 (89%)] Loss: -1193.144531\n",
      "Train Epoch: 1834 [101376/101520 (100%)] Loss: -1180.659912\n",
      "    epoch          : 1834\n",
      "    loss           : -1184.5291404532427\n",
      "    ess            : 1.965657162306896\n",
      "    log_marginal   : 1184.5629852141567\n",
      "    log_joint      : 1393.0690053048445\n",
      "    val_loss       : -1184.8580906080163\n",
      "    val_ess        : 1.9669677226439766\n",
      "    val_log_marginal: 1184.8918669327445\n",
      "    val_log_joint  : 1393.1667905061142\n",
      "Train Epoch: 1835 [0/101520 (0%)] Loss: -1184.207275\n",
      "Train Epoch: 1835 [11264/101520 (11%)] Loss: -1186.603027\n",
      "Train Epoch: 1835 [22528/101520 (22%)] Loss: -1185.246338\n",
      "Train Epoch: 1835 [33792/101520 (33%)] Loss: -1185.203491\n",
      "Train Epoch: 1835 [45056/101520 (44%)] Loss: -1188.704590\n",
      "Train Epoch: 1835 [56320/101520 (55%)] Loss: -1188.150879\n",
      "Train Epoch: 1835 [67584/101520 (67%)] Loss: -1192.341553\n",
      "Train Epoch: 1835 [78848/101520 (78%)] Loss: -1184.550781\n",
      "Train Epoch: 1835 [90112/101520 (89%)] Loss: -1179.185669\n",
      "Train Epoch: 1835 [101376/101520 (100%)] Loss: -1201.322876\n",
      "    epoch          : 1835\n",
      "    loss           : -1184.7741355704302\n",
      "    ess            : 1.9658578784022498\n",
      "    log_marginal   : 1184.8064799093122\n",
      "    log_joint      : 1393.1673473569017\n",
      "    val_loss       : -1183.156441066576\n",
      "    val_ess        : 1.967973144158073\n",
      "    val_log_marginal: 1183.1864332116168\n",
      "    val_log_joint  : 1391.7069622537365\n",
      "Train Epoch: 1836 [0/101520 (0%)] Loss: -1179.206055\n",
      "Train Epoch: 1836 [11264/101520 (11%)] Loss: -1185.889160\n",
      "Train Epoch: 1836 [22528/101520 (22%)] Loss: -1187.838501\n",
      "Train Epoch: 1836 [33792/101520 (33%)] Loss: -1187.381348\n",
      "Train Epoch: 1836 [45056/101520 (44%)] Loss: -1184.875977\n",
      "Train Epoch: 1836 [56320/101520 (55%)] Loss: -1188.559814\n",
      "Train Epoch: 1836 [67584/101520 (67%)] Loss: -1189.964111\n",
      "Train Epoch: 1836 [78848/101520 (78%)] Loss: -1181.375977\n",
      "Train Epoch: 1836 [90112/101520 (89%)] Loss: -1181.914429\n",
      "Train Epoch: 1836 [101376/101520 (100%)] Loss: -1185.492676\n",
      "    epoch          : 1836\n",
      "    loss           : -1184.6851536736417\n",
      "    ess            : 1.9654192948461178\n",
      "    log_marginal   : 1184.718701539926\n",
      "    log_joint      : 1393.2180040829146\n",
      "    val_loss       : -1184.7521707286005\n",
      "    val_ess        : 1.9667599667673525\n",
      "    val_log_marginal: 1184.783739172894\n",
      "    val_log_joint  : 1393.1143321161685\n",
      "Train Epoch: 1837 [0/101520 (0%)] Loss: -1184.547119\n",
      "Train Epoch: 1837 [11264/101520 (11%)] Loss: -1179.602783\n",
      "Train Epoch: 1837 [22528/101520 (22%)] Loss: -1185.466797\n",
      "Train Epoch: 1837 [33792/101520 (33%)] Loss: -1183.618164\n",
      "Train Epoch: 1837 [45056/101520 (44%)] Loss: -1179.877197\n",
      "Train Epoch: 1837 [56320/101520 (55%)] Loss: -1182.152100\n",
      "Train Epoch: 1837 [67584/101520 (67%)] Loss: -1185.979004\n",
      "Train Epoch: 1837 [78848/101520 (78%)] Loss: -1182.567139\n",
      "Train Epoch: 1837 [90112/101520 (89%)] Loss: -1181.415894\n",
      "Train Epoch: 1837 [101376/101520 (100%)] Loss: -1190.734741\n",
      "    epoch          : 1837\n",
      "    loss           : -1184.711341742894\n",
      "    ess            : 1.9658138536328646\n",
      "    log_marginal   : 1184.7449681267667\n",
      "    log_joint      : 1393.1815314364792\n",
      "    val_loss       : -1184.2650199558425\n",
      "    val_ess        : 1.964839131935783\n",
      "    val_log_marginal: 1184.3036737856658\n",
      "    val_log_joint  : 1392.681783924932\n",
      "Train Epoch: 1838 [0/101520 (0%)] Loss: -1189.163818\n",
      "Train Epoch: 1838 [11264/101520 (11%)] Loss: -1188.247437\n",
      "Train Epoch: 1838 [22528/101520 (22%)] Loss: -1190.609863\n",
      "Train Epoch: 1838 [33792/101520 (33%)] Loss: -1176.571533\n",
      "Train Epoch: 1838 [45056/101520 (44%)] Loss: -1182.518555\n",
      "Train Epoch: 1838 [56320/101520 (55%)] Loss: -1186.029297\n",
      "Train Epoch: 1838 [67584/101520 (67%)] Loss: -1182.296631\n",
      "Train Epoch: 1838 [78848/101520 (78%)] Loss: -1183.075073\n",
      "Train Epoch: 1838 [90112/101520 (89%)] Loss: -1180.895020\n",
      "Train Epoch: 1838 [101376/101520 (100%)] Loss: -1188.064941\n",
      "    epoch          : 1838\n",
      "    loss           : -1184.7723891675173\n",
      "    ess            : 1.9669449005893727\n",
      "    log_marginal   : 1184.803791295344\n",
      "    log_joint      : 1393.245017200259\n",
      "    val_loss       : -1181.9265295940897\n",
      "    val_ess        : 1.967373630274897\n",
      "    val_log_marginal: 1181.958002505095\n",
      "    val_log_joint  : 1390.4248046875\n",
      "Train Epoch: 1839 [0/101520 (0%)] Loss: -1178.260742\n",
      "Train Epoch: 1839 [11264/101520 (11%)] Loss: -1184.204834\n",
      "Train Epoch: 1839 [22528/101520 (22%)] Loss: -1185.723389\n",
      "Train Epoch: 1839 [33792/101520 (33%)] Loss: -1181.977539\n",
      "Train Epoch: 1839 [45056/101520 (44%)] Loss: -1186.802734\n",
      "Train Epoch: 1839 [56320/101520 (55%)] Loss: -1182.596069\n",
      "Train Epoch: 1839 [67584/101520 (67%)] Loss: -1182.745117\n",
      "Train Epoch: 1839 [78848/101520 (78%)] Loss: -1179.371094\n",
      "Train Epoch: 1839 [90112/101520 (89%)] Loss: -1187.358276\n",
      "Train Epoch: 1839 [101376/101520 (100%)] Loss: -1189.856201\n",
      "    epoch          : 1839\n",
      "    loss           : -1184.8209062892588\n",
      "    ess            : 1.9658145473231023\n",
      "    log_marginal   : 1184.8540241490657\n",
      "    log_joint      : 1393.2688319527324\n",
      "    val_loss       : -1184.7542300016983\n",
      "    val_ess        : 1.9650906998178232\n",
      "    val_log_marginal: 1184.7881071671195\n",
      "    val_log_joint  : 1393.0004511294158\n",
      "Train Epoch: 1840 [0/101520 (0%)] Loss: -1184.159912\n",
      "Train Epoch: 1840 [11264/101520 (11%)] Loss: -1186.183594\n",
      "Train Epoch: 1840 [22528/101520 (22%)] Loss: -1182.559326\n",
      "Train Epoch: 1840 [33792/101520 (33%)] Loss: -1183.424316\n",
      "Train Epoch: 1840 [45056/101520 (44%)] Loss: -1184.305786\n",
      "Train Epoch: 1840 [56320/101520 (55%)] Loss: -1186.969971\n",
      "Train Epoch: 1840 [67584/101520 (67%)] Loss: -1183.224365\n",
      "Train Epoch: 1840 [78848/101520 (78%)] Loss: -1177.812988\n",
      "Train Epoch: 1840 [90112/101520 (89%)] Loss: -1192.723389\n",
      "Train Epoch: 1840 [101376/101520 (100%)] Loss: -1180.342407\n",
      "    epoch          : 1840\n",
      "    loss           : -1184.7612844495918\n",
      "    ess            : 1.9661043540916252\n",
      "    log_marginal   : 1184.7935932101916\n",
      "    log_joint      : 1393.2451184143374\n",
      "    val_loss       : -1184.0846053413723\n",
      "    val_ess        : 1.9689195000607034\n",
      "    val_log_marginal: 1184.1122940726902\n",
      "    val_log_joint  : 1392.4825333305027\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1840.pth ...\n",
      "Train Epoch: 1841 [0/101520 (0%)] Loss: -1188.901001\n",
      "Train Epoch: 1841 [11264/101520 (11%)] Loss: -1174.690552\n",
      "Train Epoch: 1841 [22528/101520 (22%)] Loss: -1188.111572\n",
      "Train Epoch: 1841 [33792/101520 (33%)] Loss: -1181.799561\n",
      "Train Epoch: 1841 [45056/101520 (44%)] Loss: -1180.532715\n",
      "Train Epoch: 1841 [56320/101520 (55%)] Loss: -1182.159424\n",
      "Train Epoch: 1841 [67584/101520 (67%)] Loss: -1178.455566\n",
      "Train Epoch: 1841 [78848/101520 (78%)] Loss: -1182.795776\n",
      "Train Epoch: 1841 [90112/101520 (89%)] Loss: -1183.613281\n",
      "Train Epoch: 1841 [101376/101520 (100%)] Loss: -1190.382324\n",
      "    epoch          : 1841\n",
      "    loss           : -1184.8489708061793\n",
      "    ess            : 1.9668741148320874\n",
      "    log_marginal   : 1184.8812998095948\n",
      "    log_joint      : 1393.3413128876805\n",
      "    val_loss       : -1182.1692956012228\n",
      "    val_ess        : 1.9646301165870999\n",
      "    val_log_marginal: 1182.2035442849865\n",
      "    val_log_joint  : 1390.534763502038\n",
      "Train Epoch: 1842 [0/101520 (0%)] Loss: -1185.412598\n",
      "Train Epoch: 1842 [11264/101520 (11%)] Loss: -1186.043213\n",
      "Train Epoch: 1842 [22528/101520 (22%)] Loss: -1183.607422\n",
      "Train Epoch: 1842 [33792/101520 (33%)] Loss: -1190.629150\n",
      "Train Epoch: 1842 [45056/101520 (44%)] Loss: -1179.655518\n",
      "Train Epoch: 1842 [56320/101520 (55%)] Loss: -1184.113525\n",
      "Train Epoch: 1842 [67584/101520 (67%)] Loss: -1185.666748\n",
      "Train Epoch: 1842 [78848/101520 (78%)] Loss: -1177.055420\n",
      "Train Epoch: 1842 [90112/101520 (89%)] Loss: -1179.760010\n",
      "Train Epoch: 1842 [101376/101520 (100%)] Loss: -1184.520630\n",
      "    epoch          : 1842\n",
      "    loss           : -1184.800533428863\n",
      "    ess            : 1.9660381042777593\n",
      "    log_marginal   : 1184.8329108923524\n",
      "    log_joint      : 1393.28008634481\n",
      "    val_loss       : -1183.574558423913\n",
      "    val_ess        : 1.9679768862931624\n",
      "    val_log_marginal: 1183.604975161345\n",
      "    val_log_joint  : 1391.980123768682\n",
      "Train Epoch: 1843 [0/101520 (0%)] Loss: -1194.638672\n",
      "Train Epoch: 1843 [11264/101520 (11%)] Loss: -1186.606689\n",
      "Train Epoch: 1843 [22528/101520 (22%)] Loss: -1188.292236\n",
      "Train Epoch: 1843 [33792/101520 (33%)] Loss: -1182.687012\n",
      "Train Epoch: 1843 [45056/101520 (44%)] Loss: -1180.659424\n",
      "Train Epoch: 1843 [56320/101520 (55%)] Loss: -1186.470703\n",
      "Train Epoch: 1843 [67584/101520 (67%)] Loss: -1190.611328\n",
      "Train Epoch: 1843 [78848/101520 (78%)] Loss: -1176.573730\n",
      "Train Epoch: 1843 [90112/101520 (89%)] Loss: -1179.359619\n",
      "Train Epoch: 1843 [101376/101520 (100%)] Loss: -1182.294556\n",
      "    epoch          : 1843\n",
      "    loss           : -1184.841880913356\n",
      "    ess            : 1.9655980614561532\n",
      "    log_marginal   : 1184.874827015939\n",
      "    log_joint      : 1393.3338334740106\n",
      "    val_loss       : -1183.5843134341033\n",
      "    val_ess        : 1.9672917490420134\n",
      "    val_log_marginal: 1183.612846042799\n",
      "    val_log_joint  : 1392.1316767153533\n",
      "Train Epoch: 1844 [0/101520 (0%)] Loss: -1186.091064\n",
      "Train Epoch: 1844 [11264/101520 (11%)] Loss: -1184.214478\n",
      "Train Epoch: 1844 [22528/101520 (22%)] Loss: -1184.836426\n",
      "Train Epoch: 1844 [33792/101520 (33%)] Loss: -1190.834106\n",
      "Train Epoch: 1844 [45056/101520 (44%)] Loss: -1180.051392\n",
      "Train Epoch: 1844 [56320/101520 (55%)] Loss: -1187.225342\n",
      "Train Epoch: 1844 [67584/101520 (67%)] Loss: -1180.701416\n",
      "Train Epoch: 1844 [78848/101520 (78%)] Loss: -1187.694092\n",
      "Train Epoch: 1844 [90112/101520 (89%)] Loss: -1187.528564\n",
      "Train Epoch: 1844 [101376/101520 (100%)] Loss: -1190.309937\n",
      "    epoch          : 1844\n",
      "    loss           : -1185.0020592464275\n",
      "    ess            : 1.9658290908564275\n",
      "    log_marginal   : 1185.0355402500786\n",
      "    log_joint      : 1393.392987888662\n",
      "    val_loss       : -1183.0900507387908\n",
      "    val_ess        : 1.9671163662620212\n",
      "    val_log_marginal: 1183.12182086447\n",
      "    val_log_joint  : 1391.6281579059103\n",
      "Train Epoch: 1845 [0/101520 (0%)] Loss: -1184.110474\n",
      "Train Epoch: 1845 [11264/101520 (11%)] Loss: -1178.626465\n",
      "Train Epoch: 1845 [22528/101520 (22%)] Loss: -1181.590088\n",
      "Train Epoch: 1845 [33792/101520 (33%)] Loss: -1184.117432\n",
      "Train Epoch: 1845 [45056/101520 (44%)] Loss: -1182.752319\n",
      "Train Epoch: 1845 [56320/101520 (55%)] Loss: -1180.897461\n",
      "Train Epoch: 1845 [67584/101520 (67%)] Loss: -1183.948486\n",
      "Train Epoch: 1845 [78848/101520 (78%)] Loss: -1183.299927\n",
      "Train Epoch: 1845 [90112/101520 (89%)] Loss: -1183.203003\n",
      "Train Epoch: 1845 [101376/101520 (100%)] Loss: -1185.108154\n",
      "    epoch          : 1845\n",
      "    loss           : -1184.8599393451634\n",
      "    ess            : 1.9657637695571286\n",
      "    log_marginal   : 1184.8926143167007\n",
      "    log_joint      : 1393.3549277147456\n",
      "    val_loss       : -1181.4768968665082\n",
      "    val_ess        : 1.9661728247352268\n",
      "    val_log_marginal: 1181.5121486497962\n",
      "    val_log_joint  : 1389.82105022928\n",
      "Train Epoch: 1846 [0/101520 (0%)] Loss: -1189.209961\n",
      "Train Epoch: 1846 [11264/101520 (11%)] Loss: -1192.581909\n",
      "Train Epoch: 1846 [22528/101520 (22%)] Loss: -1182.179688\n",
      "Train Epoch: 1846 [33792/101520 (33%)] Loss: -1184.609253\n",
      "Train Epoch: 1846 [45056/101520 (44%)] Loss: -1181.099854\n",
      "Train Epoch: 1846 [56320/101520 (55%)] Loss: -1182.810791\n",
      "Train Epoch: 1846 [67584/101520 (67%)] Loss: -1184.495361\n",
      "Train Epoch: 1846 [78848/101520 (78%)] Loss: -1184.623779\n",
      "Train Epoch: 1846 [90112/101520 (89%)] Loss: -1187.396973\n",
      "Train Epoch: 1846 [101376/101520 (100%)] Loss: -1160.993530\n",
      "    epoch          : 1846\n",
      "    loss           : -1184.873137660961\n",
      "    ess            : 1.967260629687477\n",
      "    log_marginal   : 1184.9045207728093\n",
      "    log_joint      : 1393.3351885158213\n",
      "    val_loss       : -1183.689697265625\n",
      "    val_ess        : 1.9668325963227644\n",
      "    val_log_marginal: 1183.7259839928668\n",
      "    val_log_joint  : 1392.2351498811142\n",
      "Train Epoch: 1847 [0/101520 (0%)] Loss: -1184.548950\n",
      "Train Epoch: 1847 [11264/101520 (11%)] Loss: -1181.893799\n",
      "Train Epoch: 1847 [22528/101520 (22%)] Loss: -1177.996338\n",
      "Train Epoch: 1847 [33792/101520 (33%)] Loss: -1189.781616\n",
      "Train Epoch: 1847 [45056/101520 (44%)] Loss: -1184.982422\n",
      "Train Epoch: 1847 [56320/101520 (55%)] Loss: -1186.359131\n",
      "Train Epoch: 1847 [67584/101520 (67%)] Loss: -1183.838867\n",
      "Train Epoch: 1847 [78848/101520 (78%)] Loss: -1189.165283\n",
      "Train Epoch: 1847 [90112/101520 (89%)] Loss: -1179.496582\n",
      "Train Epoch: 1847 [101376/101520 (100%)] Loss: -1171.373047\n",
      "    epoch          : 1847\n",
      "    loss           : -1184.8338377679413\n",
      "    ess            : 1.9668167380232309\n",
      "    log_marginal   : 1184.865933058849\n",
      "    log_joint      : 1393.3688811489087\n",
      "    val_loss       : -1184.2816745923913\n",
      "    val_ess        : 1.9695855897405874\n",
      "    val_log_marginal: 1184.3113058338995\n",
      "    val_log_joint  : 1392.712259043818\n",
      "Train Epoch: 1848 [0/101520 (0%)] Loss: -1184.771851\n",
      "Train Epoch: 1848 [11264/101520 (11%)] Loss: -1179.645996\n",
      "Train Epoch: 1848 [22528/101520 (22%)] Loss: -1181.573486\n",
      "Train Epoch: 1848 [33792/101520 (33%)] Loss: -1190.560303\n",
      "Train Epoch: 1848 [45056/101520 (44%)] Loss: -1191.389160\n",
      "Train Epoch: 1848 [56320/101520 (55%)] Loss: -1183.000854\n",
      "Train Epoch: 1848 [67584/101520 (67%)] Loss: -1184.023193\n",
      "Train Epoch: 1848 [78848/101520 (78%)] Loss: -1179.341797\n",
      "Train Epoch: 1848 [90112/101520 (89%)] Loss: -1183.189819\n",
      "Train Epoch: 1848 [101376/101520 (100%)] Loss: -1193.517212\n",
      "    epoch          : 1848\n",
      "    loss           : -1185.0274707276617\n",
      "    ess            : 1.9657798466370933\n",
      "    log_marginal   : 1185.0609339421717\n",
      "    log_joint      : 1393.540725477976\n",
      "    val_loss       : -1181.6698369565217\n",
      "    val_ess        : 1.9680110267970874\n",
      "    val_log_marginal: 1181.7023288892663\n",
      "    val_log_joint  : 1390.0097974694293\n",
      "Train Epoch: 1849 [0/101520 (0%)] Loss: -1185.707642\n",
      "Train Epoch: 1849 [11264/101520 (11%)] Loss: -1187.604004\n",
      "Train Epoch: 1849 [22528/101520 (22%)] Loss: -1183.811035\n",
      "Train Epoch: 1849 [33792/101520 (33%)] Loss: -1187.385498\n",
      "Train Epoch: 1849 [45056/101520 (44%)] Loss: -1181.029297\n",
      "Train Epoch: 1849 [56320/101520 (55%)] Loss: -1174.295898\n",
      "Train Epoch: 1849 [67584/101520 (67%)] Loss: -1188.970093\n",
      "Train Epoch: 1849 [78848/101520 (78%)] Loss: -1184.758301\n",
      "Train Epoch: 1849 [90112/101520 (89%)] Loss: -1181.140869\n",
      "Train Epoch: 1849 [101376/101520 (100%)] Loss: -1196.744019\n",
      "    epoch          : 1849\n",
      "    loss           : -1185.1211176733275\n",
      "    ess            : 1.9652978978564393\n",
      "    log_marginal   : 1185.1548409773477\n",
      "    log_joint      : 1393.5874925162923\n",
      "    val_loss       : -1183.863817297894\n",
      "    val_ess        : 1.967712226121322\n",
      "    val_log_marginal: 1183.8937775985055\n",
      "    val_log_joint  : 1392.453029466712\n",
      "Train Epoch: 1850 [0/101520 (0%)] Loss: -1189.213379\n",
      "Train Epoch: 1850 [11264/101520 (11%)] Loss: -1185.913574\n",
      "Train Epoch: 1850 [22528/101520 (22%)] Loss: -1175.695312\n",
      "Train Epoch: 1850 [33792/101520 (33%)] Loss: -1180.459595\n",
      "Train Epoch: 1850 [45056/101520 (44%)] Loss: -1186.662964\n",
      "Train Epoch: 1850 [56320/101520 (55%)] Loss: -1188.361084\n",
      "Train Epoch: 1850 [67584/101520 (67%)] Loss: -1185.945923\n",
      "Train Epoch: 1850 [78848/101520 (78%)] Loss: -1188.377930\n",
      "Train Epoch: 1850 [90112/101520 (89%)] Loss: -1177.198975\n",
      "Train Epoch: 1850 [101376/101520 (100%)] Loss: -1181.673462\n",
      "    epoch          : 1850\n",
      "    loss           : -1185.0302703704067\n",
      "    ess            : 1.9664824619964139\n",
      "    log_marginal   : 1185.0621209072708\n",
      "    log_joint      : 1393.504369381085\n",
      "    val_loss       : -1183.9365128226902\n",
      "    val_ess        : 1.9650068127590676\n",
      "    val_log_marginal: 1183.9666429602582\n",
      "    val_log_joint  : 1392.8157162873642\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1850.pth ...\n",
      "Train Epoch: 1851 [0/101520 (0%)] Loss: -1182.632568\n",
      "Train Epoch: 1851 [11264/101520 (11%)] Loss: -1188.961060\n",
      "Train Epoch: 1851 [22528/101520 (22%)] Loss: -1185.558350\n",
      "Train Epoch: 1851 [33792/101520 (33%)] Loss: -1181.378174\n",
      "Train Epoch: 1851 [45056/101520 (44%)] Loss: -1188.335205\n",
      "Train Epoch: 1851 [56320/101520 (55%)] Loss: -1189.401855\n",
      "Train Epoch: 1851 [67584/101520 (67%)] Loss: -1189.119385\n",
      "Train Epoch: 1851 [78848/101520 (78%)] Loss: -1186.379883\n",
      "Train Epoch: 1851 [90112/101520 (89%)] Loss: -1173.914551\n",
      "Train Epoch: 1851 [101376/101520 (100%)] Loss: -1175.384155\n",
      "    epoch          : 1851\n",
      "    loss           : -1185.0587121398005\n",
      "    ess            : 1.9658951974993375\n",
      "    log_marginal   : 1185.0914588813207\n",
      "    log_joint      : 1393.5593856734847\n",
      "    val_loss       : -1182.1038977581522\n",
      "    val_ess        : 1.9658825397491455\n",
      "    val_log_marginal: 1182.134617017663\n",
      "    val_log_joint  : 1390.6089981742527\n",
      "Train Epoch: 1852 [0/101520 (0%)] Loss: -1184.621338\n",
      "Train Epoch: 1852 [11264/101520 (11%)] Loss: -1190.978271\n",
      "Train Epoch: 1852 [22528/101520 (22%)] Loss: -1184.492188\n",
      "Train Epoch: 1852 [33792/101520 (33%)] Loss: -1193.014526\n",
      "Train Epoch: 1852 [45056/101520 (44%)] Loss: -1186.717896\n",
      "Train Epoch: 1852 [56320/101520 (55%)] Loss: -1187.177979\n",
      "Train Epoch: 1852 [67584/101520 (67%)] Loss: -1178.701904\n",
      "Train Epoch: 1852 [78848/101520 (78%)] Loss: -1179.700195\n",
      "Train Epoch: 1852 [90112/101520 (89%)] Loss: -1191.250732\n",
      "Train Epoch: 1852 [101376/101520 (100%)] Loss: -1189.732910\n",
      "    epoch          : 1852\n",
      "    loss           : -1185.2216526970792\n",
      "    ess            : 1.9652754218135047\n",
      "    log_marginal   : 1185.2561476817682\n",
      "    log_joint      : 1393.6253502620525\n",
      "    val_loss       : -1183.7403246008832\n",
      "    val_ess        : 1.9635166966396829\n",
      "    val_log_marginal: 1183.77465289572\n",
      "    val_log_joint  : 1392.0710130774457\n",
      "Train Epoch: 1853 [0/101520 (0%)] Loss: -1183.861816\n",
      "Train Epoch: 1853 [11264/101520 (11%)] Loss: -1181.475830\n",
      "Train Epoch: 1853 [22528/101520 (22%)] Loss: -1184.286133\n",
      "Train Epoch: 1853 [33792/101520 (33%)] Loss: -1196.375977\n",
      "Train Epoch: 1853 [45056/101520 (44%)] Loss: -1179.516846\n",
      "Train Epoch: 1853 [56320/101520 (55%)] Loss: -1181.615601\n",
      "Train Epoch: 1853 [67584/101520 (67%)] Loss: -1188.851685\n",
      "Train Epoch: 1853 [78848/101520 (78%)] Loss: -1186.076050\n",
      "Train Epoch: 1853 [90112/101520 (89%)] Loss: -1186.920044\n",
      "Train Epoch: 1853 [101376/101520 (100%)] Loss: -1181.571655\n",
      "    epoch          : 1853\n",
      "    loss           : -1185.080336374254\n",
      "    ess            : 1.96526404361629\n",
      "    log_marginal   : 1185.11429891155\n",
      "    log_joint      : 1393.583375250275\n",
      "    val_loss       : -1184.5414932914402\n",
      "    val_ess        : 1.9682806989421016\n",
      "    val_log_marginal: 1184.57300335428\n",
      "    val_log_joint  : 1393.2445546025815\n",
      "Train Epoch: 1854 [0/101520 (0%)] Loss: -1185.421387\n",
      "Train Epoch: 1854 [11264/101520 (11%)] Loss: -1185.555420\n",
      "Train Epoch: 1854 [22528/101520 (22%)] Loss: -1180.927368\n",
      "Train Epoch: 1854 [33792/101520 (33%)] Loss: -1180.279419\n",
      "Train Epoch: 1854 [45056/101520 (44%)] Loss: -1185.629395\n",
      "Train Epoch: 1854 [56320/101520 (55%)] Loss: -1186.962646\n",
      "Train Epoch: 1854 [67584/101520 (67%)] Loss: -1182.617554\n",
      "Train Epoch: 1854 [78848/101520 (78%)] Loss: -1185.218994\n",
      "Train Epoch: 1854 [90112/101520 (89%)] Loss: -1185.133301\n",
      "Train Epoch: 1854 [101376/101520 (100%)] Loss: -1182.285889\n",
      "    epoch          : 1854\n",
      "    loss           : -1185.1716903609847\n",
      "    ess            : 1.9661945769535236\n",
      "    log_marginal   : 1185.2042837478407\n",
      "    log_joint      : 1393.648886522456\n",
      "    val_loss       : -1183.6835353685462\n",
      "    val_ess        : 1.962401219036268\n",
      "    val_log_marginal: 1183.7166694972825\n",
      "    val_log_joint  : 1391.997409986413\n",
      "Train Epoch: 1855 [0/101520 (0%)] Loss: -1188.161377\n",
      "Train Epoch: 1855 [11264/101520 (11%)] Loss: -1196.987793\n",
      "Train Epoch: 1855 [22528/101520 (22%)] Loss: -1186.541626\n",
      "Train Epoch: 1855 [33792/101520 (33%)] Loss: -1180.980957\n",
      "Train Epoch: 1855 [45056/101520 (44%)] Loss: -1183.122070\n",
      "Train Epoch: 1855 [56320/101520 (55%)] Loss: -1189.199707\n",
      "Train Epoch: 1855 [67584/101520 (67%)] Loss: -1183.863770\n",
      "Train Epoch: 1855 [78848/101520 (78%)] Loss: -1187.170898\n",
      "Train Epoch: 1855 [90112/101520 (89%)] Loss: -1186.014160\n",
      "Train Epoch: 1855 [101376/101520 (100%)] Loss: -1176.704712\n",
      "    epoch          : 1855\n",
      "    loss           : -1185.147763966316\n",
      "    ess            : 1.9658100730809733\n",
      "    log_marginal   : 1185.18089225424\n",
      "    log_joint      : 1393.624100114832\n",
      "    val_loss       : -1184.6264064622962\n",
      "    val_ess        : 1.9651525072429492\n",
      "    val_log_marginal: 1184.662645422894\n",
      "    val_log_joint  : 1393.087593410326\n",
      "Train Epoch: 1856 [0/101520 (0%)] Loss: -1185.743408\n",
      "Train Epoch: 1856 [11264/101520 (11%)] Loss: -1183.461426\n",
      "Train Epoch: 1856 [22528/101520 (22%)] Loss: -1181.557129\n",
      "Train Epoch: 1856 [33792/101520 (33%)] Loss: -1187.871582\n",
      "Train Epoch: 1856 [45056/101520 (44%)] Loss: -1191.010376\n",
      "Train Epoch: 1856 [56320/101520 (55%)] Loss: -1181.133545\n",
      "Train Epoch: 1856 [67584/101520 (67%)] Loss: -1184.421875\n",
      "Train Epoch: 1856 [78848/101520 (78%)] Loss: -1181.083252\n",
      "Train Epoch: 1856 [90112/101520 (89%)] Loss: -1185.351074\n",
      "Train Epoch: 1856 [101376/101520 (100%)] Loss: -1180.423096\n",
      "    epoch          : 1856\n",
      "    loss           : -1185.1665953056297\n",
      "    ess            : 1.9662405917392902\n",
      "    log_marginal   : 1185.1983335868797\n",
      "    log_joint      : 1393.6244485366285\n",
      "    val_loss       : -1183.3461595618207\n",
      "    val_ess        : 1.968337110851122\n",
      "    val_log_marginal: 1183.3792140794837\n",
      "    val_log_joint  : 1392.0872218919837\n",
      "Train Epoch: 1857 [0/101520 (0%)] Loss: -1184.497314\n",
      "Train Epoch: 1857 [11264/101520 (11%)] Loss: -1190.672852\n",
      "Train Epoch: 1857 [22528/101520 (22%)] Loss: -1186.588379\n",
      "Train Epoch: 1857 [33792/101520 (33%)] Loss: -1187.577148\n",
      "Train Epoch: 1857 [45056/101520 (44%)] Loss: -1188.063599\n",
      "Train Epoch: 1857 [56320/101520 (55%)] Loss: -1185.901978\n",
      "Train Epoch: 1857 [67584/101520 (67%)] Loss: -1186.041748\n",
      "Train Epoch: 1857 [78848/101520 (78%)] Loss: -1195.481812\n",
      "Train Epoch: 1857 [90112/101520 (89%)] Loss: -1188.272949\n",
      "Train Epoch: 1857 [101376/101520 (100%)] Loss: -1190.837158\n",
      "    epoch          : 1857\n",
      "    loss           : -1185.264081638662\n",
      "    ess            : 1.9662895010943389\n",
      "    log_marginal   : 1185.296475664455\n",
      "    log_joint      : 1393.668164430551\n",
      "    val_loss       : -1183.184814453125\n",
      "    val_ess        : 1.964133003483648\n",
      "    val_log_marginal: 1183.2219132133152\n",
      "    val_log_joint  : 1391.716993248981\n",
      "Train Epoch: 1858 [0/101520 (0%)] Loss: -1179.015503\n",
      "Train Epoch: 1858 [11264/101520 (11%)] Loss: -1185.674316\n",
      "Train Epoch: 1858 [22528/101520 (22%)] Loss: -1185.242920\n",
      "Train Epoch: 1858 [33792/101520 (33%)] Loss: -1188.545410\n",
      "Train Epoch: 1858 [45056/101520 (44%)] Loss: -1189.081421\n",
      "Train Epoch: 1858 [56320/101520 (55%)] Loss: -1187.859863\n",
      "Train Epoch: 1858 [67584/101520 (67%)] Loss: -1189.467041\n",
      "Train Epoch: 1858 [78848/101520 (78%)] Loss: -1185.748535\n",
      "Train Epoch: 1858 [90112/101520 (89%)] Loss: -1184.932373\n",
      "Train Epoch: 1858 [101376/101520 (100%)] Loss: -1190.755493\n",
      "    epoch          : 1858\n",
      "    loss           : -1185.2707096272377\n",
      "    ess            : 1.9661454561367706\n",
      "    log_marginal   : 1185.302868100267\n",
      "    log_joint      : 1393.729670692329\n",
      "    val_loss       : -1182.2143448539402\n",
      "    val_ess        : 1.9645718180614968\n",
      "    val_log_marginal: 1182.2491295855978\n",
      "    val_log_joint  : 1390.9559379245925\n",
      "Train Epoch: 1859 [0/101520 (0%)] Loss: -1180.542480\n",
      "Train Epoch: 1859 [11264/101520 (11%)] Loss: -1192.187622\n",
      "Train Epoch: 1859 [22528/101520 (22%)] Loss: -1186.105225\n",
      "Train Epoch: 1859 [33792/101520 (33%)] Loss: -1185.264404\n",
      "Train Epoch: 1859 [45056/101520 (44%)] Loss: -1188.841919\n",
      "Train Epoch: 1859 [56320/101520 (55%)] Loss: -1173.138672\n",
      "Train Epoch: 1859 [67584/101520 (67%)] Loss: -1180.230591\n",
      "Train Epoch: 1859 [78848/101520 (78%)] Loss: -1189.595825\n",
      "Train Epoch: 1859 [90112/101520 (89%)] Loss: -1182.566650\n",
      "Train Epoch: 1859 [101376/101520 (100%)] Loss: -1186.230957\n",
      "    epoch          : 1859\n",
      "    loss           : -1185.3136685625393\n",
      "    ess            : 1.9657389117245698\n",
      "    log_marginal   : 1185.347059393648\n",
      "    log_joint      : 1393.738746221341\n",
      "    val_loss       : -1181.2239034901495\n",
      "    val_ess        : 1.9670808263446973\n",
      "    val_log_marginal: 1181.253757642663\n",
      "    val_log_joint  : 1389.570455799932\n",
      "Train Epoch: 1860 [0/101520 (0%)] Loss: -1181.688477\n",
      "Train Epoch: 1860 [11264/101520 (11%)] Loss: -1185.062866\n",
      "Train Epoch: 1860 [22528/101520 (22%)] Loss: -1184.964111\n",
      "Train Epoch: 1860 [33792/101520 (33%)] Loss: -1186.395264\n",
      "Train Epoch: 1860 [45056/101520 (44%)] Loss: -1182.479492\n",
      "Train Epoch: 1860 [56320/101520 (55%)] Loss: -1183.357666\n",
      "Train Epoch: 1860 [67584/101520 (67%)] Loss: -1184.160889\n",
      "Train Epoch: 1860 [78848/101520 (78%)] Loss: -1184.317139\n",
      "Train Epoch: 1860 [90112/101520 (89%)] Loss: -1184.103027\n",
      "Train Epoch: 1860 [101376/101520 (100%)] Loss: -1190.858521\n",
      "    epoch          : 1860\n",
      "    loss           : -1185.4099256045854\n",
      "    ess            : 1.9662446652225514\n",
      "    log_marginal   : 1185.4422969338882\n",
      "    log_joint      : 1393.855479178117\n",
      "    val_loss       : -1183.964987049932\n",
      "    val_ess        : 1.9660156291464101\n",
      "    val_log_marginal: 1183.997946034307\n",
      "    val_log_joint  : 1392.2094355044158\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1860.pth ...\n",
      "Train Epoch: 1861 [0/101520 (0%)] Loss: -1183.515625\n",
      "Train Epoch: 1861 [11264/101520 (11%)] Loss: -1182.576416\n",
      "Train Epoch: 1861 [22528/101520 (22%)] Loss: -1184.871216\n",
      "Train Epoch: 1861 [33792/101520 (33%)] Loss: -1182.923706\n",
      "Train Epoch: 1861 [45056/101520 (44%)] Loss: -1185.389893\n",
      "Train Epoch: 1861 [56320/101520 (55%)] Loss: -1186.032227\n",
      "Train Epoch: 1861 [67584/101520 (67%)] Loss: -1184.856323\n",
      "Train Epoch: 1861 [78848/101520 (78%)] Loss: -1191.238892\n",
      "Train Epoch: 1861 [90112/101520 (89%)] Loss: -1186.215820\n",
      "Train Epoch: 1861 [101376/101520 (100%)] Loss: -1188.329224\n",
      "    epoch          : 1861\n",
      "    loss           : -1185.346504863183\n",
      "    ess            : 1.966018386821651\n",
      "    log_marginal   : 1185.3792374960742\n",
      "    log_joint      : 1393.85084602701\n",
      "    val_loss       : -1185.5380753226902\n",
      "    val_ess        : 1.966184377670288\n",
      "    val_log_marginal: 1185.5711245329483\n",
      "    val_log_joint  : 1393.9467083474865\n",
      "Train Epoch: 1862 [0/101520 (0%)] Loss: -1183.598877\n",
      "Train Epoch: 1862 [11264/101520 (11%)] Loss: -1185.665771\n",
      "Train Epoch: 1862 [22528/101520 (22%)] Loss: -1180.461182\n",
      "Train Epoch: 1862 [33792/101520 (33%)] Loss: -1190.295288\n",
      "Train Epoch: 1862 [45056/101520 (44%)] Loss: -1187.809692\n",
      "Train Epoch: 1862 [56320/101520 (55%)] Loss: -1185.030029\n",
      "Train Epoch: 1862 [67584/101520 (67%)] Loss: -1189.424194\n",
      "Train Epoch: 1862 [78848/101520 (78%)] Loss: -1180.572998\n",
      "Train Epoch: 1862 [90112/101520 (89%)] Loss: -1178.632080\n",
      "Train Epoch: 1862 [101376/101520 (100%)] Loss: -1197.403564\n",
      "    epoch          : 1862\n",
      "    loss           : -1185.493003960231\n",
      "    ess            : 1.9660850841196338\n",
      "    log_marginal   : 1185.5253630211605\n",
      "    log_joint      : 1393.9643585358433\n",
      "    val_loss       : -1185.005418860394\n",
      "    val_ess        : 1.9657917437346086\n",
      "    val_log_marginal: 1185.0359523607337\n",
      "    val_log_joint  : 1393.601413892663\n",
      "Train Epoch: 1863 [0/101520 (0%)] Loss: -1184.058716\n",
      "Train Epoch: 1863 [11264/101520 (11%)] Loss: -1188.976318\n",
      "Train Epoch: 1863 [22528/101520 (22%)] Loss: -1185.391602\n",
      "Train Epoch: 1863 [33792/101520 (33%)] Loss: -1182.221680\n",
      "Train Epoch: 1863 [45056/101520 (44%)] Loss: -1182.604370\n",
      "Train Epoch: 1863 [56320/101520 (55%)] Loss: -1185.126465\n",
      "Train Epoch: 1863 [67584/101520 (67%)] Loss: -1181.840576\n",
      "Train Epoch: 1863 [78848/101520 (78%)] Loss: -1183.621826\n",
      "Train Epoch: 1863 [90112/101520 (89%)] Loss: -1184.818115\n",
      "Train Epoch: 1863 [101376/101520 (100%)] Loss: -1180.033813\n",
      "    epoch          : 1863\n",
      "    loss           : -1185.3722672198885\n",
      "    ess            : 1.9657091135954736\n",
      "    log_marginal   : 1185.4053133097127\n",
      "    log_joint      : 1393.8783296364636\n",
      "    val_loss       : -1184.1350681470788\n",
      "    val_ess        : 1.9637968695682029\n",
      "    val_log_marginal: 1184.177739682405\n",
      "    val_log_joint  : 1392.891792629076\n",
      "Train Epoch: 1864 [0/101520 (0%)] Loss: -1187.633789\n",
      "Train Epoch: 1864 [11264/101520 (11%)] Loss: -1188.045410\n",
      "Train Epoch: 1864 [22528/101520 (22%)] Loss: -1181.210083\n",
      "Train Epoch: 1864 [33792/101520 (33%)] Loss: -1187.020996\n",
      "Train Epoch: 1864 [45056/101520 (44%)] Loss: -1190.835693\n",
      "Train Epoch: 1864 [56320/101520 (55%)] Loss: -1185.505371\n",
      "Train Epoch: 1864 [67584/101520 (67%)] Loss: -1188.635254\n",
      "Train Epoch: 1864 [78848/101520 (78%)] Loss: -1185.532959\n",
      "Train Epoch: 1864 [90112/101520 (89%)] Loss: -1185.001343\n",
      "Train Epoch: 1864 [101376/101520 (100%)] Loss: -1187.305786\n",
      "    epoch          : 1864\n",
      "    loss           : -1185.434960446765\n",
      "    ess            : 1.9667248881641943\n",
      "    log_marginal   : 1185.4662509323964\n",
      "    log_joint      : 1393.938681444331\n",
      "    val_loss       : -1185.8395253057065\n",
      "    val_ess        : 1.9638396397880886\n",
      "    val_log_marginal: 1185.8716987941575\n",
      "    val_log_joint  : 1394.332710597826\n",
      "Train Epoch: 1865 [0/101520 (0%)] Loss: -1193.164551\n",
      "Train Epoch: 1865 [11264/101520 (11%)] Loss: -1192.344727\n",
      "Train Epoch: 1865 [22528/101520 (22%)] Loss: -1183.073364\n",
      "Train Epoch: 1865 [33792/101520 (33%)] Loss: -1183.363281\n",
      "Train Epoch: 1865 [45056/101520 (44%)] Loss: -1184.935181\n",
      "Train Epoch: 1865 [56320/101520 (55%)] Loss: -1187.967529\n",
      "Train Epoch: 1865 [67584/101520 (67%)] Loss: -1182.229980\n",
      "Train Epoch: 1865 [78848/101520 (78%)] Loss: -1183.478027\n",
      "Train Epoch: 1865 [90112/101520 (89%)] Loss: -1186.386230\n",
      "Train Epoch: 1865 [101376/101520 (100%)] Loss: -1186.664062\n",
      "    epoch          : 1865\n",
      "    loss           : -1185.502476984532\n",
      "    ess            : 1.9660470299984343\n",
      "    log_marginal   : 1185.535245195705\n",
      "    log_joint      : 1393.9602007841945\n",
      "    val_loss       : -1186.4232761548913\n",
      "    val_ess        : 1.9636408505232439\n",
      "    val_log_marginal: 1186.4586553158967\n",
      "    val_log_joint  : 1395.2266474184783\n",
      "Train Epoch: 1866 [0/101520 (0%)] Loss: -1186.408936\n",
      "Train Epoch: 1866 [11264/101520 (11%)] Loss: -1180.298096\n",
      "Train Epoch: 1866 [22528/101520 (22%)] Loss: -1182.738770\n",
      "Train Epoch: 1866 [33792/101520 (33%)] Loss: -1185.244629\n",
      "Train Epoch: 1866 [45056/101520 (44%)] Loss: -1194.962524\n",
      "Train Epoch: 1866 [56320/101520 (55%)] Loss: -1188.740845\n",
      "Train Epoch: 1866 [67584/101520 (67%)] Loss: -1185.637451\n",
      "Train Epoch: 1866 [78848/101520 (78%)] Loss: -1187.864868\n",
      "Train Epoch: 1866 [90112/101520 (89%)] Loss: -1185.810059\n",
      "Train Epoch: 1866 [101376/101520 (100%)] Loss: -1170.342285\n",
      "    epoch          : 1866\n",
      "    loss           : -1185.4560270836605\n",
      "    ess            : 1.965870747015105\n",
      "    log_marginal   : 1185.489428956305\n",
      "    log_joint      : 1393.9066370671717\n",
      "    val_loss       : -1185.3912459663723\n",
      "    val_ess        : 1.9695526724276335\n",
      "    val_log_marginal: 1185.4206649116848\n",
      "    val_log_joint  : 1393.946389903193\n",
      "Train Epoch: 1867 [0/101520 (0%)] Loss: -1186.079102\n",
      "Train Epoch: 1867 [11264/101520 (11%)] Loss: -1191.527832\n",
      "Train Epoch: 1867 [22528/101520 (22%)] Loss: -1179.447021\n",
      "Train Epoch: 1867 [33792/101520 (33%)] Loss: -1183.925293\n",
      "Train Epoch: 1867 [45056/101520 (44%)] Loss: -1186.051025\n",
      "Train Epoch: 1867 [56320/101520 (55%)] Loss: -1186.963257\n",
      "Train Epoch: 1867 [67584/101520 (67%)] Loss: -1188.528320\n",
      "Train Epoch: 1867 [78848/101520 (78%)] Loss: -1191.683350\n",
      "Train Epoch: 1867 [90112/101520 (89%)] Loss: -1183.652100\n",
      "Train Epoch: 1867 [101376/101520 (100%)] Loss: -1187.172363\n",
      "    epoch          : 1867\n",
      "    loss           : -1185.5005962429334\n",
      "    ess            : 1.966443239743985\n",
      "    log_marginal   : 1185.5324927861966\n",
      "    log_joint      : 1393.9778703085742\n",
      "    val_loss       : -1185.1788436226223\n",
      "    val_ess        : 1.96563662653384\n",
      "    val_log_marginal: 1185.210592518682\n",
      "    val_log_joint  : 1393.7264988111413\n",
      "Train Epoch: 1868 [0/101520 (0%)] Loss: -1186.339966\n",
      "Train Epoch: 1868 [11264/101520 (11%)] Loss: -1170.468628\n",
      "Train Epoch: 1868 [22528/101520 (22%)] Loss: -1179.340088\n",
      "Train Epoch: 1868 [33792/101520 (33%)] Loss: -1177.741455\n",
      "Train Epoch: 1868 [45056/101520 (44%)] Loss: -1188.393555\n",
      "Train Epoch: 1868 [56320/101520 (55%)] Loss: -1186.137207\n",
      "Train Epoch: 1868 [67584/101520 (67%)] Loss: -1189.626465\n",
      "Train Epoch: 1868 [78848/101520 (78%)] Loss: -1182.650635\n",
      "Train Epoch: 1868 [90112/101520 (89%)] Loss: -1186.604858\n",
      "Train Epoch: 1868 [101376/101520 (100%)] Loss: -1179.791504\n",
      "    epoch          : 1868\n",
      "    loss           : -1185.5295520571608\n",
      "    ess            : 1.966502653893514\n",
      "    log_marginal   : 1185.5608192328832\n",
      "    log_joint      : 1393.9676605684674\n",
      "    val_loss       : -1186.1106222401495\n",
      "    val_ess        : 1.9677646678426992\n",
      "    val_log_marginal: 1186.1415644106658\n",
      "    val_log_joint  : 1394.7112081776495\n",
      "Train Epoch: 1869 [0/101520 (0%)] Loss: -1182.082397\n",
      "Train Epoch: 1869 [11264/101520 (11%)] Loss: -1186.937012\n",
      "Train Epoch: 1869 [22528/101520 (22%)] Loss: -1193.515625\n",
      "Train Epoch: 1869 [33792/101520 (33%)] Loss: -1184.925781\n",
      "Train Epoch: 1869 [45056/101520 (44%)] Loss: -1185.526611\n",
      "Train Epoch: 1869 [56320/101520 (55%)] Loss: -1187.591797\n",
      "Train Epoch: 1869 [67584/101520 (67%)] Loss: -1185.639648\n",
      "Train Epoch: 1869 [78848/101520 (78%)] Loss: -1180.505005\n",
      "Train Epoch: 1869 [90112/101520 (89%)] Loss: -1186.620483\n",
      "Train Epoch: 1869 [101376/101520 (100%)] Loss: -1189.084473\n",
      "    epoch          : 1869\n",
      "    loss           : -1185.5861080303864\n",
      "    ess            : 1.965523064436026\n",
      "    log_marginal   : 1185.6198006634736\n",
      "    log_joint      : 1394.0460260285804\n",
      "    val_loss       : -1183.1955778702445\n",
      "    val_ess        : 1.9640777214713718\n",
      "    val_log_marginal: 1183.2314081606658\n",
      "    val_log_joint  : 1391.7142917798913\n",
      "Train Epoch: 1870 [0/101520 (0%)] Loss: -1185.185791\n",
      "Train Epoch: 1870 [11264/101520 (11%)] Loss: -1190.134155\n",
      "Train Epoch: 1870 [22528/101520 (22%)] Loss: -1183.062378\n",
      "Train Epoch: 1870 [33792/101520 (33%)] Loss: -1190.439087\n",
      "Train Epoch: 1870 [45056/101520 (44%)] Loss: -1185.955566\n",
      "Train Epoch: 1870 [56320/101520 (55%)] Loss: -1188.091064\n",
      "Train Epoch: 1870 [67584/101520 (67%)] Loss: -1183.910278\n",
      "Train Epoch: 1870 [78848/101520 (78%)] Loss: -1179.677979\n",
      "Train Epoch: 1870 [90112/101520 (89%)] Loss: -1179.261597\n",
      "Train Epoch: 1870 [101376/101520 (100%)] Loss: -1177.184692\n",
      "    epoch          : 1870\n",
      "    loss           : -1185.4924598578832\n",
      "    ess            : 1.966400193808666\n",
      "    log_marginal   : 1185.524499327693\n",
      "    log_joint      : 1393.9994270669756\n",
      "    val_loss       : -1184.673286769701\n",
      "    val_ess        : 1.9653962902400806\n",
      "    val_log_marginal: 1184.7054018766983\n",
      "    val_log_joint  : 1393.4270868716033\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1870.pth ...\n",
      "Train Epoch: 1871 [0/101520 (0%)] Loss: -1187.758545\n",
      "Train Epoch: 1871 [11264/101520 (11%)] Loss: -1184.845581\n",
      "Train Epoch: 1871 [22528/101520 (22%)] Loss: -1186.106934\n",
      "Train Epoch: 1871 [33792/101520 (33%)] Loss: -1182.445801\n",
      "Train Epoch: 1871 [45056/101520 (44%)] Loss: -1185.466919\n",
      "Train Epoch: 1871 [56320/101520 (55%)] Loss: -1182.368774\n",
      "Train Epoch: 1871 [67584/101520 (67%)] Loss: -1191.625244\n",
      "Train Epoch: 1871 [78848/101520 (78%)] Loss: -1191.157104\n",
      "Train Epoch: 1871 [90112/101520 (89%)] Loss: -1189.651733\n",
      "Train Epoch: 1871 [101376/101520 (100%)] Loss: -1173.769043\n",
      "    epoch          : 1871\n",
      "    loss           : -1185.5784586997488\n",
      "    ess            : 1.9661503604908086\n",
      "    log_marginal   : 1185.6113428470478\n",
      "    log_joint      : 1394.0583471557004\n",
      "    val_loss       : -1185.0247802734375\n",
      "    val_ess        : 1.964617133140564\n",
      "    val_log_marginal: 1185.0580205502717\n",
      "    val_log_joint  : 1393.3699101987092\n",
      "Train Epoch: 1872 [0/101520 (0%)] Loss: -1185.800293\n",
      "Train Epoch: 1872 [11264/101520 (11%)] Loss: -1183.369507\n",
      "Train Epoch: 1872 [22528/101520 (22%)] Loss: -1190.487061\n",
      "Train Epoch: 1872 [33792/101520 (33%)] Loss: -1187.008545\n",
      "Train Epoch: 1872 [45056/101520 (44%)] Loss: -1185.439941\n",
      "Train Epoch: 1872 [56320/101520 (55%)] Loss: -1183.679688\n",
      "Train Epoch: 1872 [67584/101520 (67%)] Loss: -1184.460693\n",
      "Train Epoch: 1872 [78848/101520 (78%)] Loss: -1192.855713\n",
      "Train Epoch: 1872 [90112/101520 (89%)] Loss: -1189.948975\n",
      "Train Epoch: 1872 [101376/101520 (100%)] Loss: -1179.094604\n",
      "    epoch          : 1872\n",
      "    loss           : -1185.5717926792163\n",
      "    ess            : 1.9661963447254507\n",
      "    log_marginal   : 1185.6039615803627\n",
      "    log_joint      : 1394.067120882734\n",
      "    val_loss       : -1182.53394085428\n",
      "    val_ess        : 1.967809392058331\n",
      "    val_log_marginal: 1182.5634765625\n",
      "    val_log_joint  : 1391.10205078125\n",
      "Train Epoch: 1873 [0/101520 (0%)] Loss: -1175.437500\n",
      "Train Epoch: 1873 [11264/101520 (11%)] Loss: -1185.185913\n",
      "Train Epoch: 1873 [22528/101520 (22%)] Loss: -1185.599487\n",
      "Train Epoch: 1873 [33792/101520 (33%)] Loss: -1198.363403\n",
      "Train Epoch: 1873 [45056/101520 (44%)] Loss: -1190.626099\n",
      "Train Epoch: 1873 [56320/101520 (55%)] Loss: -1187.550781\n",
      "Train Epoch: 1873 [67584/101520 (67%)] Loss: -1183.352173\n",
      "Train Epoch: 1873 [78848/101520 (78%)] Loss: -1184.331787\n",
      "Train Epoch: 1873 [90112/101520 (89%)] Loss: -1185.780518\n",
      "Train Epoch: 1873 [101376/101520 (100%)] Loss: -1184.496094\n",
      "    epoch          : 1873\n",
      "    loss           : -1185.652527162178\n",
      "    ess            : 1.9661134092052976\n",
      "    log_marginal   : 1185.6860284086447\n",
      "    log_joint      : 1394.0737169735396\n",
      "    val_loss       : -1183.4829844599185\n",
      "    val_ess        : 1.966915488243103\n",
      "    val_log_marginal: 1183.516405188519\n",
      "    val_log_joint  : 1391.9342041015625\n",
      "Train Epoch: 1874 [0/101520 (0%)] Loss: -1192.390625\n",
      "Train Epoch: 1874 [11264/101520 (11%)] Loss: -1180.903198\n",
      "Train Epoch: 1874 [22528/101520 (22%)] Loss: -1187.374023\n",
      "Train Epoch: 1874 [33792/101520 (33%)] Loss: -1188.946289\n",
      "Train Epoch: 1874 [45056/101520 (44%)] Loss: -1193.039307\n",
      "Train Epoch: 1874 [56320/101520 (55%)] Loss: -1189.873047\n",
      "Train Epoch: 1874 [67584/101520 (67%)] Loss: -1180.463379\n",
      "Train Epoch: 1874 [78848/101520 (78%)] Loss: -1182.916016\n",
      "Train Epoch: 1874 [90112/101520 (89%)] Loss: -1180.696167\n",
      "Train Epoch: 1874 [101376/101520 (100%)] Loss: -1187.187134\n",
      "    epoch          : 1874\n",
      "    loss           : -1185.6030248900754\n",
      "    ess            : 1.96603037064998\n",
      "    log_marginal   : 1185.6354078743327\n",
      "    log_joint      : 1394.139489562068\n",
      "    val_loss       : -1185.7381432574728\n",
      "    val_ess        : 1.9649312599845554\n",
      "    val_log_marginal: 1185.7700619904892\n",
      "    val_log_joint  : 1394.1148097826087\n",
      "Train Epoch: 1875 [0/101520 (0%)] Loss: -1184.647583\n",
      "Train Epoch: 1875 [11264/101520 (11%)] Loss: -1182.863403\n",
      "Train Epoch: 1875 [22528/101520 (22%)] Loss: -1190.045532\n",
      "Train Epoch: 1875 [33792/101520 (33%)] Loss: -1190.466553\n",
      "Train Epoch: 1875 [45056/101520 (44%)] Loss: -1189.287598\n",
      "Train Epoch: 1875 [56320/101520 (55%)] Loss: -1183.168457\n",
      "Train Epoch: 1875 [67584/101520 (67%)] Loss: -1180.207764\n",
      "Train Epoch: 1875 [78848/101520 (78%)] Loss: -1190.457397\n",
      "Train Epoch: 1875 [90112/101520 (89%)] Loss: -1189.036133\n",
      "Train Epoch: 1875 [101376/101520 (100%)] Loss: -1181.145020\n",
      "    epoch          : 1875\n",
      "    loss           : -1185.6658309859847\n",
      "    ess            : 1.9660990226208863\n",
      "    log_marginal   : 1185.6989304432318\n",
      "    log_joint      : 1394.141993537021\n",
      "    val_loss       : -1185.303955078125\n",
      "    val_ess        : 1.963140176690143\n",
      "    val_log_marginal: 1185.3456341287365\n",
      "    val_log_joint  : 1394.0217975118885\n",
      "Train Epoch: 1876 [0/101520 (0%)] Loss: -1184.939087\n",
      "Train Epoch: 1876 [11264/101520 (11%)] Loss: -1183.006714\n",
      "Train Epoch: 1876 [22528/101520 (22%)] Loss: -1185.804932\n",
      "Train Epoch: 1876 [33792/101520 (33%)] Loss: -1190.739014\n",
      "Train Epoch: 1876 [45056/101520 (44%)] Loss: -1186.152710\n",
      "Train Epoch: 1876 [56320/101520 (55%)] Loss: -1186.527832\n",
      "Train Epoch: 1876 [67584/101520 (67%)] Loss: -1184.642334\n",
      "Train Epoch: 1876 [78848/101520 (78%)] Loss: -1187.388062\n",
      "Train Epoch: 1876 [90112/101520 (89%)] Loss: -1181.885010\n",
      "Train Epoch: 1876 [101376/101520 (100%)] Loss: -1194.955566\n",
      "    epoch          : 1876\n",
      "    loss           : -1185.7048793773556\n",
      "    ess            : 1.9657780884498328\n",
      "    log_marginal   : 1185.7372734031485\n",
      "    log_joint      : 1394.211016017588\n",
      "    val_loss       : -1186.931003736413\n",
      "    val_ess        : 1.9661400991937388\n",
      "    val_log_marginal: 1186.9630286175272\n",
      "    val_log_joint  : 1395.299220872962\n",
      "Train Epoch: 1877 [0/101520 (0%)] Loss: -1186.627808\n",
      "Train Epoch: 1877 [11264/101520 (11%)] Loss: -1182.412231\n",
      "Train Epoch: 1877 [22528/101520 (22%)] Loss: -1186.669189\n",
      "Train Epoch: 1877 [33792/101520 (33%)] Loss: -1178.802002\n",
      "Train Epoch: 1877 [45056/101520 (44%)] Loss: -1188.981934\n",
      "Train Epoch: 1877 [56320/101520 (55%)] Loss: -1189.705688\n",
      "Train Epoch: 1877 [67584/101520 (67%)] Loss: -1191.066040\n",
      "Train Epoch: 1877 [78848/101520 (78%)] Loss: -1186.215210\n",
      "Train Epoch: 1877 [90112/101520 (89%)] Loss: -1181.947144\n",
      "Train Epoch: 1877 [101376/101520 (100%)] Loss: -1180.801758\n",
      "    epoch          : 1877\n",
      "    loss           : -1185.7615377914965\n",
      "    ess            : 1.9656223064690979\n",
      "    log_marginal   : 1185.795176443742\n",
      "    log_joint      : 1394.1952260079695\n",
      "    val_loss       : -1186.596186098845\n",
      "    val_ess        : 1.967205638470857\n",
      "    val_log_marginal: 1186.6266240658967\n",
      "    val_log_joint  : 1394.9441979449728\n",
      "Train Epoch: 1878 [0/101520 (0%)] Loss: -1178.852783\n",
      "Train Epoch: 1878 [11264/101520 (11%)] Loss: -1191.052246\n",
      "Train Epoch: 1878 [22528/101520 (22%)] Loss: -1186.400391\n",
      "Train Epoch: 1878 [33792/101520 (33%)] Loss: -1186.171387\n",
      "Train Epoch: 1878 [45056/101520 (44%)] Loss: -1192.217041\n",
      "Train Epoch: 1878 [56320/101520 (55%)] Loss: -1188.489014\n",
      "Train Epoch: 1878 [67584/101520 (67%)] Loss: -1186.221680\n",
      "Train Epoch: 1878 [78848/101520 (78%)] Loss: -1174.370361\n",
      "Train Epoch: 1878 [90112/101520 (89%)] Loss: -1195.224854\n",
      "Train Epoch: 1878 [101376/101520 (100%)] Loss: -1176.401611\n",
      "    epoch          : 1878\n",
      "    loss           : -1185.740449684948\n",
      "    ess            : 1.9651425597655714\n",
      "    log_marginal   : 1185.7753427783448\n",
      "    log_joint      : 1394.199578826751\n",
      "    val_loss       : -1184.978075110394\n",
      "    val_ess        : 1.9630550146102905\n",
      "    val_log_marginal: 1185.0134595788043\n",
      "    val_log_joint  : 1393.5833740234375\n",
      "Train Epoch: 1879 [0/101520 (0%)] Loss: -1187.087891\n",
      "Train Epoch: 1879 [11264/101520 (11%)] Loss: -1189.433594\n",
      "Train Epoch: 1879 [22528/101520 (22%)] Loss: -1187.808105\n",
      "Train Epoch: 1879 [33792/101520 (33%)] Loss: -1191.258789\n",
      "Train Epoch: 1879 [45056/101520 (44%)] Loss: -1186.038452\n",
      "Train Epoch: 1879 [56320/101520 (55%)] Loss: -1178.421143\n",
      "Train Epoch: 1879 [67584/101520 (67%)] Loss: -1183.575195\n",
      "Train Epoch: 1879 [78848/101520 (78%)] Loss: -1192.008179\n",
      "Train Epoch: 1879 [90112/101520 (89%)] Loss: -1183.668945\n",
      "Train Epoch: 1879 [101376/101520 (100%)] Loss: -1178.597046\n",
      "    epoch          : 1879\n",
      "    loss           : -1185.759858251217\n",
      "    ess            : 1.9662260056740075\n",
      "    log_marginal   : 1185.7928362515704\n",
      "    log_joint      : 1394.2616518873665\n",
      "    val_loss       : -1185.406106700068\n",
      "    val_ess        : 1.9695818890695986\n",
      "    val_log_marginal: 1185.434225331182\n",
      "    val_log_joint  : 1393.6100437330163\n",
      "Train Epoch: 1880 [0/101520 (0%)] Loss: -1189.888916\n",
      "Train Epoch: 1880 [11264/101520 (11%)] Loss: -1191.988159\n",
      "Train Epoch: 1880 [22528/101520 (22%)] Loss: -1180.381958\n",
      "Train Epoch: 1880 [33792/101520 (33%)] Loss: -1186.343384\n",
      "Train Epoch: 1880 [45056/101520 (44%)] Loss: -1184.827148\n",
      "Train Epoch: 1880 [56320/101520 (55%)] Loss: -1183.913208\n",
      "Train Epoch: 1880 [67584/101520 (67%)] Loss: -1177.309692\n",
      "Train Epoch: 1880 [78848/101520 (78%)] Loss: -1185.229614\n",
      "Train Epoch: 1880 [90112/101520 (89%)] Loss: -1184.548340\n",
      "Train Epoch: 1880 [101376/101520 (100%)] Loss: -1197.490479\n",
      "    epoch          : 1880\n",
      "    loss           : -1185.8853569605842\n",
      "    ess            : 1.966484170463217\n",
      "    log_marginal   : 1185.917653452811\n",
      "    log_joint      : 1394.3310467130575\n",
      "    val_loss       : -1185.8719163977582\n",
      "    val_ess        : 1.9664596578349238\n",
      "    val_log_marginal: 1185.9029912533967\n",
      "    val_log_joint  : 1393.846536387568\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1880.pth ...\n",
      "Train Epoch: 1881 [0/101520 (0%)] Loss: -1187.223145\n",
      "Train Epoch: 1881 [11264/101520 (11%)] Loss: -1182.871826\n",
      "Train Epoch: 1881 [22528/101520 (22%)] Loss: -1185.089966\n",
      "Train Epoch: 1881 [33792/101520 (33%)] Loss: -1181.318115\n",
      "Train Epoch: 1881 [45056/101520 (44%)] Loss: -1183.584106\n",
      "Train Epoch: 1881 [56320/101520 (55%)] Loss: -1183.507324\n",
      "Train Epoch: 1881 [67584/101520 (67%)] Loss: -1184.283203\n",
      "Train Epoch: 1881 [78848/101520 (78%)] Loss: -1190.658936\n",
      "Train Epoch: 1881 [90112/101520 (89%)] Loss: -1184.420410\n",
      "Train Epoch: 1881 [101376/101520 (100%)] Loss: -1195.320435\n",
      "    epoch          : 1881\n",
      "    loss           : -1185.8743074503377\n",
      "    ess            : 1.9658399724481095\n",
      "    log_marginal   : 1185.9074737702183\n",
      "    log_joint      : 1394.354846743483\n",
      "    val_loss       : -1183.6746879245925\n",
      "    val_ess        : 1.96852169347846\n",
      "    val_log_marginal: 1183.7055557914402\n",
      "    val_log_joint  : 1392.2557426120925\n",
      "Train Epoch: 1882 [0/101520 (0%)] Loss: -1180.909668\n",
      "Train Epoch: 1882 [11264/101520 (11%)] Loss: -1187.879517\n",
      "Train Epoch: 1882 [22528/101520 (22%)] Loss: -1185.363281\n",
      "Train Epoch: 1882 [33792/101520 (33%)] Loss: -1184.152100\n",
      "Train Epoch: 1882 [45056/101520 (44%)] Loss: -1180.958862\n",
      "Train Epoch: 1882 [56320/101520 (55%)] Loss: -1183.047363\n",
      "Train Epoch: 1882 [67584/101520 (67%)] Loss: -1191.472412\n",
      "Train Epoch: 1882 [78848/101520 (78%)] Loss: -1182.798096\n",
      "Train Epoch: 1882 [90112/101520 (89%)] Loss: -1182.379150\n",
      "Train Epoch: 1882 [101376/101520 (100%)] Loss: -1172.230713\n",
      "    epoch          : 1882\n",
      "    loss           : -1185.8477709592887\n",
      "    ess            : 1.9666371171797938\n",
      "    log_marginal   : 1185.8798147230293\n",
      "    log_joint      : 1394.3212902893374\n",
      "    val_loss       : -1183.4726456351902\n",
      "    val_ess        : 1.9663428016330884\n",
      "    val_log_marginal: 1183.5042830757473\n",
      "    val_log_joint  : 1391.9286684782608\n",
      "Train Epoch: 1883 [0/101520 (0%)] Loss: -1186.539307\n",
      "Train Epoch: 1883 [11264/101520 (11%)] Loss: -1185.933472\n",
      "Train Epoch: 1883 [22528/101520 (22%)] Loss: -1181.954102\n",
      "Train Epoch: 1883 [33792/101520 (33%)] Loss: -1183.780762\n",
      "Train Epoch: 1883 [45056/101520 (44%)] Loss: -1179.629395\n",
      "Train Epoch: 1883 [56320/101520 (55%)] Loss: -1191.309082\n",
      "Train Epoch: 1883 [67584/101520 (67%)] Loss: -1188.645508\n",
      "Train Epoch: 1883 [78848/101520 (78%)] Loss: -1186.701172\n",
      "Train Epoch: 1883 [90112/101520 (89%)] Loss: -1185.862061\n",
      "Train Epoch: 1883 [101376/101520 (100%)] Loss: -1184.588501\n",
      "    epoch          : 1883\n",
      "    loss           : -1185.881964141999\n",
      "    ess            : 1.966032286385196\n",
      "    log_marginal   : 1185.9156027942447\n",
      "    log_joint      : 1394.3511956756438\n",
      "    val_loss       : -1182.9662449048913\n",
      "    val_ess        : 1.9665055171303127\n",
      "    val_log_marginal: 1182.9988482931385\n",
      "    val_log_joint  : 1391.197021484375\n",
      "Train Epoch: 1884 [0/101520 (0%)] Loss: -1184.284668\n",
      "Train Epoch: 1884 [11264/101520 (11%)] Loss: -1183.423828\n",
      "Train Epoch: 1884 [22528/101520 (22%)] Loss: -1181.876709\n",
      "Train Epoch: 1884 [33792/101520 (33%)] Loss: -1181.261230\n",
      "Train Epoch: 1884 [45056/101520 (44%)] Loss: -1186.529663\n",
      "Train Epoch: 1884 [56320/101520 (55%)] Loss: -1186.039307\n",
      "Train Epoch: 1884 [67584/101520 (67%)] Loss: -1183.959229\n",
      "Train Epoch: 1884 [78848/101520 (78%)] Loss: -1193.957764\n",
      "Train Epoch: 1884 [90112/101520 (89%)] Loss: -1183.861816\n",
      "Train Epoch: 1884 [101376/101520 (100%)] Loss: -1193.816040\n",
      "    epoch          : 1884\n",
      "    loss           : -1185.9339495328204\n",
      "    ess            : 1.9658509265238315\n",
      "    log_marginal   : 1185.967720070077\n",
      "    log_joint      : 1394.428212228133\n",
      "    val_loss       : -1184.875005307405\n",
      "    val_ess        : 1.9674190749292788\n",
      "    val_log_marginal: 1184.9060377038043\n",
      "    val_log_joint  : 1393.0369289232337\n",
      "Train Epoch: 1885 [0/101520 (0%)] Loss: -1188.040771\n",
      "Train Epoch: 1885 [11264/101520 (11%)] Loss: -1191.090088\n",
      "Train Epoch: 1885 [22528/101520 (22%)] Loss: -1185.277832\n",
      "Train Epoch: 1885 [33792/101520 (33%)] Loss: -1183.099121\n",
      "Train Epoch: 1885 [45056/101520 (44%)] Loss: -1187.830444\n",
      "Train Epoch: 1885 [56320/101520 (55%)] Loss: -1187.454102\n",
      "Train Epoch: 1885 [67584/101520 (67%)] Loss: -1186.110352\n",
      "Train Epoch: 1885 [78848/101520 (78%)] Loss: -1189.396362\n",
      "Train Epoch: 1885 [90112/101520 (89%)] Loss: -1181.130615\n",
      "Train Epoch: 1885 [101376/101520 (100%)] Loss: -1174.937744\n",
      "    epoch          : 1885\n",
      "    loss           : -1185.8160633489715\n",
      "    ess            : 1.9658259590666498\n",
      "    log_marginal   : 1185.849325975581\n",
      "    log_joint      : 1394.3210847940877\n",
      "    val_loss       : -1183.964350161345\n",
      "    val_ess        : 1.967401105424632\n",
      "    val_log_marginal: 1183.995552394701\n",
      "    val_log_joint  : 1392.2311799422555\n",
      "Train Epoch: 1886 [0/101520 (0%)] Loss: -1187.841187\n",
      "Train Epoch: 1886 [11264/101520 (11%)] Loss: -1195.020142\n",
      "Train Epoch: 1886 [22528/101520 (22%)] Loss: -1187.898682\n",
      "Train Epoch: 1886 [33792/101520 (33%)] Loss: -1180.172974\n",
      "Train Epoch: 1886 [45056/101520 (44%)] Loss: -1182.328979\n",
      "Train Epoch: 1886 [56320/101520 (55%)] Loss: -1184.327881\n",
      "Train Epoch: 1886 [67584/101520 (67%)] Loss: -1189.661865\n",
      "Train Epoch: 1886 [78848/101520 (78%)] Loss: -1189.168701\n",
      "Train Epoch: 1886 [90112/101520 (89%)] Loss: -1185.245850\n",
      "Train Epoch: 1886 [101376/101520 (100%)] Loss: -1184.695557\n",
      "    epoch          : 1886\n",
      "    loss           : -1185.902308171718\n",
      "    ess            : 1.9662176897777386\n",
      "    log_marginal   : 1185.9342004210505\n",
      "    log_joint      : 1394.3798944674545\n",
      "    val_loss       : -1184.2877621858017\n",
      "    val_ess        : 1.963275406671607\n",
      "    val_log_marginal: 1184.3231041949728\n",
      "    val_log_joint  : 1392.5066554857337\n",
      "Train Epoch: 1887 [0/101520 (0%)] Loss: -1188.279297\n",
      "Train Epoch: 1887 [11264/101520 (11%)] Loss: -1184.065186\n",
      "Train Epoch: 1887 [22528/101520 (22%)] Loss: -1192.126221\n",
      "Train Epoch: 1887 [33792/101520 (33%)] Loss: -1192.769043\n",
      "Train Epoch: 1887 [45056/101520 (44%)] Loss: -1181.637695\n",
      "Train Epoch: 1887 [56320/101520 (55%)] Loss: -1187.849243\n",
      "Train Epoch: 1887 [67584/101520 (67%)] Loss: -1182.492432\n",
      "Train Epoch: 1887 [78848/101520 (78%)] Loss: -1192.699585\n",
      "Train Epoch: 1887 [90112/101520 (89%)] Loss: -1184.034790\n",
      "Train Epoch: 1887 [101376/101520 (100%)] Loss: -1181.508789\n",
      "    epoch          : 1887\n",
      "    loss           : -1185.901876938403\n",
      "    ess            : 1.9654489647802995\n",
      "    log_marginal   : 1185.9355996290044\n",
      "    log_joint      : 1394.3817347234217\n",
      "    val_loss       : -1184.5712306810462\n",
      "    val_ess        : 1.96619423057722\n",
      "    val_log_marginal: 1184.6027513586957\n",
      "    val_log_joint  : 1393.352491295856\n",
      "Train Epoch: 1888 [0/101520 (0%)] Loss: -1184.436890\n",
      "Train Epoch: 1888 [11264/101520 (11%)] Loss: -1195.953003\n",
      "Train Epoch: 1888 [22528/101520 (22%)] Loss: -1188.496338\n",
      "Train Epoch: 1888 [33792/101520 (33%)] Loss: -1187.628540\n",
      "Train Epoch: 1888 [45056/101520 (44%)] Loss: -1187.635010\n",
      "Train Epoch: 1888 [56320/101520 (55%)] Loss: -1192.125488\n",
      "Train Epoch: 1888 [67584/101520 (67%)] Loss: -1189.587402\n",
      "Train Epoch: 1888 [78848/101520 (78%)] Loss: -1185.340820\n",
      "Train Epoch: 1888 [90112/101520 (89%)] Loss: -1182.006226\n",
      "Train Epoch: 1888 [101376/101520 (100%)] Loss: -1180.451538\n",
      "    epoch          : 1888\n",
      "    loss           : -1185.968995980881\n",
      "    ess            : 1.965798020362854\n",
      "    log_marginal   : 1186.001650096184\n",
      "    log_joint      : 1394.4423601160097\n",
      "    val_loss       : -1184.8414625084918\n",
      "    val_ess        : 1.9670736012251482\n",
      "    val_log_marginal: 1184.8719482421875\n",
      "    val_log_joint  : 1393.3703294836957\n",
      "Train Epoch: 1889 [0/101520 (0%)] Loss: -1185.077881\n",
      "Train Epoch: 1889 [11264/101520 (11%)] Loss: -1181.220703\n",
      "Train Epoch: 1889 [22528/101520 (22%)] Loss: -1180.398926\n",
      "Train Epoch: 1889 [33792/101520 (33%)] Loss: -1188.872192\n",
      "Train Epoch: 1889 [45056/101520 (44%)] Loss: -1183.585815\n",
      "Train Epoch: 1889 [56320/101520 (55%)] Loss: -1185.789551\n",
      "Train Epoch: 1889 [67584/101520 (67%)] Loss: -1186.694702\n",
      "Train Epoch: 1889 [78848/101520 (78%)] Loss: -1182.011108\n",
      "Train Epoch: 1889 [90112/101520 (89%)] Loss: -1186.933472\n",
      "Train Epoch: 1889 [101376/101520 (100%)] Loss: -1203.624390\n",
      "    epoch          : 1889\n",
      "    loss           : -1186.020407825259\n",
      "    ess            : 1.9664894144738738\n",
      "    log_marginal   : 1186.0524196912295\n",
      "    log_joint      : 1394.5116788777873\n",
      "    val_loss       : -1185.2959886633832\n",
      "    val_ess        : 1.9647512798723967\n",
      "    val_log_marginal: 1185.3305027173913\n",
      "    val_log_joint  : 1393.758544921875\n",
      "Train Epoch: 1890 [0/101520 (0%)] Loss: -1188.995361\n",
      "Train Epoch: 1890 [11264/101520 (11%)] Loss: -1188.236694\n",
      "Train Epoch: 1890 [22528/101520 (22%)] Loss: -1181.174805\n",
      "Train Epoch: 1890 [33792/101520 (33%)] Loss: -1193.466064\n",
      "Train Epoch: 1890 [45056/101520 (44%)] Loss: -1188.750977\n",
      "Train Epoch: 1890 [56320/101520 (55%)] Loss: -1185.976074\n",
      "Train Epoch: 1890 [67584/101520 (67%)] Loss: -1191.407471\n",
      "Train Epoch: 1890 [78848/101520 (78%)] Loss: -1180.042969\n",
      "Train Epoch: 1890 [90112/101520 (89%)] Loss: -1188.430298\n",
      "Train Epoch: 1890 [101376/101520 (100%)] Loss: -1202.401489\n",
      "    epoch          : 1890\n",
      "    loss           : -1186.185206427646\n",
      "    ess            : 1.9662689443808705\n",
      "    log_marginal   : 1186.218129833739\n",
      "    log_joint      : 1394.557812868051\n",
      "    val_loss       : -1184.125100840693\n",
      "    val_ess        : 1.9663160728371663\n",
      "    val_log_marginal: 1184.158887780231\n",
      "    val_log_joint  : 1392.4420325237772\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1890.pth ...\n",
      "Train Epoch: 1891 [0/101520 (0%)] Loss: -1186.158691\n",
      "Train Epoch: 1891 [11264/101520 (11%)] Loss: -1185.814209\n",
      "Train Epoch: 1891 [22528/101520 (22%)] Loss: -1178.119873\n",
      "Train Epoch: 1891 [33792/101520 (33%)] Loss: -1192.252075\n",
      "Train Epoch: 1891 [45056/101520 (44%)] Loss: -1190.337402\n",
      "Train Epoch: 1891 [56320/101520 (55%)] Loss: -1183.560669\n",
      "Train Epoch: 1891 [67584/101520 (67%)] Loss: -1183.082031\n",
      "Train Epoch: 1891 [78848/101520 (78%)] Loss: -1198.949219\n",
      "Train Epoch: 1891 [90112/101520 (89%)] Loss: -1185.447388\n",
      "Train Epoch: 1891 [101376/101520 (100%)] Loss: -1182.223145\n",
      "    epoch          : 1891\n",
      "    loss           : -1185.952989434477\n",
      "    ess            : 1.9656840443012102\n",
      "    log_marginal   : 1185.9863753582365\n",
      "    log_joint      : 1394.4091882753612\n",
      "    val_loss       : -1185.4305419921875\n",
      "    val_ess        : 1.9692518348279207\n",
      "    val_log_marginal: 1185.4594620414402\n",
      "    val_log_joint  : 1394.0697870669158\n",
      "Train Epoch: 1892 [0/101520 (0%)] Loss: -1186.996582\n",
      "Train Epoch: 1892 [11264/101520 (11%)] Loss: -1188.331177\n",
      "Train Epoch: 1892 [22528/101520 (22%)] Loss: -1178.532349\n",
      "Train Epoch: 1892 [33792/101520 (33%)] Loss: -1181.121338\n",
      "Train Epoch: 1892 [45056/101520 (44%)] Loss: -1193.442871\n",
      "Train Epoch: 1892 [56320/101520 (55%)] Loss: -1179.172974\n",
      "Train Epoch: 1892 [67584/101520 (67%)] Loss: -1187.386963\n",
      "Train Epoch: 1892 [78848/101520 (78%)] Loss: -1180.152588\n",
      "Train Epoch: 1892 [90112/101520 (89%)] Loss: -1188.849121\n",
      "Train Epoch: 1892 [101376/101520 (100%)] Loss: -1182.966553\n",
      "    epoch          : 1892\n",
      "    loss           : -1185.977909567368\n",
      "    ess            : 1.966118846107368\n",
      "    log_marginal   : 1186.010512155504\n",
      "    log_joint      : 1394.419512111338\n",
      "    val_loss       : -1185.3539402173913\n",
      "    val_ess        : 1.9653188663980234\n",
      "    val_log_marginal: 1185.3874617866848\n",
      "    val_log_joint  : 1393.998439622962\n",
      "Train Epoch: 1893 [0/101520 (0%)] Loss: -1182.091064\n",
      "Train Epoch: 1893 [11264/101520 (11%)] Loss: -1176.819824\n",
      "Train Epoch: 1893 [22528/101520 (22%)] Loss: -1180.530518\n",
      "Train Epoch: 1893 [33792/101520 (33%)] Loss: -1193.553955\n",
      "Train Epoch: 1893 [45056/101520 (44%)] Loss: -1193.737183\n",
      "Train Epoch: 1893 [56320/101520 (55%)] Loss: -1190.443726\n",
      "Train Epoch: 1893 [67584/101520 (67%)] Loss: -1188.552002\n",
      "Train Epoch: 1893 [78848/101520 (78%)] Loss: -1193.977051\n",
      "Train Epoch: 1893 [90112/101520 (89%)] Loss: -1187.015991\n",
      "Train Epoch: 1893 [101376/101520 (100%)] Loss: -1189.965942\n",
      "    epoch          : 1893\n",
      "    loss           : -1185.96141657997\n",
      "    ess            : 1.9661045140357474\n",
      "    log_marginal   : 1185.993457890036\n",
      "    log_joint      : 1394.4782997016332\n",
      "    val_loss       : -1185.8965215268342\n",
      "    val_ess        : 1.9668484304262244\n",
      "    val_log_marginal: 1185.9293212890625\n",
      "    val_log_joint  : 1394.4595045006793\n",
      "Train Epoch: 1894 [0/101520 (0%)] Loss: -1190.831299\n",
      "Train Epoch: 1894 [11264/101520 (11%)] Loss: -1186.870728\n",
      "Train Epoch: 1894 [22528/101520 (22%)] Loss: -1187.251587\n",
      "Train Epoch: 1894 [33792/101520 (33%)] Loss: -1190.165405\n",
      "Train Epoch: 1894 [45056/101520 (44%)] Loss: -1182.822021\n",
      "Train Epoch: 1894 [56320/101520 (55%)] Loss: -1186.722900\n",
      "Train Epoch: 1894 [67584/101520 (67%)] Loss: -1185.477051\n",
      "Train Epoch: 1894 [78848/101520 (78%)] Loss: -1193.185303\n",
      "Train Epoch: 1894 [90112/101520 (89%)] Loss: -1181.817505\n",
      "Train Epoch: 1894 [101376/101520 (100%)] Loss: -1187.148438\n",
      "    epoch          : 1894\n",
      "    loss           : -1186.0101864301978\n",
      "    ess            : 1.9661912852196237\n",
      "    log_marginal   : 1186.041848034116\n",
      "    log_joint      : 1394.5431552292714\n",
      "    val_loss       : -1186.2699452275815\n",
      "    val_ess        : 1.966697615125905\n",
      "    val_log_marginal: 1186.3007653277853\n",
      "    val_log_joint  : 1394.765816066576\n",
      "Train Epoch: 1895 [0/101520 (0%)] Loss: -1187.036377\n",
      "Train Epoch: 1895 [11264/101520 (11%)] Loss: -1183.426270\n",
      "Train Epoch: 1895 [22528/101520 (22%)] Loss: -1187.227295\n",
      "Train Epoch: 1895 [33792/101520 (33%)] Loss: -1177.483643\n",
      "Train Epoch: 1895 [45056/101520 (44%)] Loss: -1180.139648\n",
      "Train Epoch: 1895 [56320/101520 (55%)] Loss: -1177.073486\n",
      "Train Epoch: 1895 [67584/101520 (67%)] Loss: -1183.847412\n",
      "Train Epoch: 1895 [78848/101520 (78%)] Loss: -1189.267578\n",
      "Train Epoch: 1895 [90112/101520 (89%)] Loss: -1188.090820\n",
      "Train Epoch: 1895 [101376/101520 (100%)] Loss: -1176.260254\n",
      "    epoch          : 1895\n",
      "    loss           : -1186.0562977239715\n",
      "    ess            : 1.9666357286012353\n",
      "    log_marginal   : 1186.0878280562972\n",
      "    log_joint      : 1394.541972558103\n",
      "    val_loss       : -1183.7442202360733\n",
      "    val_ess        : 1.966176784556845\n",
      "    val_log_marginal: 1183.7770359205163\n",
      "    val_log_joint  : 1392.6824791949728\n",
      "Train Epoch: 1896 [0/101520 (0%)] Loss: -1193.205322\n",
      "Train Epoch: 1896 [11264/101520 (11%)] Loss: -1189.298584\n",
      "Train Epoch: 1896 [22528/101520 (22%)] Loss: -1182.289429\n",
      "Train Epoch: 1896 [33792/101520 (33%)] Loss: -1185.148438\n",
      "Train Epoch: 1896 [45056/101520 (44%)] Loss: -1183.193848\n",
      "Train Epoch: 1896 [56320/101520 (55%)] Loss: -1189.895996\n",
      "Train Epoch: 1896 [67584/101520 (67%)] Loss: -1185.392822\n",
      "Train Epoch: 1896 [78848/101520 (78%)] Loss: -1187.815674\n",
      "Train Epoch: 1896 [90112/101520 (89%)] Loss: -1183.863281\n",
      "Train Epoch: 1896 [101376/101520 (100%)] Loss: -1180.540649\n",
      "    epoch          : 1896\n",
      "    loss           : -1186.0836071225267\n",
      "    ess            : 1.9668372522047417\n",
      "    log_marginal   : 1186.1159177233826\n",
      "    log_joint      : 1394.5705038866206\n",
      "    val_loss       : -1183.1749373726223\n",
      "    val_ess        : 1.9640185677486917\n",
      "    val_log_marginal: 1183.210252844769\n",
      "    val_log_joint  : 1391.7015274711277\n",
      "Train Epoch: 1897 [0/101520 (0%)] Loss: -1186.749023\n",
      "Train Epoch: 1897 [11264/101520 (11%)] Loss: -1186.205811\n",
      "Train Epoch: 1897 [22528/101520 (22%)] Loss: -1192.551880\n",
      "Train Epoch: 1897 [33792/101520 (33%)] Loss: -1181.880859\n",
      "Train Epoch: 1897 [45056/101520 (44%)] Loss: -1187.861572\n",
      "Train Epoch: 1897 [56320/101520 (55%)] Loss: -1188.380127\n",
      "Train Epoch: 1897 [67584/101520 (67%)] Loss: -1188.930664\n",
      "Train Epoch: 1897 [78848/101520 (78%)] Loss: -1190.156250\n",
      "Train Epoch: 1897 [90112/101520 (89%)] Loss: -1177.665039\n",
      "Train Epoch: 1897 [101376/101520 (100%)] Loss: -1191.960815\n",
      "    epoch          : 1897\n",
      "    loss           : -1186.2617813187028\n",
      "    ess            : 1.9660273359049505\n",
      "    log_marginal   : 1186.2949028590217\n",
      "    log_joint      : 1394.642184310223\n",
      "    val_loss       : -1186.4048966117527\n",
      "    val_ess        : 1.9653761905172598\n",
      "    val_log_marginal: 1186.4354194972825\n",
      "    val_log_joint  : 1394.8284912109375\n",
      "Train Epoch: 1898 [0/101520 (0%)] Loss: -1184.200073\n",
      "Train Epoch: 1898 [11264/101520 (11%)] Loss: -1184.822998\n",
      "Train Epoch: 1898 [22528/101520 (22%)] Loss: -1186.813965\n",
      "Train Epoch: 1898 [33792/101520 (33%)] Loss: -1186.609375\n",
      "Train Epoch: 1898 [45056/101520 (44%)] Loss: -1185.215942\n",
      "Train Epoch: 1898 [56320/101520 (55%)] Loss: -1183.101196\n",
      "Train Epoch: 1898 [67584/101520 (67%)] Loss: -1193.562866\n",
      "Train Epoch: 1898 [78848/101520 (78%)] Loss: -1185.369507\n",
      "Train Epoch: 1898 [90112/101520 (89%)] Loss: -1177.153076\n",
      "Train Epoch: 1898 [101376/101520 (100%)] Loss: -1199.989868\n",
      "    epoch          : 1898\n",
      "    loss           : -1186.2052087831737\n",
      "    ess            : 1.966417173644406\n",
      "    log_marginal   : 1186.2382150007852\n",
      "    log_joint      : 1394.6864259039337\n",
      "    val_loss       : -1185.3188051970108\n",
      "    val_ess        : 1.9659455237181291\n",
      "    val_log_marginal: 1185.350437330163\n",
      "    val_log_joint  : 1393.7394170346467\n",
      "Train Epoch: 1899 [0/101520 (0%)] Loss: -1180.584229\n",
      "Train Epoch: 1899 [11264/101520 (11%)] Loss: -1192.622314\n",
      "Train Epoch: 1899 [22528/101520 (22%)] Loss: -1187.773926\n",
      "Train Epoch: 1899 [33792/101520 (33%)] Loss: -1184.330566\n",
      "Train Epoch: 1899 [45056/101520 (44%)] Loss: -1184.807251\n",
      "Train Epoch: 1899 [56320/101520 (55%)] Loss: -1186.765381\n",
      "Train Epoch: 1899 [67584/101520 (67%)] Loss: -1186.796997\n",
      "Train Epoch: 1899 [78848/101520 (78%)] Loss: -1186.262207\n",
      "Train Epoch: 1899 [90112/101520 (89%)] Loss: -1181.116943\n",
      "Train Epoch: 1899 [101376/101520 (100%)] Loss: -1178.002686\n",
      "    epoch          : 1899\n",
      "    loss           : -1186.1761916270807\n",
      "    ess            : 1.9662387718507393\n",
      "    log_marginal   : 1186.209232196137\n",
      "    log_joint      : 1394.604133951005\n",
      "    val_loss       : -1184.4465703549592\n",
      "    val_ess        : 1.9667910233787869\n",
      "    val_log_marginal: 1184.4767217221467\n",
      "    val_log_joint  : 1393.0430112092392\n",
      "Train Epoch: 1900 [0/101520 (0%)] Loss: -1184.502441\n",
      "Train Epoch: 1900 [11264/101520 (11%)] Loss: -1186.371460\n",
      "Train Epoch: 1900 [22528/101520 (22%)] Loss: -1189.568359\n",
      "Train Epoch: 1900 [33792/101520 (33%)] Loss: -1193.511230\n",
      "Train Epoch: 1900 [45056/101520 (44%)] Loss: -1182.784912\n",
      "Train Epoch: 1900 [56320/101520 (55%)] Loss: -1184.588135\n",
      "Train Epoch: 1900 [67584/101520 (67%)] Loss: -1181.627319\n",
      "Train Epoch: 1900 [78848/101520 (78%)] Loss: -1191.857544\n",
      "Train Epoch: 1900 [90112/101520 (89%)] Loss: -1187.609741\n",
      "Train Epoch: 1900 [101376/101520 (100%)] Loss: -1178.709717\n",
      "    epoch          : 1900\n",
      "    loss           : -1186.2510808436714\n",
      "    ess            : 1.966156378463285\n",
      "    log_marginal   : 1186.2837864861417\n",
      "    log_joint      : 1394.6524333091238\n",
      "    val_loss       : -1184.540139903193\n",
      "    val_ess        : 1.9683206651521765\n",
      "    val_log_marginal: 1184.5712519106658\n",
      "    val_log_joint  : 1393.4631613026495\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1900.pth ...\n",
      "Train Epoch: 1901 [0/101520 (0%)] Loss: -1189.522339\n",
      "Train Epoch: 1901 [11264/101520 (11%)] Loss: -1188.830933\n",
      "Train Epoch: 1901 [22528/101520 (22%)] Loss: -1188.023071\n",
      "Train Epoch: 1901 [33792/101520 (33%)] Loss: -1183.671875\n",
      "Train Epoch: 1901 [45056/101520 (44%)] Loss: -1186.751831\n",
      "Train Epoch: 1901 [56320/101520 (55%)] Loss: -1184.229126\n",
      "Train Epoch: 1901 [67584/101520 (67%)] Loss: -1187.753906\n",
      "Train Epoch: 1901 [78848/101520 (78%)] Loss: -1186.901978\n",
      "Train Epoch: 1901 [90112/101520 (89%)] Loss: -1188.145508\n",
      "Train Epoch: 1901 [101376/101520 (100%)] Loss: -1196.458740\n",
      "    epoch          : 1901\n",
      "    loss           : -1186.197536142627\n",
      "    ess            : 1.9656986333617015\n",
      "    log_marginal   : 1186.2310195999528\n",
      "    log_joint      : 1394.740441097087\n",
      "    val_loss       : -1184.8521675441575\n",
      "    val_ess        : 1.967695863350578\n",
      "    val_log_marginal: 1184.8824356742527\n",
      "    val_log_joint  : 1393.2550632642663\n",
      "Train Epoch: 1902 [0/101520 (0%)] Loss: -1182.036255\n",
      "Train Epoch: 1902 [11264/101520 (11%)] Loss: -1183.531982\n",
      "Train Epoch: 1902 [22528/101520 (22%)] Loss: -1186.587646\n",
      "Train Epoch: 1902 [33792/101520 (33%)] Loss: -1181.584595\n",
      "Train Epoch: 1902 [45056/101520 (44%)] Loss: -1185.008789\n",
      "Train Epoch: 1902 [56320/101520 (55%)] Loss: -1184.894287\n",
      "Train Epoch: 1902 [67584/101520 (67%)] Loss: -1180.575439\n",
      "Train Epoch: 1902 [78848/101520 (78%)] Loss: -1189.418579\n",
      "Train Epoch: 1902 [90112/101520 (89%)] Loss: -1186.817627\n",
      "Train Epoch: 1902 [101376/101520 (100%)] Loss: -1184.903564\n",
      "    epoch          : 1902\n",
      "    loss           : -1186.280237859218\n",
      "    ess            : 1.9662284731265887\n",
      "    log_marginal   : 1186.3124957060695\n",
      "    log_joint      : 1394.7699085024733\n",
      "    val_loss       : -1186.330226732337\n",
      "    val_ess        : 1.966487283292024\n",
      "    val_log_marginal: 1186.3621932319973\n",
      "    val_log_joint  : 1394.6575025475543\n",
      "Train Epoch: 1903 [0/101520 (0%)] Loss: -1189.250977\n",
      "Train Epoch: 1903 [11264/101520 (11%)] Loss: -1182.675293\n",
      "Train Epoch: 1903 [22528/101520 (22%)] Loss: -1186.562256\n",
      "Train Epoch: 1903 [33792/101520 (33%)] Loss: -1186.203003\n",
      "Train Epoch: 1903 [45056/101520 (44%)] Loss: -1191.239258\n",
      "Train Epoch: 1903 [56320/101520 (55%)] Loss: -1183.845947\n",
      "Train Epoch: 1903 [67584/101520 (67%)] Loss: -1195.561035\n",
      "Train Epoch: 1903 [78848/101520 (78%)] Loss: -1184.782471\n",
      "Train Epoch: 1903 [90112/101520 (89%)] Loss: -1192.693481\n",
      "Train Epoch: 1903 [101376/101520 (100%)] Loss: -1184.702148\n",
      "    epoch          : 1903\n",
      "    loss           : -1186.300610106195\n",
      "    ess            : 1.9660738466972083\n",
      "    log_marginal   : 1186.333456834956\n",
      "    log_joint      : 1394.7728774487673\n",
      "    val_loss       : -1187.6619236158288\n",
      "    val_ess        : 1.966528902883115\n",
      "    val_log_marginal: 1187.6945110818615\n",
      "    val_log_joint  : 1396.1159986413043\n",
      "Train Epoch: 1904 [0/101520 (0%)] Loss: -1182.753174\n",
      "Train Epoch: 1904 [11264/101520 (11%)] Loss: -1187.174194\n",
      "Train Epoch: 1904 [22528/101520 (22%)] Loss: -1187.961426\n",
      "Train Epoch: 1904 [33792/101520 (33%)] Loss: -1198.541016\n",
      "Train Epoch: 1904 [45056/101520 (44%)] Loss: -1192.397461\n",
      "Train Epoch: 1904 [56320/101520 (55%)] Loss: -1184.897949\n",
      "Train Epoch: 1904 [67584/101520 (67%)] Loss: -1181.130615\n",
      "Train Epoch: 1904 [78848/101520 (78%)] Loss: -1189.248169\n",
      "Train Epoch: 1904 [90112/101520 (89%)] Loss: -1185.281738\n",
      "Train Epoch: 1904 [101376/101520 (100%)] Loss: -1190.207397\n",
      "    epoch          : 1904\n",
      "    loss           : -1186.339792222833\n",
      "    ess            : 1.966274175212611\n",
      "    log_marginal   : 1186.373574415044\n",
      "    log_joint      : 1394.7886981293184\n",
      "    val_loss       : -1186.0809963060462\n",
      "    val_ess        : 1.968138337135315\n",
      "    val_log_marginal: 1186.1100968070652\n",
      "    val_log_joint  : 1394.7497770889945\n",
      "Train Epoch: 1905 [0/101520 (0%)] Loss: -1191.806641\n",
      "Train Epoch: 1905 [11264/101520 (11%)] Loss: -1188.066040\n",
      "Train Epoch: 1905 [22528/101520 (22%)] Loss: -1186.358276\n",
      "Train Epoch: 1905 [33792/101520 (33%)] Loss: -1188.522339\n",
      "Train Epoch: 1905 [45056/101520 (44%)] Loss: -1180.308350\n",
      "Train Epoch: 1905 [56320/101520 (55%)] Loss: -1184.749268\n",
      "Train Epoch: 1905 [67584/101520 (67%)] Loss: -1185.795654\n",
      "Train Epoch: 1905 [78848/101520 (78%)] Loss: -1180.560791\n",
      "Train Epoch: 1905 [90112/101520 (89%)] Loss: -1179.573730\n",
      "Train Epoch: 1905 [101376/101520 (100%)] Loss: -1189.857422\n",
      "    epoch          : 1905\n",
      "    loss           : -1186.2829632783055\n",
      "    ess            : 1.9657573280622012\n",
      "    log_marginal   : 1186.3160615087154\n",
      "    log_joint      : 1394.8191157202025\n",
      "    val_loss       : -1186.1110096807065\n",
      "    val_ess        : 1.9592143089874932\n",
      "    val_log_marginal: 1186.1547108525815\n",
      "    val_log_joint  : 1394.7144244650135\n",
      "Train Epoch: 1906 [0/101520 (0%)] Loss: -1190.436401\n",
      "Train Epoch: 1906 [11264/101520 (11%)] Loss: -1184.686279\n",
      "Train Epoch: 1906 [22528/101520 (22%)] Loss: -1185.852905\n",
      "Train Epoch: 1906 [33792/101520 (33%)] Loss: -1186.559448\n",
      "Train Epoch: 1906 [45056/101520 (44%)] Loss: -1189.871704\n",
      "Train Epoch: 1906 [56320/101520 (55%)] Loss: -1188.838501\n",
      "Train Epoch: 1906 [67584/101520 (67%)] Loss: -1187.960571\n",
      "Train Epoch: 1906 [78848/101520 (78%)] Loss: -1195.051514\n",
      "Train Epoch: 1906 [90112/101520 (89%)] Loss: -1183.337646\n",
      "Train Epoch: 1906 [101376/101520 (100%)] Loss: -1182.572754\n",
      "    epoch          : 1906\n",
      "    loss           : -1186.3437904856312\n",
      "    ess            : 1.966036610866911\n",
      "    log_marginal   : 1186.3771396042714\n",
      "    log_joint      : 1394.7572364998823\n",
      "    val_loss       : -1184.404684315557\n",
      "    val_ess        : 1.9637255772300388\n",
      "    val_log_marginal: 1184.4446703040082\n",
      "    val_log_joint  : 1393.0490032693615\n",
      "Train Epoch: 1907 [0/101520 (0%)] Loss: -1184.301880\n",
      "Train Epoch: 1907 [11264/101520 (11%)] Loss: -1190.195312\n",
      "Train Epoch: 1907 [22528/101520 (22%)] Loss: -1185.330322\n",
      "Train Epoch: 1907 [33792/101520 (33%)] Loss: -1175.362183\n",
      "Train Epoch: 1907 [45056/101520 (44%)] Loss: -1185.616333\n",
      "Train Epoch: 1907 [56320/101520 (55%)] Loss: -1181.682495\n",
      "Train Epoch: 1907 [67584/101520 (67%)] Loss: -1183.105225\n",
      "Train Epoch: 1907 [78848/101520 (78%)] Loss: -1184.555908\n",
      "Train Epoch: 1907 [90112/101520 (89%)] Loss: -1186.961670\n",
      "Train Epoch: 1907 [101376/101520 (100%)] Loss: -1189.710938\n",
      "    epoch          : 1907\n",
      "    loss           : -1186.315518633205\n",
      "    ess            : 1.9662610603936355\n",
      "    log_marginal   : 1186.3479065248116\n",
      "    log_joint      : 1394.8064418773556\n",
      "    val_loss       : -1184.0362389605978\n",
      "    val_ess        : 1.9657946825027466\n",
      "    val_log_marginal: 1184.0687574303668\n",
      "    val_log_joint  : 1393.1451256793478\n",
      "Train Epoch: 1908 [0/101520 (0%)] Loss: -1194.316040\n",
      "Train Epoch: 1908 [11264/101520 (11%)] Loss: -1188.415527\n",
      "Train Epoch: 1908 [22528/101520 (22%)] Loss: -1185.254517\n",
      "Train Epoch: 1908 [33792/101520 (33%)] Loss: -1183.153076\n",
      "Train Epoch: 1908 [45056/101520 (44%)] Loss: -1192.724854\n",
      "Train Epoch: 1908 [56320/101520 (55%)] Loss: -1176.754395\n",
      "Train Epoch: 1908 [67584/101520 (67%)] Loss: -1185.835449\n",
      "Train Epoch: 1908 [78848/101520 (78%)] Loss: -1190.564697\n",
      "Train Epoch: 1908 [90112/101520 (89%)] Loss: -1184.412354\n",
      "Train Epoch: 1908 [101376/101520 (100%)] Loss: -1189.157837\n",
      "    epoch          : 1908\n",
      "    loss           : -1186.301417365146\n",
      "    ess            : 1.9658146683295168\n",
      "    log_marginal   : 1186.3351547777952\n",
      "    log_joint      : 1394.825687274262\n",
      "    val_loss       : -1185.8827063519022\n",
      "    val_ess        : 1.961541077365046\n",
      "    val_log_marginal: 1185.9197414232337\n",
      "    val_log_joint  : 1394.0534986413043\n",
      "Train Epoch: 1909 [0/101520 (0%)] Loss: -1184.390869\n",
      "Train Epoch: 1909 [11264/101520 (11%)] Loss: -1190.231079\n",
      "Train Epoch: 1909 [22528/101520 (22%)] Loss: -1185.976807\n",
      "Train Epoch: 1909 [33792/101520 (33%)] Loss: -1190.830078\n",
      "Train Epoch: 1909 [45056/101520 (44%)] Loss: -1190.247803\n",
      "Train Epoch: 1909 [56320/101520 (55%)] Loss: -1189.104858\n",
      "Train Epoch: 1909 [67584/101520 (67%)] Loss: -1181.227173\n",
      "Train Epoch: 1909 [78848/101520 (78%)] Loss: -1194.426758\n",
      "Train Epoch: 1909 [90112/101520 (89%)] Loss: -1183.983887\n",
      "Train Epoch: 1909 [101376/101520 (100%)] Loss: -1198.479736\n",
      "    epoch          : 1909\n",
      "    loss           : -1186.4221252748116\n",
      "    ess            : 1.965359028859354\n",
      "    log_marginal   : 1186.455759019708\n",
      "    log_joint      : 1394.855657682946\n",
      "    val_loss       : -1184.946968410326\n",
      "    val_ess        : 1.9655349254608154\n",
      "    val_log_marginal: 1184.9783776324728\n",
      "    val_log_joint  : 1393.6227178158967\n",
      "Train Epoch: 1910 [0/101520 (0%)] Loss: -1189.445801\n",
      "Train Epoch: 1910 [11264/101520 (11%)] Loss: -1181.008911\n",
      "Train Epoch: 1910 [22528/101520 (22%)] Loss: -1181.107178\n",
      "Train Epoch: 1910 [33792/101520 (33%)] Loss: -1188.255127\n",
      "Train Epoch: 1910 [45056/101520 (44%)] Loss: -1187.211182\n",
      "Train Epoch: 1910 [56320/101520 (55%)] Loss: -1181.607300\n",
      "Train Epoch: 1910 [67584/101520 (67%)] Loss: -1185.409180\n",
      "Train Epoch: 1910 [78848/101520 (78%)] Loss: -1186.535645\n",
      "Train Epoch: 1910 [90112/101520 (89%)] Loss: -1189.550293\n",
      "Train Epoch: 1910 [101376/101520 (100%)] Loss: -1193.779907\n",
      "    epoch          : 1910\n",
      "    loss           : -1186.4426944291772\n",
      "    ess            : 1.9662940244578837\n",
      "    log_marginal   : 1186.4753816690877\n",
      "    log_joint      : 1394.8727505937893\n",
      "    val_loss       : -1184.9510020380435\n",
      "    val_ess        : 1.9661893326303232\n",
      "    val_log_marginal: 1184.985404636549\n",
      "    val_log_joint  : 1393.26831585428\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1910.pth ...\n",
      "Train Epoch: 1911 [0/101520 (0%)] Loss: -1181.374268\n",
      "Train Epoch: 1911 [11264/101520 (11%)] Loss: -1184.571167\n",
      "Train Epoch: 1911 [22528/101520 (22%)] Loss: -1186.067139\n",
      "Train Epoch: 1911 [33792/101520 (33%)] Loss: -1185.553467\n",
      "Train Epoch: 1911 [45056/101520 (44%)] Loss: -1192.571777\n",
      "Train Epoch: 1911 [56320/101520 (55%)] Loss: -1191.078491\n",
      "Train Epoch: 1911 [67584/101520 (67%)] Loss: -1188.921631\n",
      "Train Epoch: 1911 [78848/101520 (78%)] Loss: -1188.851196\n",
      "Train Epoch: 1911 [90112/101520 (89%)] Loss: -1186.104736\n",
      "Train Epoch: 1911 [101376/101520 (100%)] Loss: -1181.829468\n",
      "    epoch          : 1911\n",
      "    loss           : -1186.3624482274654\n",
      "    ess            : 1.9657942284291714\n",
      "    log_marginal   : 1186.395611480253\n",
      "    log_joint      : 1394.888343696019\n",
      "    val_loss       : -1183.7927829908288\n",
      "    val_ess        : 1.9682236806206082\n",
      "    val_log_marginal: 1183.8233642578125\n",
      "    val_log_joint  : 1392.4923467221467\n",
      "Train Epoch: 1912 [0/101520 (0%)] Loss: -1187.373413\n",
      "Train Epoch: 1912 [11264/101520 (11%)] Loss: -1185.294556\n",
      "Train Epoch: 1912 [22528/101520 (22%)] Loss: -1190.422852\n",
      "Train Epoch: 1912 [33792/101520 (33%)] Loss: -1186.123535\n",
      "Train Epoch: 1912 [45056/101520 (44%)] Loss: -1192.776489\n",
      "Train Epoch: 1912 [56320/101520 (55%)] Loss: -1186.191284\n",
      "Train Epoch: 1912 [67584/101520 (67%)] Loss: -1198.812988\n",
      "Train Epoch: 1912 [78848/101520 (78%)] Loss: -1187.373047\n",
      "Train Epoch: 1912 [90112/101520 (89%)] Loss: -1187.474854\n",
      "Train Epoch: 1912 [101376/101520 (100%)] Loss: -1195.028931\n",
      "    epoch          : 1912\n",
      "    loss           : -1186.4707215275596\n",
      "    ess            : 1.966764436295284\n",
      "    log_marginal   : 1186.501903438089\n",
      "    log_joint      : 1394.9410467866678\n",
      "    val_loss       : -1183.5040230129075\n",
      "    val_ess        : 1.9674798716669497\n",
      "    val_log_marginal: 1183.5365892493207\n",
      "    val_log_joint  : 1391.878465735394\n",
      "Train Epoch: 1913 [0/101520 (0%)] Loss: -1191.338745\n",
      "Train Epoch: 1913 [11264/101520 (11%)] Loss: -1186.794434\n",
      "Train Epoch: 1913 [22528/101520 (22%)] Loss: -1189.205444\n",
      "Train Epoch: 1913 [33792/101520 (33%)] Loss: -1186.036377\n",
      "Train Epoch: 1913 [45056/101520 (44%)] Loss: -1194.839600\n",
      "Train Epoch: 1913 [56320/101520 (55%)] Loss: -1183.692139\n",
      "Train Epoch: 1913 [67584/101520 (67%)] Loss: -1190.724854\n",
      "Train Epoch: 1913 [78848/101520 (78%)] Loss: -1189.839966\n",
      "Train Epoch: 1913 [90112/101520 (89%)] Loss: -1186.094727\n",
      "Train Epoch: 1913 [101376/101520 (100%)] Loss: -1205.449341\n",
      "    epoch          : 1913\n",
      "    loss           : -1186.4847258754712\n",
      "    ess            : 1.96659798717978\n",
      "    log_marginal   : 1186.516505255771\n",
      "    log_joint      : 1394.9875966747802\n",
      "    val_loss       : -1184.1913266389267\n",
      "    val_ess        : 1.9663697066514387\n",
      "    val_log_marginal: 1184.2257982336957\n",
      "    val_log_joint  : 1393.016845703125\n",
      "Train Epoch: 1914 [0/101520 (0%)] Loss: -1194.752808\n",
      "Train Epoch: 1914 [11264/101520 (11%)] Loss: -1188.489746\n",
      "Train Epoch: 1914 [22528/101520 (22%)] Loss: -1190.161987\n",
      "Train Epoch: 1914 [33792/101520 (33%)] Loss: -1187.782227\n",
      "Train Epoch: 1914 [45056/101520 (44%)] Loss: -1187.468262\n",
      "Train Epoch: 1914 [56320/101520 (55%)] Loss: -1183.976440\n",
      "Train Epoch: 1914 [67584/101520 (67%)] Loss: -1187.647949\n",
      "Train Epoch: 1914 [78848/101520 (78%)] Loss: -1188.901367\n",
      "Train Epoch: 1914 [90112/101520 (89%)] Loss: -1186.510986\n",
      "Train Epoch: 1914 [101376/101520 (100%)] Loss: -1186.624146\n",
      "    epoch          : 1914\n",
      "    loss           : -1186.420427945391\n",
      "    ess            : 1.9661137662341248\n",
      "    log_marginal   : 1186.453016424898\n",
      "    log_joint      : 1394.9123007616206\n",
      "    val_loss       : -1186.326612389606\n",
      "    val_ess        : 1.965237249498782\n",
      "    val_log_marginal: 1186.3587752632473\n",
      "    val_log_joint  : 1394.8573051120925\n",
      "Train Epoch: 1915 [0/101520 (0%)] Loss: -1188.120605\n",
      "Train Epoch: 1915 [11264/101520 (11%)] Loss: -1181.784668\n",
      "Train Epoch: 1915 [22528/101520 (22%)] Loss: -1184.778564\n",
      "Train Epoch: 1915 [33792/101520 (33%)] Loss: -1187.933960\n",
      "Train Epoch: 1915 [45056/101520 (44%)] Loss: -1184.455322\n",
      "Train Epoch: 1915 [56320/101520 (55%)] Loss: -1187.386475\n",
      "Train Epoch: 1915 [67584/101520 (67%)] Loss: -1197.847778\n",
      "Train Epoch: 1915 [78848/101520 (78%)] Loss: -1188.155273\n",
      "Train Epoch: 1915 [90112/101520 (89%)] Loss: -1184.624512\n",
      "Train Epoch: 1915 [101376/101520 (100%)] Loss: -1170.737427\n",
      "    epoch          : 1915\n",
      "    loss           : -1186.369849736966\n",
      "    ess            : 1.9654575544386055\n",
      "    log_marginal   : 1186.4021897819173\n",
      "    log_joint      : 1394.9534979585428\n",
      "    val_loss       : -1184.9228303328805\n",
      "    val_ess        : 1.9664167580397234\n",
      "    val_log_marginal: 1184.9544146993885\n",
      "    val_log_joint  : 1393.033882472826\n",
      "Train Epoch: 1916 [0/101520 (0%)] Loss: -1190.107178\n",
      "Train Epoch: 1916 [11264/101520 (11%)] Loss: -1186.356689\n",
      "Train Epoch: 1916 [22528/101520 (22%)] Loss: -1188.499268\n",
      "Train Epoch: 1916 [33792/101520 (33%)] Loss: -1181.364136\n",
      "Train Epoch: 1916 [45056/101520 (44%)] Loss: -1188.446045\n",
      "Train Epoch: 1916 [56320/101520 (55%)] Loss: -1186.483154\n",
      "Train Epoch: 1916 [67584/101520 (67%)] Loss: -1183.920898\n",
      "Train Epoch: 1916 [78848/101520 (78%)] Loss: -1184.370117\n",
      "Train Epoch: 1916 [90112/101520 (89%)] Loss: -1178.649902\n",
      "Train Epoch: 1916 [101376/101520 (100%)] Loss: -1168.382446\n",
      "    epoch          : 1916\n",
      "    loss           : -1186.4667619101367\n",
      "    ess            : 1.9662870372360075\n",
      "    log_marginal   : 1186.4987480125235\n",
      "    log_joint      : 1394.9504664435458\n",
      "    val_loss       : -1185.518257472826\n",
      "    val_ess        : 1.966183532839236\n",
      "    val_log_marginal: 1185.5513438349185\n",
      "    val_log_joint  : 1393.8958634086277\n",
      "Train Epoch: 1917 [0/101520 (0%)] Loss: -1180.496582\n",
      "Train Epoch: 1917 [11264/101520 (11%)] Loss: -1182.067017\n",
      "Train Epoch: 1917 [22528/101520 (22%)] Loss: -1190.189209\n",
      "Train Epoch: 1917 [33792/101520 (33%)] Loss: -1191.233154\n",
      "Train Epoch: 1917 [45056/101520 (44%)] Loss: -1183.953003\n",
      "Train Epoch: 1917 [56320/101520 (55%)] Loss: -1190.587158\n",
      "Train Epoch: 1917 [67584/101520 (67%)] Loss: -1185.136230\n",
      "Train Epoch: 1917 [78848/101520 (78%)] Loss: -1190.943604\n",
      "Train Epoch: 1917 [90112/101520 (89%)] Loss: -1185.232910\n",
      "Train Epoch: 1917 [101376/101520 (100%)] Loss: -1189.577026\n",
      "    epoch          : 1917\n",
      "    loss           : -1186.6115961889525\n",
      "    ess            : 1.9661860657696748\n",
      "    log_marginal   : 1186.6444380103644\n",
      "    log_joint      : 1395.0385533625158\n",
      "    val_loss       : -1186.3955184273098\n",
      "    val_ess        : 1.9663516594016033\n",
      "    val_log_marginal: 1186.4271824048913\n",
      "    val_log_joint  : 1395.0749140200408\n",
      "Train Epoch: 1918 [0/101520 (0%)] Loss: -1185.394775\n",
      "Train Epoch: 1918 [11264/101520 (11%)] Loss: -1189.920044\n",
      "Train Epoch: 1918 [22528/101520 (22%)] Loss: -1187.696533\n",
      "Train Epoch: 1918 [33792/101520 (33%)] Loss: -1183.560059\n",
      "Train Epoch: 1918 [45056/101520 (44%)] Loss: -1179.869141\n",
      "Train Epoch: 1918 [56320/101520 (55%)] Loss: -1180.040771\n",
      "Train Epoch: 1918 [67584/101520 (67%)] Loss: -1182.482910\n",
      "Train Epoch: 1918 [78848/101520 (78%)] Loss: -1180.385498\n",
      "Train Epoch: 1918 [90112/101520 (89%)] Loss: -1189.203979\n",
      "Train Epoch: 1918 [101376/101520 (100%)] Loss: -1181.037354\n",
      "    epoch          : 1918\n",
      "    loss           : -1186.5058691896984\n",
      "    ess            : 1.965791067885394\n",
      "    log_marginal   : 1186.538039931101\n",
      "    log_joint      : 1395.014622060498\n",
      "    val_loss       : -1185.7262546705163\n",
      "    val_ess        : 1.965000701987225\n",
      "    val_log_marginal: 1185.7592189622962\n",
      "    val_log_joint  : 1394.083501401155\n",
      "Train Epoch: 1919 [0/101520 (0%)] Loss: -1187.044678\n",
      "Train Epoch: 1919 [11264/101520 (11%)] Loss: -1181.548462\n",
      "Train Epoch: 1919 [22528/101520 (22%)] Loss: -1184.126831\n",
      "Train Epoch: 1919 [33792/101520 (33%)] Loss: -1186.823730\n",
      "Train Epoch: 1919 [45056/101520 (44%)] Loss: -1189.791626\n",
      "Train Epoch: 1919 [56320/101520 (55%)] Loss: -1188.957397\n",
      "Train Epoch: 1919 [67584/101520 (67%)] Loss: -1188.354248\n",
      "Train Epoch: 1919 [78848/101520 (78%)] Loss: -1186.663452\n",
      "Train Epoch: 1919 [90112/101520 (89%)] Loss: -1181.104248\n",
      "Train Epoch: 1919 [101376/101520 (100%)] Loss: -1177.477417\n",
      "    epoch          : 1919\n",
      "    loss           : -1186.5915104084877\n",
      "    ess            : 1.966756745798504\n",
      "    log_marginal   : 1186.6242197314698\n",
      "    log_joint      : 1395.0866159410332\n",
      "    val_loss       : -1185.486227284307\n",
      "    val_ess        : 1.9671383992485378\n",
      "    val_log_marginal: 1185.5185228430707\n",
      "    val_log_joint  : 1393.9957381538723\n",
      "Train Epoch: 1920 [0/101520 (0%)] Loss: -1183.199951\n",
      "Train Epoch: 1920 [11264/101520 (11%)] Loss: -1191.697266\n",
      "Train Epoch: 1920 [22528/101520 (22%)] Loss: -1191.499512\n",
      "Train Epoch: 1920 [33792/101520 (33%)] Loss: -1183.237183\n",
      "Train Epoch: 1920 [45056/101520 (44%)] Loss: -1185.298462\n",
      "Train Epoch: 1920 [56320/101520 (55%)] Loss: -1185.938477\n",
      "Train Epoch: 1920 [67584/101520 (67%)] Loss: -1188.701294\n",
      "Train Epoch: 1920 [78848/101520 (78%)] Loss: -1189.287598\n",
      "Train Epoch: 1920 [90112/101520 (89%)] Loss: -1182.073120\n",
      "Train Epoch: 1920 [101376/101520 (100%)] Loss: -1194.707031\n",
      "    epoch          : 1920\n",
      "    loss           : -1186.6338571519707\n",
      "    ess            : 1.9655202543316175\n",
      "    log_marginal   : 1186.6671823472832\n",
      "    log_joint      : 1395.1301882949906\n",
      "    val_loss       : -1184.6560695482337\n",
      "    val_ess        : 1.9660715123881465\n",
      "    val_log_marginal: 1184.6905942170517\n",
      "    val_log_joint  : 1392.970947265625\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1920.pth ...\n",
      "Train Epoch: 1921 [0/101520 (0%)] Loss: -1184.538086\n",
      "Train Epoch: 1921 [11264/101520 (11%)] Loss: -1187.890137\n",
      "Train Epoch: 1921 [22528/101520 (22%)] Loss: -1182.763428\n",
      "Train Epoch: 1921 [33792/101520 (33%)] Loss: -1188.306763\n",
      "Train Epoch: 1921 [45056/101520 (44%)] Loss: -1181.913940\n",
      "Train Epoch: 1921 [56320/101520 (55%)] Loss: -1188.429565\n",
      "Train Epoch: 1921 [67584/101520 (67%)] Loss: -1190.650879\n",
      "Train Epoch: 1921 [78848/101520 (78%)] Loss: -1185.967041\n",
      "Train Epoch: 1921 [90112/101520 (89%)] Loss: -1184.250732\n",
      "Train Epoch: 1921 [101376/101520 (100%)] Loss: -1188.742065\n",
      "    epoch          : 1921\n",
      "    loss           : -1186.731384584053\n",
      "    ess            : 1.9647915818583426\n",
      "    log_marginal   : 1186.765610891371\n",
      "    log_joint      : 1395.1883686008166\n",
      "    val_loss       : -1185.125732421875\n",
      "    val_ess        : 1.9631767739420352\n",
      "    val_log_marginal: 1185.1588983950408\n",
      "    val_log_joint  : 1393.828560207201\n",
      "Train Epoch: 1922 [0/101520 (0%)] Loss: -1188.217407\n",
      "Train Epoch: 1922 [11264/101520 (11%)] Loss: -1185.155273\n",
      "Train Epoch: 1922 [22528/101520 (22%)] Loss: -1185.015259\n",
      "Train Epoch: 1922 [33792/101520 (33%)] Loss: -1184.570435\n",
      "Train Epoch: 1922 [45056/101520 (44%)] Loss: -1190.199219\n",
      "Train Epoch: 1922 [56320/101520 (55%)] Loss: -1180.404785\n",
      "Train Epoch: 1922 [67584/101520 (67%)] Loss: -1190.322510\n",
      "Train Epoch: 1922 [78848/101520 (78%)] Loss: -1183.881348\n",
      "Train Epoch: 1922 [90112/101520 (89%)] Loss: -1185.808960\n",
      "Train Epoch: 1922 [101376/101520 (100%)] Loss: -1194.787964\n",
      "    epoch          : 1922\n",
      "    loss           : -1186.666127880614\n",
      "    ess            : 1.9658821443816525\n",
      "    log_marginal   : 1186.698787516685\n",
      "    log_joint      : 1395.1913357068545\n",
      "    val_loss       : -1185.7658585258152\n",
      "    val_ess        : 1.967601195625637\n",
      "    val_log_marginal: 1185.7973208220108\n",
      "    val_log_joint  : 1394.2366571841033\n",
      "Train Epoch: 1923 [0/101520 (0%)] Loss: -1189.491943\n",
      "Train Epoch: 1923 [11264/101520 (11%)] Loss: -1190.263794\n",
      "Train Epoch: 1923 [22528/101520 (22%)] Loss: -1182.639282\n",
      "Train Epoch: 1923 [33792/101520 (33%)] Loss: -1181.382690\n",
      "Train Epoch: 1923 [45056/101520 (44%)] Loss: -1183.895630\n",
      "Train Epoch: 1923 [56320/101520 (55%)] Loss: -1193.003540\n",
      "Train Epoch: 1923 [67584/101520 (67%)] Loss: -1190.338379\n",
      "Train Epoch: 1923 [78848/101520 (78%)] Loss: -1182.051758\n",
      "Train Epoch: 1923 [90112/101520 (89%)] Loss: -1187.719238\n",
      "Train Epoch: 1923 [101376/101520 (100%)] Loss: -1178.755615\n",
      "    epoch          : 1923\n",
      "    loss           : -1186.598905783802\n",
      "    ess            : 1.9659185235823817\n",
      "    log_marginal   : 1186.6322499950927\n",
      "    log_joint      : 1395.0997725443624\n",
      "    val_loss       : -1185.034572435462\n",
      "    val_ess        : 1.9679682410281638\n",
      "    val_log_marginal: 1185.0641824473505\n",
      "    val_log_joint  : 1393.8320047129755\n",
      "Train Epoch: 1924 [0/101520 (0%)] Loss: -1186.124023\n",
      "Train Epoch: 1924 [11264/101520 (11%)] Loss: -1187.774658\n",
      "Train Epoch: 1924 [22528/101520 (22%)] Loss: -1186.038452\n",
      "Train Epoch: 1924 [33792/101520 (33%)] Loss: -1185.712280\n",
      "Train Epoch: 1924 [45056/101520 (44%)] Loss: -1185.635498\n",
      "Train Epoch: 1924 [56320/101520 (55%)] Loss: -1185.231323\n",
      "Train Epoch: 1924 [67584/101520 (67%)] Loss: -1191.412109\n",
      "Train Epoch: 1924 [78848/101520 (78%)] Loss: -1183.508423\n",
      "Train Epoch: 1924 [90112/101520 (89%)] Loss: -1183.055420\n",
      "Train Epoch: 1924 [101376/101520 (100%)] Loss: -1185.304688\n",
      "    epoch          : 1924\n",
      "    loss           : -1186.7097321323413\n",
      "    ess            : 1.9654599506052295\n",
      "    log_marginal   : 1186.7427512317447\n",
      "    log_joint      : 1395.1853499676115\n",
      "    val_loss       : -1185.5708273182745\n",
      "    val_ess        : 1.9669194169666455\n",
      "    val_log_marginal: 1185.6038977581522\n",
      "    val_log_joint  : 1394.1836733610733\n",
      "Train Epoch: 1925 [0/101520 (0%)] Loss: -1192.081299\n",
      "Train Epoch: 1925 [11264/101520 (11%)] Loss: -1188.859131\n",
      "Train Epoch: 1925 [22528/101520 (22%)] Loss: -1186.183228\n",
      "Train Epoch: 1925 [33792/101520 (33%)] Loss: -1189.013916\n",
      "Train Epoch: 1925 [45056/101520 (44%)] Loss: -1189.604736\n",
      "Train Epoch: 1925 [56320/101520 (55%)] Loss: -1186.967896\n",
      "Train Epoch: 1925 [67584/101520 (67%)] Loss: -1186.508301\n",
      "Train Epoch: 1925 [78848/101520 (78%)] Loss: -1186.717285\n",
      "Train Epoch: 1925 [90112/101520 (89%)] Loss: -1191.526611\n",
      "Train Epoch: 1925 [101376/101520 (100%)] Loss: -1183.353394\n",
      "    epoch          : 1925\n",
      "    loss           : -1186.7507949905778\n",
      "    ess            : 1.9666607943012486\n",
      "    log_marginal   : 1186.783106818271\n",
      "    log_joint      : 1395.1985850885285\n",
      "    val_loss       : -1186.6066310716712\n",
      "    val_ess        : 1.9654531997183096\n",
      "    val_log_marginal: 1186.6427055027175\n",
      "    val_log_joint  : 1395.0435260275135\n",
      "Train Epoch: 1926 [0/101520 (0%)] Loss: -1185.905273\n",
      "Train Epoch: 1926 [11264/101520 (11%)] Loss: -1188.151733\n",
      "Train Epoch: 1926 [22528/101520 (22%)] Loss: -1191.258057\n",
      "Train Epoch: 1926 [33792/101520 (33%)] Loss: -1184.094482\n",
      "Train Epoch: 1926 [45056/101520 (44%)] Loss: -1186.709229\n",
      "Train Epoch: 1926 [56320/101520 (55%)] Loss: -1186.834351\n",
      "Train Epoch: 1926 [67584/101520 (67%)] Loss: -1190.616699\n",
      "Train Epoch: 1926 [78848/101520 (78%)] Loss: -1183.843506\n",
      "Train Epoch: 1926 [90112/101520 (89%)] Loss: -1192.445068\n",
      "Train Epoch: 1926 [101376/101520 (100%)] Loss: -1178.983887\n",
      "    epoch          : 1926\n",
      "    loss           : -1186.6483503945508\n",
      "    ess            : 1.966402086780299\n",
      "    log_marginal   : 1186.6812100051036\n",
      "    log_joint      : 1395.1314445763976\n",
      "    val_loss       : -1184.1596414317255\n",
      "    val_ess        : 1.9657372288081958\n",
      "    val_log_marginal: 1184.1920643682065\n",
      "    val_log_joint  : 1392.9605394446332\n",
      "Train Epoch: 1927 [0/101520 (0%)] Loss: -1182.207397\n",
      "Train Epoch: 1927 [11264/101520 (11%)] Loss: -1188.653320\n",
      "Train Epoch: 1927 [22528/101520 (22%)] Loss: -1189.004883\n",
      "Train Epoch: 1927 [33792/101520 (33%)] Loss: -1193.001709\n",
      "Train Epoch: 1927 [45056/101520 (44%)] Loss: -1185.912354\n",
      "Train Epoch: 1927 [56320/101520 (55%)] Loss: -1184.636230\n",
      "Train Epoch: 1927 [67584/101520 (67%)] Loss: -1188.113525\n",
      "Train Epoch: 1927 [78848/101520 (78%)] Loss: -1182.482910\n",
      "Train Epoch: 1927 [90112/101520 (89%)] Loss: -1187.457520\n",
      "Train Epoch: 1927 [101376/101520 (100%)] Loss: -1193.222290\n",
      "    epoch          : 1927\n",
      "    loss           : -1186.773963199788\n",
      "    ess            : 1.9657879390908246\n",
      "    log_marginal   : 1186.8072798072394\n",
      "    log_joint      : 1395.2729461516567\n",
      "    val_loss       : -1185.3169529127038\n",
      "    val_ess        : 1.9653114080429077\n",
      "    val_log_marginal: 1185.3525655995245\n",
      "    val_log_joint  : 1393.5901993461277\n",
      "Train Epoch: 1928 [0/101520 (0%)] Loss: -1186.860352\n",
      "Train Epoch: 1928 [11264/101520 (11%)] Loss: -1184.714600\n",
      "Train Epoch: 1928 [22528/101520 (22%)] Loss: -1182.522949\n",
      "Train Epoch: 1928 [33792/101520 (33%)] Loss: -1183.444214\n",
      "Train Epoch: 1928 [45056/101520 (44%)] Loss: -1192.068848\n",
      "Train Epoch: 1928 [56320/101520 (55%)] Loss: -1190.099243\n",
      "Train Epoch: 1928 [67584/101520 (67%)] Loss: -1187.098389\n",
      "Train Epoch: 1928 [78848/101520 (78%)] Loss: -1177.140381\n",
      "Train Epoch: 1928 [90112/101520 (89%)] Loss: -1182.578125\n",
      "Train Epoch: 1928 [101376/101520 (100%)] Loss: -1186.736572\n",
      "    epoch          : 1928\n",
      "    loss           : -1186.7639877856077\n",
      "    ess            : 1.9662081865809071\n",
      "    log_marginal   : 1186.7965511149498\n",
      "    log_joint      : 1395.3088648810458\n",
      "    val_loss       : -1185.7489332116168\n",
      "    val_ess        : 1.967407869256061\n",
      "    val_log_marginal: 1185.779243800951\n",
      "    val_log_joint  : 1394.2425430961277\n",
      "Train Epoch: 1929 [0/101520 (0%)] Loss: -1184.139648\n",
      "Train Epoch: 1929 [11264/101520 (11%)] Loss: -1183.531860\n",
      "Train Epoch: 1929 [22528/101520 (22%)] Loss: -1193.253906\n",
      "Train Epoch: 1929 [33792/101520 (33%)] Loss: -1187.338135\n",
      "Train Epoch: 1929 [45056/101520 (44%)] Loss: -1176.302734\n",
      "Train Epoch: 1929 [56320/101520 (55%)] Loss: -1190.957520\n",
      "Train Epoch: 1929 [67584/101520 (67%)] Loss: -1190.171631\n",
      "Train Epoch: 1929 [78848/101520 (78%)] Loss: -1194.793579\n",
      "Train Epoch: 1929 [90112/101520 (89%)] Loss: -1183.917236\n",
      "Train Epoch: 1929 [101376/101520 (100%)] Loss: -1185.612183\n",
      "    epoch          : 1929\n",
      "    loss           : -1186.7715334484924\n",
      "    ess            : 1.9657124670306643\n",
      "    log_marginal   : 1186.8045384392667\n",
      "    log_joint      : 1395.334531544441\n",
      "    val_loss       : -1186.5143936820652\n",
      "    val_ess        : 1.9577305835226309\n",
      "    val_log_marginal: 1186.5617304262908\n",
      "    val_log_joint  : 1395.2646378226902\n",
      "Train Epoch: 1930 [0/101520 (0%)] Loss: -1189.613281\n",
      "Train Epoch: 1930 [11264/101520 (11%)] Loss: -1190.298950\n",
      "Train Epoch: 1930 [22528/101520 (22%)] Loss: -1185.995483\n",
      "Train Epoch: 1930 [33792/101520 (33%)] Loss: -1184.454590\n",
      "Train Epoch: 1930 [45056/101520 (44%)] Loss: -1187.826782\n",
      "Train Epoch: 1930 [56320/101520 (55%)] Loss: -1192.322754\n",
      "Train Epoch: 1930 [67584/101520 (67%)] Loss: -1184.437988\n",
      "Train Epoch: 1930 [78848/101520 (78%)] Loss: -1185.191162\n",
      "Train Epoch: 1930 [90112/101520 (89%)] Loss: -1181.045410\n",
      "Train Epoch: 1930 [101376/101520 (100%)] Loss: -1176.793457\n",
      "    epoch          : 1930\n",
      "    loss           : -1186.8924290642667\n",
      "    ess            : 1.9661338742653929\n",
      "    log_marginal   : 1186.9247807641725\n",
      "    log_joint      : 1395.3961500618327\n",
      "    val_loss       : -1185.2500902258832\n",
      "    val_ess        : 1.9668364939482317\n",
      "    val_log_marginal: 1185.2814835258152\n",
      "    val_log_joint  : 1393.3804453974185\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1930.pth ...\n",
      "Train Epoch: 1931 [0/101520 (0%)] Loss: -1194.690674\n",
      "Train Epoch: 1931 [11264/101520 (11%)] Loss: -1193.322510\n",
      "Train Epoch: 1931 [22528/101520 (22%)] Loss: -1181.791504\n",
      "Train Epoch: 1931 [33792/101520 (33%)] Loss: -1190.174072\n",
      "Train Epoch: 1931 [45056/101520 (44%)] Loss: -1181.799194\n",
      "Train Epoch: 1931 [56320/101520 (55%)] Loss: -1188.898193\n",
      "Train Epoch: 1931 [67584/101520 (67%)] Loss: -1189.657959\n",
      "Train Epoch: 1931 [78848/101520 (78%)] Loss: -1191.410645\n",
      "Train Epoch: 1931 [90112/101520 (89%)] Loss: -1179.479980\n",
      "Train Epoch: 1931 [101376/101520 (100%)] Loss: -1191.120239\n",
      "    epoch          : 1931\n",
      "    loss           : -1186.94963403443\n",
      "    ess            : 1.9657949179261174\n",
      "    log_marginal   : 1186.9825237024968\n",
      "    log_joint      : 1395.4221203674624\n",
      "    val_loss       : -1185.2548403532608\n",
      "    val_ess        : 1.9693008609440015\n",
      "    val_log_marginal: 1185.2813667629075\n",
      "    val_log_joint  : 1393.650337550951\n",
      "Train Epoch: 1932 [0/101520 (0%)] Loss: -1177.845703\n",
      "Train Epoch: 1932 [11264/101520 (11%)] Loss: -1187.178955\n",
      "Train Epoch: 1932 [22528/101520 (22%)] Loss: -1193.854736\n",
      "Train Epoch: 1932 [33792/101520 (33%)] Loss: -1188.520020\n",
      "Train Epoch: 1932 [45056/101520 (44%)] Loss: -1186.599121\n",
      "Train Epoch: 1932 [56320/101520 (55%)] Loss: -1183.231201\n",
      "Train Epoch: 1932 [67584/101520 (67%)] Loss: -1190.308350\n",
      "Train Epoch: 1932 [78848/101520 (78%)] Loss: -1185.743042\n",
      "Train Epoch: 1932 [90112/101520 (89%)] Loss: -1190.831299\n",
      "Train Epoch: 1932 [101376/101520 (100%)] Loss: -1198.930664\n",
      "    epoch          : 1932\n",
      "    loss           : -1186.9440592856863\n",
      "    ess            : 1.9656725145464566\n",
      "    log_marginal   : 1186.976981464942\n",
      "    log_joint      : 1395.4399616490657\n",
      "    val_loss       : -1187.1256050441575\n",
      "    val_ess        : 1.962242173111957\n",
      "    val_log_marginal: 1187.1623429008152\n",
      "    val_log_joint  : 1395.7724237856658\n",
      "Train Epoch: 1933 [0/101520 (0%)] Loss: -1184.652100\n",
      "Train Epoch: 1933 [11264/101520 (11%)] Loss: -1184.292969\n",
      "Train Epoch: 1933 [22528/101520 (22%)] Loss: -1185.005737\n",
      "Train Epoch: 1933 [33792/101520 (33%)] Loss: -1186.192627\n",
      "Train Epoch: 1933 [45056/101520 (44%)] Loss: -1189.536133\n",
      "Train Epoch: 1933 [56320/101520 (55%)] Loss: -1190.273071\n",
      "Train Epoch: 1933 [67584/101520 (67%)] Loss: -1189.473755\n",
      "Train Epoch: 1933 [78848/101520 (78%)] Loss: -1185.054321\n",
      "Train Epoch: 1933 [90112/101520 (89%)] Loss: -1184.401855\n",
      "Train Epoch: 1933 [101376/101520 (100%)] Loss: -1170.405273\n",
      "    epoch          : 1933\n",
      "    loss           : -1186.943124435655\n",
      "    ess            : 1.9668552869528382\n",
      "    log_marginal   : 1186.974320454813\n",
      "    log_joint      : 1395.3672776725423\n",
      "    val_loss       : -1186.3849407693615\n",
      "    val_ess        : 1.9673610200052676\n",
      "    val_log_marginal: 1186.4145083220108\n",
      "    val_log_joint  : 1394.7788988196332\n",
      "Train Epoch: 1934 [0/101520 (0%)] Loss: -1185.211670\n",
      "Train Epoch: 1934 [11264/101520 (11%)] Loss: -1180.027100\n",
      "Train Epoch: 1934 [22528/101520 (22%)] Loss: -1184.020142\n",
      "Train Epoch: 1934 [33792/101520 (33%)] Loss: -1189.051270\n",
      "Train Epoch: 1934 [45056/101520 (44%)] Loss: -1184.079102\n",
      "Train Epoch: 1934 [56320/101520 (55%)] Loss: -1194.211426\n",
      "Train Epoch: 1934 [67584/101520 (67%)] Loss: -1183.105225\n",
      "Train Epoch: 1934 [78848/101520 (78%)] Loss: -1191.909912\n",
      "Train Epoch: 1934 [90112/101520 (89%)] Loss: -1187.863403\n",
      "Train Epoch: 1934 [101376/101520 (100%)] Loss: -1197.117554\n",
      "    epoch          : 1934\n",
      "    loss           : -1187.0192288346027\n",
      "    ess            : 1.96629065544761\n",
      "    log_marginal   : 1187.0518154738536\n",
      "    log_joint      : 1395.5092883852858\n",
      "    val_loss       : -1186.0691077190897\n",
      "    val_ess        : 1.9647743753764941\n",
      "    val_log_marginal: 1186.1017482591712\n",
      "    val_log_joint  : 1394.247165845788\n",
      "Train Epoch: 1935 [0/101520 (0%)] Loss: -1188.735596\n",
      "Train Epoch: 1935 [11264/101520 (11%)] Loss: -1187.173584\n",
      "Train Epoch: 1935 [22528/101520 (22%)] Loss: -1185.876465\n",
      "Train Epoch: 1935 [33792/101520 (33%)] Loss: -1186.101074\n",
      "Train Epoch: 1935 [45056/101520 (44%)] Loss: -1181.285522\n",
      "Train Epoch: 1935 [56320/101520 (55%)] Loss: -1195.060181\n",
      "Train Epoch: 1935 [67584/101520 (67%)] Loss: -1187.677124\n",
      "Train Epoch: 1935 [78848/101520 (78%)] Loss: -1184.023804\n",
      "Train Epoch: 1935 [90112/101520 (89%)] Loss: -1187.062622\n",
      "Train Epoch: 1935 [101376/101520 (100%)] Loss: -1181.311035\n",
      "    epoch          : 1935\n",
      "    loss           : -1186.995820778698\n",
      "    ess            : 1.9661200130405139\n",
      "    log_marginal   : 1187.0286865234375\n",
      "    log_joint      : 1395.4871329302764\n",
      "    val_loss       : -1185.3248768682065\n",
      "    val_ess        : 1.9684180695077647\n",
      "    val_log_marginal: 1185.353467858356\n",
      "    val_log_joint  : 1393.737309994905\n",
      "Train Epoch: 1936 [0/101520 (0%)] Loss: -1184.761963\n",
      "Train Epoch: 1936 [11264/101520 (11%)] Loss: -1187.228027\n",
      "Train Epoch: 1936 [22528/101520 (22%)] Loss: -1195.545410\n",
      "Train Epoch: 1936 [33792/101520 (33%)] Loss: -1195.533569\n",
      "Train Epoch: 1936 [45056/101520 (44%)] Loss: -1192.751831\n",
      "Train Epoch: 1936 [56320/101520 (55%)] Loss: -1191.005615\n",
      "Train Epoch: 1936 [67584/101520 (67%)] Loss: -1187.918701\n",
      "Train Epoch: 1936 [78848/101520 (78%)] Loss: -1181.766602\n",
      "Train Epoch: 1936 [90112/101520 (89%)] Loss: -1187.652832\n",
      "Train Epoch: 1936 [101376/101520 (100%)] Loss: -1189.252563\n",
      "    epoch          : 1936\n",
      "    loss           : -1187.1170212635443\n",
      "    ess            : 1.9658711687404307\n",
      "    log_marginal   : 1187.15001643962\n",
      "    log_joint      : 1395.615149109807\n",
      "    val_loss       : -1185.7259893002717\n",
      "    val_ess        : 1.969064510386923\n",
      "    val_log_marginal: 1185.7551959493885\n",
      "    val_log_joint  : 1393.9772630774457\n",
      "Train Epoch: 1937 [0/101520 (0%)] Loss: -1190.420166\n",
      "Train Epoch: 1937 [11264/101520 (11%)] Loss: -1193.971924\n",
      "Train Epoch: 1937 [22528/101520 (22%)] Loss: -1191.337036\n",
      "Train Epoch: 1937 [33792/101520 (33%)] Loss: -1187.168579\n",
      "Train Epoch: 1937 [45056/101520 (44%)] Loss: -1187.790894\n",
      "Train Epoch: 1937 [56320/101520 (55%)] Loss: -1187.434692\n",
      "Train Epoch: 1937 [67584/101520 (67%)] Loss: -1193.579834\n",
      "Train Epoch: 1937 [78848/101520 (78%)] Loss: -1185.539551\n",
      "Train Epoch: 1937 [90112/101520 (89%)] Loss: -1182.307739\n",
      "Train Epoch: 1937 [101376/101520 (100%)] Loss: -1200.381470\n",
      "    epoch          : 1937\n",
      "    loss           : -1187.1714732107805\n",
      "    ess            : 1.9654910187026364\n",
      "    log_marginal   : 1187.2051842464275\n",
      "    log_joint      : 1395.6855775459328\n",
      "    val_loss       : -1187.0361009680707\n",
      "    val_ess        : 1.9660402484562085\n",
      "    val_log_marginal: 1187.0693465523098\n",
      "    val_log_joint  : 1395.6058190387228\n",
      "Train Epoch: 1938 [0/101520 (0%)] Loss: -1184.728638\n",
      "Train Epoch: 1938 [11264/101520 (11%)] Loss: -1186.615112\n",
      "Train Epoch: 1938 [22528/101520 (22%)] Loss: -1186.475098\n",
      "Train Epoch: 1938 [33792/101520 (33%)] Loss: -1189.386719\n",
      "Train Epoch: 1938 [45056/101520 (44%)] Loss: -1190.701782\n",
      "Train Epoch: 1938 [56320/101520 (55%)] Loss: -1191.918823\n",
      "Train Epoch: 1938 [67584/101520 (67%)] Loss: -1187.970947\n",
      "Train Epoch: 1938 [78848/101520 (78%)] Loss: -1179.593506\n",
      "Train Epoch: 1938 [90112/101520 (89%)] Loss: -1181.949585\n",
      "Train Epoch: 1938 [101376/101520 (100%)] Loss: -1205.448608\n",
      "    epoch          : 1938\n",
      "    loss           : -1187.222461796286\n",
      "    ess            : 1.9653124306070147\n",
      "    log_marginal   : 1187.2561078095555\n",
      "    log_joint      : 1395.7205080578674\n",
      "    val_loss       : -1184.8319622537365\n",
      "    val_ess        : 1.964787094489388\n",
      "    val_log_marginal: 1184.8649530825408\n",
      "    val_log_joint  : 1393.215523097826\n",
      "Train Epoch: 1939 [0/101520 (0%)] Loss: -1196.807129\n",
      "Train Epoch: 1939 [11264/101520 (11%)] Loss: -1187.663330\n",
      "Train Epoch: 1939 [22528/101520 (22%)] Loss: -1185.454346\n",
      "Train Epoch: 1939 [33792/101520 (33%)] Loss: -1187.682129\n",
      "Train Epoch: 1939 [45056/101520 (44%)] Loss: -1179.811646\n",
      "Train Epoch: 1939 [56320/101520 (55%)] Loss: -1190.241211\n",
      "Train Epoch: 1939 [67584/101520 (67%)] Loss: -1184.969482\n",
      "Train Epoch: 1939 [78848/101520 (78%)] Loss: -1186.983154\n",
      "Train Epoch: 1939 [90112/101520 (89%)] Loss: -1192.839600\n",
      "Train Epoch: 1939 [101376/101520 (100%)] Loss: -1195.649780\n",
      "    epoch          : 1939\n",
      "    loss           : -1187.26637337076\n",
      "    ess            : 1.9663073483423972\n",
      "    log_marginal   : 1187.2990146042714\n",
      "    log_joint      : 1395.694936474364\n",
      "    val_loss       : -1185.9302553923233\n",
      "    val_ess        : 1.9682384366574495\n",
      "    val_log_marginal: 1185.9612028702445\n",
      "    val_log_joint  : 1394.261766516644\n",
      "Train Epoch: 1940 [0/101520 (0%)] Loss: -1181.285889\n",
      "Train Epoch: 1940 [11264/101520 (11%)] Loss: -1188.043945\n",
      "Train Epoch: 1940 [22528/101520 (22%)] Loss: -1183.840088\n",
      "Train Epoch: 1940 [33792/101520 (33%)] Loss: -1185.979126\n",
      "Train Epoch: 1940 [45056/101520 (44%)] Loss: -1179.249512\n",
      "Train Epoch: 1940 [56320/101520 (55%)] Loss: -1183.481934\n",
      "Train Epoch: 1940 [67584/101520 (67%)] Loss: -1182.838745\n",
      "Train Epoch: 1940 [78848/101520 (78%)] Loss: -1189.457520\n",
      "Train Epoch: 1940 [90112/101520 (89%)] Loss: -1184.813477\n",
      "Train Epoch: 1940 [101376/101520 (100%)] Loss: -1183.102051\n",
      "    epoch          : 1940\n",
      "    loss           : -1187.2148198266725\n",
      "    ess            : 1.9663582507090354\n",
      "    log_marginal   : 1187.247702747134\n",
      "    log_joint      : 1395.6691980409862\n",
      "    val_loss       : -1185.9905475118885\n",
      "    val_ess        : 1.9595784311709197\n",
      "    val_log_marginal: 1186.0350447944973\n",
      "    val_log_joint  : 1394.4896558678668\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1940.pth ...\n",
      "Train Epoch: 1941 [0/101520 (0%)] Loss: -1181.194458\n",
      "Train Epoch: 1941 [11264/101520 (11%)] Loss: -1186.756958\n",
      "Train Epoch: 1941 [22528/101520 (22%)] Loss: -1191.176025\n",
      "Train Epoch: 1941 [33792/101520 (33%)] Loss: -1183.750244\n",
      "Train Epoch: 1941 [45056/101520 (44%)] Loss: -1185.721802\n",
      "Train Epoch: 1941 [56320/101520 (55%)] Loss: -1184.894287\n",
      "Train Epoch: 1941 [67584/101520 (67%)] Loss: -1180.084106\n",
      "Train Epoch: 1941 [78848/101520 (78%)] Loss: -1191.277222\n",
      "Train Epoch: 1941 [90112/101520 (89%)] Loss: -1189.050049\n",
      "Train Epoch: 1941 [101376/101520 (100%)] Loss: -1186.301880\n",
      "    epoch          : 1941\n",
      "    loss           : -1187.214845590256\n",
      "    ess            : 1.9656912328010827\n",
      "    log_marginal   : 1187.247540804609\n",
      "    log_joint      : 1395.6994708650675\n",
      "    val_loss       : -1185.062982973845\n",
      "    val_ess        : 1.9668653477793154\n",
      "    val_log_marginal: 1185.095554517663\n",
      "    val_log_joint  : 1393.608058763587\n",
      "Train Epoch: 1942 [0/101520 (0%)] Loss: -1194.430786\n",
      "Train Epoch: 1942 [11264/101520 (11%)] Loss: -1189.573242\n",
      "Train Epoch: 1942 [22528/101520 (22%)] Loss: -1189.467163\n",
      "Train Epoch: 1942 [33792/101520 (33%)] Loss: -1184.932861\n",
      "Train Epoch: 1942 [45056/101520 (44%)] Loss: -1180.647339\n",
      "Train Epoch: 1942 [56320/101520 (55%)] Loss: -1183.803467\n",
      "Train Epoch: 1942 [67584/101520 (67%)] Loss: -1188.019165\n",
      "Train Epoch: 1942 [78848/101520 (78%)] Loss: -1190.906860\n",
      "Train Epoch: 1942 [90112/101520 (89%)] Loss: -1187.680786\n",
      "Train Epoch: 1942 [101376/101520 (100%)] Loss: -1190.206299\n",
      "    epoch          : 1942\n",
      "    loss           : -1187.306881085113\n",
      "    ess            : 1.9660498868280918\n",
      "    log_marginal   : 1187.3394928745288\n",
      "    log_joint      : 1395.7078710201397\n",
      "    val_loss       : -1184.205810546875\n",
      "    val_ess        : 1.9663413659386013\n",
      "    val_log_marginal: 1184.2384935461957\n",
      "    val_log_joint  : 1392.9438848080842\n",
      "Train Epoch: 1943 [0/101520 (0%)] Loss: -1188.859863\n",
      "Train Epoch: 1943 [11264/101520 (11%)] Loss: -1183.489502\n",
      "Train Epoch: 1943 [22528/101520 (22%)] Loss: -1199.440430\n",
      "Train Epoch: 1943 [33792/101520 (33%)] Loss: -1188.496094\n",
      "Train Epoch: 1943 [45056/101520 (44%)] Loss: -1179.867676\n",
      "Train Epoch: 1943 [56320/101520 (55%)] Loss: -1186.226196\n",
      "Train Epoch: 1943 [67584/101520 (67%)] Loss: -1192.355103\n",
      "Train Epoch: 1943 [78848/101520 (78%)] Loss: -1180.640381\n",
      "Train Epoch: 1943 [90112/101520 (89%)] Loss: -1189.873779\n",
      "Train Epoch: 1943 [101376/101520 (100%)] Loss: -1201.971313\n",
      "    epoch          : 1943\n",
      "    loss           : -1187.3245739194017\n",
      "    ess            : 1.9658962847599433\n",
      "    log_marginal   : 1187.3574402775596\n",
      "    log_joint      : 1395.743948011542\n",
      "    val_loss       : -1185.2601212211277\n",
      "    val_ess        : 1.9663249513377314\n",
      "    val_log_marginal: 1185.2942266049592\n",
      "    val_log_joint  : 1393.7587519106658\n",
      "Train Epoch: 1944 [0/101520 (0%)] Loss: -1191.268799\n",
      "Train Epoch: 1944 [11264/101520 (11%)] Loss: -1188.683594\n",
      "Train Epoch: 1944 [22528/101520 (22%)] Loss: -1190.923340\n",
      "Train Epoch: 1944 [33792/101520 (33%)] Loss: -1185.093750\n",
      "Train Epoch: 1944 [45056/101520 (44%)] Loss: -1181.621582\n",
      "Train Epoch: 1944 [56320/101520 (55%)] Loss: -1187.321899\n",
      "Train Epoch: 1944 [67584/101520 (67%)] Loss: -1184.109985\n",
      "Train Epoch: 1944 [78848/101520 (78%)] Loss: -1192.686768\n",
      "Train Epoch: 1944 [90112/101520 (89%)] Loss: -1188.016846\n",
      "Train Epoch: 1944 [101376/101520 (100%)] Loss: -1177.148438\n",
      "    epoch          : 1944\n",
      "    loss           : -1187.180697800526\n",
      "    ess            : 1.966769641967275\n",
      "    log_marginal   : 1187.2125544715766\n",
      "    log_joint      : 1395.689538390193\n",
      "    val_loss       : -1185.959132982337\n",
      "    val_ess        : 1.9629921498505964\n",
      "    val_log_marginal: 1185.9959132982337\n",
      "    val_log_joint  : 1394.2285421620245\n",
      "Train Epoch: 1945 [0/101520 (0%)] Loss: -1192.881592\n",
      "Train Epoch: 1945 [11264/101520 (11%)] Loss: -1193.872314\n",
      "Train Epoch: 1945 [22528/101520 (22%)] Loss: -1194.864990\n",
      "Train Epoch: 1945 [33792/101520 (33%)] Loss: -1184.841309\n",
      "Train Epoch: 1945 [45056/101520 (44%)] Loss: -1189.919067\n",
      "Train Epoch: 1945 [56320/101520 (55%)] Loss: -1186.971191\n",
      "Train Epoch: 1945 [67584/101520 (67%)] Loss: -1188.270020\n",
      "Train Epoch: 1945 [78848/101520 (78%)] Loss: -1183.739990\n",
      "Train Epoch: 1945 [90112/101520 (89%)] Loss: -1187.267334\n",
      "Train Epoch: 1945 [101376/101520 (100%)] Loss: -1184.575195\n",
      "    epoch          : 1945\n",
      "    loss           : -1187.23068774046\n",
      "    ess            : 1.9663199252219656\n",
      "    log_marginal   : 1187.2631615106784\n",
      "    log_joint      : 1395.7156730920226\n",
      "    val_loss       : -1186.2461998980978\n",
      "    val_ess        : 1.9648451286813486\n",
      "    val_log_marginal: 1186.2794136379075\n",
      "    val_log_joint  : 1394.6275050951087\n",
      "Train Epoch: 1946 [0/101520 (0%)] Loss: -1182.512329\n",
      "Train Epoch: 1946 [11264/101520 (11%)] Loss: -1186.401367\n",
      "Train Epoch: 1946 [22528/101520 (22%)] Loss: -1184.467529\n",
      "Train Epoch: 1946 [33792/101520 (33%)] Loss: -1180.611816\n",
      "Train Epoch: 1946 [45056/101520 (44%)] Loss: -1188.512939\n",
      "Train Epoch: 1946 [56320/101520 (55%)] Loss: -1183.638306\n",
      "Train Epoch: 1946 [67584/101520 (67%)] Loss: -1185.592529\n",
      "Train Epoch: 1946 [78848/101520 (78%)] Loss: -1185.740356\n",
      "Train Epoch: 1946 [90112/101520 (89%)] Loss: -1186.502197\n",
      "Train Epoch: 1946 [101376/101520 (100%)] Loss: -1187.069702\n",
      "    epoch          : 1946\n",
      "    loss           : -1187.200763951594\n",
      "    ess            : 1.9661462181177571\n",
      "    log_marginal   : 1187.2341854536353\n",
      "    log_joint      : 1395.7261539631752\n",
      "    val_loss       : -1184.5517471976902\n",
      "    val_ess        : 1.9684135499207869\n",
      "    val_log_marginal: 1184.5804921025815\n",
      "    val_log_joint  : 1392.9433965268342\n",
      "Train Epoch: 1947 [0/101520 (0%)] Loss: -1186.938232\n",
      "Train Epoch: 1947 [11264/101520 (11%)] Loss: -1189.322998\n",
      "Train Epoch: 1947 [22528/101520 (22%)] Loss: -1197.580322\n",
      "Train Epoch: 1947 [33792/101520 (33%)] Loss: -1179.540039\n",
      "Train Epoch: 1947 [45056/101520 (44%)] Loss: -1188.002930\n",
      "Train Epoch: 1947 [56320/101520 (55%)] Loss: -1185.212646\n",
      "Train Epoch: 1947 [67584/101520 (67%)] Loss: -1182.857910\n",
      "Train Epoch: 1947 [78848/101520 (78%)] Loss: -1181.542480\n",
      "Train Epoch: 1947 [90112/101520 (89%)] Loss: -1186.027832\n",
      "Train Epoch: 1947 [101376/101520 (100%)] Loss: -1177.193481\n",
      "    epoch          : 1947\n",
      "    loss           : -1187.178236764879\n",
      "    ess            : 1.9662340382235732\n",
      "    log_marginal   : 1187.210638151696\n",
      "    log_joint      : 1395.6287185438914\n",
      "    val_loss       : -1185.801906419837\n",
      "    val_ess        : 1.965191353922305\n",
      "    val_log_marginal: 1185.8354332965353\n",
      "    val_log_joint  : 1394.3975193189538\n",
      "Train Epoch: 1948 [0/101520 (0%)] Loss: -1183.678589\n",
      "Train Epoch: 1948 [11264/101520 (11%)] Loss: -1180.944092\n",
      "Train Epoch: 1948 [22528/101520 (22%)] Loss: -1182.401489\n",
      "Train Epoch: 1948 [33792/101520 (33%)] Loss: -1185.022095\n",
      "Train Epoch: 1948 [45056/101520 (44%)] Loss: -1189.126709\n",
      "Train Epoch: 1948 [56320/101520 (55%)] Loss: -1178.759399\n",
      "Train Epoch: 1948 [67584/101520 (67%)] Loss: -1190.456055\n",
      "Train Epoch: 1948 [78848/101520 (78%)] Loss: -1183.607910\n",
      "Train Epoch: 1948 [90112/101520 (89%)] Loss: -1188.402710\n",
      "Train Epoch: 1948 [101376/101520 (100%)] Loss: -1200.068481\n",
      "    epoch          : 1948\n",
      "    loss           : -1187.2543472980135\n",
      "    ess            : 1.9655300091259444\n",
      "    log_marginal   : 1187.288050359218\n",
      "    log_joint      : 1395.7386315120525\n",
      "    val_loss       : -1185.438036047894\n",
      "    val_ess        : 1.9687310664550117\n",
      "    val_log_marginal: 1185.468994140625\n",
      "    val_log_joint  : 1393.8623418393342\n",
      "Train Epoch: 1949 [0/101520 (0%)] Loss: -1193.883545\n",
      "Train Epoch: 1949 [11264/101520 (11%)] Loss: -1190.633667\n",
      "Train Epoch: 1949 [22528/101520 (22%)] Loss: -1184.652344\n",
      "Train Epoch: 1949 [33792/101520 (33%)] Loss: -1192.717529\n",
      "Train Epoch: 1949 [45056/101520 (44%)] Loss: -1186.203369\n",
      "Train Epoch: 1949 [56320/101520 (55%)] Loss: -1185.760254\n",
      "Train Epoch: 1949 [67584/101520 (67%)] Loss: -1184.469238\n",
      "Train Epoch: 1949 [78848/101520 (78%)] Loss: -1192.991699\n",
      "Train Epoch: 1949 [90112/101520 (89%)] Loss: -1179.731934\n",
      "Train Epoch: 1949 [101376/101520 (100%)] Loss: -1203.203979\n",
      "    epoch          : 1949\n",
      "    loss           : -1187.2003241304178\n",
      "    ess            : 1.9652717053590707\n",
      "    log_marginal   : 1187.2340903737438\n",
      "    log_joint      : 1395.7816769393844\n",
      "    val_loss       : -1186.122017238451\n",
      "    val_ess        : 1.9646196106205815\n",
      "    val_log_marginal: 1186.1525560461957\n",
      "    val_log_joint  : 1394.4217423148777\n",
      "Train Epoch: 1950 [0/101520 (0%)] Loss: -1190.762939\n",
      "Train Epoch: 1950 [11264/101520 (11%)] Loss: -1192.242432\n",
      "Train Epoch: 1950 [22528/101520 (22%)] Loss: -1184.451904\n",
      "Train Epoch: 1950 [33792/101520 (33%)] Loss: -1180.932861\n",
      "Train Epoch: 1950 [45056/101520 (44%)] Loss: -1190.751831\n",
      "Train Epoch: 1950 [56320/101520 (55%)] Loss: -1188.746338\n",
      "Train Epoch: 1950 [67584/101520 (67%)] Loss: -1180.296875\n",
      "Train Epoch: 1950 [78848/101520 (78%)] Loss: -1190.956665\n",
      "Train Epoch: 1950 [90112/101520 (89%)] Loss: -1189.357178\n",
      "Train Epoch: 1950 [101376/101520 (100%)] Loss: -1189.459473\n",
      "    epoch          : 1950\n",
      "    loss           : -1187.2335652873744\n",
      "    ess            : 1.9659574954353984\n",
      "    log_marginal   : 1187.2672297032036\n",
      "    log_joint      : 1395.7199621152638\n",
      "    val_loss       : -1186.436183763587\n",
      "    val_ess        : 1.966677468755971\n",
      "    val_log_marginal: 1186.4669826341712\n",
      "    val_log_joint  : 1395.043701171875\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1950.pth ...\n",
      "Train Epoch: 1951 [0/101520 (0%)] Loss: -1182.100586\n",
      "Train Epoch: 1951 [11264/101520 (11%)] Loss: -1192.906494\n",
      "Train Epoch: 1951 [22528/101520 (22%)] Loss: -1187.027344\n",
      "Train Epoch: 1951 [33792/101520 (33%)] Loss: -1186.607666\n",
      "Train Epoch: 1951 [45056/101520 (44%)] Loss: -1187.189453\n",
      "Train Epoch: 1951 [56320/101520 (55%)] Loss: -1183.971924\n",
      "Train Epoch: 1951 [67584/101520 (67%)] Loss: -1184.821411\n",
      "Train Epoch: 1951 [78848/101520 (78%)] Loss: -1194.747437\n",
      "Train Epoch: 1951 [90112/101520 (89%)] Loss: -1185.171631\n",
      "Train Epoch: 1951 [101376/101520 (100%)] Loss: -1199.220459\n",
      "    epoch          : 1951\n",
      "    loss           : -1187.220421565837\n",
      "    ess            : 1.9650747482501083\n",
      "    log_marginal   : 1187.2541436430197\n",
      "    log_joint      : 1395.7500239233275\n",
      "    val_loss       : -1185.9158404806385\n",
      "    val_ess        : 1.9671495582746423\n",
      "    val_log_marginal: 1185.9475734544837\n",
      "    val_log_joint  : 1394.4428445567255\n",
      "Train Epoch: 1952 [0/101520 (0%)] Loss: -1182.350708\n",
      "Train Epoch: 1952 [11264/101520 (11%)] Loss: -1187.925293\n",
      "Train Epoch: 1952 [22528/101520 (22%)] Loss: -1191.920654\n",
      "Train Epoch: 1952 [33792/101520 (33%)] Loss: -1188.168335\n",
      "Train Epoch: 1952 [45056/101520 (44%)] Loss: -1180.792725\n",
      "Train Epoch: 1952 [56320/101520 (55%)] Loss: -1186.584473\n",
      "Train Epoch: 1952 [67584/101520 (67%)] Loss: -1184.949951\n",
      "Train Epoch: 1952 [78848/101520 (78%)] Loss: -1186.046143\n",
      "Train Epoch: 1952 [90112/101520 (89%)] Loss: -1186.394775\n",
      "Train Epoch: 1952 [101376/101520 (100%)] Loss: -1192.161133\n",
      "    epoch          : 1952\n",
      "    loss           : -1187.346234345556\n",
      "    ess            : 1.9657853985551614\n",
      "    log_marginal   : 1187.3797699434674\n",
      "    log_joint      : 1395.78501025636\n",
      "    val_loss       : -1187.5567096212635\n",
      "    val_ess        : 1.9684494630150173\n",
      "    val_log_marginal: 1187.5858525815217\n",
      "    val_log_joint  : 1396.0787194293478\n",
      "Train Epoch: 1953 [0/101520 (0%)] Loss: -1189.348145\n",
      "Train Epoch: 1953 [11264/101520 (11%)] Loss: -1185.062256\n",
      "Train Epoch: 1953 [22528/101520 (22%)] Loss: -1183.067627\n",
      "Train Epoch: 1953 [33792/101520 (33%)] Loss: -1187.190430\n",
      "Train Epoch: 1953 [45056/101520 (44%)] Loss: -1188.335083\n",
      "Train Epoch: 1953 [56320/101520 (55%)] Loss: -1189.186401\n",
      "Train Epoch: 1953 [67584/101520 (67%)] Loss: -1183.998291\n",
      "Train Epoch: 1953 [78848/101520 (78%)] Loss: -1191.788086\n",
      "Train Epoch: 1953 [90112/101520 (89%)] Loss: -1183.198730\n",
      "Train Epoch: 1953 [101376/101520 (100%)] Loss: -1193.856079\n",
      "    epoch          : 1953\n",
      "    loss           : -1187.351817068742\n",
      "    ess            : 1.9665158975064454\n",
      "    log_marginal   : 1187.384544794284\n",
      "    log_joint      : 1395.7991869749137\n",
      "    val_loss       : -1184.4796620244565\n",
      "    val_ess        : 1.9650880512983904\n",
      "    val_log_marginal: 1184.5101424507473\n",
      "    val_log_joint  : 1393.3830672554348\n",
      "Train Epoch: 1954 [0/101520 (0%)] Loss: -1183.943481\n",
      "Train Epoch: 1954 [11264/101520 (11%)] Loss: -1183.425659\n",
      "Train Epoch: 1954 [22528/101520 (22%)] Loss: -1181.683594\n",
      "Train Epoch: 1954 [33792/101520 (33%)] Loss: -1197.058594\n",
      "Train Epoch: 1954 [45056/101520 (44%)] Loss: -1191.251831\n",
      "Train Epoch: 1954 [56320/101520 (55%)] Loss: -1179.525879\n",
      "Train Epoch: 1954 [67584/101520 (67%)] Loss: -1184.158447\n",
      "Train Epoch: 1954 [78848/101520 (78%)] Loss: -1192.174805\n",
      "Train Epoch: 1954 [90112/101520 (89%)] Loss: -1186.249512\n",
      "Train Epoch: 1954 [101376/101520 (100%)] Loss: -1186.806885\n",
      "    epoch          : 1954\n",
      "    loss           : -1187.318728039612\n",
      "    ess            : 1.965788015169115\n",
      "    log_marginal   : 1187.352346449042\n",
      "    log_joint      : 1395.8051567652717\n",
      "    val_loss       : -1186.816549549932\n",
      "    val_ess        : 1.9665220975875854\n",
      "    val_log_marginal: 1186.851222826087\n",
      "    val_log_joint  : 1395.5309315557065\n",
      "Train Epoch: 1955 [0/101520 (0%)] Loss: -1192.787109\n",
      "Train Epoch: 1955 [11264/101520 (11%)] Loss: -1190.430908\n",
      "Train Epoch: 1955 [22528/101520 (22%)] Loss: -1190.634644\n",
      "Train Epoch: 1955 [33792/101520 (33%)] Loss: -1186.536377\n",
      "Train Epoch: 1955 [45056/101520 (44%)] Loss: -1189.301270\n",
      "Train Epoch: 1955 [56320/101520 (55%)] Loss: -1194.791138\n",
      "Train Epoch: 1955 [67584/101520 (67%)] Loss: -1180.833496\n",
      "Train Epoch: 1955 [78848/101520 (78%)] Loss: -1193.629761\n",
      "Train Epoch: 1955 [90112/101520 (89%)] Loss: -1192.831909\n",
      "Train Epoch: 1955 [101376/101520 (100%)] Loss: -1189.116089\n",
      "    epoch          : 1955\n",
      "    loss           : -1187.42243627807\n",
      "    ess            : 1.966143947150839\n",
      "    log_marginal   : 1187.4556866363066\n",
      "    log_joint      : 1395.7940287374372\n",
      "    val_loss       : -1186.8288202700408\n",
      "    val_ess        : 1.9659819499306057\n",
      "    val_log_marginal: 1186.8615934952445\n",
      "    val_log_joint  : 1395.4150019106658\n",
      "Train Epoch: 1956 [0/101520 (0%)] Loss: -1196.221191\n",
      "Train Epoch: 1956 [11264/101520 (11%)] Loss: -1184.579834\n",
      "Train Epoch: 1956 [22528/101520 (22%)] Loss: -1186.412598\n",
      "Train Epoch: 1956 [33792/101520 (33%)] Loss: -1192.076416\n",
      "Train Epoch: 1956 [45056/101520 (44%)] Loss: -1185.388306\n",
      "Train Epoch: 1956 [56320/101520 (55%)] Loss: -1187.068848\n",
      "Train Epoch: 1956 [67584/101520 (67%)] Loss: -1181.964355\n",
      "Train Epoch: 1956 [78848/101520 (78%)] Loss: -1182.691040\n",
      "Train Epoch: 1956 [90112/101520 (89%)] Loss: -1191.428711\n",
      "Train Epoch: 1956 [101376/101520 (100%)] Loss: -1187.907227\n",
      "    epoch          : 1956\n",
      "    loss           : -1187.3633928921954\n",
      "    ess            : 1.965564003541841\n",
      "    log_marginal   : 1187.396194227976\n",
      "    log_joint      : 1395.8530512670775\n",
      "    val_loss       : -1186.3863949983017\n",
      "    val_ess        : 1.9671762404234514\n",
      "    val_log_marginal: 1186.418844471807\n",
      "    val_log_joint  : 1394.8952742866848\n",
      "Train Epoch: 1957 [0/101520 (0%)] Loss: -1182.806152\n",
      "Train Epoch: 1957 [11264/101520 (11%)] Loss: -1180.759766\n",
      "Train Epoch: 1957 [22528/101520 (22%)] Loss: -1188.760254\n",
      "Train Epoch: 1957 [33792/101520 (33%)] Loss: -1187.930786\n",
      "Train Epoch: 1957 [45056/101520 (44%)] Loss: -1184.750732\n",
      "Train Epoch: 1957 [56320/101520 (55%)] Loss: -1183.719482\n",
      "Train Epoch: 1957 [67584/101520 (67%)] Loss: -1183.808594\n",
      "Train Epoch: 1957 [78848/101520 (78%)] Loss: -1187.896484\n",
      "Train Epoch: 1957 [90112/101520 (89%)] Loss: -1193.552979\n",
      "Train Epoch: 1957 [101376/101520 (100%)] Loss: -1186.906494\n",
      "    epoch          : 1957\n",
      "    loss           : -1187.3631597931062\n",
      "    ess            : 1.966155422392802\n",
      "    log_marginal   : 1187.3960402598932\n",
      "    log_joint      : 1395.875384613497\n",
      "    val_loss       : -1188.0206723420517\n",
      "    val_ess        : 1.9631469353385593\n",
      "    val_log_marginal: 1188.0554570737092\n",
      "    val_log_joint  : 1396.6240818189538\n",
      "Train Epoch: 1958 [0/101520 (0%)] Loss: -1182.572754\n",
      "Train Epoch: 1958 [11264/101520 (11%)] Loss: -1183.376221\n",
      "Train Epoch: 1958 [22528/101520 (22%)] Loss: -1183.067993\n",
      "Train Epoch: 1958 [33792/101520 (33%)] Loss: -1187.305420\n",
      "Train Epoch: 1958 [45056/101520 (44%)] Loss: -1187.426025\n",
      "Train Epoch: 1958 [56320/101520 (55%)] Loss: -1190.416260\n",
      "Train Epoch: 1958 [67584/101520 (67%)] Loss: -1183.893066\n",
      "Train Epoch: 1958 [78848/101520 (78%)] Loss: -1185.967896\n",
      "Train Epoch: 1958 [90112/101520 (89%)] Loss: -1178.474365\n",
      "Train Epoch: 1958 [101376/101520 (100%)] Loss: -1207.012939\n",
      "    epoch          : 1958\n",
      "    loss           : -1187.4079712527482\n",
      "    ess            : 1.9659518668399982\n",
      "    log_marginal   : 1187.4403272465845\n",
      "    log_joint      : 1395.8823039759343\n",
      "    val_loss       : -1185.6971276324728\n",
      "    val_ess        : 1.9659725272137185\n",
      "    val_log_marginal: 1185.7263289741848\n",
      "    val_log_joint  : 1394.1334653108017\n",
      "Train Epoch: 1959 [0/101520 (0%)] Loss: -1191.284790\n",
      "Train Epoch: 1959 [11264/101520 (11%)] Loss: -1184.724243\n",
      "Train Epoch: 1959 [22528/101520 (22%)] Loss: -1192.942383\n",
      "Train Epoch: 1959 [33792/101520 (33%)] Loss: -1193.603271\n",
      "Train Epoch: 1959 [45056/101520 (44%)] Loss: -1190.924927\n",
      "Train Epoch: 1959 [56320/101520 (55%)] Loss: -1187.039307\n",
      "Train Epoch: 1959 [67584/101520 (67%)] Loss: -1177.441162\n",
      "Train Epoch: 1959 [78848/101520 (78%)] Loss: -1192.149536\n",
      "Train Epoch: 1959 [90112/101520 (89%)] Loss: -1185.695312\n",
      "Train Epoch: 1959 [101376/101520 (100%)] Loss: -1187.495605\n",
      "    epoch          : 1959\n",
      "    loss           : -1187.424207831148\n",
      "    ess            : 1.9667767843409398\n",
      "    log_marginal   : 1187.456188412767\n",
      "    log_joint      : 1395.8264558878377\n",
      "    val_loss       : -1187.1521367612092\n",
      "    val_ess        : 1.9621940332910288\n",
      "    val_log_marginal: 1187.193656589674\n",
      "    val_log_joint  : 1395.529349949049\n",
      "Train Epoch: 1960 [0/101520 (0%)] Loss: -1178.563843\n",
      "Train Epoch: 1960 [11264/101520 (11%)] Loss: -1185.912598\n",
      "Train Epoch: 1960 [22528/101520 (22%)] Loss: -1189.359985\n",
      "Train Epoch: 1960 [33792/101520 (33%)] Loss: -1186.771729\n",
      "Train Epoch: 1960 [45056/101520 (44%)] Loss: -1184.137939\n",
      "Train Epoch: 1960 [56320/101520 (55%)] Loss: -1186.173584\n",
      "Train Epoch: 1960 [67584/101520 (67%)] Loss: -1180.914429\n",
      "Train Epoch: 1960 [78848/101520 (78%)] Loss: -1182.907715\n",
      "Train Epoch: 1960 [90112/101520 (89%)] Loss: -1188.539795\n",
      "Train Epoch: 1960 [101376/101520 (100%)] Loss: -1178.102173\n",
      "    epoch          : 1960\n",
      "    loss           : -1187.3512619248586\n",
      "    ess            : 1.9661075032536108\n",
      "    log_marginal   : 1187.3850287816033\n",
      "    log_joint      : 1395.8605036903266\n",
      "    val_loss       : -1186.5162937330163\n",
      "    val_ess        : 1.9644219978995945\n",
      "    val_log_marginal: 1186.5526600713315\n",
      "    val_log_joint  : 1395.130907141644\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1960.pth ...\n",
      "Train Epoch: 1961 [0/101520 (0%)] Loss: -1187.551758\n",
      "Train Epoch: 1961 [11264/101520 (11%)] Loss: -1186.885254\n",
      "Train Epoch: 1961 [22528/101520 (22%)] Loss: -1186.724609\n",
      "Train Epoch: 1961 [33792/101520 (33%)] Loss: -1185.945068\n",
      "Train Epoch: 1961 [45056/101520 (44%)] Loss: -1190.849976\n",
      "Train Epoch: 1961 [56320/101520 (55%)] Loss: -1189.652100\n",
      "Train Epoch: 1961 [67584/101520 (67%)] Loss: -1182.528809\n",
      "Train Epoch: 1961 [78848/101520 (78%)] Loss: -1188.113037\n",
      "Train Epoch: 1961 [90112/101520 (89%)] Loss: -1191.688843\n",
      "Train Epoch: 1961 [101376/101520 (100%)] Loss: -1183.653687\n",
      "    epoch          : 1961\n",
      "    loss           : -1187.4143311773712\n",
      "    ess            : 1.9655709739905507\n",
      "    log_marginal   : 1187.44738892215\n",
      "    log_joint      : 1395.8720709259187\n",
      "    val_loss       : -1186.1248407778533\n",
      "    val_ess        : 1.9679209926854009\n",
      "    val_log_marginal: 1186.155666185462\n",
      "    val_log_joint  : 1394.3419614045517\n",
      "Train Epoch: 1962 [0/101520 (0%)] Loss: -1183.210815\n",
      "Train Epoch: 1962 [11264/101520 (11%)] Loss: -1186.574951\n",
      "Train Epoch: 1962 [22528/101520 (22%)] Loss: -1188.719971\n",
      "Train Epoch: 1962 [33792/101520 (33%)] Loss: -1185.044189\n",
      "Train Epoch: 1962 [45056/101520 (44%)] Loss: -1182.779297\n",
      "Train Epoch: 1962 [56320/101520 (55%)] Loss: -1185.512207\n",
      "Train Epoch: 1962 [67584/101520 (67%)] Loss: -1186.840088\n",
      "Train Epoch: 1962 [78848/101520 (78%)] Loss: -1184.543335\n",
      "Train Epoch: 1962 [90112/101520 (89%)] Loss: -1185.849365\n",
      "Train Epoch: 1962 [101376/101520 (100%)] Loss: -1177.639282\n",
      "    epoch          : 1962\n",
      "    loss           : -1187.4122265379633\n",
      "    ess            : 1.9653861354942896\n",
      "    log_marginal   : 1187.4458130496232\n",
      "    log_joint      : 1395.8886988654208\n",
      "    val_loss       : -1186.4236635954483\n",
      "    val_ess        : 1.9659062364827031\n",
      "    val_log_marginal: 1186.4524615743885\n",
      "    val_log_joint  : 1394.810552182405\n",
      "Train Epoch: 1963 [0/101520 (0%)] Loss: -1186.824219\n",
      "Train Epoch: 1963 [11264/101520 (11%)] Loss: -1183.494385\n",
      "Train Epoch: 1963 [22528/101520 (22%)] Loss: -1185.008301\n",
      "Train Epoch: 1963 [33792/101520 (33%)] Loss: -1184.072632\n",
      "Train Epoch: 1963 [45056/101520 (44%)] Loss: -1195.155884\n",
      "Train Epoch: 1963 [56320/101520 (55%)] Loss: -1189.617432\n",
      "Train Epoch: 1963 [67584/101520 (67%)] Loss: -1180.758301\n",
      "Train Epoch: 1963 [78848/101520 (78%)] Loss: -1185.810425\n",
      "Train Epoch: 1963 [90112/101520 (89%)] Loss: -1187.273682\n",
      "Train Epoch: 1963 [101376/101520 (100%)] Loss: -1193.909302\n",
      "    epoch          : 1963\n",
      "    loss           : -1187.4829917409313\n",
      "    ess            : 1.9658282228450679\n",
      "    log_marginal   : 1187.51652856568\n",
      "    log_joint      : 1395.9674041594692\n",
      "    val_loss       : -1185.9095512058425\n",
      "    val_ess        : 1.964177333790323\n",
      "    val_log_marginal: 1185.9487623131793\n",
      "    val_log_joint  : 1394.453714121943\n",
      "Train Epoch: 1964 [0/101520 (0%)] Loss: -1185.177490\n",
      "Train Epoch: 1964 [11264/101520 (11%)] Loss: -1184.754395\n",
      "Train Epoch: 1964 [22528/101520 (22%)] Loss: -1182.694336\n",
      "Train Epoch: 1964 [33792/101520 (33%)] Loss: -1190.711304\n",
      "Train Epoch: 1964 [45056/101520 (44%)] Loss: -1188.471436\n",
      "Train Epoch: 1964 [56320/101520 (55%)] Loss: -1194.297363\n",
      "Train Epoch: 1964 [67584/101520 (67%)] Loss: -1181.855591\n",
      "Train Epoch: 1964 [78848/101520 (78%)] Loss: -1188.571533\n",
      "Train Epoch: 1964 [90112/101520 (89%)] Loss: -1183.328125\n",
      "Train Epoch: 1964 [101376/101520 (100%)] Loss: -1199.718628\n",
      "    epoch          : 1964\n",
      "    loss           : -1187.5717650753768\n",
      "    ess            : 1.9660271238442042\n",
      "    log_marginal   : 1187.6045216315954\n",
      "    log_joint      : 1396.009582212822\n",
      "    val_loss       : -1185.7126570991848\n",
      "    val_ess        : 1.9630663965059363\n",
      "    val_log_marginal: 1185.7525687839675\n",
      "    val_log_joint  : 1394.1212158203125\n",
      "Train Epoch: 1965 [0/101520 (0%)] Loss: -1191.058838\n",
      "Train Epoch: 1965 [11264/101520 (11%)] Loss: -1187.440430\n",
      "Train Epoch: 1965 [22528/101520 (22%)] Loss: -1186.349609\n",
      "Train Epoch: 1965 [33792/101520 (33%)] Loss: -1192.207764\n",
      "Train Epoch: 1965 [45056/101520 (44%)] Loss: -1189.753418\n",
      "Train Epoch: 1965 [56320/101520 (55%)] Loss: -1194.050049\n",
      "Train Epoch: 1965 [67584/101520 (67%)] Loss: -1179.747070\n",
      "Train Epoch: 1965 [78848/101520 (78%)] Loss: -1185.140869\n",
      "Train Epoch: 1965 [90112/101520 (89%)] Loss: -1187.522949\n",
      "Train Epoch: 1965 [101376/101520 (100%)] Loss: -1195.796875\n",
      "    epoch          : 1965\n",
      "    loss           : -1187.5392955990892\n",
      "    ess            : 1.9658868348778193\n",
      "    log_marginal   : 1187.57162214883\n",
      "    log_joint      : 1396.0069972666065\n",
      "    val_loss       : -1185.424465013587\n",
      "    val_ess        : 1.967287955076798\n",
      "    val_log_marginal: 1185.4546057659647\n",
      "    val_log_joint  : 1394.056688391644\n",
      "Train Epoch: 1966 [0/101520 (0%)] Loss: -1183.420654\n",
      "Train Epoch: 1966 [11264/101520 (11%)] Loss: -1187.411865\n",
      "Train Epoch: 1966 [22528/101520 (22%)] Loss: -1183.992188\n",
      "Train Epoch: 1966 [33792/101520 (33%)] Loss: -1185.157715\n",
      "Train Epoch: 1966 [45056/101520 (44%)] Loss: -1186.744873\n",
      "Train Epoch: 1966 [56320/101520 (55%)] Loss: -1187.623169\n",
      "Train Epoch: 1966 [67584/101520 (67%)] Loss: -1181.838379\n",
      "Train Epoch: 1966 [78848/101520 (78%)] Loss: -1187.784180\n",
      "Train Epoch: 1966 [90112/101520 (89%)] Loss: -1185.643311\n",
      "Train Epoch: 1966 [101376/101520 (100%)] Loss: -1189.723267\n",
      "    epoch          : 1966\n",
      "    loss           : -1187.4925880623823\n",
      "    ess            : 1.9657169676306259\n",
      "    log_marginal   : 1187.5255470467573\n",
      "    log_joint      : 1395.9089539494346\n",
      "    val_loss       : -1185.6301057235055\n",
      "    val_ess        : 1.965162510457246\n",
      "    val_log_marginal: 1185.6615414826767\n",
      "    val_log_joint  : 1393.825439453125\n",
      "Train Epoch: 1967 [0/101520 (0%)] Loss: -1197.535889\n",
      "Train Epoch: 1967 [11264/101520 (11%)] Loss: -1184.232666\n",
      "Train Epoch: 1967 [22528/101520 (22%)] Loss: -1182.934082\n",
      "Train Epoch: 1967 [33792/101520 (33%)] Loss: -1190.441162\n",
      "Train Epoch: 1967 [45056/101520 (44%)] Loss: -1183.424561\n",
      "Train Epoch: 1967 [56320/101520 (55%)] Loss: -1188.093018\n",
      "Train Epoch: 1967 [67584/101520 (67%)] Loss: -1186.996338\n",
      "Train Epoch: 1967 [78848/101520 (78%)] Loss: -1181.147461\n",
      "Train Epoch: 1967 [90112/101520 (89%)] Loss: -1198.148193\n",
      "Train Epoch: 1967 [101376/101520 (100%)] Loss: -1197.577637\n",
      "    epoch          : 1967\n",
      "    loss           : -1187.4865833071608\n",
      "    ess            : 1.9663524316183885\n",
      "    log_marginal   : 1187.5188528089668\n",
      "    log_joint      : 1395.8989699473932\n",
      "    val_loss       : -1187.9157077955163\n",
      "    val_ess        : 1.9676835277806157\n",
      "    val_log_marginal: 1187.9446384595788\n",
      "    val_log_joint  : 1396.1899201766305\n",
      "Train Epoch: 1968 [0/101520 (0%)] Loss: -1187.517700\n",
      "Train Epoch: 1968 [11264/101520 (11%)] Loss: -1194.647827\n",
      "Train Epoch: 1968 [22528/101520 (22%)] Loss: -1195.109131\n",
      "Train Epoch: 1968 [33792/101520 (33%)] Loss: -1193.477539\n",
      "Train Epoch: 1968 [45056/101520 (44%)] Loss: -1186.614746\n",
      "Train Epoch: 1968 [56320/101520 (55%)] Loss: -1191.099121\n",
      "Train Epoch: 1968 [67584/101520 (67%)] Loss: -1194.911865\n",
      "Train Epoch: 1968 [78848/101520 (78%)] Loss: -1189.107544\n",
      "Train Epoch: 1968 [90112/101520 (89%)] Loss: -1190.192871\n",
      "Train Epoch: 1968 [101376/101520 (100%)] Loss: -1187.227539\n",
      "    epoch          : 1968\n",
      "    loss           : -1187.468238408841\n",
      "    ess            : 1.9667986667335933\n",
      "    log_marginal   : 1187.5004214186165\n",
      "    log_joint      : 1395.9262308858747\n",
      "    val_loss       : -1186.3613387398098\n",
      "    val_ess        : 1.9648269881372866\n",
      "    val_log_marginal: 1186.394536557405\n",
      "    val_log_joint  : 1394.8902640964675\n",
      "Train Epoch: 1969 [0/101520 (0%)] Loss: -1194.345337\n",
      "Train Epoch: 1969 [11264/101520 (11%)] Loss: -1180.582764\n",
      "Train Epoch: 1969 [22528/101520 (22%)] Loss: -1187.501099\n",
      "Train Epoch: 1969 [33792/101520 (33%)] Loss: -1195.104370\n",
      "Train Epoch: 1969 [45056/101520 (44%)] Loss: -1189.167725\n",
      "Train Epoch: 1969 [56320/101520 (55%)] Loss: -1196.052979\n",
      "Train Epoch: 1969 [67584/101520 (67%)] Loss: -1190.022705\n",
      "Train Epoch: 1969 [78848/101520 (78%)] Loss: -1184.345459\n",
      "Train Epoch: 1969 [90112/101520 (89%)] Loss: -1183.110107\n",
      "Train Epoch: 1969 [101376/101520 (100%)] Loss: -1183.508667\n",
      "    epoch          : 1969\n",
      "    loss           : -1187.4134680973225\n",
      "    ess            : 1.9661340407989731\n",
      "    log_marginal   : 1187.447167478015\n",
      "    log_joint      : 1395.9134515350188\n",
      "    val_loss       : -1185.7683901579483\n",
      "    val_ess        : 1.9667098988657412\n",
      "    val_log_marginal: 1185.8006167204483\n",
      "    val_log_joint  : 1393.9924210258152\n",
      "Train Epoch: 1970 [0/101520 (0%)] Loss: -1184.111572\n",
      "Train Epoch: 1970 [11264/101520 (11%)] Loss: -1172.154175\n",
      "Train Epoch: 1970 [22528/101520 (22%)] Loss: -1186.396240\n",
      "Train Epoch: 1970 [33792/101520 (33%)] Loss: -1191.468628\n",
      "Train Epoch: 1970 [45056/101520 (44%)] Loss: -1184.745850\n",
      "Train Epoch: 1970 [56320/101520 (55%)] Loss: -1192.407593\n",
      "Train Epoch: 1970 [67584/101520 (67%)] Loss: -1188.877197\n",
      "Train Epoch: 1970 [78848/101520 (78%)] Loss: -1193.753174\n",
      "Train Epoch: 1970 [90112/101520 (89%)] Loss: -1184.707642\n",
      "Train Epoch: 1970 [101376/101520 (100%)] Loss: -1192.186768\n",
      "    epoch          : 1970\n",
      "    loss           : -1187.5769245396907\n",
      "    ess            : 1.9655987928860152\n",
      "    log_marginal   : 1187.6104687254633\n",
      "    log_joint      : 1396.0120739194017\n",
      "    val_loss       : -1185.3257419752038\n",
      "    val_ess        : 1.9671435874441396\n",
      "    val_log_marginal: 1185.3550176205842\n",
      "    val_log_joint  : 1393.8634617017663\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1970.pth ...\n",
      "Train Epoch: 1971 [0/101520 (0%)] Loss: -1192.076660\n",
      "Train Epoch: 1971 [11264/101520 (11%)] Loss: -1182.108154\n",
      "Train Epoch: 1971 [22528/101520 (22%)] Loss: -1191.316406\n",
      "Train Epoch: 1971 [33792/101520 (33%)] Loss: -1185.868774\n",
      "Train Epoch: 1971 [45056/101520 (44%)] Loss: -1194.028809\n",
      "Train Epoch: 1971 [56320/101520 (55%)] Loss: -1189.098267\n",
      "Train Epoch: 1971 [67584/101520 (67%)] Loss: -1189.149658\n",
      "Train Epoch: 1971 [78848/101520 (78%)] Loss: -1190.477905\n",
      "Train Epoch: 1971 [90112/101520 (89%)] Loss: -1188.471069\n",
      "Train Epoch: 1971 [101376/101520 (100%)] Loss: -1193.067261\n",
      "    epoch          : 1971\n",
      "    loss           : -1187.603619292753\n",
      "    ess            : 1.966413919051089\n",
      "    log_marginal   : 1187.6357771523633\n",
      "    log_joint      : 1396.07095597617\n",
      "    val_loss       : -1184.0759861158288\n",
      "    val_ess        : 1.9652725406315015\n",
      "    val_log_marginal: 1184.1068486752717\n",
      "    val_log_joint  : 1392.5244299847147\n",
      "Train Epoch: 1972 [0/101520 (0%)] Loss: -1179.474731\n",
      "Train Epoch: 1972 [11264/101520 (11%)] Loss: -1189.036621\n",
      "Train Epoch: 1972 [22528/101520 (22%)] Loss: -1190.434204\n",
      "Train Epoch: 1972 [33792/101520 (33%)] Loss: -1192.339355\n",
      "Train Epoch: 1972 [45056/101520 (44%)] Loss: -1180.625244\n",
      "Train Epoch: 1972 [56320/101520 (55%)] Loss: -1183.616455\n",
      "Train Epoch: 1972 [67584/101520 (67%)] Loss: -1185.027100\n",
      "Train Epoch: 1972 [78848/101520 (78%)] Loss: -1178.237549\n",
      "Train Epoch: 1972 [90112/101520 (89%)] Loss: -1187.410889\n",
      "Train Epoch: 1972 [101376/101520 (100%)] Loss: -1170.772461\n",
      "    epoch          : 1972\n",
      "    loss           : -1187.4964348107726\n",
      "    ess            : 1.966189395243199\n",
      "    log_marginal   : 1187.5294612711998\n",
      "    log_joint      : 1395.979362142745\n",
      "    val_loss       : -1188.103515625\n",
      "    val_ess        : 1.9664121918056323\n",
      "    val_log_marginal: 1188.1362623131793\n",
      "    val_log_joint  : 1396.863716457201\n",
      "Train Epoch: 1973 [0/101520 (0%)] Loss: -1184.816284\n",
      "Train Epoch: 1973 [11264/101520 (11%)] Loss: -1190.397217\n",
      "Train Epoch: 1973 [22528/101520 (22%)] Loss: -1189.309326\n",
      "Train Epoch: 1973 [33792/101520 (33%)] Loss: -1188.016846\n",
      "Train Epoch: 1973 [45056/101520 (44%)] Loss: -1188.173096\n",
      "Train Epoch: 1973 [56320/101520 (55%)] Loss: -1185.912109\n",
      "Train Epoch: 1973 [67584/101520 (67%)] Loss: -1182.479004\n",
      "Train Epoch: 1973 [78848/101520 (78%)] Loss: -1185.470459\n",
      "Train Epoch: 1973 [90112/101520 (89%)] Loss: -1187.312988\n",
      "Train Epoch: 1973 [101376/101520 (100%)] Loss: -1188.031860\n",
      "    epoch          : 1973\n",
      "    loss           : -1187.629355272456\n",
      "    ess            : 1.9658394938138262\n",
      "    log_marginal   : 1187.6622854261543\n",
      "    log_joint      : 1396.0275388171326\n",
      "    val_loss       : -1187.6636803668478\n",
      "    val_ess        : 1.967719114345053\n",
      "    val_log_marginal: 1187.694580078125\n",
      "    val_log_joint  : 1396.0749140200408\n",
      "Train Epoch: 1974 [0/101520 (0%)] Loss: -1190.189941\n",
      "Train Epoch: 1974 [11264/101520 (11%)] Loss: -1193.277832\n",
      "Train Epoch: 1974 [22528/101520 (22%)] Loss: -1180.795898\n",
      "Train Epoch: 1974 [33792/101520 (33%)] Loss: -1179.095093\n",
      "Train Epoch: 1974 [45056/101520 (44%)] Loss: -1186.513184\n",
      "Train Epoch: 1974 [56320/101520 (55%)] Loss: -1187.987549\n",
      "Train Epoch: 1974 [67584/101520 (67%)] Loss: -1183.048950\n",
      "Train Epoch: 1974 [78848/101520 (78%)] Loss: -1190.051025\n",
      "Train Epoch: 1974 [90112/101520 (89%)] Loss: -1192.076660\n",
      "Train Epoch: 1974 [101376/101520 (100%)] Loss: -1198.159546\n",
      "    epoch          : 1974\n",
      "    loss           : -1187.59063321981\n",
      "    ess            : 1.9657807434024523\n",
      "    log_marginal   : 1187.6244552842336\n",
      "    log_joint      : 1396.1413739841787\n",
      "    val_loss       : -1186.5863886294158\n",
      "    val_ess        : 1.9663074742192808\n",
      "    val_log_marginal: 1186.6173997961957\n",
      "    val_log_joint  : 1394.957423997962\n",
      "Train Epoch: 1975 [0/101520 (0%)] Loss: -1188.993042\n",
      "Train Epoch: 1975 [11264/101520 (11%)] Loss: -1188.011597\n",
      "Train Epoch: 1975 [22528/101520 (22%)] Loss: -1190.748291\n",
      "Train Epoch: 1975 [33792/101520 (33%)] Loss: -1188.144531\n",
      "Train Epoch: 1975 [45056/101520 (44%)] Loss: -1186.938843\n",
      "Train Epoch: 1975 [56320/101520 (55%)] Loss: -1181.500977\n",
      "Train Epoch: 1975 [67584/101520 (67%)] Loss: -1182.341064\n",
      "Train Epoch: 1975 [78848/101520 (78%)] Loss: -1189.124512\n",
      "Train Epoch: 1975 [90112/101520 (89%)] Loss: -1183.738525\n",
      "Train Epoch: 1975 [101376/101520 (100%)] Loss: -1187.757202\n",
      "    epoch          : 1975\n",
      "    loss           : -1187.6454391096106\n",
      "    ess            : 1.965875212271609\n",
      "    log_marginal   : 1187.679067333739\n",
      "    log_joint      : 1396.1447152755968\n",
      "    val_loss       : -1186.539545473845\n",
      "    val_ess        : 1.9668463572211887\n",
      "    val_log_marginal: 1186.5717932659647\n",
      "    val_log_joint  : 1394.7540442425272\n",
      "Train Epoch: 1976 [0/101520 (0%)] Loss: -1185.968262\n",
      "Train Epoch: 1976 [11264/101520 (11%)] Loss: -1183.996338\n",
      "Train Epoch: 1976 [22528/101520 (22%)] Loss: -1184.416260\n",
      "Train Epoch: 1976 [33792/101520 (33%)] Loss: -1185.422241\n",
      "Train Epoch: 1976 [45056/101520 (44%)] Loss: -1191.250244\n",
      "Train Epoch: 1976 [56320/101520 (55%)] Loss: -1187.404785\n",
      "Train Epoch: 1976 [67584/101520 (67%)] Loss: -1181.547974\n",
      "Train Epoch: 1976 [78848/101520 (78%)] Loss: -1185.669189\n",
      "Train Epoch: 1976 [90112/101520 (89%)] Loss: -1180.460449\n",
      "Train Epoch: 1976 [101376/101520 (100%)] Loss: -1185.529663\n",
      "    epoch          : 1976\n",
      "    loss           : -1187.7602778295775\n",
      "    ess            : 1.9656241982426477\n",
      "    log_marginal   : 1187.7926025390625\n",
      "    log_joint      : 1396.2130415259894\n",
      "    val_loss       : -1187.4052628226902\n",
      "    val_ess        : 1.9660250622293223\n",
      "    val_log_marginal: 1187.4381050441575\n",
      "    val_log_joint  : 1395.6975309952445\n",
      "Train Epoch: 1977 [0/101520 (0%)] Loss: -1194.230347\n",
      "Train Epoch: 1977 [11264/101520 (11%)] Loss: -1181.693848\n",
      "Train Epoch: 1977 [22528/101520 (22%)] Loss: -1182.924438\n",
      "Train Epoch: 1977 [33792/101520 (33%)] Loss: -1184.298096\n",
      "Train Epoch: 1977 [45056/101520 (44%)] Loss: -1191.502930\n",
      "Train Epoch: 1977 [56320/101520 (55%)] Loss: -1190.626221\n",
      "Train Epoch: 1977 [67584/101520 (67%)] Loss: -1185.251465\n",
      "Train Epoch: 1977 [78848/101520 (78%)] Loss: -1184.476318\n",
      "Train Epoch: 1977 [90112/101520 (89%)] Loss: -1186.172729\n",
      "Train Epoch: 1977 [101376/101520 (100%)] Loss: -1197.995605\n",
      "    epoch          : 1977\n",
      "    loss           : -1187.835073193114\n",
      "    ess            : 1.9665011395162075\n",
      "    log_marginal   : 1187.866858094182\n",
      "    log_joint      : 1396.2995439845713\n",
      "    val_loss       : -1187.9182235054348\n",
      "    val_ess        : 1.9637380838394165\n",
      "    val_log_marginal: 1187.9548499065897\n",
      "    val_log_joint  : 1396.6453751273777\n",
      "Train Epoch: 1978 [0/101520 (0%)] Loss: -1183.710693\n",
      "Train Epoch: 1978 [11264/101520 (11%)] Loss: -1190.435059\n",
      "Train Epoch: 1978 [22528/101520 (22%)] Loss: -1178.661255\n",
      "Train Epoch: 1978 [33792/101520 (33%)] Loss: -1192.510864\n",
      "Train Epoch: 1978 [45056/101520 (44%)] Loss: -1191.843140\n",
      "Train Epoch: 1978 [56320/101520 (55%)] Loss: -1189.086670\n",
      "Train Epoch: 1978 [67584/101520 (67%)] Loss: -1185.149170\n",
      "Train Epoch: 1978 [78848/101520 (78%)] Loss: -1186.924683\n",
      "Train Epoch: 1978 [90112/101520 (89%)] Loss: -1185.381104\n",
      "Train Epoch: 1978 [101376/101520 (100%)] Loss: -1185.376953\n",
      "    epoch          : 1978\n",
      "    loss           : -1187.7968811341866\n",
      "    ess            : 1.9653722886464104\n",
      "    log_marginal   : 1187.8307099462154\n",
      "    log_joint      : 1396.2412980429492\n",
      "    val_loss       : -1188.0354853091033\n",
      "    val_ess        : 1.9639205051505046\n",
      "    val_log_marginal: 1188.0763364045517\n",
      "    val_log_joint  : 1396.5164954144022\n",
      "Train Epoch: 1979 [0/101520 (0%)] Loss: -1186.985962\n",
      "Train Epoch: 1979 [11264/101520 (11%)] Loss: -1191.326050\n",
      "Train Epoch: 1979 [22528/101520 (22%)] Loss: -1189.346680\n",
      "Train Epoch: 1979 [33792/101520 (33%)] Loss: -1181.937012\n",
      "Train Epoch: 1979 [45056/101520 (44%)] Loss: -1182.847534\n",
      "Train Epoch: 1979 [56320/101520 (55%)] Loss: -1185.993530\n",
      "Train Epoch: 1979 [67584/101520 (67%)] Loss: -1191.075195\n",
      "Train Epoch: 1979 [78848/101520 (78%)] Loss: -1186.907349\n",
      "Train Epoch: 1979 [90112/101520 (89%)] Loss: -1180.766846\n",
      "Train Epoch: 1979 [101376/101520 (100%)] Loss: -1185.065552\n",
      "    epoch          : 1979\n",
      "    loss           : -1187.767497767156\n",
      "    ess            : 1.965884780164939\n",
      "    log_marginal   : 1187.8002034096262\n",
      "    log_joint      : 1396.2822314698492\n",
      "    val_loss       : -1186.5825354534647\n",
      "    val_ess        : 1.9681028801461924\n",
      "    val_log_marginal: 1186.6134563943615\n",
      "    val_log_joint  : 1395.1026027513587\n",
      "Train Epoch: 1980 [0/101520 (0%)] Loss: -1179.974854\n",
      "Train Epoch: 1980 [11264/101520 (11%)] Loss: -1182.715576\n",
      "Train Epoch: 1980 [22528/101520 (22%)] Loss: -1184.495117\n",
      "Train Epoch: 1980 [33792/101520 (33%)] Loss: -1188.802490\n",
      "Train Epoch: 1980 [45056/101520 (44%)] Loss: -1194.480957\n",
      "Train Epoch: 1980 [56320/101520 (55%)] Loss: -1178.708374\n",
      "Train Epoch: 1980 [67584/101520 (67%)] Loss: -1193.022705\n",
      "Train Epoch: 1980 [78848/101520 (78%)] Loss: -1186.243652\n",
      "Train Epoch: 1980 [90112/101520 (89%)] Loss: -1187.558105\n",
      "Train Epoch: 1980 [101376/101520 (100%)] Loss: -1166.046021\n",
      "    epoch          : 1980\n",
      "    loss           : -1187.6951579184988\n",
      "    ess            : 1.9655233334057296\n",
      "    log_marginal   : 1187.7289339765232\n",
      "    log_joint      : 1396.1816884716552\n",
      "    val_loss       : -1184.986322817595\n",
      "    val_ess        : 1.9673778647961824\n",
      "    val_log_marginal: 1185.0168563179348\n",
      "    val_log_joint  : 1393.3686841881793\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1980.pth ...\n",
      "Train Epoch: 1981 [0/101520 (0%)] Loss: -1184.820801\n",
      "Train Epoch: 1981 [11264/101520 (11%)] Loss: -1183.221191\n",
      "Train Epoch: 1981 [22528/101520 (22%)] Loss: -1187.976685\n",
      "Train Epoch: 1981 [33792/101520 (33%)] Loss: -1195.184814\n",
      "Train Epoch: 1981 [45056/101520 (44%)] Loss: -1189.805420\n",
      "Train Epoch: 1981 [56320/101520 (55%)] Loss: -1191.130737\n",
      "Train Epoch: 1981 [67584/101520 (67%)] Loss: -1190.329346\n",
      "Train Epoch: 1981 [78848/101520 (78%)] Loss: -1182.692383\n",
      "Train Epoch: 1981 [90112/101520 (89%)] Loss: -1189.017578\n",
      "Train Epoch: 1981 [101376/101520 (100%)] Loss: -1172.439087\n",
      "    epoch          : 1981\n",
      "    loss           : -1187.7151596606077\n",
      "    ess            : 1.9659920553466184\n",
      "    log_marginal   : 1187.7472635393765\n",
      "    log_joint      : 1396.2321071912295\n",
      "    val_loss       : -1188.0063582710598\n",
      "    val_ess        : 1.9687289725179258\n",
      "    val_log_marginal: 1188.0363504161005\n",
      "    val_log_joint  : 1396.3538499915082\n",
      "Train Epoch: 1982 [0/101520 (0%)] Loss: -1190.701416\n",
      "Train Epoch: 1982 [11264/101520 (11%)] Loss: -1180.411987\n",
      "Train Epoch: 1982 [22528/101520 (22%)] Loss: -1187.872314\n",
      "Train Epoch: 1982 [33792/101520 (33%)] Loss: -1187.486816\n",
      "Train Epoch: 1982 [45056/101520 (44%)] Loss: -1187.070679\n",
      "Train Epoch: 1982 [56320/101520 (55%)] Loss: -1184.598267\n",
      "Train Epoch: 1982 [67584/101520 (67%)] Loss: -1193.632568\n",
      "Train Epoch: 1982 [78848/101520 (78%)] Loss: -1187.396729\n",
      "Train Epoch: 1982 [90112/101520 (89%)] Loss: -1182.953369\n",
      "Train Epoch: 1982 [101376/101520 (100%)] Loss: -1185.034668\n",
      "    epoch          : 1982\n",
      "    loss           : -1187.7966296325376\n",
      "    ess            : 1.9660628644665281\n",
      "    log_marginal   : 1187.829668361338\n",
      "    log_joint      : 1396.240562553981\n",
      "    val_loss       : -1184.9156918733017\n",
      "    val_ess        : 1.9699094554652339\n",
      "    val_log_marginal: 1184.943115234375\n",
      "    val_log_joint  : 1393.2151197350543\n",
      "Train Epoch: 1983 [0/101520 (0%)] Loss: -1188.674438\n",
      "Train Epoch: 1983 [11264/101520 (11%)] Loss: -1191.862305\n",
      "Train Epoch: 1983 [22528/101520 (22%)] Loss: -1182.393921\n",
      "Train Epoch: 1983 [33792/101520 (33%)] Loss: -1185.647339\n",
      "Train Epoch: 1983 [45056/101520 (44%)] Loss: -1195.697510\n",
      "Train Epoch: 1983 [56320/101520 (55%)] Loss: -1189.130371\n",
      "Train Epoch: 1983 [67584/101520 (67%)] Loss: -1189.684937\n",
      "Train Epoch: 1983 [78848/101520 (78%)] Loss: -1188.243652\n",
      "Train Epoch: 1983 [90112/101520 (89%)] Loss: -1185.260010\n",
      "Train Epoch: 1983 [101376/101520 (100%)] Loss: -1192.498657\n",
      "    epoch          : 1983\n",
      "    loss           : -1187.844931444331\n",
      "    ess            : 1.9656647036423036\n",
      "    log_marginal   : 1187.87819897829\n",
      "    log_joint      : 1396.3222484492776\n",
      "    val_loss       : -1185.265821373981\n",
      "    val_ess        : 1.963172181792881\n",
      "    val_log_marginal: 1185.3024265455163\n",
      "    val_log_joint  : 1393.5442478345788\n",
      "Train Epoch: 1984 [0/101520 (0%)] Loss: -1178.540771\n",
      "Train Epoch: 1984 [11264/101520 (11%)] Loss: -1188.769043\n",
      "Train Epoch: 1984 [22528/101520 (22%)] Loss: -1190.650635\n",
      "Train Epoch: 1984 [33792/101520 (33%)] Loss: -1187.097900\n",
      "Train Epoch: 1984 [45056/101520 (44%)] Loss: -1185.989136\n",
      "Train Epoch: 1984 [56320/101520 (55%)] Loss: -1191.286865\n",
      "Train Epoch: 1984 [67584/101520 (67%)] Loss: -1185.150391\n",
      "Train Epoch: 1984 [78848/101520 (78%)] Loss: -1184.579224\n",
      "Train Epoch: 1984 [90112/101520 (89%)] Loss: -1187.766357\n",
      "Train Epoch: 1984 [101376/101520 (100%)] Loss: -1178.615845\n",
      "    epoch          : 1984\n",
      "    loss           : -1187.827447785804\n",
      "    ess            : 1.9666873881565266\n",
      "    log_marginal   : 1187.8599767637013\n",
      "    log_joint      : 1396.290525503494\n",
      "    val_loss       : -1187.3118471891983\n",
      "    val_ess        : 1.964612655017687\n",
      "    val_log_marginal: 1187.3450396993885\n",
      "    val_log_joint  : 1395.7699027683425\n",
      "Train Epoch: 1985 [0/101520 (0%)] Loss: -1187.984253\n",
      "Train Epoch: 1985 [11264/101520 (11%)] Loss: -1182.779541\n",
      "Train Epoch: 1985 [22528/101520 (22%)] Loss: -1179.137085\n",
      "Train Epoch: 1985 [33792/101520 (33%)] Loss: -1192.737549\n",
      "Train Epoch: 1985 [45056/101520 (44%)] Loss: -1188.815430\n",
      "Train Epoch: 1985 [56320/101520 (55%)] Loss: -1189.202393\n",
      "Train Epoch: 1985 [67584/101520 (67%)] Loss: -1185.077393\n",
      "Train Epoch: 1985 [78848/101520 (78%)] Loss: -1192.504272\n",
      "Train Epoch: 1985 [90112/101520 (89%)] Loss: -1185.812622\n",
      "Train Epoch: 1985 [101376/101520 (100%)] Loss: -1174.711914\n",
      "    epoch          : 1985\n",
      "    loss           : -1187.8552111141646\n",
      "    ess            : 1.9661632554614963\n",
      "    log_marginal   : 1187.888438162492\n",
      "    log_joint      : 1396.33853410117\n",
      "    val_loss       : -1187.835693359375\n",
      "    val_ess        : 1.966092736824699\n",
      "    val_log_marginal: 1187.8682224439538\n",
      "    val_log_joint  : 1396.3723887567935\n",
      "Train Epoch: 1986 [0/101520 (0%)] Loss: -1194.231689\n",
      "Train Epoch: 1986 [11264/101520 (11%)] Loss: -1187.734863\n",
      "Train Epoch: 1986 [22528/101520 (22%)] Loss: -1185.892822\n",
      "Train Epoch: 1986 [33792/101520 (33%)] Loss: -1189.382324\n",
      "Train Epoch: 1986 [45056/101520 (44%)] Loss: -1192.828857\n",
      "Train Epoch: 1986 [56320/101520 (55%)] Loss: -1183.383057\n",
      "Train Epoch: 1986 [67584/101520 (67%)] Loss: -1194.586670\n",
      "Train Epoch: 1986 [78848/101520 (78%)] Loss: -1187.635376\n",
      "Train Epoch: 1986 [90112/101520 (89%)] Loss: -1188.791748\n",
      "Train Epoch: 1986 [101376/101520 (100%)] Loss: -1189.529053\n",
      "    epoch          : 1986\n",
      "    loss           : -1187.9324503376256\n",
      "    ess            : 1.96629582457806\n",
      "    log_marginal   : 1187.9652136414495\n",
      "    log_joint      : 1396.4405192466238\n",
      "    val_loss       : -1186.8599747367527\n",
      "    val_ess        : 1.9693843022636746\n",
      "    val_log_marginal: 1186.8884861158288\n",
      "    val_log_joint  : 1395.2875286599865\n",
      "Train Epoch: 1987 [0/101520 (0%)] Loss: -1184.849121\n",
      "Train Epoch: 1987 [11264/101520 (11%)] Loss: -1185.746460\n",
      "Train Epoch: 1987 [22528/101520 (22%)] Loss: -1187.004517\n",
      "Train Epoch: 1987 [33792/101520 (33%)] Loss: -1195.812256\n",
      "Train Epoch: 1987 [45056/101520 (44%)] Loss: -1184.265869\n",
      "Train Epoch: 1987 [56320/101520 (55%)] Loss: -1187.206543\n",
      "Train Epoch: 1987 [67584/101520 (67%)] Loss: -1192.884277\n",
      "Train Epoch: 1987 [78848/101520 (78%)] Loss: -1182.835571\n",
      "Train Epoch: 1987 [90112/101520 (89%)] Loss: -1185.784302\n",
      "Train Epoch: 1987 [101376/101520 (100%)] Loss: -1204.297485\n",
      "    epoch          : 1987\n",
      "    loss           : -1187.981907216748\n",
      "    ess            : 1.966363176628573\n",
      "    log_marginal   : 1188.0143963224325\n",
      "    log_joint      : 1396.537471292007\n",
      "    val_loss       : -1185.5265370244565\n",
      "    val_ess        : 1.9667591374853384\n",
      "    val_log_marginal: 1185.5560674252717\n",
      "    val_log_joint  : 1394.159614894701\n",
      "Train Epoch: 1988 [0/101520 (0%)] Loss: -1194.448242\n",
      "Train Epoch: 1988 [11264/101520 (11%)] Loss: -1183.976074\n",
      "Train Epoch: 1988 [22528/101520 (22%)] Loss: -1186.672729\n",
      "Train Epoch: 1988 [33792/101520 (33%)] Loss: -1191.161133\n",
      "Train Epoch: 1988 [45056/101520 (44%)] Loss: -1185.287598\n",
      "Train Epoch: 1988 [56320/101520 (55%)] Loss: -1196.246094\n",
      "Train Epoch: 1988 [67584/101520 (67%)] Loss: -1192.326904\n",
      "Train Epoch: 1988 [78848/101520 (78%)] Loss: -1190.537109\n",
      "Train Epoch: 1988 [90112/101520 (89%)] Loss: -1191.757690\n",
      "Train Epoch: 1988 [101376/101520 (100%)] Loss: -1188.763306\n",
      "    epoch          : 1988\n",
      "    loss           : -1188.0708958611417\n",
      "    ess            : 1.9664905143143543\n",
      "    log_marginal   : 1188.103225477976\n",
      "    log_joint      : 1396.5283374882224\n",
      "    val_loss       : -1186.6449080757473\n",
      "    val_ess        : 1.9675017595291138\n",
      "    val_log_marginal: 1186.6751125169837\n",
      "    val_log_joint  : 1394.950296153193\n",
      "Train Epoch: 1989 [0/101520 (0%)] Loss: -1188.588379\n",
      "Train Epoch: 1989 [11264/101520 (11%)] Loss: -1184.917236\n",
      "Train Epoch: 1989 [22528/101520 (22%)] Loss: -1187.758789\n",
      "Train Epoch: 1989 [33792/101520 (33%)] Loss: -1186.248535\n",
      "Train Epoch: 1989 [45056/101520 (44%)] Loss: -1186.600098\n",
      "Train Epoch: 1989 [56320/101520 (55%)] Loss: -1192.979004\n",
      "Train Epoch: 1989 [67584/101520 (67%)] Loss: -1187.822632\n",
      "Train Epoch: 1989 [78848/101520 (78%)] Loss: -1186.984009\n",
      "Train Epoch: 1989 [90112/101520 (89%)] Loss: -1189.438843\n",
      "Train Epoch: 1989 [101376/101520 (100%)] Loss: -1192.341553\n",
      "    epoch          : 1989\n",
      "    loss           : -1188.0908264466866\n",
      "    ess            : 1.9658044978002807\n",
      "    log_marginal   : 1188.124670594182\n",
      "    log_joint      : 1396.5933175398477\n",
      "    val_loss       : -1188.7022652004075\n",
      "    val_ess        : 1.968269793883614\n",
      "    val_log_marginal: 1188.733255137568\n",
      "    val_log_joint  : 1397.416211998981\n",
      "Train Epoch: 1990 [0/101520 (0%)] Loss: -1190.819824\n",
      "Train Epoch: 1990 [11264/101520 (11%)] Loss: -1182.026611\n",
      "Train Epoch: 1990 [22528/101520 (22%)] Loss: -1186.604004\n",
      "Train Epoch: 1990 [33792/101520 (33%)] Loss: -1190.730957\n",
      "Train Epoch: 1990 [45056/101520 (44%)] Loss: -1181.430786\n",
      "Train Epoch: 1990 [56320/101520 (55%)] Loss: -1187.826782\n",
      "Train Epoch: 1990 [67584/101520 (67%)] Loss: -1182.822998\n",
      "Train Epoch: 1990 [78848/101520 (78%)] Loss: -1185.639526\n",
      "Train Epoch: 1990 [90112/101520 (89%)] Loss: -1190.560547\n",
      "Train Epoch: 1990 [101376/101520 (100%)] Loss: -1193.101562\n",
      "    epoch          : 1990\n",
      "    loss           : -1188.1789360621467\n",
      "    ess            : 1.9665387802986642\n",
      "    log_marginal   : 1188.2108148162688\n",
      "    log_joint      : 1396.5940935144472\n",
      "    val_loss       : -1186.696145762568\n",
      "    val_ess        : 1.9666678749996682\n",
      "    val_log_marginal: 1186.726758873981\n",
      "    val_log_joint  : 1395.1734194548233\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch1990.pth ...\n",
      "Train Epoch: 1991 [0/101520 (0%)] Loss: -1187.470947\n",
      "Train Epoch: 1991 [11264/101520 (11%)] Loss: -1184.328857\n",
      "Train Epoch: 1991 [22528/101520 (22%)] Loss: -1190.304688\n",
      "Train Epoch: 1991 [33792/101520 (33%)] Loss: -1185.763306\n",
      "Train Epoch: 1991 [45056/101520 (44%)] Loss: -1181.687866\n",
      "Train Epoch: 1991 [56320/101520 (55%)] Loss: -1189.606445\n",
      "Train Epoch: 1991 [67584/101520 (67%)] Loss: -1187.513184\n",
      "Train Epoch: 1991 [78848/101520 (78%)] Loss: -1185.532227\n",
      "Train Epoch: 1991 [90112/101520 (89%)] Loss: -1189.590576\n",
      "Train Epoch: 1991 [101376/101520 (100%)] Loss: -1186.016235\n",
      "    epoch          : 1991\n",
      "    loss           : -1188.0999608638897\n",
      "    ess            : 1.9653648114084599\n",
      "    log_marginal   : 1188.1338829155543\n",
      "    log_joint      : 1396.585356592533\n",
      "    val_loss       : -1186.1556714928668\n",
      "    val_ess        : 1.9653106461400571\n",
      "    val_log_marginal: 1186.1885667883832\n",
      "    val_log_joint  : 1394.6465533712635\n",
      "Train Epoch: 1992 [0/101520 (0%)] Loss: -1188.958862\n",
      "Train Epoch: 1992 [11264/101520 (11%)] Loss: -1188.912109\n",
      "Train Epoch: 1992 [22528/101520 (22%)] Loss: -1187.298096\n",
      "Train Epoch: 1992 [33792/101520 (33%)] Loss: -1178.236450\n",
      "Train Epoch: 1992 [45056/101520 (44%)] Loss: -1185.576416\n",
      "Train Epoch: 1992 [56320/101520 (55%)] Loss: -1186.863525\n",
      "Train Epoch: 1992 [67584/101520 (67%)] Loss: -1194.510986\n",
      "Train Epoch: 1992 [78848/101520 (78%)] Loss: -1184.103149\n",
      "Train Epoch: 1992 [90112/101520 (89%)] Loss: -1185.435425\n",
      "Train Epoch: 1992 [101376/101520 (100%)] Loss: -1190.401978\n",
      "    epoch          : 1992\n",
      "    loss           : -1188.1096185272063\n",
      "    ess            : 1.9661154681114694\n",
      "    log_marginal   : 1188.1417647318624\n",
      "    log_joint      : 1396.5784464313756\n",
      "    val_loss       : -1187.7633693529212\n",
      "    val_ess        : 1.9679277876149053\n",
      "    val_log_marginal: 1187.7926662279212\n",
      "    val_log_joint  : 1395.9400316321332\n",
      "Train Epoch: 1993 [0/101520 (0%)] Loss: -1187.615234\n",
      "Train Epoch: 1993 [11264/101520 (11%)] Loss: -1190.079590\n",
      "Train Epoch: 1993 [22528/101520 (22%)] Loss: -1184.118286\n",
      "Train Epoch: 1993 [33792/101520 (33%)] Loss: -1191.147949\n",
      "Train Epoch: 1993 [45056/101520 (44%)] Loss: -1187.851440\n",
      "Train Epoch: 1993 [56320/101520 (55%)] Loss: -1189.541626\n",
      "Train Epoch: 1993 [67584/101520 (67%)] Loss: -1189.433716\n",
      "Train Epoch: 1993 [78848/101520 (78%)] Loss: -1190.069336\n",
      "Train Epoch: 1993 [90112/101520 (89%)] Loss: -1187.883789\n",
      "Train Epoch: 1993 [101376/101520 (100%)] Loss: -1191.914062\n",
      "    epoch          : 1993\n",
      "    loss           : -1188.0923569262327\n",
      "    ess            : 1.9654097503154122\n",
      "    log_marginal   : 1188.1260237957365\n",
      "    log_joint      : 1396.5823827388897\n",
      "    val_loss       : -1186.112113620924\n",
      "    val_ess        : 1.9648573554080466\n",
      "    val_log_marginal: 1186.1439527428668\n",
      "    val_log_joint  : 1394.4573496942935\n",
      "Train Epoch: 1994 [0/101520 (0%)] Loss: -1190.079102\n",
      "Train Epoch: 1994 [11264/101520 (11%)] Loss: -1179.435303\n",
      "Train Epoch: 1994 [22528/101520 (22%)] Loss: -1186.557129\n",
      "Train Epoch: 1994 [33792/101520 (33%)] Loss: -1194.123169\n",
      "Train Epoch: 1994 [45056/101520 (44%)] Loss: -1190.314209\n",
      "Train Epoch: 1994 [56320/101520 (55%)] Loss: -1191.702393\n",
      "Train Epoch: 1994 [67584/101520 (67%)] Loss: -1182.038452\n",
      "Train Epoch: 1994 [78848/101520 (78%)] Loss: -1184.461914\n",
      "Train Epoch: 1994 [90112/101520 (89%)] Loss: -1189.216187\n",
      "Train Epoch: 1994 [101376/101520 (100%)] Loss: -1191.133423\n",
      "    epoch          : 1994\n",
      "    loss           : -1188.1442325151147\n",
      "    ess            : 1.9659822136912513\n",
      "    log_marginal   : 1188.1771093013897\n",
      "    log_joint      : 1396.633354148673\n",
      "    val_loss       : -1187.2084217900815\n",
      "    val_ess        : 1.9677535917447961\n",
      "    val_log_marginal: 1187.2375753651495\n",
      "    val_log_joint  : 1395.897896144701\n",
      "Train Epoch: 1995 [0/101520 (0%)] Loss: -1195.529419\n",
      "Train Epoch: 1995 [11264/101520 (11%)] Loss: -1179.028198\n",
      "Train Epoch: 1995 [22528/101520 (22%)] Loss: -1190.348022\n",
      "Train Epoch: 1995 [33792/101520 (33%)] Loss: -1190.897217\n",
      "Train Epoch: 1995 [45056/101520 (44%)] Loss: -1186.688965\n",
      "Train Epoch: 1995 [56320/101520 (55%)] Loss: -1178.314697\n",
      "Train Epoch: 1995 [67584/101520 (67%)] Loss: -1187.246338\n",
      "Train Epoch: 1995 [78848/101520 (78%)] Loss: -1191.707275\n",
      "Train Epoch: 1995 [90112/101520 (89%)] Loss: -1187.070557\n",
      "Train Epoch: 1995 [101376/101520 (100%)] Loss: -1185.426880\n",
      "    epoch          : 1995\n",
      "    loss           : -1188.1652807494504\n",
      "    ess            : 1.9656610848316596\n",
      "    log_marginal   : 1188.1976115931218\n",
      "    log_joint      : 1396.6443257547503\n",
      "    val_loss       : -1187.8473059612772\n",
      "    val_ess        : 1.9670891969100288\n",
      "    val_log_marginal: 1187.879537831182\n",
      "    val_log_joint  : 1396.185106360394\n",
      "Train Epoch: 1996 [0/101520 (0%)] Loss: -1186.057251\n",
      "Train Epoch: 1996 [11264/101520 (11%)] Loss: -1188.668457\n",
      "Train Epoch: 1996 [22528/101520 (22%)] Loss: -1192.493652\n",
      "Train Epoch: 1996 [33792/101520 (33%)] Loss: -1188.565430\n",
      "Train Epoch: 1996 [45056/101520 (44%)] Loss: -1189.837646\n",
      "Train Epoch: 1996 [56320/101520 (55%)] Loss: -1189.295898\n",
      "Train Epoch: 1996 [67584/101520 (67%)] Loss: -1190.623047\n",
      "Train Epoch: 1996 [78848/101520 (78%)] Loss: -1190.270264\n",
      "Train Epoch: 1996 [90112/101520 (89%)] Loss: -1190.162354\n",
      "Train Epoch: 1996 [101376/101520 (100%)] Loss: -1199.695435\n",
      "    epoch          : 1996\n",
      "    loss           : -1188.193321956462\n",
      "    ess            : 1.9665069526164376\n",
      "    log_marginal   : 1188.2254638671875\n",
      "    log_joint      : 1396.7175875716473\n",
      "    val_loss       : -1186.4517344599185\n",
      "    val_ess        : 1.9654793117357336\n",
      "    val_log_marginal: 1186.4879574983017\n",
      "    val_log_joint  : 1394.907720151155\n",
      "Train Epoch: 1997 [0/101520 (0%)] Loss: -1185.106934\n",
      "Train Epoch: 1997 [11264/101520 (11%)] Loss: -1193.623047\n",
      "Train Epoch: 1997 [22528/101520 (22%)] Loss: -1188.497559\n",
      "Train Epoch: 1997 [33792/101520 (33%)] Loss: -1189.124023\n",
      "Train Epoch: 1997 [45056/101520 (44%)] Loss: -1193.585205\n",
      "Train Epoch: 1997 [56320/101520 (55%)] Loss: -1186.285767\n",
      "Train Epoch: 1997 [67584/101520 (67%)] Loss: -1192.239990\n",
      "Train Epoch: 1997 [78848/101520 (78%)] Loss: -1185.265381\n",
      "Train Epoch: 1997 [90112/101520 (89%)] Loss: -1191.419189\n",
      "Train Epoch: 1997 [101376/101520 (100%)] Loss: -1194.288696\n",
      "    epoch          : 1997\n",
      "    loss           : -1188.288772352976\n",
      "    ess            : 1.9663409126463847\n",
      "    log_marginal   : 1188.3217822510992\n",
      "    log_joint      : 1396.7739822157664\n",
      "    val_loss       : -1186.163091244905\n",
      "    val_ess        : 1.9684818723927373\n",
      "    val_log_marginal: 1186.1943147078805\n",
      "    val_log_joint  : 1394.503126061481\n",
      "Train Epoch: 1998 [0/101520 (0%)] Loss: -1184.184204\n",
      "Train Epoch: 1998 [11264/101520 (11%)] Loss: -1186.342529\n",
      "Train Epoch: 1998 [22528/101520 (22%)] Loss: -1185.827881\n",
      "Train Epoch: 1998 [33792/101520 (33%)] Loss: -1188.228027\n",
      "Train Epoch: 1998 [45056/101520 (44%)] Loss: -1187.877930\n",
      "Train Epoch: 1998 [56320/101520 (55%)] Loss: -1186.266724\n",
      "Train Epoch: 1998 [67584/101520 (67%)] Loss: -1183.390625\n",
      "Train Epoch: 1998 [78848/101520 (78%)] Loss: -1184.384644\n",
      "Train Epoch: 1998 [90112/101520 (89%)] Loss: -1190.145752\n",
      "Train Epoch: 1998 [101376/101520 (100%)] Loss: -1178.451050\n",
      "    epoch          : 1998\n",
      "    loss           : -1188.2988170834642\n",
      "    ess            : 1.9659421875249201\n",
      "    log_marginal   : 1188.3323459337705\n",
      "    log_joint      : 1396.7623971910332\n",
      "    val_loss       : -1187.9796142578125\n",
      "    val_ess        : 1.9657509534255317\n",
      "    val_log_marginal: 1188.0114321501358\n",
      "    val_log_joint  : 1396.6530443274457\n",
      "Train Epoch: 1999 [0/101520 (0%)] Loss: -1188.692139\n",
      "Train Epoch: 1999 [11264/101520 (11%)] Loss: -1191.808960\n",
      "Train Epoch: 1999 [22528/101520 (22%)] Loss: -1187.756348\n",
      "Train Epoch: 1999 [33792/101520 (33%)] Loss: -1183.063477\n",
      "Train Epoch: 1999 [45056/101520 (44%)] Loss: -1194.122559\n",
      "Train Epoch: 1999 [56320/101520 (55%)] Loss: -1187.784546\n",
      "Train Epoch: 1999 [67584/101520 (67%)] Loss: -1190.935181\n",
      "Train Epoch: 1999 [78848/101520 (78%)] Loss: -1191.076782\n",
      "Train Epoch: 1999 [90112/101520 (89%)] Loss: -1179.104248\n",
      "Train Epoch: 1999 [101376/101520 (100%)] Loss: -1196.819946\n",
      "    epoch          : 1999\n",
      "    loss           : -1188.361158821451\n",
      "    ess            : 1.9653146919892661\n",
      "    log_marginal   : 1188.3946570008245\n",
      "    log_joint      : 1396.8529242894158\n",
      "    val_loss       : -1186.1299730383832\n",
      "    val_ess        : 1.9672123297401096\n",
      "    val_log_marginal: 1186.1615998641305\n",
      "    val_log_joint  : 1394.7359512992527\n",
      "Train Epoch: 2000 [0/101520 (0%)] Loss: -1184.027100\n",
      "Train Epoch: 2000 [11264/101520 (11%)] Loss: -1185.567871\n",
      "Train Epoch: 2000 [22528/101520 (22%)] Loss: -1185.955200\n",
      "Train Epoch: 2000 [33792/101520 (33%)] Loss: -1187.888916\n",
      "Train Epoch: 2000 [45056/101520 (44%)] Loss: -1187.854248\n",
      "Train Epoch: 2000 [56320/101520 (55%)] Loss: -1189.820557\n",
      "Train Epoch: 2000 [67584/101520 (67%)] Loss: -1191.508911\n",
      "Train Epoch: 2000 [78848/101520 (78%)] Loss: -1186.360229\n",
      "Train Epoch: 2000 [90112/101520 (89%)] Loss: -1182.234619\n",
      "Train Epoch: 2000 [101376/101520 (100%)] Loss: -1194.423462\n",
      "    epoch          : 2000\n",
      "    loss           : -1188.3905385079695\n",
      "    ess            : 1.9660609643063953\n",
      "    log_marginal   : 1188.42401521769\n",
      "    log_joint      : 1396.8985773594536\n",
      "    val_loss       : -1185.7683476987092\n",
      "    val_ess        : 1.9638027367384538\n",
      "    val_log_marginal: 1185.801216457201\n",
      "    val_log_joint  : 1393.9627526324728\n",
      "Saving checkpoint: saved/models/EMnist_Ppc/0504_153821/checkpoint-epoch2000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa5b900-0121-4cf6-91b6-2c9b0b12716f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParticleDict(\n",
       "    (z1): Parameter containing: [torch.FloatTensor of size 2x11280x20]\n",
       "    (z2): Parameter containing: [torch.FloatTensor of size 2x11280x128]\n",
       "    (z3): Parameter containing: [torch.FloatTensor of size 2x11280x256]\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "trainer.model.cpu()\n",
    "trainer.cpu()\n",
    "trainer.train_particles.cpu()\n",
    "trainer.valid_particles.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d528cb-d45e-4a37-be86-9622aec9ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.graph.clear()\n",
    "trainer._load_particles(range(trainer.data_loader.batch_size), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e26726a6-74e5-4e0f-81d5-746dba00f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in trainer.model.graph.nodes:\n",
    "    trainer.model.graph.nodes[site]['is_observed'] = trainer.model.graph.nodes[site]['value'] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cb94c06-7112-4c5e-b781-0ac215a7e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60a2eeb8-32e3-467e-9e9a-0ac1e2af9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyro.plate_stack(\"forward\", (trainer.num_particles, trainer.data_loader.batch_size)):\n",
    "    model = pyro.condition(trainer.model, data={k: v['value'] for k, v in trainer.model.graph.nodes.items()})\n",
    "    xs = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6e9ba59-7b26-4ef3-9bfd-23f7c8a26464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b1509d2-9afc-444e-821f-b7ccda42aa28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    plt.imshow(xs.mean(dim=0)[i].squeeze().detach().cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2029315-afe9-4ae8-9f64-c342ef8b830b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34a71d-84a2-48f0-8150-3607e77ed6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
