{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ef93e3-8eb5-4883-8265-5fbe97d06e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/shai_hulud/ppc_experiments\n",
      "env: TORCH_CUDNN_SDPA_ENABLED=1\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%env TORCH_CUDNN_SDPA_ENABLED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94520ed3-45be-4735-a0f2-b8f380838c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import numpy as np\n",
    "import pyro\n",
    "import torch\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "import trainer.trainer as module_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ac4be4-19eb-4713-a88d-9bec118c7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75ea7b9-6fe5-4f8a-a825-3572e421242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1445d1-45bb-43f9-893d-7a2b2bad1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "\n",
    "config = read_json(\"experiments/ppc_flowers_config.json\")\n",
    "config = ConfigParser(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca35026-5f64-47f8-98c7-a5e228013d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionPpc(\n",
      "  (diffusion): DiffusionStep(\n",
      "    (unet): Unet(\n",
      "      (init_conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (time_mlp): Sequential(\n",
      "        (0): SinusoidalPosEmb()\n",
      "        (1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (downs): ModuleList(\n",
      "        (0): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "            (block1): Block(\n",
      "              (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (block2): Block(\n",
      "              (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (res_conv): Identity()\n",
      "          )\n",
      "          (2): LinearAttention(\n",
      "            (norm): RMSNorm()\n",
      "            (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): RMSNorm()\n",
      "            )\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
      "            (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (1): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "            (block1): Block(\n",
      "              (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (block2): Block(\n",
      "              (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (res_conv): Identity()\n",
      "          )\n",
      "          (2): LinearAttention(\n",
      "            (norm): RMSNorm()\n",
      "            (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): RMSNorm()\n",
      "            )\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
      "            (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (2): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (block1): Block(\n",
      "              (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (block2): Block(\n",
      "              (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (res_conv): Identity()\n",
      "          )\n",
      "          (2): LinearAttention(\n",
      "            (norm): RMSNorm()\n",
      "            (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): RMSNorm()\n",
      "            )\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
      "            (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (3): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "            )\n",
      "            (block1): Block(\n",
      "              (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (block2): Block(\n",
      "              (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (res_conv): Identity()\n",
      "          )\n",
      "          (2): Attention(\n",
      "            (norm): RMSNorm()\n",
      "            (attend): Attend(\n",
      "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (to_out): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (ups): ModuleList(\n",
      "        (0): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            )\n",
      "            (block1): Block(\n",
      "              (proj): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (block2): Block(\n",
      "              (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (res_conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Attention(\n",
      "            (norm): RMSNorm()\n",
      "            (attend): Attend(\n",
      "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (to_out): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "            (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (1): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "            )\n",
      "            (block1): Block(\n",
      "              (proj): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (block2): Block(\n",
      "              (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (res_conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): LinearAttention(\n",
      "            (norm): RMSNorm()\n",
      "            (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): RMSNorm()\n",
      "            )\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "            (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (2): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (block1): Block(\n",
      "              (proj): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (block2): Block(\n",
      "              (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (res_conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): LinearAttention(\n",
      "            (norm): RMSNorm()\n",
      "            (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): RMSNorm()\n",
      "            )\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "            (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (3): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock(\n",
      "            (mlp): Sequential(\n",
      "              (0): SiLU()\n",
      "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "            )\n",
      "            (block1): Block(\n",
      "              (proj): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (block2): Block(\n",
      "              (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "            (res_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): LinearAttention(\n",
      "            (norm): RMSNorm()\n",
      "            (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): RMSNorm()\n",
      "            )\n",
      "          )\n",
      "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (mid_block1): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        )\n",
      "        (block1): Block(\n",
      "          (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (block2): Block(\n",
      "          (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (res_conv): Identity()\n",
      "      )\n",
      "      (mid_attn): Attention(\n",
      "        (norm): RMSNorm()\n",
      "        (attend): Attend(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (to_out): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (mid_block2): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        )\n",
      "        (block1): Block(\n",
      "          (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (block2): Block(\n",
      "          (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (res_conv): Identity()\n",
      "      )\n",
      "      (final_res_block): ResnetBlock(\n",
      "        (mlp): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (block1): Block(\n",
      "          (proj): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (block2): Block(\n",
      "          (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (res_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (final_conv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (prior): DiffusionPrior()\n",
      "  (graph): PpcGraphicalModel()\n",
      ")\n",
      "Trainable parameters: 35719555\n",
      "Initialize particles: train batch 0\n",
      "Initialize particles: train batch 1\n",
      "Initialize particles: train batch 2\n",
      "Initialize particles: train batch 3\n",
      "Initialize particles: train batch 4\n",
      "Initialize particles: train batch 5\n",
      "Initialize particles: train batch 6\n",
      "Initialize particles: train batch 7\n",
      "Initialize particles: train batch 8\n",
      "Initialize particles: train batch 9\n",
      "Initialize particles: train batch 10\n",
      "Initialize particles: train batch 11\n",
      "Initialize particles: train batch 12\n",
      "Initialize particles: train batch 13\n",
      "Initialize particles: train batch 14\n",
      "Initialize particles: train batch 15\n",
      "Initialize particles: train batch 16\n",
      "Initialize particles: train batch 17\n",
      "Initialize particles: train batch 18\n",
      "Initialize particles: train batch 19\n",
      "Initialize particles: train batch 20\n",
      "Initialize particles: train batch 21\n",
      "Initialize particles: train batch 22\n",
      "Initialize particles: train batch 23\n",
      "Initialize particles: train batch 24\n",
      "Initialize particles: train batch 25\n",
      "Initialize particles: train batch 26\n",
      "Initialize particles: train batch 27\n",
      "Initialize particles: train batch 28\n",
      "Initialize particles: train batch 29\n",
      "Initialize particles: train batch 30\n",
      "Initialize particles: train batch 31\n",
      "Initialize particles: train batch 32\n",
      "Initialize particles: train batch 33\n",
      "Initialize particles: train batch 34\n",
      "Initialize particles: train batch 35\n",
      "Initialize particles: train batch 36\n",
      "Initialize particles: train batch 37\n",
      "Initialize particles: train batch 38\n",
      "Initialize particles: train batch 39\n",
      "Initialize particles: train batch 40\n",
      "Initialize particles: train batch 41\n",
      "Initialize particles: train batch 42\n",
      "Initialize particles: train batch 43\n",
      "Initialize particles: train batch 44\n",
      "Initialize particles: train batch 45\n",
      "Initialize particles: train batch 46\n",
      "Initialize particles: train batch 47\n",
      "Initialize particles: train batch 48\n",
      "Initialize particles: train batch 49\n",
      "Initialize particles: train batch 50\n",
      "Initialize particles: train batch 51\n",
      "Initialize particles: train batch 52\n",
      "Initialize particles: train batch 53\n",
      "Initialize particles: train batch 54\n",
      "Initialize particles: train batch 55\n",
      "Initialize particles: train batch 56\n",
      "Initialize particles: train batch 57\n",
      "Initialize particles: train batch 58\n",
      "Initialize particles: train batch 59\n",
      "Initialize particles: train batch 60\n",
      "Initialize particles: train batch 61\n",
      "Initialize particles: train batch 62\n",
      "Initialize particles: train batch 63\n",
      "Initialize particles: train batch 64\n",
      "Initialize particles: train batch 65\n",
      "Initialize particles: train batch 66\n",
      "Initialize particles: train batch 67\n",
      "Initialize particles: train batch 68\n",
      "Initialize particles: train batch 69\n",
      "Initialize particles: train batch 70\n",
      "Initialize particles: train batch 71\n",
      "Initialize particles: train batch 72\n",
      "Initialize particles: train batch 73\n",
      "Initialize particles: train batch 74\n",
      "Initialize particles: train batch 75\n",
      "Initialize particles: train batch 76\n",
      "Initialize particles: train batch 77\n",
      "Initialize particles: train batch 78\n",
      "Initialize particles: train batch 79\n",
      "Initialize particles: train batch 80\n",
      "Initialize particles: train batch 81\n",
      "Initialize particles: train batch 82\n",
      "Initialize particles: train batch 83\n",
      "Initialize particles: train batch 84\n",
      "Initialize particles: train batch 85\n",
      "Initialize particles: train batch 86\n",
      "Initialize particles: train batch 87\n",
      "Initialize particles: train batch 88\n",
      "Initialize particles: train batch 89\n",
      "Initialize particles: train batch 90\n",
      "Initialize particles: train batch 91\n",
      "Initialize particles: train batch 92\n",
      "Initialize particles: train batch 93\n",
      "Initialize particles: train batch 94\n",
      "Initialize particles: train batch 95\n",
      "Initialize particles: train batch 96\n",
      "Initialize particles: train batch 97\n",
      "Initialize particles: train batch 98\n",
      "Initialize particles: train batch 99\n",
      "Initialize particles: train batch 100\n",
      "Initialize particles: train batch 101\n",
      "Initialize particles: train batch 102\n",
      "Initialize particles: train batch 103\n",
      "Initialize particles: train batch 104\n",
      "Initialize particles: train batch 105\n",
      "Initialize particles: train batch 106\n",
      "Initialize particles: train batch 107\n",
      "Initialize particles: train batch 108\n",
      "Initialize particles: train batch 109\n",
      "Initialize particles: train batch 110\n",
      "Initialize particles: train batch 111\n",
      "Initialize particles: train batch 112\n",
      "Initialize particles: train batch 113\n",
      "Initialize particles: train batch 114\n",
      "Initialize particles: valid batch 0\n",
      "Initialize particles: valid batch 1\n",
      "Initialize particles: valid batch 2\n",
      "Initialize particles: valid batch 3\n",
      "Initialize particles: valid batch 4\n",
      "Initialize particles: valid batch 5\n",
      "Initialize particles: valid batch 6\n",
      "Initialize particles: valid batch 7\n",
      "Initialize particles: valid batch 8\n",
      "Initialize particles: valid batch 9\n",
      "Initialize particles: valid batch 10\n",
      "Initialize particles: valid batch 11\n",
      "Initialize particles: valid batch 12\n"
     ]
    }
   ],
   "source": [
    "logger = config.get_logger('train')\n",
    "\n",
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()\n",
    "\n",
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "logger.info(model)\n",
    "\n",
    "# get function handles of metrics\n",
    "metrics = [getattr(module_metric, met) for met in config['metrics']]\n",
    "\n",
    "# build optimizer.\n",
    "if \"lr_scheduler\" in config:\n",
    "    lr_scheduler = getattr(pyro.optim, config[\"lr_scheduler\"][\"type\"])\n",
    "    lr_scheduler = optimizer = lr_scheduler({\n",
    "        \"optimizer\": getattr(torch.optim, config[\"optimizer\"][\"type\"]),\n",
    "        \"optim_args\": config[\"optimizer\"][\"args\"][\"optim_args\"],\n",
    "        **config[\"lr_scheduler\"][\"args\"]\n",
    "    })\n",
    "else:\n",
    "    optimizer = config.init_obj('optimizer', pyro.optim)\n",
    "    lr_scheduler = None\n",
    "\n",
    "# build trainer\n",
    "# kwargs = config['trainer'].pop('args')\n",
    "trainer = config.init_obj('trainer', module_trainer, model, metrics, optimizer,\n",
    "                          config=config, data_loader=data_loader,\n",
    "                          valid_data_loader=valid_data_loader,\n",
    "                          lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5cdd31-7fae-4e6f-8fe2-ef5c023919de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved/log/FlowersDiffusion_Ppc/0426_014324\n"
     ]
    }
   ],
   "source": [
    "logger.info(trainer.config.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9820b982-1a39-4bde-850e-1a1d9a92557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/918 (0%)] Loss: 2944920.500000\n",
      "Train Epoch: 1 [16/918 (2%)] Loss: 2314598.000000\n",
      "Train Epoch: 1 [32/918 (3%)] Loss: 2452550.000000\n",
      "Train Epoch: 1 [48/918 (5%)] Loss: 1879770.250000\n",
      "Train Epoch: 1 [64/918 (7%)] Loss: 2102946.750000\n",
      "Train Epoch: 1 [80/918 (9%)] Loss: 2251886.500000\n",
      "Train Epoch: 1 [96/918 (10%)] Loss: 1876882.000000\n",
      "Train Epoch: 1 [112/918 (12%)] Loss: 1661908.875000\n",
      "Train Epoch: 1 [128/918 (14%)] Loss: 1996919.125000\n",
      "Train Epoch: 1 [144/918 (16%)] Loss: 1825555.250000\n",
      "Train Epoch: 1 [160/918 (17%)] Loss: 1848060.750000\n",
      "Train Epoch: 1 [176/918 (19%)] Loss: 1893916.625000\n",
      "Train Epoch: 1 [192/918 (21%)] Loss: 1773853.750000\n",
      "Train Epoch: 1 [208/918 (23%)] Loss: 1777014.500000\n",
      "Train Epoch: 1 [224/918 (24%)] Loss: 1750127.250000\n",
      "Train Epoch: 1 [240/918 (26%)] Loss: 1492784.750000\n",
      "Train Epoch: 1 [256/918 (28%)] Loss: 1606607.750000\n",
      "Train Epoch: 1 [272/918 (30%)] Loss: 1239083.500000\n",
      "Train Epoch: 1 [288/918 (31%)] Loss: 1414853.250000\n",
      "Train Epoch: 1 [304/918 (33%)] Loss: 1586622.000000\n",
      "Train Epoch: 1 [320/918 (35%)] Loss: 1257428.750000\n",
      "Train Epoch: 1 [336/918 (37%)] Loss: 1242558.250000\n",
      "Train Epoch: 1 [352/918 (38%)] Loss: 1353810.000000\n",
      "Train Epoch: 1 [368/918 (40%)] Loss: 1522781.375000\n",
      "Train Epoch: 1 [384/918 (42%)] Loss: 1490124.500000\n",
      "Train Epoch: 1 [400/918 (44%)] Loss: 1136867.250000\n",
      "Train Epoch: 1 [416/918 (45%)] Loss: 1068222.375000\n",
      "Train Epoch: 1 [432/918 (47%)] Loss: 1085241.500000\n",
      "Train Epoch: 1 [448/918 (49%)] Loss: 1118039.875000\n",
      "Train Epoch: 1 [464/918 (51%)] Loss: 987194.812500\n",
      "Train Epoch: 1 [480/918 (52%)] Loss: 1173901.250000\n",
      "Train Epoch: 1 [496/918 (54%)] Loss: 1130590.000000\n",
      "Train Epoch: 1 [512/918 (56%)] Loss: 1064696.000000\n",
      "Train Epoch: 1 [528/918 (58%)] Loss: 1108352.000000\n",
      "Train Epoch: 1 [544/918 (59%)] Loss: 1098317.125000\n",
      "Train Epoch: 1 [560/918 (61%)] Loss: 948928.437500\n",
      "Train Epoch: 1 [576/918 (63%)] Loss: 1043151.375000\n",
      "Train Epoch: 1 [592/918 (64%)] Loss: 1060602.250000\n",
      "Train Epoch: 1 [608/918 (66%)] Loss: 1033386.875000\n",
      "Train Epoch: 1 [624/918 (68%)] Loss: 890674.687500\n",
      "Train Epoch: 1 [640/918 (70%)] Loss: 1034468.750000\n",
      "Train Epoch: 1 [656/918 (71%)] Loss: 939229.875000\n",
      "Train Epoch: 1 [672/918 (73%)] Loss: 970443.625000\n",
      "Train Epoch: 1 [688/918 (75%)] Loss: 900961.875000\n",
      "Train Epoch: 1 [704/918 (77%)] Loss: 901574.875000\n",
      "Train Epoch: 1 [720/918 (78%)] Loss: 982221.750000\n",
      "Train Epoch: 1 [736/918 (80%)] Loss: 1036559.750000\n",
      "Train Epoch: 1 [752/918 (82%)] Loss: 848267.000000\n",
      "Train Epoch: 1 [768/918 (84%)] Loss: 1005232.312500\n",
      "Train Epoch: 1 [784/918 (85%)] Loss: 858698.625000\n",
      "Train Epoch: 1 [800/918 (87%)] Loss: 1015064.500000\n",
      "Train Epoch: 1 [816/918 (89%)] Loss: 1208801.250000\n",
      "Train Epoch: 1 [832/918 (91%)] Loss: 1201212.750000\n",
      "Train Epoch: 1 [848/918 (92%)] Loss: 859851.750000\n",
      "Train Epoch: 1 [864/918 (94%)] Loss: 995657.375000\n",
      "Train Epoch: 1 [880/918 (96%)] Loss: 960040.375000\n",
      "Train Epoch: 1 [896/918 (98%)] Loss: 998875.125000\n",
      "Train Epoch: 1 [912/918 (99%)] Loss: 807497.500000\n",
      "    epoch          : 1\n",
      "    loss           : 1350953.785326087\n",
      "    ess            : 2.085200812505639\n",
      "    log_marginal   : -1350953.785326087\n",
      "    val_loss       : 968653.8605769231\n",
      "    val_ess        : 1.9216643021656916\n",
      "    val_log_marginal: -968653.8605769231\n",
      "Train Epoch: 2 [0/918 (0%)] Loss: 932736.687500\n",
      "Train Epoch: 2 [16/918 (2%)] Loss: 1010605.500000\n",
      "Train Epoch: 2 [32/918 (3%)] Loss: 836567.625000\n",
      "Train Epoch: 2 [48/918 (5%)] Loss: 875540.250000\n",
      "Train Epoch: 2 [64/918 (7%)] Loss: 1221586.750000\n",
      "Train Epoch: 2 [80/918 (9%)] Loss: 1044293.375000\n",
      "Train Epoch: 2 [96/918 (10%)] Loss: 896770.125000\n",
      "Train Epoch: 2 [112/918 (12%)] Loss: 857080.750000\n",
      "Train Epoch: 2 [128/918 (14%)] Loss: 913575.437500\n",
      "Train Epoch: 2 [144/918 (16%)] Loss: 837269.312500\n",
      "Train Epoch: 2 [160/918 (17%)] Loss: 957116.125000\n",
      "Train Epoch: 2 [176/918 (19%)] Loss: 998529.750000\n",
      "Train Epoch: 2 [192/918 (21%)] Loss: 811147.125000\n",
      "Train Epoch: 2 [208/918 (23%)] Loss: 828077.125000\n",
      "Train Epoch: 2 [224/918 (24%)] Loss: 774402.062500\n",
      "Train Epoch: 2 [240/918 (26%)] Loss: 826724.687500\n",
      "Train Epoch: 2 [256/918 (28%)] Loss: 810546.562500\n",
      "Train Epoch: 2 [272/918 (30%)] Loss: 893291.312500\n",
      "Train Epoch: 2 [288/918 (31%)] Loss: 881136.500000\n",
      "Train Epoch: 2 [304/918 (33%)] Loss: 933959.000000\n",
      "Train Epoch: 2 [320/918 (35%)] Loss: 856536.625000\n",
      "Train Epoch: 2 [336/918 (37%)] Loss: 988753.125000\n",
      "Train Epoch: 2 [352/918 (38%)] Loss: 786347.750000\n",
      "Train Epoch: 2 [368/918 (40%)] Loss: 765580.625000\n",
      "Train Epoch: 2 [384/918 (42%)] Loss: 898445.812500\n",
      "Train Epoch: 2 [400/918 (44%)] Loss: 816614.750000\n",
      "Train Epoch: 2 [416/918 (45%)] Loss: 833116.500000\n",
      "Train Epoch: 2 [432/918 (47%)] Loss: 827330.375000\n",
      "Train Epoch: 2 [448/918 (49%)] Loss: 770981.250000\n",
      "Train Epoch: 2 [464/918 (51%)] Loss: 772466.687500\n",
      "Train Epoch: 2 [480/918 (52%)] Loss: 852132.375000\n",
      "Train Epoch: 2 [496/918 (54%)] Loss: 911648.875000\n",
      "Train Epoch: 2 [512/918 (56%)] Loss: 953770.312500\n",
      "Train Epoch: 2 [528/918 (58%)] Loss: 905350.375000\n",
      "Train Epoch: 2 [544/918 (59%)] Loss: 898866.187500\n",
      "Train Epoch: 2 [560/918 (61%)] Loss: 837023.187500\n",
      "Train Epoch: 2 [576/918 (63%)] Loss: 840649.875000\n",
      "Train Epoch: 2 [592/918 (64%)] Loss: 824116.000000\n",
      "Train Epoch: 2 [608/918 (66%)] Loss: 856421.125000\n",
      "Train Epoch: 2 [624/918 (68%)] Loss: 830190.125000\n",
      "Train Epoch: 2 [640/918 (70%)] Loss: 889926.312500\n",
      "Train Epoch: 2 [656/918 (71%)] Loss: 853519.250000\n",
      "Train Epoch: 2 [672/918 (73%)] Loss: 732089.500000\n",
      "Train Epoch: 2 [688/918 (75%)] Loss: 772351.125000\n",
      "Train Epoch: 2 [704/918 (77%)] Loss: 805517.187500\n",
      "Train Epoch: 2 [720/918 (78%)] Loss: 791476.000000\n",
      "Train Epoch: 2 [736/918 (80%)] Loss: 781558.500000\n",
      "Train Epoch: 2 [752/918 (82%)] Loss: 772767.375000\n",
      "Train Epoch: 2 [768/918 (84%)] Loss: 792293.625000\n",
      "Train Epoch: 2 [784/918 (85%)] Loss: 810320.562500\n",
      "Train Epoch: 2 [800/918 (87%)] Loss: 819203.187500\n",
      "Train Epoch: 2 [816/918 (89%)] Loss: 820907.062500\n",
      "Train Epoch: 2 [832/918 (91%)] Loss: 817182.125000\n",
      "Train Epoch: 2 [848/918 (92%)] Loss: 985314.125000\n",
      "Train Epoch: 2 [864/918 (94%)] Loss: 789023.500000\n",
      "Train Epoch: 2 [880/918 (96%)] Loss: 824832.875000\n",
      "Train Epoch: 2 [896/918 (98%)] Loss: 698816.500000\n",
      "Train Epoch: 2 [912/918 (99%)] Loss: 778217.375000\n",
      "    epoch          : 2\n",
      "    loss           : 860085.5902173913\n",
      "    ess            : 1.9050397230231244\n",
      "    log_marginal   : -860085.5842391305\n",
      "    val_loss       : 807672.5769230769\n",
      "    val_ess        : 1.879855330173786\n",
      "    val_log_marginal: -807672.5625\n",
      "Train Epoch: 3 [0/918 (0%)] Loss: 791942.812500\n",
      "Train Epoch: 3 [16/918 (2%)] Loss: 646722.625000\n",
      "Train Epoch: 3 [32/918 (3%)] Loss: 773628.750000\n",
      "Train Epoch: 3 [48/918 (5%)] Loss: 802034.625000\n",
      "Train Epoch: 3 [64/918 (7%)] Loss: 771746.375000\n",
      "Train Epoch: 3 [80/918 (9%)] Loss: 785571.500000\n",
      "Train Epoch: 3 [96/918 (10%)] Loss: 631276.687500\n",
      "Train Epoch: 3 [112/918 (12%)] Loss: 799518.375000\n",
      "Train Epoch: 3 [128/918 (14%)] Loss: 806370.250000\n",
      "Train Epoch: 3 [144/918 (16%)] Loss: 876533.375000\n",
      "Train Epoch: 3 [160/918 (17%)] Loss: 760816.312500\n",
      "Train Epoch: 3 [176/918 (19%)] Loss: 774300.500000\n",
      "Train Epoch: 3 [192/918 (21%)] Loss: 767511.250000\n",
      "Train Epoch: 3 [208/918 (23%)] Loss: 866781.437500\n",
      "Train Epoch: 3 [224/918 (24%)] Loss: 758432.500000\n",
      "Train Epoch: 3 [240/918 (26%)] Loss: 853816.187500\n",
      "Train Epoch: 3 [256/918 (28%)] Loss: 725593.125000\n",
      "Train Epoch: 3 [272/918 (30%)] Loss: 746006.250000\n",
      "Train Epoch: 3 [288/918 (31%)] Loss: 688807.125000\n",
      "Train Epoch: 3 [304/918 (33%)] Loss: 811769.687500\n",
      "Train Epoch: 3 [320/918 (35%)] Loss: 769058.187500\n",
      "Train Epoch: 3 [336/918 (37%)] Loss: 804583.375000\n",
      "Train Epoch: 3 [352/918 (38%)] Loss: 762937.187500\n",
      "Train Epoch: 3 [368/918 (40%)] Loss: 847911.187500\n",
      "Train Epoch: 3 [384/918 (42%)] Loss: 816819.062500\n",
      "Train Epoch: 3 [400/918 (44%)] Loss: 752880.500000\n",
      "Train Epoch: 3 [416/918 (45%)] Loss: 765375.000000\n",
      "Train Epoch: 3 [432/918 (47%)] Loss: 643474.500000\n",
      "Train Epoch: 3 [448/918 (49%)] Loss: 812484.625000\n",
      "Train Epoch: 3 [464/918 (51%)] Loss: 766851.000000\n",
      "Train Epoch: 3 [480/918 (52%)] Loss: 711370.375000\n",
      "Train Epoch: 3 [496/918 (54%)] Loss: 662566.437500\n",
      "Train Epoch: 3 [512/918 (56%)] Loss: 785524.125000\n",
      "Train Epoch: 3 [528/918 (58%)] Loss: 665570.375000\n",
      "Train Epoch: 3 [544/918 (59%)] Loss: 757070.500000\n",
      "Train Epoch: 3 [560/918 (61%)] Loss: 824757.937500\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5b900-0121-4cf6-91b6-2c9b0b12716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.eval()\n",
    "trainer.model.cpu()\n",
    "trainer.cpu()\n",
    "trainer.train_particles.cpu()\n",
    "trainer.valid_particles.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d528cb-d45e-4a37-be86-9622aec9ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.graph.clear()\n",
    "trainer._load_particles(range(trainer.data_loader.batch_size), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26726a6-74e5-4e0f-81d5-746dba00f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in trainer.model.graph.nodes:\n",
    "    trainer.model.graph.nodes[site]['is_observed'] = trainer.model.graph.nodes[site]['value'] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb94c06-7112-4c5e-b781-0ac215a7e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2eeb8-32e3-467e-9e9a-0ac1e2af9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyro.plate_stack(\"forward\", (trainer.num_particles, trainer.data_loader.batch_size)):\n",
    "    model = pyro.condition(trainer.model, data={k: v['value'] for k, v in trainer.model.graph.nodes.items()})\n",
    "    xs = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9ba59-7b26-4ef3-9bfd-23f7c8a26464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1509d2-9afc-444e-821f-b7ccda42aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.imshow(xs.mean(dim=0)[i].squeeze().detach().cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2029315-afe9-4ae8-9f64-c342ef8b830b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34a71d-84a2-48f0-8150-3607e77ed6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
