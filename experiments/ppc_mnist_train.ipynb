{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ef93e3-8eb5-4883-8265-5fbe97d06e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/ppc_experiments\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94520ed3-45be-4735-a0f2-b8f380838c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import numpy as np\n",
    "import pyro\n",
    "import torch\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "import trainer.trainer as module_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ac4be4-19eb-4713-a88d-9bec118c7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75ea7b9-6fe5-4f8a-a825-3572e421242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1445d1-45bb-43f9-893d-7a2b2bad1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "\n",
    "config = read_json(\"experiments/ppc_mnist_config.json\")\n",
    "config = ConfigParser(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca35026-5f64-47f8-98c7-a5e228013d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MnistPpc(\n",
      "  (digit_features): DigitFeatures()\n",
      "  (decoder): DigitDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=10, out_features=200, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=200, out_features=400, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=400, out_features=784, bias=True)\n",
      "      (5): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (graph): GraphicalModel()\n",
      ")\n",
      "Trainable parameters: 396984\n",
      "Initialize particles: train batch 0\n",
      "Initialize particles: train batch 1\n",
      "Initialize particles: train batch 2\n",
      "Initialize particles: train batch 3\n",
      "Initialize particles: train batch 4\n",
      "Initialize particles: train batch 5\n",
      "Initialize particles: train batch 6\n",
      "Initialize particles: train batch 7\n",
      "Initialize particles: train batch 8\n",
      "Initialize particles: train batch 9\n",
      "Initialize particles: train batch 10\n",
      "Initialize particles: train batch 11\n",
      "Initialize particles: train batch 12\n",
      "Initialize particles: train batch 13\n",
      "Initialize particles: train batch 14\n",
      "Initialize particles: train batch 15\n",
      "Initialize particles: train batch 16\n",
      "Initialize particles: train batch 17\n",
      "Initialize particles: train batch 18\n",
      "Initialize particles: train batch 19\n",
      "Initialize particles: train batch 20\n",
      "Initialize particles: train batch 21\n",
      "Initialize particles: train batch 22\n",
      "Initialize particles: train batch 23\n",
      "Initialize particles: train batch 24\n",
      "Initialize particles: train batch 25\n",
      "Initialize particles: train batch 26\n",
      "Initialize particles: train batch 27\n",
      "Initialize particles: train batch 28\n",
      "Initialize particles: train batch 29\n",
      "Initialize particles: train batch 30\n",
      "Initialize particles: train batch 31\n",
      "Initialize particles: train batch 32\n",
      "Initialize particles: train batch 33\n",
      "Initialize particles: train batch 34\n",
      "Initialize particles: train batch 35\n",
      "Initialize particles: train batch 36\n",
      "Initialize particles: train batch 37\n",
      "Initialize particles: train batch 38\n",
      "Initialize particles: train batch 39\n",
      "Initialize particles: train batch 40\n",
      "Initialize particles: train batch 41\n",
      "Initialize particles: train batch 42\n",
      "Initialize particles: train batch 43\n",
      "Initialize particles: train batch 44\n",
      "Initialize particles: train batch 45\n",
      "Initialize particles: train batch 46\n",
      "Initialize particles: train batch 47\n",
      "Initialize particles: train batch 48\n",
      "Initialize particles: train batch 49\n",
      "Initialize particles: train batch 50\n",
      "Initialize particles: train batch 51\n",
      "Initialize particles: train batch 52\n",
      "Initialize particles: train batch 53\n",
      "Initialize particles: train batch 54\n",
      "Initialize particles: train batch 55\n",
      "Initialize particles: train batch 56\n",
      "Initialize particles: train batch 57\n",
      "Initialize particles: train batch 58\n",
      "Initialize particles: train batch 59\n",
      "Initialize particles: train batch 60\n",
      "Initialize particles: train batch 61\n",
      "Initialize particles: train batch 62\n",
      "Initialize particles: train batch 63\n",
      "Initialize particles: train batch 64\n",
      "Initialize particles: train batch 65\n",
      "Initialize particles: train batch 66\n",
      "Initialize particles: train batch 67\n",
      "Initialize particles: train batch 68\n",
      "Initialize particles: train batch 69\n",
      "Initialize particles: train batch 70\n",
      "Initialize particles: train batch 71\n",
      "Initialize particles: train batch 72\n",
      "Initialize particles: train batch 73\n",
      "Initialize particles: train batch 74\n",
      "Initialize particles: train batch 75\n",
      "Initialize particles: train batch 76\n",
      "Initialize particles: train batch 77\n",
      "Initialize particles: train batch 78\n",
      "Initialize particles: train batch 79\n",
      "Initialize particles: train batch 80\n",
      "Initialize particles: train batch 81\n",
      "Initialize particles: train batch 82\n",
      "Initialize particles: train batch 83\n",
      "Initialize particles: train batch 84\n",
      "Initialize particles: train batch 85\n",
      "Initialize particles: train batch 86\n",
      "Initialize particles: train batch 87\n",
      "Initialize particles: train batch 88\n",
      "Initialize particles: train batch 89\n",
      "Initialize particles: train batch 90\n",
      "Initialize particles: train batch 91\n",
      "Initialize particles: train batch 92\n",
      "Initialize particles: train batch 93\n",
      "Initialize particles: train batch 94\n",
      "Initialize particles: train batch 95\n",
      "Initialize particles: train batch 96\n",
      "Initialize particles: train batch 97\n",
      "Initialize particles: train batch 98\n",
      "Initialize particles: train batch 99\n",
      "Initialize particles: train batch 100\n",
      "Initialize particles: train batch 101\n",
      "Initialize particles: train batch 102\n",
      "Initialize particles: train batch 103\n",
      "Initialize particles: train batch 104\n",
      "Initialize particles: train batch 105\n",
      "Initialize particles: valid batch 0\n",
      "Initialize particles: valid batch 1\n",
      "Initialize particles: valid batch 2\n",
      "Initialize particles: valid batch 3\n",
      "Initialize particles: valid batch 4\n",
      "Initialize particles: valid batch 5\n",
      "Initialize particles: valid batch 6\n",
      "Initialize particles: valid batch 7\n",
      "Initialize particles: valid batch 8\n",
      "Initialize particles: valid batch 9\n",
      "Initialize particles: valid batch 10\n",
      "Initialize particles: valid batch 11\n"
     ]
    }
   ],
   "source": [
    "logger = config.get_logger('train')\n",
    "\n",
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()\n",
    "\n",
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "logger.info(model)\n",
    "\n",
    "# get function handles of metrics\n",
    "metrics = [getattr(module_metric, met) for met in config['metrics']]\n",
    "\n",
    "# build optimizer.\n",
    "if \"lr_scheduler\" in config:\n",
    "    lr_scheduler = getattr(pyro.optim, config[\"lr_scheduler\"][\"type\"])\n",
    "    lr_scheduler = optimizer = lr_scheduler({\n",
    "        \"optimizer\": getattr(torch.optim, config[\"optimizer\"][\"type\"]),\n",
    "        \"optim_args\": config[\"optimizer\"][\"args\"][\"optim_args\"],\n",
    "        **config[\"lr_scheduler\"][\"args\"]\n",
    "    })\n",
    "else:\n",
    "    optimizer = config.init_obj('optimizer', pyro.optim)\n",
    "    lr_scheduler = None\n",
    "\n",
    "# build trainer\n",
    "# kwargs = config['trainer'].pop('args')\n",
    "trainer = config.init_obj('trainer', module_trainer, model, metrics, optimizer,\n",
    "                          config=config, data_loader=data_loader,\n",
    "                          valid_data_loader=valid_data_loader,\n",
    "                          lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5cdd31-7fae-4e6f-8fe2-ef5c023919de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved/log/Mnist_Ppc/0222_160001\n"
     ]
    }
   ],
   "source": [
    "logger.info(trainer.config.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9820b982-1a39-4bde-850e-1a1d9a92557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54000 (0%)] Loss: 16.229351\n",
      "Train Epoch: 1 [11264/54000 (21%)] Loss: -1373.407349\n",
      "Train Epoch: 1 [22528/54000 (42%)] Loss: -1397.655273\n",
      "Train Epoch: 1 [33792/54000 (63%)] Loss: -1454.890869\n",
      "Train Epoch: 1 [45056/54000 (83%)] Loss: -1497.555176\n",
      "    epoch          : 1\n",
      "    loss           : -1391.889636525568\n",
      "    ess            : 8.001125308702576\n",
      "    log_marginal   : 1391.8896353739613\n",
      "    val_loss       : -1509.3681131998699\n",
      "    val_ess        : 8.00118080774943\n",
      "    val_log_marginal: 1509.3681131998699\n",
      "Train Epoch: 2 [0/54000 (0%)] Loss: -1488.680176\n",
      "Train Epoch: 2 [11264/54000 (21%)] Loss: -1511.282715\n",
      "Train Epoch: 2 [22528/54000 (42%)] Loss: -1502.158691\n",
      "Train Epoch: 2 [33792/54000 (63%)] Loss: -1502.542114\n",
      "Train Epoch: 2 [45056/54000 (83%)] Loss: -1495.854248\n",
      "    epoch          : 2\n",
      "    loss           : -1505.7764650740714\n",
      "    ess            : 8.001171552909994\n",
      "    log_marginal   : 1505.7764639224647\n",
      "    val_loss       : -1566.4869588216145\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1566.4869384765625\n",
      "Train Epoch: 3 [0/54000 (0%)] Loss: -1585.952881\n",
      "Train Epoch: 3 [11264/54000 (21%)] Loss: -1560.135498\n",
      "Train Epoch: 3 [22528/54000 (42%)] Loss: -1529.720947\n",
      "Train Epoch: 3 [33792/54000 (63%)] Loss: -1550.858643\n",
      "Train Epoch: 3 [45056/54000 (83%)] Loss: -1576.288086\n",
      "    epoch          : 3\n",
      "    loss           : -1557.6897502395343\n",
      "    ess            : 8.00117830060563\n",
      "    log_marginal   : 1557.6897502395343\n",
      "    val_loss       : -1565.0954996744792\n",
      "    val_ess        : 8.001183986663818\n",
      "    val_log_marginal: 1565.0955098470051\n",
      "Train Epoch: 4 [0/54000 (0%)] Loss: -1581.164307\n",
      "Train Epoch: 4 [11264/54000 (21%)] Loss: -1593.563477\n",
      "Train Epoch: 4 [22528/54000 (42%)] Loss: -1584.550049\n",
      "Train Epoch: 4 [33792/54000 (63%)] Loss: -1586.917969\n",
      "Train Epoch: 4 [45056/54000 (83%)] Loss: -1578.749023\n",
      "    epoch          : 4\n",
      "    loss           : -1583.0642619582843\n",
      "    ess            : 8.001181719438085\n",
      "    log_marginal   : 1583.0642619582843\n",
      "    val_loss       : -1587.6875610351562\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1587.6875712076824\n",
      "Train Epoch: 5 [0/54000 (0%)] Loss: -1613.215454\n",
      "Train Epoch: 5 [11264/54000 (21%)] Loss: -1635.780273\n",
      "Train Epoch: 5 [22528/54000 (42%)] Loss: -1637.781372\n",
      "Train Epoch: 5 [33792/54000 (63%)] Loss: -1611.462280\n",
      "Train Epoch: 5 [45056/54000 (83%)] Loss: -1636.625977\n",
      "    epoch          : 5\n",
      "    loss           : -1617.313762160967\n",
      "    ess            : 8.001183779734486\n",
      "    log_marginal   : 1617.313762160967\n",
      "    val_loss       : -1633.7456461588542\n",
      "    val_ess        : 8.001184701919556\n",
      "    val_log_marginal: 1633.7456461588542\n",
      "Train Epoch: 6 [0/54000 (0%)] Loss: -1655.012451\n",
      "Train Epoch: 6 [11264/54000 (21%)] Loss: -1610.342285\n",
      "Train Epoch: 6 [22528/54000 (42%)] Loss: -1615.727295\n",
      "Train Epoch: 6 [33792/54000 (63%)] Loss: -1622.560547\n",
      "Train Epoch: 6 [45056/54000 (83%)] Loss: -1590.360596\n",
      "    epoch          : 6\n",
      "    loss           : -1626.4766304447967\n",
      "    ess            : 8.001184346540919\n",
      "    log_marginal   : 1626.4766304447967\n",
      "    val_loss       : -1645.2907206217449\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1645.2907206217449\n",
      "Train Epoch: 7 [0/54000 (0%)] Loss: -1645.791748\n",
      "Train Epoch: 7 [11264/54000 (21%)] Loss: -1649.101196\n",
      "Train Epoch: 7 [22528/54000 (42%)] Loss: -1647.094482\n",
      "Train Epoch: 7 [33792/54000 (63%)] Loss: -1654.272217\n",
      "Train Epoch: 7 [45056/54000 (83%)] Loss: -1668.638184\n",
      "    epoch          : 7\n",
      "    loss           : -1652.7248258770637\n",
      "    ess            : 8.001184985322773\n",
      "    log_marginal   : 1652.7248258770637\n",
      "    val_loss       : -1673.4010925292969\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1673.4010925292969\n",
      "Train Epoch: 8 [0/54000 (0%)] Loss: -1696.098633\n",
      "Train Epoch: 8 [11264/54000 (21%)] Loss: -1678.677734\n",
      "Train Epoch: 8 [22528/54000 (42%)] Loss: -1684.195557\n",
      "Train Epoch: 8 [33792/54000 (63%)] Loss: -1675.063721\n",
      "Train Epoch: 8 [45056/54000 (83%)] Loss: -1675.204834\n",
      "    epoch          : 8\n",
      "    loss           : -1675.0766290628685\n",
      "    ess            : 8.001185255230597\n",
      "    log_marginal   : 1675.0766302144752\n",
      "    val_loss       : -1689.6587626139324\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1689.6587524414062\n",
      "Train Epoch: 9 [0/54000 (0%)] Loss: -1705.440674\n",
      "Train Epoch: 9 [11264/54000 (21%)] Loss: -1662.125244\n",
      "Train Epoch: 9 [22528/54000 (42%)] Loss: -1692.364990\n",
      "Train Epoch: 9 [33792/54000 (63%)] Loss: -1679.348267\n",
      "Train Epoch: 9 [45056/54000 (83%)] Loss: -1690.747925\n",
      "    epoch          : 9\n",
      "    loss           : -1679.52943737102\n",
      "    ess            : 8.001186028966364\n",
      "    log_marginal   : 1679.52943737102\n",
      "    val_loss       : -1689.0432637532551\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1689.0432434082031\n",
      "Train Epoch: 10 [0/54000 (0%)] Loss: -1700.635620\n",
      "Train Epoch: 10 [11264/54000 (21%)] Loss: -1690.142578\n",
      "Train Epoch: 10 [22528/54000 (42%)] Loss: -1708.060059\n",
      "Train Epoch: 10 [33792/54000 (63%)] Loss: -1688.808472\n",
      "Train Epoch: 10 [45056/54000 (83%)] Loss: -1703.895264\n",
      "    epoch          : 10\n",
      "    loss           : -1699.5624884839328\n",
      "    ess            : 8.00118593899709\n",
      "    log_marginal   : 1699.5624884839328\n",
      "    val_loss       : -1706.7267557779949\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1706.7267557779949\n",
      "Train Epoch: 11 [0/54000 (0%)] Loss: -1712.685303\n",
      "Train Epoch: 11 [11264/54000 (21%)] Loss: -1692.854248\n",
      "Train Epoch: 11 [22528/54000 (42%)] Loss: -1678.713135\n",
      "Train Epoch: 11 [33792/54000 (63%)] Loss: -1707.106079\n",
      "Train Epoch: 11 [45056/54000 (83%)] Loss: -1704.168945\n",
      "    epoch          : 11\n",
      "    loss           : -1696.3835921377506\n",
      "    ess            : 8.001185309212163\n",
      "    log_marginal   : 1696.3835921377506\n",
      "    val_loss       : -1703.8496602376301\n",
      "    val_ess        : 8.00118605295817\n",
      "    val_log_marginal: 1703.8496602376301\n",
      "Train Epoch: 12 [0/54000 (0%)] Loss: -1695.574707\n",
      "Train Epoch: 12 [11264/54000 (21%)] Loss: -1703.154297\n",
      "Train Epoch: 12 [22528/54000 (42%)] Loss: -1696.979614\n",
      "Train Epoch: 12 [33792/54000 (63%)] Loss: -1703.467896\n",
      "Train Epoch: 12 [45056/54000 (83%)] Loss: -1717.195435\n",
      "    epoch          : 12\n",
      "    loss           : -1705.082560989092\n",
      "    ess            : 8.001185651095408\n",
      "    log_marginal   : 1705.082560989092\n",
      "    val_loss       : -1717.1215515136719\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1717.1215515136719\n",
      "Train Epoch: 13 [0/54000 (0%)] Loss: -1716.674316\n",
      "Train Epoch: 13 [11264/54000 (21%)] Loss: -1723.993164\n",
      "Train Epoch: 13 [22528/54000 (42%)] Loss: -1713.783325\n",
      "Train Epoch: 13 [33792/54000 (63%)] Loss: -1716.716553\n",
      "Train Epoch: 13 [45056/54000 (83%)] Loss: -1720.913940\n",
      "    epoch          : 13\n",
      "    loss           : -1723.3276390219635\n",
      "    ess            : 8.001185147267467\n",
      "    log_marginal   : 1723.3276390219635\n",
      "    val_loss       : -1723.0465799967449\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1723.0465799967449\n",
      "Train Epoch: 14 [0/54000 (0%)] Loss: -1730.239990\n",
      "Train Epoch: 14 [11264/54000 (21%)] Loss: -1713.435181\n",
      "Train Epoch: 14 [22528/54000 (42%)] Loss: -1713.566772\n",
      "Train Epoch: 14 [33792/54000 (63%)] Loss: -1717.030762\n",
      "Train Epoch: 14 [45056/54000 (83%)] Loss: -1727.151855\n",
      "    epoch          : 14\n",
      "    loss           : -1718.2973529167896\n",
      "    ess            : 8.001185669089264\n",
      "    log_marginal   : 1718.2973529167896\n",
      "    val_loss       : -1733.6435953776042\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1733.643575032552\n",
      "Train Epoch: 15 [0/54000 (0%)] Loss: -1742.787720\n",
      "Train Epoch: 15 [11264/54000 (21%)] Loss: -1734.014893\n",
      "Train Epoch: 15 [22528/54000 (42%)] Loss: -1728.063232\n",
      "Train Epoch: 15 [33792/54000 (63%)] Loss: -1723.458618\n",
      "Train Epoch: 15 [45056/54000 (83%)] Loss: -1719.404297\n",
      "    epoch          : 15\n",
      "    loss           : -1728.695055691701\n",
      "    ess            : 8.001185633101553\n",
      "    log_marginal   : 1728.695055691701\n",
      "    val_loss       : -1743.7549947102864\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1743.7549947102864\n",
      "Train Epoch: 16 [0/54000 (0%)] Loss: -1742.904297\n",
      "Train Epoch: 16 [11264/54000 (21%)] Loss: -1727.350586\n",
      "Train Epoch: 16 [22528/54000 (42%)] Loss: -1730.143799\n",
      "Train Epoch: 16 [33792/54000 (63%)] Loss: -1720.076660\n",
      "Train Epoch: 16 [45056/54000 (83%)] Loss: -1730.397583\n",
      "    epoch          : 16\n",
      "    loss           : -1730.6606986567658\n",
      "    ess            : 8.00118539018451\n",
      "    log_marginal   : 1730.6606986567658\n",
      "    val_loss       : -1742.9831034342449\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1742.9831034342449\n",
      "Train Epoch: 17 [0/54000 (0%)] Loss: -1754.068115\n",
      "Train Epoch: 17 [11264/54000 (21%)] Loss: -1744.128906\n",
      "Train Epoch: 17 [22528/54000 (42%)] Loss: -1738.773926\n",
      "Train Epoch: 17 [33792/54000 (63%)] Loss: -1728.380981\n",
      "Train Epoch: 17 [45056/54000 (83%)] Loss: -1736.645264\n",
      "    epoch          : 17\n",
      "    loss           : -1740.1825377266362\n",
      "    ess            : 8.001184625445672\n",
      "    log_marginal   : 1740.1825400298496\n",
      "    val_loss       : -1751.3914184570312\n",
      "    val_ess        : 8.00118605295817\n",
      "    val_log_marginal: 1751.3913981119792\n",
      "Train Epoch: 18 [0/54000 (0%)] Loss: -1764.329712\n",
      "Train Epoch: 18 [11264/54000 (21%)] Loss: -1742.356201\n",
      "Train Epoch: 18 [22528/54000 (42%)] Loss: -1739.720337\n",
      "Train Epoch: 18 [33792/54000 (63%)] Loss: -1742.951660\n",
      "Train Epoch: 18 [45056/54000 (83%)] Loss: -1764.997314\n",
      "    epoch          : 18\n",
      "    loss           : -1746.6192626953125\n",
      "    ess            : 8.001184589457962\n",
      "    log_marginal   : 1746.6192626953125\n",
      "    val_loss       : -1742.5391845703125\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1742.5391845703125\n",
      "Train Epoch: 19 [0/54000 (0%)] Loss: -1750.677490\n",
      "Train Epoch: 19 [11264/54000 (21%)] Loss: -1743.605347\n",
      "Train Epoch: 19 [22528/54000 (42%)] Loss: -1747.557373\n",
      "Train Epoch: 19 [33792/54000 (63%)] Loss: -1747.770508\n",
      "Train Epoch: 19 [45056/54000 (83%)] Loss: -1735.618652\n",
      "    epoch          : 19\n",
      "    loss           : -1742.6248295622052\n",
      "    ess            : 8.001184436510194\n",
      "    log_marginal   : 1742.624826107385\n",
      "    val_loss       : -1748.5591125488281\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1748.5591125488281\n",
      "Train Epoch: 20 [0/54000 (0%)] Loss: -1753.289062\n",
      "Train Epoch: 20 [11264/54000 (21%)] Loss: -1755.564941\n",
      "Train Epoch: 20 [22528/54000 (42%)] Loss: -1757.933105\n",
      "Train Epoch: 20 [33792/54000 (63%)] Loss: -1750.661377\n",
      "Train Epoch: 20 [45056/54000 (83%)] Loss: -1749.872192\n",
      "    epoch          : 20\n",
      "    loss           : -1753.5410513248084\n",
      "    ess            : 8.001184031648457\n",
      "    log_marginal   : 1753.5410524764152\n",
      "    val_loss       : -1761.6016133626301\n",
      "    val_ess        : 8.001184542973837\n",
      "    val_log_marginal: 1761.6016133626301\n",
      "Train Epoch: 21 [0/54000 (0%)] Loss: -1771.867676\n",
      "Train Epoch: 21 [11264/54000 (21%)] Loss: -1753.841064\n",
      "Train Epoch: 21 [22528/54000 (42%)] Loss: -1757.329956\n",
      "Train Epoch: 21 [33792/54000 (63%)] Loss: -1740.041260\n",
      "Train Epoch: 21 [45056/54000 (83%)] Loss: -1743.245850\n",
      "    epoch          : 21\n",
      "    loss           : -1750.5179523971844\n",
      "    ess            : 8.001185273224452\n",
      "    log_marginal   : 1750.5179523971844\n",
      "    val_loss       : -1754.5579325358074\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1754.5579325358074\n",
      "Train Epoch: 22 [0/54000 (0%)] Loss: -1756.968506\n",
      "Train Epoch: 22 [11264/54000 (21%)] Loss: -1752.726807\n",
      "Train Epoch: 22 [22528/54000 (42%)] Loss: -1731.871216\n",
      "Train Epoch: 22 [33792/54000 (63%)] Loss: -1754.143799\n",
      "Train Epoch: 22 [45056/54000 (83%)] Loss: -1756.146973\n",
      "    epoch          : 22\n",
      "    loss           : -1752.1357053360848\n",
      "    ess            : 8.001184553470251\n",
      "    log_marginal   : 1752.1357053360848\n",
      "    val_loss       : -1767.7110493977864\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1767.7110493977864\n",
      "Train Epoch: 23 [0/54000 (0%)] Loss: -1771.217285\n",
      "Train Epoch: 23 [11264/54000 (21%)] Loss: -1756.037842\n",
      "Train Epoch: 23 [22528/54000 (42%)] Loss: -1746.028809\n",
      "Train Epoch: 23 [33792/54000 (63%)] Loss: -1758.369629\n",
      "Train Epoch: 23 [45056/54000 (83%)] Loss: -1750.510376\n",
      "    epoch          : 23\n",
      "    loss           : -1753.4401498470666\n",
      "    ess            : 8.001184265568572\n",
      "    log_marginal   : 1753.4401498470666\n",
      "    val_loss       : -1763.010518391927\n",
      "    val_ess        : 8.001183986663818\n",
      "    val_log_marginal: 1763.010518391927\n",
      "Train Epoch: 24 [0/54000 (0%)] Loss: -1766.127197\n",
      "Train Epoch: 24 [11264/54000 (21%)] Loss: -1739.846436\n",
      "Train Epoch: 24 [22528/54000 (42%)] Loss: -1761.089111\n",
      "Train Epoch: 24 [33792/54000 (63%)] Loss: -1730.948975\n",
      "Train Epoch: 24 [45056/54000 (83%)] Loss: -1765.382812\n",
      "    epoch          : 24\n",
      "    loss           : -1752.589856417674\n",
      "    ess            : 8.001184670430309\n",
      "    log_marginal   : 1752.589856417674\n",
      "    val_loss       : -1751.7606201171875\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1751.7606201171875\n",
      "Train Epoch: 25 [0/54000 (0%)] Loss: -1755.196411\n",
      "Train Epoch: 25 [11264/54000 (21%)] Loss: -1766.140137\n",
      "Train Epoch: 25 [22528/54000 (42%)] Loss: -1753.784424\n",
      "Train Epoch: 25 [33792/54000 (63%)] Loss: -1764.359863\n",
      "Train Epoch: 25 [45056/54000 (83%)] Loss: -1761.936890\n",
      "    epoch          : 25\n",
      "    loss           : -1762.0964378500885\n",
      "    ess            : 8.001183770737558\n",
      "    log_marginal   : 1762.0964366984817\n",
      "    val_loss       : -1767.6340738932292\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1767.6340738932292\n",
      "Train Epoch: 26 [0/54000 (0%)] Loss: -1775.182739\n",
      "Train Epoch: 26 [11264/54000 (21%)] Loss: -1760.924683\n",
      "Train Epoch: 26 [22528/54000 (42%)] Loss: -1771.440674\n",
      "Train Epoch: 26 [33792/54000 (63%)] Loss: -1763.512573\n",
      "Train Epoch: 26 [45056/54000 (83%)] Loss: -1763.928711\n",
      "    epoch          : 26\n",
      "    loss           : -1765.1223363336528\n",
      "    ess            : 8.001184427513266\n",
      "    log_marginal   : 1765.122335182046\n",
      "    val_loss       : -1775.7403157552083\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1775.7403259277344\n",
      "Train Epoch: 27 [0/54000 (0%)] Loss: -1787.623291\n",
      "Train Epoch: 27 [11264/54000 (21%)] Loss: -1743.146606\n",
      "Train Epoch: 27 [22528/54000 (42%)] Loss: -1759.361816\n",
      "Train Epoch: 27 [33792/54000 (63%)] Loss: -1751.150024\n",
      "Train Epoch: 27 [45056/54000 (83%)] Loss: -1757.581299\n",
      "    epoch          : 27\n",
      "    loss           : -1759.4705983287884\n",
      "    ess            : 8.001184445507121\n",
      "    log_marginal   : 1759.4705983287884\n",
      "    val_loss       : -1771.5174153645833\n",
      "    val_ess        : 8.001185337702433\n",
      "    val_log_marginal: 1771.5174153645833\n",
      "Train Epoch: 28 [0/54000 (0%)] Loss: -1776.916504\n",
      "Train Epoch: 28 [11264/54000 (21%)] Loss: -1769.232666\n",
      "Train Epoch: 28 [22528/54000 (42%)] Loss: -1768.323730\n",
      "Train Epoch: 28 [33792/54000 (63%)] Loss: -1757.670654\n",
      "Train Epoch: 28 [45056/54000 (83%)] Loss: -1776.627563\n",
      "    epoch          : 28\n",
      "    loss           : -1766.216677107901\n",
      "    ess            : 8.001183518823588\n",
      "    log_marginal   : 1766.2166782595077\n",
      "    val_loss       : -1772.5274454752605\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1772.5274251302083\n",
      "Train Epoch: 29 [0/54000 (0%)] Loss: -1777.121338\n",
      "Train Epoch: 29 [11264/54000 (21%)] Loss: -1775.869629\n",
      "Train Epoch: 29 [22528/54000 (42%)] Loss: -1780.003662\n",
      "Train Epoch: 29 [33792/54000 (63%)] Loss: -1753.470337\n",
      "Train Epoch: 29 [45056/54000 (83%)] Loss: -1761.774902\n",
      "    epoch          : 29\n",
      "    loss           : -1767.7140744767098\n",
      "    ess            : 8.00118347383895\n",
      "    log_marginal   : 1767.714073325103\n",
      "    val_loss       : -1763.8815511067708\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1763.8815511067708\n",
      "Train Epoch: 30 [0/54000 (0%)] Loss: -1769.916504\n",
      "Train Epoch: 30 [11264/54000 (21%)] Loss: -1779.855957\n",
      "Train Epoch: 30 [22528/54000 (42%)] Loss: -1767.320679\n",
      "Train Epoch: 30 [33792/54000 (63%)] Loss: -1761.689453\n",
      "Train Epoch: 30 [45056/54000 (83%)] Loss: -1760.419434\n",
      "    epoch          : 30\n",
      "    loss           : -1768.5481210384728\n",
      "    ess            : 8.001184517482542\n",
      "    log_marginal   : 1768.5481210384728\n",
      "    val_loss       : -1763.5185445149739\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1763.5185445149739\n",
      "Train Epoch: 31 [0/54000 (0%)] Loss: -1767.035278\n",
      "Train Epoch: 31 [11264/54000 (21%)] Loss: -1784.623047\n",
      "Train Epoch: 31 [22528/54000 (42%)] Loss: -1762.098267\n",
      "Train Epoch: 31 [33792/54000 (63%)] Loss: -1768.057129\n",
      "Train Epoch: 31 [45056/54000 (83%)] Loss: -1783.614258\n",
      "    epoch          : 31\n",
      "    loss           : -1771.423389362839\n",
      "    ess            : 8.001183671771356\n",
      "    log_marginal   : 1771.423389362839\n",
      "    val_loss       : -1780.9001159667969\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1780.9001159667969\n",
      "Train Epoch: 32 [0/54000 (0%)] Loss: -1786.582031\n",
      "Train Epoch: 32 [11264/54000 (21%)] Loss: -1778.934814\n",
      "Train Epoch: 32 [22528/54000 (42%)] Loss: -1749.994263\n",
      "Train Epoch: 32 [33792/54000 (63%)] Loss: -1765.263428\n",
      "Train Epoch: 32 [45056/54000 (83%)] Loss: -1757.346436\n",
      "    epoch          : 32\n",
      "    loss           : -1765.1071892504422\n",
      "    ess            : 8.001185201249033\n",
      "    log_marginal   : 1765.1071892504422\n",
      "    val_loss       : -1775.2323201497395\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1775.2323201497395\n",
      "Train Epoch: 33 [0/54000 (0%)] Loss: -1784.912720\n",
      "Train Epoch: 33 [11264/54000 (21%)] Loss: -1757.198242\n",
      "Train Epoch: 33 [22528/54000 (42%)] Loss: -1765.460938\n",
      "Train Epoch: 33 [33792/54000 (63%)] Loss: -1767.787109\n",
      "Train Epoch: 33 [45056/54000 (83%)] Loss: -1770.145630\n",
      "    epoch          : 33\n",
      "    loss           : -1768.4072012271522\n",
      "    ess            : 8.001184472497904\n",
      "    log_marginal   : 1768.4072012271522\n",
      "    val_loss       : -1773.1700846354167\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1773.1700744628906\n",
      "Train Epoch: 34 [0/54000 (0%)] Loss: -1779.991211\n",
      "Train Epoch: 34 [11264/54000 (21%)] Loss: -1784.661133\n",
      "Train Epoch: 34 [22528/54000 (42%)] Loss: -1784.981567\n",
      "Train Epoch: 34 [33792/54000 (63%)] Loss: -1780.147949\n",
      "Train Epoch: 34 [45056/54000 (83%)] Loss: -1770.163818\n",
      "    epoch          : 34\n",
      "    loss           : -1777.3488516177772\n",
      "    ess            : 8.001183806725269\n",
      "    log_marginal   : 1777.3488516177772\n",
      "    val_loss       : -1777.604512532552\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1777.6045328776042\n",
      "Train Epoch: 35 [0/54000 (0%)] Loss: -1781.174805\n",
      "Train Epoch: 35 [11264/54000 (21%)] Loss: -1785.988037\n",
      "Train Epoch: 35 [22528/54000 (42%)] Loss: -1770.699463\n",
      "Train Epoch: 35 [33792/54000 (63%)] Loss: -1765.567261\n",
      "Train Epoch: 35 [45056/54000 (83%)] Loss: -1775.898682\n",
      "    epoch          : 35\n",
      "    loss           : -1775.049568608122\n",
      "    ess            : 8.001184157605442\n",
      "    log_marginal   : 1775.049568608122\n",
      "    val_loss       : -1777.7094014485676\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1777.7093811035156\n",
      "Train Epoch: 36 [0/54000 (0%)] Loss: -1786.623291\n",
      "Train Epoch: 36 [11264/54000 (21%)] Loss: -1768.478271\n",
      "Train Epoch: 36 [22528/54000 (42%)] Loss: -1779.382812\n",
      "Train Epoch: 36 [33792/54000 (63%)] Loss: -1769.157959\n",
      "Train Epoch: 36 [45056/54000 (83%)] Loss: -1765.765259\n",
      "    epoch          : 36\n",
      "    loss           : -1769.6184381448998\n",
      "    ess            : 8.0011846344426\n",
      "    log_marginal   : 1769.6184392965065\n",
      "    val_loss       : -1783.2106526692708\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1783.2106526692708\n",
      "Train Epoch: 37 [0/54000 (0%)] Loss: -1782.226318\n",
      "Train Epoch: 37 [11264/54000 (21%)] Loss: -1749.822021\n",
      "Train Epoch: 37 [22528/54000 (42%)] Loss: -1781.851196\n",
      "Train Epoch: 37 [33792/54000 (63%)] Loss: -1777.109985\n",
      "Train Epoch: 37 [45056/54000 (83%)] Loss: -1790.604492\n",
      "    epoch          : 37\n",
      "    loss           : -1776.845417526533\n",
      "    ess            : 8.001184517482542\n",
      "    log_marginal   : 1776.845417526533\n",
      "    val_loss       : -1788.274149576823\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1788.274149576823\n",
      "Train Epoch: 38 [0/54000 (0%)] Loss: -1796.490967\n",
      "Train Epoch: 38 [11264/54000 (21%)] Loss: -1786.931396\n",
      "Train Epoch: 38 [22528/54000 (42%)] Loss: -1770.694092\n",
      "Train Epoch: 38 [33792/54000 (63%)] Loss: -1774.721436\n",
      "Train Epoch: 38 [45056/54000 (83%)] Loss: -1766.985107\n",
      "    epoch          : 38\n",
      "    loss           : -1775.891307902786\n",
      "    ess            : 8.001184562467179\n",
      "    log_marginal   : 1775.891307902786\n",
      "    val_loss       : -1768.638671875\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1768.638692220052\n",
      "Train Epoch: 39 [0/54000 (0%)] Loss: -1757.569580\n",
      "Train Epoch: 39 [11264/54000 (21%)] Loss: -1776.589600\n",
      "Train Epoch: 39 [22528/54000 (42%)] Loss: -1768.148193\n",
      "Train Epoch: 39 [33792/54000 (63%)] Loss: -1786.562744\n",
      "Train Epoch: 39 [45056/54000 (83%)] Loss: -1780.283325\n",
      "    epoch          : 39\n",
      "    loss           : -1775.8360745411999\n",
      "    ess            : 8.001184373531702\n",
      "    log_marginal   : 1775.8360745411999\n",
      "    val_loss       : -1787.812764485677\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1787.8127543131511\n",
      "Train Epoch: 40 [0/54000 (0%)] Loss: -1795.330078\n",
      "Train Epoch: 40 [11264/54000 (21%)] Loss: -1764.150513\n",
      "Train Epoch: 40 [22528/54000 (42%)] Loss: -1776.579102\n",
      "Train Epoch: 40 [33792/54000 (63%)] Loss: -1773.846802\n",
      "Train Epoch: 40 [45056/54000 (83%)] Loss: -1777.870117\n",
      "    epoch          : 40\n",
      "    loss           : -1776.6259961398143\n",
      "    ess            : 8.001184211587006\n",
      "    log_marginal   : 1776.6259961398143\n",
      "    val_loss       : -1786.465311686198\n",
      "    val_ess        : 8.001184701919556\n",
      "    val_log_marginal: 1786.4653422037761\n",
      "Train Epoch: 41 [0/54000 (0%)] Loss: -1799.981812\n",
      "Train Epoch: 41 [11264/54000 (21%)] Loss: -1770.931274\n",
      "Train Epoch: 41 [22528/54000 (42%)] Loss: -1775.755859\n",
      "Train Epoch: 41 [33792/54000 (63%)] Loss: -1776.876099\n",
      "Train Epoch: 41 [45056/54000 (83%)] Loss: -1781.883789\n",
      "    epoch          : 41\n",
      "    loss           : -1778.595368007444\n",
      "    ess            : 8.001184337543991\n",
      "    log_marginal   : 1778.595368007444\n",
      "    val_loss       : -1788.5347290039062\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1788.5347290039062\n",
      "Train Epoch: 42 [0/54000 (0%)] Loss: -1800.721313\n",
      "Train Epoch: 42 [11264/54000 (21%)] Loss: -1779.767090\n",
      "Train Epoch: 42 [22528/54000 (42%)] Loss: -1776.316406\n",
      "Train Epoch: 42 [33792/54000 (63%)] Loss: -1765.817505\n",
      "Train Epoch: 42 [45056/54000 (83%)] Loss: -1780.828247\n",
      "    epoch          : 42\n",
      "    loss           : -1779.3807039080925\n",
      "    ess            : 8.001183779734486\n",
      "    log_marginal   : 1779.3807039080925\n",
      "    val_loss       : -1784.9939270019531\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1784.9939270019531\n",
      "Train Epoch: 43 [0/54000 (0%)] Loss: -1793.492920\n",
      "Train Epoch: 43 [11264/54000 (21%)] Loss: -1768.973877\n",
      "Train Epoch: 43 [22528/54000 (42%)] Loss: -1782.707275\n",
      "Train Epoch: 43 [33792/54000 (63%)] Loss: -1781.079346\n",
      "Train Epoch: 43 [45056/54000 (83%)] Loss: -1783.622803\n",
      "    epoch          : 43\n",
      "    loss           : -1777.8033953972583\n",
      "    ess            : 8.001183923685327\n",
      "    log_marginal   : 1777.8033953972583\n",
      "    val_loss       : -1780.6599731445312\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1780.6599731445312\n",
      "Train Epoch: 44 [0/54000 (0%)] Loss: -1791.450928\n",
      "Train Epoch: 44 [11264/54000 (21%)] Loss: -1779.712158\n",
      "Train Epoch: 44 [22528/54000 (42%)] Loss: -1755.164795\n",
      "Train Epoch: 44 [33792/54000 (63%)] Loss: -1730.964478\n",
      "Train Epoch: 44 [45056/54000 (83%)] Loss: -1761.531982\n",
      "    epoch          : 44\n",
      "    loss           : -1767.2199453677772\n",
      "    ess            : 8.001184211587006\n",
      "    log_marginal   : 1767.2199453677772\n",
      "    val_loss       : -1768.7694091796875\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1768.7694091796875\n",
      "Train Epoch: 45 [0/54000 (0%)] Loss: -1785.618042\n",
      "Train Epoch: 45 [11264/54000 (21%)] Loss: -1764.003662\n",
      "Train Epoch: 45 [22528/54000 (42%)] Loss: -1770.124878\n",
      "Train Epoch: 45 [33792/54000 (63%)] Loss: -1771.507812\n",
      "Train Epoch: 45 [45056/54000 (83%)] Loss: -1758.057617\n",
      "    epoch          : 45\n",
      "    loss           : -1768.8509429355838\n",
      "    ess            : 8.001185687083119\n",
      "    log_marginal   : 1768.8509429355838\n",
      "    val_loss       : -1775.4760335286458\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1775.476053873698\n",
      "Train Epoch: 46 [0/54000 (0%)] Loss: -1775.932373\n",
      "Train Epoch: 46 [11264/54000 (21%)] Loss: -1769.430542\n",
      "Train Epoch: 46 [22528/54000 (42%)] Loss: -1770.311890\n",
      "Train Epoch: 46 [33792/54000 (63%)] Loss: -1770.977539\n",
      "Train Epoch: 46 [45056/54000 (83%)] Loss: -1774.157959\n",
      "    epoch          : 46\n",
      "    loss           : -1772.705239349941\n",
      "    ess            : 8.001184895353497\n",
      "    log_marginal   : 1772.705239349941\n",
      "    val_loss       : -1785.7323404947917\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1785.7323404947917\n",
      "Train Epoch: 47 [0/54000 (0%)] Loss: -1785.387451\n",
      "Train Epoch: 47 [11264/54000 (21%)] Loss: -1765.975220\n",
      "Train Epoch: 47 [22528/54000 (42%)] Loss: -1773.299316\n",
      "Train Epoch: 47 [33792/54000 (63%)] Loss: -1786.288330\n",
      "Train Epoch: 47 [45056/54000 (83%)] Loss: -1782.982178\n",
      "    epoch          : 47\n",
      "    loss           : -1778.435939572892\n",
      "    ess            : 8.00118459845489\n",
      "    log_marginal   : 1778.4359384212853\n",
      "    val_loss       : -1785.8360595703125\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1785.8360595703125\n",
      "Train Epoch: 48 [0/54000 (0%)] Loss: -1797.241089\n",
      "Train Epoch: 48 [11264/54000 (21%)] Loss: -1777.791748\n",
      "Train Epoch: 48 [22528/54000 (42%)] Loss: -1755.915405\n",
      "Train Epoch: 48 [33792/54000 (63%)] Loss: -1779.013428\n",
      "Train Epoch: 48 [45056/54000 (83%)] Loss: -1777.276367\n",
      "    epoch          : 48\n",
      "    loss           : -1779.210138284935\n",
      "    ess            : 8.001184571464107\n",
      "    log_marginal   : 1779.2101405881485\n",
      "    val_loss       : -1783.8005269368489\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1783.8005269368489\n",
      "Train Epoch: 49 [0/54000 (0%)] Loss: -1788.477295\n",
      "Train Epoch: 49 [11264/54000 (21%)] Loss: -1794.464600\n",
      "Train Epoch: 49 [22528/54000 (42%)] Loss: -1776.976074\n",
      "Train Epoch: 49 [33792/54000 (63%)] Loss: -1780.558105\n",
      "Train Epoch: 49 [45056/54000 (83%)] Loss: -1777.893799\n",
      "    epoch          : 49\n",
      "    loss           : -1781.7519105155513\n",
      "    ess            : 8.001183698762137\n",
      "    log_marginal   : 1781.7519105155513\n",
      "    val_loss       : -1786.5049947102864\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1786.5049947102864\n",
      "Train Epoch: 50 [0/54000 (0%)] Loss: -1791.789795\n",
      "Train Epoch: 50 [11264/54000 (21%)] Loss: -1780.904541\n",
      "Train Epoch: 50 [22528/54000 (42%)] Loss: -1781.030518\n",
      "Train Epoch: 50 [33792/54000 (63%)] Loss: -1767.588745\n",
      "Train Epoch: 50 [45056/54000 (83%)] Loss: -1774.065674\n",
      "    epoch          : 50\n",
      "    loss           : -1778.3078129606427\n",
      "    ess            : 8.001184328547064\n",
      "    log_marginal   : 1778.307811809036\n",
      "    val_loss       : -1788.9646301269531\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1788.9646301269531\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch50.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 51 [0/54000 (0%)] Loss: -1791.234863\n",
      "Train Epoch: 51 [11264/54000 (21%)] Loss: -1790.399536\n",
      "Train Epoch: 51 [22528/54000 (42%)] Loss: -1776.251709\n",
      "Train Epoch: 51 [33792/54000 (63%)] Loss: -1780.349121\n",
      "Train Epoch: 51 [45056/54000 (83%)] Loss: -1781.016357\n",
      "    epoch          : 51\n",
      "    loss           : -1782.267445690227\n",
      "    ess            : 8.001184103623876\n",
      "    log_marginal   : 1782.2674445386203\n",
      "    val_loss       : -1791.4756571451824\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1791.4756571451824\n",
      "Train Epoch: 52 [0/54000 (0%)] Loss: -1794.705078\n",
      "Train Epoch: 52 [11264/54000 (21%)] Loss: -1780.086182\n",
      "Train Epoch: 52 [22528/54000 (42%)] Loss: -1782.177490\n",
      "Train Epoch: 52 [33792/54000 (63%)] Loss: -1779.926758\n",
      "Train Epoch: 52 [45056/54000 (83%)] Loss: -1782.276855\n",
      "    epoch          : 52\n",
      "    loss           : -1783.4322141251473\n",
      "    ess            : 8.001184445507121\n",
      "    log_marginal   : 1783.4322141251473\n",
      "    val_loss       : -1791.2454935709636\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1791.2454935709636\n",
      "Train Epoch: 53 [0/54000 (0%)] Loss: -1785.405151\n",
      "Train Epoch: 53 [11264/54000 (21%)] Loss: -1777.317505\n",
      "Train Epoch: 53 [22528/54000 (42%)] Loss: -1778.589844\n",
      "Train Epoch: 53 [33792/54000 (63%)] Loss: -1757.449463\n",
      "Train Epoch: 53 [45056/54000 (83%)] Loss: -1777.371338\n",
      "    epoch          : 53\n",
      "    loss           : -1776.8843118919517\n",
      "    ess            : 8.001184499488687\n",
      "    log_marginal   : 1776.8843118919517\n",
      "    val_loss       : -1789.3147379557292\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1789.3147583007812\n",
      "Train Epoch: 54 [0/54000 (0%)] Loss: -1792.385254\n",
      "Train Epoch: 54 [11264/54000 (21%)] Loss: -1786.213013\n",
      "Train Epoch: 54 [22528/54000 (42%)] Loss: -1778.183960\n",
      "Train Epoch: 54 [33792/54000 (63%)] Loss: -1783.872803\n",
      "Train Epoch: 54 [45056/54000 (83%)] Loss: -1777.134155\n",
      "    epoch          : 54\n",
      "    loss           : -1775.944915195681\n",
      "    ess            : 8.001184877359643\n",
      "    log_marginal   : 1775.944915195681\n",
      "    val_loss       : -1778.5730590820312\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1778.5730590820312\n",
      "Train Epoch: 55 [0/54000 (0%)] Loss: -1793.231445\n",
      "Train Epoch: 55 [11264/54000 (21%)] Loss: -1787.416992\n",
      "Train Epoch: 55 [22528/54000 (42%)] Loss: -1798.097046\n",
      "Train Epoch: 55 [33792/54000 (63%)] Loss: -1788.913330\n",
      "Train Epoch: 55 [45056/54000 (83%)] Loss: -1788.002197\n",
      "    epoch          : 55\n",
      "    loss           : -1788.3189835458431\n",
      "    ess            : 8.001183707759065\n",
      "    log_marginal   : 1788.3189823942364\n",
      "    val_loss       : -1789.9048970540364\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1789.9048970540364\n",
      "Train Epoch: 56 [0/54000 (0%)] Loss: -1803.249512\n",
      "Train Epoch: 56 [11264/54000 (21%)] Loss: -1793.041016\n",
      "Train Epoch: 56 [22528/54000 (42%)] Loss: -1777.804199\n",
      "Train Epoch: 56 [33792/54000 (63%)] Loss: -1764.084961\n",
      "Train Epoch: 56 [45056/54000 (83%)] Loss: -1769.813965\n",
      "    epoch          : 56\n",
      "    loss           : -1778.491359494767\n",
      "    ess            : 8.001184319550136\n",
      "    log_marginal   : 1778.491359494767\n",
      "    val_loss       : -1777.9922688802083\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1777.9922688802083\n",
      "Train Epoch: 57 [0/54000 (0%)] Loss: -1784.837524\n",
      "Train Epoch: 57 [11264/54000 (21%)] Loss: -1780.213379\n",
      "Train Epoch: 57 [22528/54000 (42%)] Loss: -1791.425903\n",
      "Train Epoch: 57 [33792/54000 (63%)] Loss: -1788.155151\n",
      "Train Epoch: 57 [45056/54000 (83%)] Loss: -1786.226074\n",
      "    epoch          : 57\n",
      "    loss           : -1785.4874256062058\n",
      "    ess            : 8.001184229580861\n",
      "    log_marginal   : 1785.4874256062058\n",
      "    val_loss       : -1794.0880126953125\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1794.0880025227864\n",
      "Train Epoch: 58 [0/54000 (0%)] Loss: -1801.352051\n",
      "Train Epoch: 58 [11264/54000 (21%)] Loss: -1762.933838\n",
      "Train Epoch: 58 [22528/54000 (42%)] Loss: -1780.277832\n",
      "Train Epoch: 58 [33792/54000 (63%)] Loss: -1767.683960\n",
      "Train Epoch: 58 [45056/54000 (83%)] Loss: -1783.453979\n",
      "    epoch          : 58\n",
      "    loss           : -1778.0325432543484\n",
      "    ess            : 8.001185003316627\n",
      "    log_marginal   : 1778.0325432543484\n",
      "    val_loss       : -1787.8149515787761\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1787.8149515787761\n",
      "Train Epoch: 59 [0/54000 (0%)] Loss: -1793.658691\n",
      "Train Epoch: 59 [11264/54000 (21%)] Loss: -1791.374268\n",
      "Train Epoch: 59 [22528/54000 (42%)] Loss: -1774.548584\n",
      "Train Epoch: 59 [33792/54000 (63%)] Loss: -1779.893188\n",
      "Train Epoch: 59 [45056/54000 (83%)] Loss: -1790.739502\n",
      "    epoch          : 59\n",
      "    loss           : -1784.600188633181\n",
      "    ess            : 8.001184157605442\n",
      "    log_marginal   : 1784.600188633181\n",
      "    val_loss       : -1794.8686625162761\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1794.868672688802\n",
      "Train Epoch: 60 [0/54000 (0%)] Loss: -1798.044434\n",
      "Train Epoch: 60 [11264/54000 (21%)] Loss: -1758.760010\n",
      "Train Epoch: 60 [22528/54000 (42%)] Loss: -1782.430176\n",
      "Train Epoch: 60 [33792/54000 (63%)] Loss: -1765.279053\n",
      "Train Epoch: 60 [45056/54000 (83%)] Loss: -1773.833252\n",
      "    epoch          : 60\n",
      "    loss           : -1774.4081374834168\n",
      "    ess            : 8.001184391525557\n",
      "    log_marginal   : 1774.4081374834168\n",
      "    val_loss       : -1780.1006673177083\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1780.1006673177083\n",
      "Train Epoch: 61 [0/54000 (0%)] Loss: -1787.528564\n",
      "Train Epoch: 61 [11264/54000 (21%)] Loss: -1776.840576\n",
      "Train Epoch: 61 [22528/54000 (42%)] Loss: -1790.886597\n",
      "Train Epoch: 61 [33792/54000 (63%)] Loss: -1799.114868\n",
      "Train Epoch: 61 [45056/54000 (83%)] Loss: -1795.393066\n",
      "    epoch          : 61\n",
      "    loss           : -1786.0347140330189\n",
      "    ess            : 8.001183959673035\n",
      "    log_marginal   : 1786.0347151846256\n",
      "    val_loss       : -1794.7135620117188\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1794.7135620117188\n",
      "Train Epoch: 62 [0/54000 (0%)] Loss: -1804.362061\n",
      "Train Epoch: 62 [11264/54000 (21%)] Loss: -1761.854614\n",
      "Train Epoch: 62 [22528/54000 (42%)] Loss: -1787.981934\n",
      "Train Epoch: 62 [33792/54000 (63%)] Loss: -1789.250000\n",
      "Train Epoch: 62 [45056/54000 (83%)] Loss: -1784.587402\n",
      "    epoch          : 62\n",
      "    loss           : -1785.9027721477005\n",
      "    ess            : 8.00118376174063\n",
      "    log_marginal   : 1785.9027732993072\n",
      "    val_loss       : -1795.6649881998699\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1795.6649983723958\n",
      "Train Epoch: 63 [0/54000 (0%)] Loss: -1799.753174\n",
      "Train Epoch: 63 [11264/54000 (21%)] Loss: -1780.193604\n",
      "Train Epoch: 63 [22528/54000 (42%)] Loss: -1797.438477\n",
      "Train Epoch: 63 [33792/54000 (63%)] Loss: -1786.937378\n",
      "Train Epoch: 63 [45056/54000 (83%)] Loss: -1789.880859\n",
      "    epoch          : 63\n",
      "    loss           : -1784.064946012677\n",
      "    ess            : 8.001184913347352\n",
      "    log_marginal   : 1784.064946012677\n",
      "    val_loss       : -1779.7325032552083\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1779.7324829101562\n",
      "Train Epoch: 64 [0/54000 (0%)] Loss: -1786.640259\n",
      "Train Epoch: 64 [11264/54000 (21%)] Loss: -1784.404053\n",
      "Train Epoch: 64 [22528/54000 (42%)] Loss: -1791.895508\n",
      "Train Epoch: 64 [33792/54000 (63%)] Loss: -1778.329468\n",
      "Train Epoch: 64 [45056/54000 (83%)] Loss: -1781.536865\n",
      "    epoch          : 64\n",
      "    loss           : -1783.7493850420105\n",
      "    ess            : 8.001184463500977\n",
      "    log_marginal   : 1783.7493850420105\n",
      "    val_loss       : -1782.4791768391926\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1782.4791870117188\n",
      "Train Epoch: 65 [0/54000 (0%)] Loss: -1798.722412\n",
      "Train Epoch: 65 [11264/54000 (21%)] Loss: -1786.828857\n",
      "Train Epoch: 65 [22528/54000 (42%)] Loss: -1783.779297\n",
      "Train Epoch: 65 [33792/54000 (63%)] Loss: -1783.831299\n",
      "Train Epoch: 65 [45056/54000 (83%)] Loss: -1790.568115\n",
      "    epoch          : 65\n",
      "    loss           : -1788.5497321362766\n",
      "    ess            : 8.00118386970376\n",
      "    log_marginal   : 1788.5497321362766\n",
      "    val_loss       : -1793.6842244466145\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1793.6842244466145\n",
      "Train Epoch: 66 [0/54000 (0%)] Loss: -1796.346069\n",
      "Train Epoch: 66 [11264/54000 (21%)] Loss: -1795.297974\n",
      "Train Epoch: 66 [22528/54000 (42%)] Loss: -1783.351807\n",
      "Train Epoch: 66 [33792/54000 (63%)] Loss: -1784.939087\n",
      "Train Epoch: 66 [45056/54000 (83%)] Loss: -1780.852783\n",
      "    epoch          : 66\n",
      "    loss           : -1788.6513395489387\n",
      "    ess            : 8.001184535476396\n",
      "    log_marginal   : 1788.6513407005455\n",
      "    val_loss       : -1798.8898824055989\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1798.8898824055989\n",
      "Train Epoch: 67 [0/54000 (0%)] Loss: -1793.656738\n",
      "Train Epoch: 67 [11264/54000 (21%)] Loss: -1786.013794\n",
      "Train Epoch: 67 [22528/54000 (42%)] Loss: -1779.089600\n",
      "Train Epoch: 67 [33792/54000 (63%)] Loss: -1790.938721\n",
      "Train Epoch: 67 [45056/54000 (83%)] Loss: -1774.317871\n",
      "    epoch          : 67\n",
      "    loss           : -1785.6586522516216\n",
      "    ess            : 8.001184769396511\n",
      "    log_marginal   : 1785.6586522516216\n",
      "    val_loss       : -1776.6049296061199\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1776.6049296061199\n",
      "Train Epoch: 68 [0/54000 (0%)] Loss: -1780.656494\n",
      "Train Epoch: 68 [11264/54000 (21%)] Loss: -1753.858643\n",
      "Train Epoch: 68 [22528/54000 (42%)] Loss: -1780.656982\n",
      "Train Epoch: 68 [33792/54000 (63%)] Loss: -1768.004272\n",
      "Train Epoch: 68 [45056/54000 (83%)] Loss: -1782.714478\n",
      "    epoch          : 68\n",
      "    loss           : -1768.0769296322228\n",
      "    ess            : 8.001185525138423\n",
      "    log_marginal   : 1768.0769296322228\n",
      "    val_loss       : -1776.4271138509114\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1776.4271138509114\n",
      "Train Epoch: 69 [0/54000 (0%)] Loss: -1774.249756\n",
      "Train Epoch: 69 [11264/54000 (21%)] Loss: -1796.149902\n",
      "Train Epoch: 69 [22528/54000 (42%)] Loss: -1789.868286\n",
      "Train Epoch: 69 [33792/54000 (63%)] Loss: -1768.516113\n",
      "Train Epoch: 69 [45056/54000 (83%)] Loss: -1777.913818\n",
      "    epoch          : 69\n",
      "    loss           : -1782.3915831367924\n",
      "    ess            : 8.001185309212163\n",
      "    log_marginal   : 1782.3915831367924\n",
      "    val_loss       : -1779.4949951171875\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1779.4949951171875\n",
      "Train Epoch: 70 [0/54000 (0%)] Loss: -1798.289551\n",
      "Train Epoch: 70 [11264/54000 (21%)] Loss: -1794.147705\n",
      "Train Epoch: 70 [22528/54000 (42%)] Loss: -1792.574585\n",
      "Train Epoch: 70 [33792/54000 (63%)] Loss: -1785.804932\n",
      "Train Epoch: 70 [45056/54000 (83%)] Loss: -1753.959717\n",
      "    epoch          : 70\n",
      "    loss           : -1783.719180700914\n",
      "    ess            : 8.001184913347352\n",
      "    log_marginal   : 1783.7191795493072\n",
      "    val_loss       : -1787.2830505371094\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1787.2830505371094\n",
      "Train Epoch: 71 [0/54000 (0%)] Loss: -1781.910767\n",
      "Train Epoch: 71 [11264/54000 (21%)] Loss: -1791.511108\n",
      "Train Epoch: 71 [22528/54000 (42%)] Loss: -1797.347168\n",
      "Train Epoch: 71 [33792/54000 (63%)] Loss: -1783.885742\n",
      "Train Epoch: 71 [45056/54000 (83%)] Loss: -1785.697388\n",
      "    epoch          : 71\n",
      "    loss           : -1789.4989497346698\n",
      "    ess            : 8.001184931341207\n",
      "    log_marginal   : 1789.4989497346698\n",
      "    val_loss       : -1799.6150817871094\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.6151021321614\n",
      "Train Epoch: 72 [0/54000 (0%)] Loss: -1807.767334\n",
      "Train Epoch: 72 [11264/54000 (21%)] Loss: -1799.504150\n",
      "Train Epoch: 72 [22528/54000 (42%)] Loss: -1801.512695\n",
      "Train Epoch: 72 [33792/54000 (63%)] Loss: -1776.753418\n",
      "Train Epoch: 72 [45056/54000 (83%)] Loss: -1780.402832\n",
      "    epoch          : 72\n",
      "    loss           : -1786.5656104897553\n",
      "    ess            : 8.001184544473324\n",
      "    log_marginal   : 1786.5656093381485\n",
      "    val_loss       : -1790.1706441243489\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1790.1706441243489\n",
      "Train Epoch: 73 [0/54000 (0%)] Loss: -1788.709595\n",
      "Train Epoch: 73 [11264/54000 (21%)] Loss: -1797.330200\n",
      "Train Epoch: 73 [22528/54000 (42%)] Loss: -1786.161743\n",
      "Train Epoch: 73 [33792/54000 (63%)] Loss: -1790.149170\n",
      "Train Epoch: 73 [45056/54000 (83%)] Loss: -1785.701416\n",
      "    epoch          : 73\n",
      "    loss           : -1789.5358172722583\n",
      "    ess            : 8.001184796387294\n",
      "    log_marginal   : 1789.5358161206516\n",
      "    val_loss       : -1796.8314514160156\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1796.8314514160156\n",
      "Train Epoch: 74 [0/54000 (0%)] Loss: -1803.997070\n",
      "Train Epoch: 74 [11264/54000 (21%)] Loss: -1765.394775\n",
      "Train Epoch: 74 [22528/54000 (42%)] Loss: -1786.920532\n",
      "Train Epoch: 74 [33792/54000 (63%)] Loss: -1789.966797\n",
      "Train Epoch: 74 [45056/54000 (83%)] Loss: -1770.381104\n",
      "    epoch          : 74\n",
      "    loss           : -1784.8586160911705\n",
      "    ess            : 8.001184688424164\n",
      "    log_marginal   : 1784.8586160911705\n",
      "    val_loss       : -1789.4666239420574\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1789.4666239420574\n",
      "Train Epoch: 75 [0/54000 (0%)] Loss: -1783.963379\n",
      "Train Epoch: 75 [11264/54000 (21%)] Loss: -1785.651245\n",
      "Train Epoch: 75 [22528/54000 (42%)] Loss: -1786.845703\n",
      "Train Epoch: 75 [33792/54000 (63%)] Loss: -1790.530029\n",
      "Train Epoch: 75 [45056/54000 (83%)] Loss: -1790.003784\n",
      "    epoch          : 75\n",
      "    loss           : -1787.6003740418632\n",
      "    ess            : 8.001184733408802\n",
      "    log_marginal   : 1787.6003728902565\n",
      "    val_loss       : -1795.7320556640625\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1795.7320556640625\n",
      "Train Epoch: 76 [0/54000 (0%)] Loss: -1800.461060\n",
      "Train Epoch: 76 [11264/54000 (21%)] Loss: -1785.336426\n",
      "Train Epoch: 76 [22528/54000 (42%)] Loss: -1786.622070\n",
      "Train Epoch: 76 [33792/54000 (63%)] Loss: -1786.071045\n",
      "Train Epoch: 76 [45056/54000 (83%)] Loss: -1801.903809\n",
      "    epoch          : 76\n",
      "    loss           : -1792.0905335624263\n",
      "    ess            : 8.00118440951941\n",
      "    log_marginal   : 1792.0905335624263\n",
      "    val_loss       : -1798.918680826823\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1798.918680826823\n",
      "Train Epoch: 77 [0/54000 (0%)] Loss: -1803.319580\n",
      "Train Epoch: 77 [11264/54000 (21%)] Loss: -1801.419922\n",
      "Train Epoch: 77 [22528/54000 (42%)] Loss: -1750.636963\n",
      "Train Epoch: 77 [33792/54000 (63%)] Loss: -1781.871948\n",
      "Train Epoch: 77 [45056/54000 (83%)] Loss: -1786.303955\n",
      "    epoch          : 77\n",
      "    loss           : -1786.0513973595962\n",
      "    ess            : 8.001184751402658\n",
      "    log_marginal   : 1786.0513973595962\n",
      "    val_loss       : -1793.6631469726562\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1793.6631571451824\n",
      "Train Epoch: 78 [0/54000 (0%)] Loss: -1802.098999\n",
      "Train Epoch: 78 [11264/54000 (21%)] Loss: -1776.675049\n",
      "Train Epoch: 78 [22528/54000 (42%)] Loss: -1776.673218\n",
      "Train Epoch: 78 [33792/54000 (63%)] Loss: -1792.635376\n",
      "Train Epoch: 78 [45056/54000 (83%)] Loss: -1795.161133\n",
      "    epoch          : 78\n",
      "    loss           : -1785.5293291199882\n",
      "    ess            : 8.001184877359643\n",
      "    log_marginal   : 1785.5293291199882\n",
      "    val_loss       : -1799.8935852050781\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1799.893575032552\n",
      "Train Epoch: 79 [0/54000 (0%)] Loss: -1807.114746\n",
      "Train Epoch: 79 [11264/54000 (21%)] Loss: -1784.304077\n",
      "Train Epoch: 79 [22528/54000 (42%)] Loss: -1783.582031\n",
      "Train Epoch: 79 [33792/54000 (63%)] Loss: -1779.884033\n",
      "Train Epoch: 79 [45056/54000 (83%)] Loss: -1787.187378\n",
      "    epoch          : 79\n",
      "    loss           : -1784.955805940448\n",
      "    ess            : 8.001184481494832\n",
      "    log_marginal   : 1784.9558047888413\n",
      "    val_loss       : -1783.5396525065105\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1783.5396525065105\n",
      "Train Epoch: 80 [0/54000 (0%)] Loss: -1791.211914\n",
      "Train Epoch: 80 [11264/54000 (21%)] Loss: -1784.319092\n",
      "Train Epoch: 80 [22528/54000 (42%)] Loss: -1781.338379\n",
      "Train Epoch: 80 [33792/54000 (63%)] Loss: -1786.605469\n",
      "Train Epoch: 80 [45056/54000 (83%)] Loss: -1787.839478\n",
      "    epoch          : 80\n",
      "    loss           : -1782.981457980174\n",
      "    ess            : 8.001184877359643\n",
      "    log_marginal   : 1782.9814591317806\n",
      "    val_loss       : -1789.2994893391926\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1789.2994893391926\n",
      "Train Epoch: 81 [0/54000 (0%)] Loss: -1804.107056\n",
      "Train Epoch: 81 [11264/54000 (21%)] Loss: -1795.808350\n",
      "Train Epoch: 81 [22528/54000 (42%)] Loss: -1790.971191\n",
      "Train Epoch: 81 [33792/54000 (63%)] Loss: -1776.223633\n",
      "Train Epoch: 81 [45056/54000 (83%)] Loss: -1780.840332\n",
      "    epoch          : 81\n",
      "    loss           : -1787.8518100954452\n",
      "    ess            : 8.001184589457962\n",
      "    log_marginal   : 1787.8518100954452\n",
      "    val_loss       : -1795.8247375488281\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1795.8247375488281\n",
      "Train Epoch: 82 [0/54000 (0%)] Loss: -1814.338501\n",
      "Train Epoch: 82 [11264/54000 (21%)] Loss: -1797.894531\n",
      "Train Epoch: 82 [22528/54000 (42%)] Loss: -1779.688599\n",
      "Train Epoch: 82 [33792/54000 (63%)] Loss: -1793.901855\n",
      "Train Epoch: 82 [45056/54000 (83%)] Loss: -1794.515747\n",
      "    epoch          : 82\n",
      "    loss           : -1791.7228854197376\n",
      "    ess            : 8.001184481494832\n",
      "    log_marginal   : 1791.7228854197376\n",
      "    val_loss       : -1799.6497395833333\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.6497395833333\n",
      "Train Epoch: 83 [0/54000 (0%)] Loss: -1799.910522\n",
      "Train Epoch: 83 [11264/54000 (21%)] Loss: -1796.588867\n",
      "Train Epoch: 83 [22528/54000 (42%)] Loss: -1780.382324\n",
      "Train Epoch: 83 [33792/54000 (63%)] Loss: -1788.501221\n",
      "Train Epoch: 83 [45056/54000 (83%)] Loss: -1799.498901\n",
      "    epoch          : 83\n",
      "    loss           : -1791.2611072468308\n",
      "    ess            : 8.001184391525557\n",
      "    log_marginal   : 1791.2611072468308\n",
      "    val_loss       : -1767.3743794759114\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1767.3743794759114\n",
      "Train Epoch: 84 [0/54000 (0%)] Loss: -1778.966919\n",
      "Train Epoch: 84 [11264/54000 (21%)] Loss: -1770.719360\n",
      "Train Epoch: 84 [22528/54000 (42%)] Loss: -1761.206421\n",
      "Train Epoch: 84 [33792/54000 (63%)] Loss: -1766.434204\n",
      "Train Epoch: 84 [45056/54000 (83%)] Loss: -1756.408203\n",
      "    epoch          : 84\n",
      "    loss           : -1769.670298450398\n",
      "    ess            : 8.001185309212163\n",
      "    log_marginal   : 1769.6702961471844\n",
      "    val_loss       : -1782.5105794270833\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1782.5105590820312\n",
      "Train Epoch: 85 [0/54000 (0%)] Loss: -1792.182861\n",
      "Train Epoch: 85 [11264/54000 (21%)] Loss: -1791.710571\n",
      "Train Epoch: 85 [22528/54000 (42%)] Loss: -1781.585693\n",
      "Train Epoch: 85 [33792/54000 (63%)] Loss: -1765.075806\n",
      "Train Epoch: 85 [45056/54000 (83%)] Loss: -1774.274780\n",
      "    epoch          : 85\n",
      "    loss           : -1785.2987429061027\n",
      "    ess            : 8.001184094626948\n",
      "    log_marginal   : 1785.2987429061027\n",
      "    val_loss       : -1794.5289408365886\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1794.5289408365886\n",
      "Train Epoch: 86 [0/54000 (0%)] Loss: -1805.268188\n",
      "Train Epoch: 86 [11264/54000 (21%)] Loss: -1791.641235\n",
      "Train Epoch: 86 [22528/54000 (42%)] Loss: -1787.557251\n",
      "Train Epoch: 86 [33792/54000 (63%)] Loss: -1786.612793\n",
      "Train Epoch: 86 [45056/54000 (83%)] Loss: -1783.260986\n",
      "    epoch          : 86\n",
      "    loss           : -1785.7400305406102\n",
      "    ess            : 8.001184553470251\n",
      "    log_marginal   : 1785.7400305406102\n",
      "    val_loss       : -1792.1249593098958\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1792.1249593098958\n",
      "Train Epoch: 87 [0/54000 (0%)] Loss: -1801.941650\n",
      "Train Epoch: 87 [11264/54000 (21%)] Loss: -1791.715576\n",
      "Train Epoch: 87 [22528/54000 (42%)] Loss: -1790.285889\n",
      "Train Epoch: 87 [33792/54000 (63%)] Loss: -1795.333740\n",
      "Train Epoch: 87 [45056/54000 (83%)] Loss: -1800.100586\n",
      "    epoch          : 87\n",
      "    loss           : -1794.1924196639152\n",
      "    ess            : 8.001183815722195\n",
      "    log_marginal   : 1794.1924196639152\n",
      "    val_loss       : -1797.2934672037761\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1797.2934672037761\n",
      "Train Epoch: 88 [0/54000 (0%)] Loss: -1809.049927\n",
      "Train Epoch: 88 [11264/54000 (21%)] Loss: -1798.562500\n",
      "Train Epoch: 88 [22528/54000 (42%)] Loss: -1778.731812\n",
      "Train Epoch: 88 [33792/54000 (63%)] Loss: -1795.244873\n",
      "Train Epoch: 88 [45056/54000 (83%)] Loss: -1789.026245\n",
      "    epoch          : 88\n",
      "    loss           : -1792.3177709039653\n",
      "    ess            : 8.001184319550136\n",
      "    log_marginal   : 1792.3177720555718\n",
      "    val_loss       : -1800.4700113932292\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1800.4700113932292\n",
      "Train Epoch: 89 [0/54000 (0%)] Loss: -1803.373901\n",
      "Train Epoch: 89 [11264/54000 (21%)] Loss: -1758.174561\n",
      "Train Epoch: 89 [22528/54000 (42%)] Loss: -1787.041748\n",
      "Train Epoch: 89 [33792/54000 (63%)] Loss: -1782.494141\n",
      "Train Epoch: 89 [45056/54000 (83%)] Loss: -1781.055664\n",
      "    epoch          : 89\n",
      "    loss           : -1789.4804595371463\n",
      "    ess            : 8.001184643439526\n",
      "    log_marginal   : 1789.4804583855396\n",
      "    val_loss       : -1793.7937825520833\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1793.7937825520833\n",
      "Train Epoch: 90 [0/54000 (0%)] Loss: -1806.628662\n",
      "Train Epoch: 90 [11264/54000 (21%)] Loss: -1792.335693\n",
      "Train Epoch: 90 [22528/54000 (42%)] Loss: -1796.325439\n",
      "Train Epoch: 90 [33792/54000 (63%)] Loss: -1786.503418\n",
      "Train Epoch: 90 [45056/54000 (83%)] Loss: -1785.560913\n",
      "    epoch          : 90\n",
      "    loss           : -1787.8273753040241\n",
      "    ess            : 8.001185381187582\n",
      "    log_marginal   : 1787.8273776072376\n",
      "    val_loss       : -1776.9058227539062\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1776.9058227539062\n",
      "Train Epoch: 91 [0/54000 (0%)] Loss: -1773.924438\n",
      "Train Epoch: 91 [11264/54000 (21%)] Loss: -1761.468506\n",
      "Train Epoch: 91 [22528/54000 (42%)] Loss: -1789.110107\n",
      "Train Epoch: 91 [33792/54000 (63%)] Loss: -1770.567627\n",
      "Train Epoch: 91 [45056/54000 (83%)] Loss: -1777.037598\n",
      "    epoch          : 91\n",
      "    loss           : -1774.1246383954895\n",
      "    ess            : 8.001185543132278\n",
      "    log_marginal   : 1774.1246383954895\n",
      "    val_loss       : -1779.3629659016926\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1779.3629659016926\n",
      "Train Epoch: 92 [0/54000 (0%)] Loss: -1791.838135\n",
      "Train Epoch: 92 [11264/54000 (21%)] Loss: -1774.756348\n",
      "Train Epoch: 92 [22528/54000 (42%)] Loss: -1766.553223\n",
      "Train Epoch: 92 [33792/54000 (63%)] Loss: -1785.403687\n",
      "Train Epoch: 92 [45056/54000 (83%)] Loss: -1787.571655\n",
      "    epoch          : 92\n",
      "    loss           : -1782.0164910082547\n",
      "    ess            : 8.001185219242888\n",
      "    log_marginal   : 1782.0164944630749\n",
      "    val_loss       : -1794.8152465820312\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1794.8152465820312\n",
      "Train Epoch: 93 [0/54000 (0%)] Loss: -1797.198486\n",
      "Train Epoch: 93 [11264/54000 (21%)] Loss: -1785.919800\n",
      "Train Epoch: 93 [22528/54000 (42%)] Loss: -1779.483398\n",
      "Train Epoch: 93 [33792/54000 (63%)] Loss: -1769.703369\n",
      "Train Epoch: 93 [45056/54000 (83%)] Loss: -1782.062988\n",
      "    epoch          : 93\n",
      "    loss           : -1783.3991641638413\n",
      "    ess            : 8.001185057298192\n",
      "    log_marginal   : 1783.3991630122346\n",
      "    val_loss       : -1794.5925598144531\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1794.5925598144531\n",
      "Train Epoch: 94 [0/54000 (0%)] Loss: -1795.433594\n",
      "Train Epoch: 94 [11264/54000 (21%)] Loss: -1797.124023\n",
      "Train Epoch: 94 [22528/54000 (42%)] Loss: -1792.369507\n",
      "Train Epoch: 94 [33792/54000 (63%)] Loss: -1788.528320\n",
      "Train Epoch: 94 [45056/54000 (83%)] Loss: -1789.932617\n",
      "    epoch          : 94\n",
      "    loss           : -1793.8802064139888\n",
      "    ess            : 8.001185075292048\n",
      "    log_marginal   : 1793.8802064139888\n",
      "    val_loss       : -1795.5611673990886\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1795.5611673990886\n",
      "Train Epoch: 95 [0/54000 (0%)] Loss: -1804.181396\n",
      "Train Epoch: 95 [11264/54000 (21%)] Loss: -1798.639160\n",
      "Train Epoch: 95 [22528/54000 (42%)] Loss: -1790.526123\n",
      "Train Epoch: 95 [33792/54000 (63%)] Loss: -1787.404175\n",
      "Train Epoch: 95 [45056/54000 (83%)] Loss: -1790.170410\n",
      "    epoch          : 95\n",
      "    loss           : -1790.0814969044811\n",
      "    ess            : 8.001184931341207\n",
      "    log_marginal   : 1790.0814969044811\n",
      "    val_loss       : -1793.5107320149739\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1793.5107320149739\n",
      "Train Epoch: 96 [0/54000 (0%)] Loss: -1804.139038\n",
      "Train Epoch: 96 [11264/54000 (21%)] Loss: -1789.337158\n",
      "Train Epoch: 96 [22528/54000 (42%)] Loss: -1770.456055\n",
      "Train Epoch: 96 [33792/54000 (63%)] Loss: -1790.716064\n",
      "Train Epoch: 96 [45056/54000 (83%)] Loss: -1791.049561\n",
      "    epoch          : 96\n",
      "    loss           : -1790.7290683962265\n",
      "    ess            : 8.001184787390367\n",
      "    log_marginal   : 1790.7290683962265\n",
      "    val_loss       : -1787.9310201009114\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1787.9309895833333\n",
      "Train Epoch: 97 [0/54000 (0%)] Loss: -1799.715088\n",
      "Train Epoch: 97 [11264/54000 (21%)] Loss: -1799.426880\n",
      "Train Epoch: 97 [22528/54000 (42%)] Loss: -1784.843994\n",
      "Train Epoch: 97 [33792/54000 (63%)] Loss: -1792.421143\n",
      "Train Epoch: 97 [45056/54000 (83%)] Loss: -1789.754639\n",
      "    epoch          : 97\n",
      "    loss           : -1795.2112737691627\n",
      "    ess            : 8.001184391525557\n",
      "    log_marginal   : 1795.211272617556\n",
      "    val_loss       : -1801.3811747233074\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.3811848958333\n",
      "Train Epoch: 98 [0/54000 (0%)] Loss: -1803.583984\n",
      "Train Epoch: 98 [11264/54000 (21%)] Loss: -1797.675659\n",
      "Train Epoch: 98 [22528/54000 (42%)] Loss: -1776.343872\n",
      "Train Epoch: 98 [33792/54000 (63%)] Loss: -1782.912842\n",
      "Train Epoch: 98 [45056/54000 (83%)] Loss: -1783.329224\n",
      "    epoch          : 98\n",
      "    loss           : -1785.9724397479363\n",
      "    ess            : 8.001185129273612\n",
      "    log_marginal   : 1785.9724397479363\n",
      "    val_loss       : -1789.0341796875\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1789.0341796875\n",
      "Train Epoch: 99 [0/54000 (0%)] Loss: -1790.265625\n",
      "Train Epoch: 99 [11264/54000 (21%)] Loss: -1800.835938\n",
      "Train Epoch: 99 [22528/54000 (42%)] Loss: -1752.056396\n",
      "Train Epoch: 99 [33792/54000 (63%)] Loss: -1780.481934\n",
      "Train Epoch: 99 [45056/54000 (83%)] Loss: -1771.102783\n",
      "    epoch          : 99\n",
      "    loss           : -1779.457597840507\n",
      "    ess            : 8.001185057298192\n",
      "    log_marginal   : 1779.457597840507\n",
      "    val_loss       : -1791.9299418131511\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1791.9299418131511\n",
      "Train Epoch: 100 [0/54000 (0%)] Loss: -1787.574463\n",
      "Train Epoch: 100 [11264/54000 (21%)] Loss: -1804.929321\n",
      "Train Epoch: 100 [22528/54000 (42%)] Loss: -1790.281738\n",
      "Train Epoch: 100 [33792/54000 (63%)] Loss: -1784.970947\n",
      "Train Epoch: 100 [45056/54000 (83%)] Loss: -1779.661987\n",
      "    epoch          : 100\n",
      "    loss           : -1790.1290398363797\n",
      "    ess            : 8.001184337543991\n",
      "    log_marginal   : 1790.1290398363797\n",
      "    val_loss       : -1799.2156372070312\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1799.2156473795574\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch100.pth ...\n",
      "Train Epoch: 101 [0/54000 (0%)] Loss: -1807.121826\n",
      "Train Epoch: 101 [11264/54000 (21%)] Loss: -1780.988281\n",
      "Train Epoch: 101 [22528/54000 (42%)] Loss: -1778.841064\n",
      "Train Epoch: 101 [33792/54000 (63%)] Loss: -1801.758789\n",
      "Train Epoch: 101 [45056/54000 (83%)] Loss: -1795.774658\n",
      "    epoch          : 101\n",
      "    loss           : -1789.3127809920402\n",
      "    ess            : 8.001184805384222\n",
      "    log_marginal   : 1789.3127809920402\n",
      "    val_loss       : -1799.9535522460938\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.9535624186199\n",
      "Train Epoch: 102 [0/54000 (0%)] Loss: -1804.331299\n",
      "Train Epoch: 102 [11264/54000 (21%)] Loss: -1806.623291\n",
      "Train Epoch: 102 [22528/54000 (42%)] Loss: -1798.409302\n",
      "Train Epoch: 102 [33792/54000 (63%)] Loss: -1799.038696\n",
      "Train Epoch: 102 [45056/54000 (83%)] Loss: -1784.420166\n",
      "    epoch          : 102\n",
      "    loss           : -1795.975962512898\n",
      "    ess            : 8.001184103623876\n",
      "    log_marginal   : 1795.9759613612912\n",
      "    val_loss       : -1798.3625895182292\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1798.3625895182292\n",
      "Train Epoch: 103 [0/54000 (0%)] Loss: -1797.048462\n",
      "Train Epoch: 103 [11264/54000 (21%)] Loss: -1791.399414\n",
      "Train Epoch: 103 [22528/54000 (42%)] Loss: -1786.575928\n",
      "Train Epoch: 103 [33792/54000 (63%)] Loss: -1768.672241\n",
      "Train Epoch: 103 [45056/54000 (83%)] Loss: -1766.667725\n",
      "    epoch          : 103\n",
      "    loss           : -1785.5997475678066\n",
      "    ess            : 8.001184679427237\n",
      "    log_marginal   : 1785.5997452645931\n",
      "    val_loss       : -1788.1837870279949\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1788.1837870279949\n",
      "Train Epoch: 104 [0/54000 (0%)] Loss: -1778.975830\n",
      "Train Epoch: 104 [11264/54000 (21%)] Loss: -1796.894165\n",
      "Train Epoch: 104 [22528/54000 (42%)] Loss: -1800.181152\n",
      "Train Epoch: 104 [33792/54000 (63%)] Loss: -1794.466797\n",
      "Train Epoch: 104 [45056/54000 (83%)] Loss: -1796.781738\n",
      "    epoch          : 104\n",
      "    loss           : -1791.5705911888267\n",
      "    ess            : 8.001184751402658\n",
      "    log_marginal   : 1791.57059003722\n",
      "    val_loss       : -1803.9464111328125\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1803.9464111328125\n",
      "Train Epoch: 105 [0/54000 (0%)] Loss: -1805.907959\n",
      "Train Epoch: 105 [11264/54000 (21%)] Loss: -1806.238525\n",
      "Train Epoch: 105 [22528/54000 (42%)] Loss: -1755.507568\n",
      "Train Epoch: 105 [33792/54000 (63%)] Loss: -1747.658936\n",
      "Train Epoch: 105 [45056/54000 (83%)] Loss: -1760.009277\n",
      "    epoch          : 105\n",
      "    loss           : -1774.0030102999706\n",
      "    ess            : 8.001185273224452\n",
      "    log_marginal   : 1774.0030102999706\n",
      "    val_loss       : -1779.6223449707031\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1779.6223449707031\n",
      "Train Epoch: 106 [0/54000 (0%)] Loss: -1780.350220\n",
      "Train Epoch: 106 [11264/54000 (21%)] Loss: -1788.600342\n",
      "Train Epoch: 106 [22528/54000 (42%)] Loss: -1790.263672\n",
      "Train Epoch: 106 [33792/54000 (63%)] Loss: -1782.527954\n",
      "Train Epoch: 106 [45056/54000 (83%)] Loss: -1781.139038\n",
      "    epoch          : 106\n",
      "    loss           : -1785.9615973706516\n",
      "    ess            : 8.001184751402658\n",
      "    log_marginal   : 1785.9615973706516\n",
      "    val_loss       : -1800.0348103841145\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1800.0348205566406\n",
      "Train Epoch: 107 [0/54000 (0%)] Loss: -1798.701904\n",
      "Train Epoch: 107 [11264/54000 (21%)] Loss: -1803.194824\n",
      "Train Epoch: 107 [22528/54000 (42%)] Loss: -1786.520020\n",
      "Train Epoch: 107 [33792/54000 (63%)] Loss: -1788.470703\n",
      "Train Epoch: 107 [45056/54000 (83%)] Loss: -1776.602295\n",
      "    epoch          : 107\n",
      "    loss           : -1789.4229206589032\n",
      "    ess            : 8.001184877359643\n",
      "    log_marginal   : 1789.4229206589032\n",
      "    val_loss       : -1797.7046813964844\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1797.7046813964844\n",
      "Train Epoch: 108 [0/54000 (0%)] Loss: -1800.814575\n",
      "Train Epoch: 108 [11264/54000 (21%)] Loss: -1806.291016\n",
      "Train Epoch: 108 [22528/54000 (42%)] Loss: -1787.238770\n",
      "Train Epoch: 108 [33792/54000 (63%)] Loss: -1799.430786\n",
      "Train Epoch: 108 [45056/54000 (83%)] Loss: -1791.485962\n",
      "    epoch          : 108\n",
      "    loss           : -1796.8472336103332\n",
      "    ess            : 8.001184535476396\n",
      "    log_marginal   : 1796.8472336103332\n",
      "    val_loss       : -1805.1671651204426\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1805.1671651204426\n",
      "Train Epoch: 109 [0/54000 (0%)] Loss: -1809.792969\n",
      "Train Epoch: 109 [11264/54000 (21%)] Loss: -1805.108643\n",
      "Train Epoch: 109 [22528/54000 (42%)] Loss: -1787.347412\n",
      "Train Epoch: 109 [33792/54000 (63%)] Loss: -1769.521484\n",
      "Train Epoch: 109 [45056/54000 (83%)] Loss: -1794.981445\n",
      "    epoch          : 109\n",
      "    loss           : -1793.7917457436615\n",
      "    ess            : 8.001184463500977\n",
      "    log_marginal   : 1793.7917457436615\n",
      "    val_loss       : -1799.2720642089844\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1799.2720642089844\n",
      "Train Epoch: 110 [0/54000 (0%)] Loss: -1800.034424\n",
      "Train Epoch: 110 [11264/54000 (21%)] Loss: -1787.193848\n",
      "Train Epoch: 110 [22528/54000 (42%)] Loss: -1779.623047\n",
      "Train Epoch: 110 [33792/54000 (63%)] Loss: -1790.733154\n",
      "Train Epoch: 110 [45056/54000 (83%)] Loss: -1788.471436\n",
      "    epoch          : 110\n",
      "    loss           : -1790.8228644604953\n",
      "    ess            : 8.001184499488687\n",
      "    log_marginal   : 1790.8228633088886\n",
      "    val_loss       : -1796.9783935546875\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1796.9784138997395\n",
      "Train Epoch: 111 [0/54000 (0%)] Loss: -1806.593262\n",
      "Train Epoch: 111 [11264/54000 (21%)] Loss: -1788.614746\n",
      "Train Epoch: 111 [22528/54000 (42%)] Loss: -1780.677490\n",
      "Train Epoch: 111 [33792/54000 (63%)] Loss: -1799.920654\n",
      "Train Epoch: 111 [45056/54000 (83%)] Loss: -1793.521484\n",
      "    epoch          : 111\n",
      "    loss           : -1789.2545373304836\n",
      "    ess            : 8.001184823378077\n",
      "    log_marginal   : 1789.2545373304836\n",
      "    val_loss       : -1799.3076578776042\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1799.3076578776042\n",
      "Train Epoch: 112 [0/54000 (0%)] Loss: -1787.166870\n",
      "Train Epoch: 112 [11264/54000 (21%)] Loss: -1806.285400\n",
      "Train Epoch: 112 [22528/54000 (42%)] Loss: -1781.317139\n",
      "Train Epoch: 112 [33792/54000 (63%)] Loss: -1785.395264\n",
      "Train Epoch: 112 [45056/54000 (83%)] Loss: -1765.399780\n",
      "    epoch          : 112\n",
      "    loss           : -1786.5801771631782\n",
      "    ess            : 8.001184607451817\n",
      "    log_marginal   : 1786.5801771631782\n",
      "    val_loss       : -1781.7034098307292\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1781.7034098307292\n",
      "Train Epoch: 113 [0/54000 (0%)] Loss: -1793.973022\n",
      "Train Epoch: 113 [11264/54000 (21%)] Loss: -1771.452026\n",
      "Train Epoch: 113 [22528/54000 (42%)] Loss: -1787.582764\n",
      "Train Epoch: 113 [33792/54000 (63%)] Loss: -1772.778931\n",
      "Train Epoch: 113 [45056/54000 (83%)] Loss: -1775.279785\n",
      "    epoch          : 113\n",
      "    loss           : -1778.609135465802\n",
      "    ess            : 8.001185777052394\n",
      "    log_marginal   : 1778.6091366174087\n",
      "    val_loss       : -1795.7128804524739\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1795.7128601074219\n",
      "Train Epoch: 114 [0/54000 (0%)] Loss: -1788.380615\n",
      "Train Epoch: 114 [11264/54000 (21%)] Loss: -1777.303711\n",
      "Train Epoch: 114 [22528/54000 (42%)] Loss: -1787.009521\n",
      "Train Epoch: 114 [33792/54000 (63%)] Loss: -1783.689209\n",
      "Train Epoch: 114 [45056/54000 (83%)] Loss: -1787.568848\n",
      "    epoch          : 114\n",
      "    loss           : -1785.4160674473026\n",
      "    ess            : 8.001185741064683\n",
      "    log_marginal   : 1785.4160674473026\n",
      "    val_loss       : -1788.0196431477864\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1788.0196431477864\n",
      "Train Epoch: 115 [0/54000 (0%)] Loss: -1789.241821\n",
      "Train Epoch: 115 [11264/54000 (21%)] Loss: -1798.331299\n",
      "Train Epoch: 115 [22528/54000 (42%)] Loss: -1786.289062\n",
      "Train Epoch: 115 [33792/54000 (63%)] Loss: -1801.164062\n",
      "Train Epoch: 115 [45056/54000 (83%)] Loss: -1775.470093\n",
      "    epoch          : 115\n",
      "    loss           : -1786.0647490879276\n",
      "    ess            : 8.001185561126134\n",
      "    log_marginal   : 1786.0647490879276\n",
      "    val_loss       : -1795.808125813802\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1795.808125813802\n",
      "Train Epoch: 116 [0/54000 (0%)] Loss: -1794.635742\n",
      "Train Epoch: 116 [11264/54000 (21%)] Loss: -1789.656982\n",
      "Train Epoch: 116 [22528/54000 (42%)] Loss: -1773.854004\n",
      "Train Epoch: 116 [33792/54000 (63%)] Loss: -1772.273315\n",
      "Train Epoch: 116 [45056/54000 (83%)] Loss: -1766.801758\n",
      "    epoch          : 116\n",
      "    loss           : -1780.621912542379\n",
      "    ess            : 8.001185651095408\n",
      "    log_marginal   : 1780.6219136939858\n",
      "    val_loss       : -1772.84130859375\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1772.84130859375\n",
      "Train Epoch: 117 [0/54000 (0%)] Loss: -1772.675293\n",
      "Train Epoch: 117 [11264/54000 (21%)] Loss: -1785.227051\n",
      "Train Epoch: 117 [22528/54000 (42%)] Loss: -1771.300537\n",
      "Train Epoch: 117 [33792/54000 (63%)] Loss: -1780.560791\n",
      "Train Epoch: 117 [45056/54000 (83%)] Loss: -1785.659424\n",
      "    epoch          : 117\n",
      "    loss           : -1783.04499212301\n",
      "    ess            : 8.001185381187582\n",
      "    log_marginal   : 1783.04499212301\n",
      "    val_loss       : -1775.7434692382812\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1775.7434692382812\n",
      "Train Epoch: 118 [0/54000 (0%)] Loss: -1796.236572\n",
      "Train Epoch: 118 [11264/54000 (21%)] Loss: -1789.461426\n",
      "Train Epoch: 118 [22528/54000 (42%)] Loss: -1789.186279\n",
      "Train Epoch: 118 [33792/54000 (63%)] Loss: -1803.582153\n",
      "Train Epoch: 118 [45056/54000 (83%)] Loss: -1796.934814\n",
      "    epoch          : 118\n",
      "    loss           : -1794.512703373747\n",
      "    ess            : 8.001185237236744\n",
      "    log_marginal   : 1794.512703373747\n",
      "    val_loss       : -1796.8216756184895\n",
      "    val_ess        : 8.001185337702433\n",
      "    val_log_marginal: 1796.8216756184895\n",
      "Train Epoch: 119 [0/54000 (0%)] Loss: -1812.763550\n",
      "Train Epoch: 119 [11264/54000 (21%)] Loss: -1806.654907\n",
      "Train Epoch: 119 [22528/54000 (42%)] Loss: -1793.781372\n",
      "Train Epoch: 119 [33792/54000 (63%)] Loss: -1788.281494\n",
      "Train Epoch: 119 [45056/54000 (83%)] Loss: -1793.440430\n",
      "    epoch          : 119\n",
      "    loss           : -1796.063642393868\n",
      "    ess            : 8.001185291218308\n",
      "    log_marginal   : 1796.063642393868\n",
      "    val_loss       : -1799.1904195149739\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1799.190409342448\n",
      "Train Epoch: 120 [0/54000 (0%)] Loss: -1811.295410\n",
      "Train Epoch: 120 [11264/54000 (21%)] Loss: -1790.402100\n",
      "Train Epoch: 120 [22528/54000 (42%)] Loss: -1796.584717\n",
      "Train Epoch: 120 [33792/54000 (63%)] Loss: -1795.203247\n",
      "Train Epoch: 120 [45056/54000 (83%)] Loss: -1796.733643\n",
      "    epoch          : 120\n",
      "    loss           : -1795.1298897221404\n",
      "    ess            : 8.001185183255178\n",
      "    log_marginal   : 1795.129890873747\n",
      "    val_loss       : -1795.1087849934895\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1795.1087849934895\n",
      "Train Epoch: 121 [0/54000 (0%)] Loss: -1805.952881\n",
      "Train Epoch: 121 [11264/54000 (21%)] Loss: -1784.605469\n",
      "Train Epoch: 121 [22528/54000 (42%)] Loss: -1783.236450\n",
      "Train Epoch: 121 [33792/54000 (63%)] Loss: -1780.288208\n",
      "Train Epoch: 121 [45056/54000 (83%)] Loss: -1792.666870\n",
      "    epoch          : 121\n",
      "    loss           : -1786.6193790075913\n",
      "    ess            : 8.001185273224452\n",
      "    log_marginal   : 1786.6193790075913\n",
      "    val_loss       : -1796.3625895182292\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1796.3625996907551\n",
      "Train Epoch: 122 [0/54000 (0%)] Loss: -1810.496216\n",
      "Train Epoch: 122 [11264/54000 (21%)] Loss: -1798.552979\n",
      "Train Epoch: 122 [22528/54000 (42%)] Loss: -1791.979004\n",
      "Train Epoch: 122 [33792/54000 (63%)] Loss: -1794.540771\n",
      "Train Epoch: 122 [45056/54000 (83%)] Loss: -1793.938232\n",
      "    epoch          : 122\n",
      "    loss           : -1792.8786874447228\n",
      "    ess            : 8.001185489150712\n",
      "    log_marginal   : 1792.878686293116\n",
      "    val_loss       : -1803.8573811848958\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1803.8573811848958\n",
      "Train Epoch: 123 [0/54000 (0%)] Loss: -1802.785889\n",
      "Train Epoch: 123 [11264/54000 (21%)] Loss: -1790.477539\n",
      "Train Epoch: 123 [22528/54000 (42%)] Loss: -1796.228516\n",
      "Train Epoch: 123 [33792/54000 (63%)] Loss: -1791.999878\n",
      "Train Epoch: 123 [45056/54000 (83%)] Loss: -1790.186646\n",
      "    epoch          : 123\n",
      "    loss           : -1793.8895136995136\n",
      "    ess            : 8.001185543132278\n",
      "    log_marginal   : 1793.8895148511203\n",
      "    val_loss       : -1805.9361979166667\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.9361979166667\n",
      "Train Epoch: 124 [0/54000 (0%)] Loss: -1805.205811\n",
      "Train Epoch: 124 [11264/54000 (21%)] Loss: -1790.300049\n",
      "Train Epoch: 124 [22528/54000 (42%)] Loss: -1785.285278\n",
      "Train Epoch: 124 [33792/54000 (63%)] Loss: -1787.607178\n",
      "Train Epoch: 124 [45056/54000 (83%)] Loss: -1793.277100\n",
      "    epoch          : 124\n",
      "    loss           : -1790.283298708358\n",
      "    ess            : 8.001185345199874\n",
      "    log_marginal   : 1790.283298708358\n",
      "    val_loss       : -1792.4333597819011\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1792.4333597819011\n",
      "Train Epoch: 125 [0/54000 (0%)] Loss: -1798.562500\n",
      "Train Epoch: 125 [11264/54000 (21%)] Loss: -1805.308472\n",
      "Train Epoch: 125 [22528/54000 (42%)] Loss: -1794.902588\n",
      "Train Epoch: 125 [33792/54000 (63%)] Loss: -1797.088379\n",
      "Train Epoch: 125 [45056/54000 (83%)] Loss: -1800.471191\n",
      "    epoch          : 125\n",
      "    loss           : -1799.3126554669075\n",
      "    ess            : 8.001185579119989\n",
      "    log_marginal   : 1799.3126566185142\n",
      "    val_loss       : -1807.6978963216145\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.6979166666667\n",
      "Train Epoch: 126 [0/54000 (0%)] Loss: -1821.484009\n",
      "Train Epoch: 126 [11264/54000 (21%)] Loss: -1790.935303\n",
      "Train Epoch: 126 [22528/54000 (42%)] Loss: -1772.096802\n",
      "Train Epoch: 126 [33792/54000 (63%)] Loss: -1789.061279\n",
      "Train Epoch: 126 [45056/54000 (83%)] Loss: -1774.784668\n",
      "    epoch          : 126\n",
      "    loss           : -1784.8790663233344\n",
      "    ess            : 8.001185273224452\n",
      "    log_marginal   : 1784.8790663233344\n",
      "    val_loss       : -1783.3997294108074\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1783.3997294108074\n",
      "Train Epoch: 127 [0/54000 (0%)] Loss: -1769.121582\n",
      "Train Epoch: 127 [11264/54000 (21%)] Loss: -1778.060303\n",
      "Train Epoch: 127 [22528/54000 (42%)] Loss: -1786.554443\n",
      "Train Epoch: 127 [33792/54000 (63%)] Loss: -1784.105225\n",
      "Train Epoch: 127 [45056/54000 (83%)] Loss: -1775.187500\n",
      "    epoch          : 127\n",
      "    loss           : -1781.114928047612\n",
      "    ess            : 8.00118517425825\n",
      "    log_marginal   : 1781.114928047612\n",
      "    val_loss       : -1800.8019917805989\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1800.8019917805989\n",
      "Train Epoch: 128 [0/54000 (0%)] Loss: -1796.847900\n",
      "Train Epoch: 128 [11264/54000 (21%)] Loss: -1793.403076\n",
      "Train Epoch: 128 [22528/54000 (42%)] Loss: -1801.581543\n",
      "Train Epoch: 128 [33792/54000 (63%)] Loss: -1797.237305\n",
      "Train Epoch: 128 [45056/54000 (83%)] Loss: -1792.051758\n",
      "    epoch          : 128\n",
      "    loss           : -1795.758797123747\n",
      "    ess            : 8.001184769396511\n",
      "    log_marginal   : 1795.758797123747\n",
      "    val_loss       : -1806.4120992024739\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1806.412089029948\n",
      "Train Epoch: 129 [0/54000 (0%)] Loss: -1806.621582\n",
      "Train Epoch: 129 [11264/54000 (21%)] Loss: -1806.394287\n",
      "Train Epoch: 129 [22528/54000 (42%)] Loss: -1796.496338\n",
      "Train Epoch: 129 [33792/54000 (63%)] Loss: -1799.057007\n",
      "Train Epoch: 129 [45056/54000 (83%)] Loss: -1797.147095\n",
      "    epoch          : 129\n",
      "    loss           : -1796.5769526643573\n",
      "    ess            : 8.001184967328918\n",
      "    log_marginal   : 1796.5769526643573\n",
      "    val_loss       : -1802.2469075520833\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1802.2469075520833\n",
      "Train Epoch: 130 [0/54000 (0%)] Loss: -1805.835571\n",
      "Train Epoch: 130 [11264/54000 (21%)] Loss: -1802.089111\n",
      "Train Epoch: 130 [22528/54000 (42%)] Loss: -1790.546753\n",
      "Train Epoch: 130 [33792/54000 (63%)] Loss: -1792.080078\n",
      "Train Epoch: 130 [45056/54000 (83%)] Loss: -1795.779907\n",
      "    epoch          : 130\n",
      "    loss           : -1795.9358428379276\n",
      "    ess            : 8.001185039304337\n",
      "    log_marginal   : 1795.9358428379276\n",
      "    val_loss       : -1794.3956502278645\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1794.3956502278645\n",
      "Train Epoch: 131 [0/54000 (0%)] Loss: -1806.909668\n",
      "Train Epoch: 131 [11264/54000 (21%)] Loss: -1799.084351\n",
      "Train Epoch: 131 [22528/54000 (42%)] Loss: -1776.896240\n",
      "Train Epoch: 131 [33792/54000 (63%)] Loss: -1794.332397\n",
      "Train Epoch: 131 [45056/54000 (83%)] Loss: -1787.991089\n",
      "    epoch          : 131\n",
      "    loss           : -1793.7664426407723\n",
      "    ess            : 8.001185264227525\n",
      "    log_marginal   : 1793.7664426407723\n",
      "    val_loss       : -1793.9847005208333\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1793.9847005208333\n",
      "Train Epoch: 132 [0/54000 (0%)] Loss: -1802.102295\n",
      "Train Epoch: 132 [11264/54000 (21%)] Loss: -1788.254761\n",
      "Train Epoch: 132 [22528/54000 (42%)] Loss: -1789.746338\n",
      "Train Epoch: 132 [33792/54000 (63%)] Loss: -1807.723389\n",
      "Train Epoch: 132 [45056/54000 (83%)] Loss: -1798.645508\n",
      "    epoch          : 132\n",
      "    loss           : -1795.0984427973908\n",
      "    ess            : 8.001185327206018\n",
      "    log_marginal   : 1795.0984427973908\n",
      "    val_loss       : -1789.4386291503906\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1789.4386291503906\n",
      "Train Epoch: 133 [0/54000 (0%)] Loss: -1806.567871\n",
      "Train Epoch: 133 [11264/54000 (21%)] Loss: -1766.903076\n",
      "Train Epoch: 133 [22528/54000 (42%)] Loss: -1797.320801\n",
      "Train Epoch: 133 [33792/54000 (63%)] Loss: -1752.011475\n",
      "Train Epoch: 133 [45056/54000 (83%)] Loss: -1764.191162\n",
      "    epoch          : 133\n",
      "    loss           : -1773.2448004956516\n",
      "    ess            : 8.001185885015524\n",
      "    log_marginal   : 1773.2448004956516\n",
      "    val_loss       : -1778.4336853027344\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1778.4336853027344\n",
      "Train Epoch: 134 [0/54000 (0%)] Loss: -1789.323486\n",
      "Train Epoch: 134 [11264/54000 (21%)] Loss: -1785.978638\n",
      "Train Epoch: 134 [22528/54000 (42%)] Loss: -1776.354248\n",
      "Train Epoch: 134 [33792/54000 (63%)] Loss: -1786.056396\n",
      "Train Epoch: 134 [45056/54000 (83%)] Loss: -1790.927246\n",
      "    epoch          : 134\n",
      "    loss           : -1788.4961098724941\n",
      "    ess            : 8.001185777052394\n",
      "    log_marginal   : 1788.4961098724941\n",
      "    val_loss       : -1794.5604858398438\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1794.5604858398438\n",
      "Train Epoch: 135 [0/54000 (0%)] Loss: -1803.865479\n",
      "Train Epoch: 135 [11264/54000 (21%)] Loss: -1784.933838\n",
      "Train Epoch: 135 [22528/54000 (42%)] Loss: -1784.190430\n",
      "Train Epoch: 135 [33792/54000 (63%)] Loss: -1792.997803\n",
      "Train Epoch: 135 [45056/54000 (83%)] Loss: -1793.886475\n",
      "    epoch          : 135\n",
      "    loss           : -1793.4891265293338\n",
      "    ess            : 8.001185615107698\n",
      "    log_marginal   : 1793.4891265293338\n",
      "    val_loss       : -1804.8514200846355\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.8514200846355\n",
      "Train Epoch: 136 [0/54000 (0%)] Loss: -1812.651367\n",
      "Train Epoch: 136 [11264/54000 (21%)] Loss: -1800.174805\n",
      "Train Epoch: 136 [22528/54000 (42%)] Loss: -1774.391846\n",
      "Train Epoch: 136 [33792/54000 (63%)] Loss: -1800.704590\n",
      "Train Epoch: 136 [45056/54000 (83%)] Loss: -1791.143677\n",
      "    epoch          : 136\n",
      "    loss           : -1794.4003629864387\n",
      "    ess            : 8.001185399181438\n",
      "    log_marginal   : 1794.400361834832\n",
      "    val_loss       : -1792.5404561360676\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1792.5404561360676\n",
      "Train Epoch: 137 [0/54000 (0%)] Loss: -1805.107910\n",
      "Train Epoch: 137 [11264/54000 (21%)] Loss: -1798.613037\n",
      "Train Epoch: 137 [22528/54000 (42%)] Loss: -1772.197266\n",
      "Train Epoch: 137 [33792/54000 (63%)] Loss: -1783.342041\n",
      "Train Epoch: 137 [45056/54000 (83%)] Loss: -1759.611816\n",
      "    epoch          : 137\n",
      "    loss           : -1781.2216025298496\n",
      "    ess            : 8.001185453163004\n",
      "    log_marginal   : 1781.221601378243\n",
      "    val_loss       : -1791.4818522135417\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1791.4818522135417\n",
      "Train Epoch: 138 [0/54000 (0%)] Loss: -1790.171631\n",
      "Train Epoch: 138 [11264/54000 (21%)] Loss: -1785.720215\n",
      "Train Epoch: 138 [22528/54000 (42%)] Loss: -1793.614868\n",
      "Train Epoch: 138 [33792/54000 (63%)] Loss: -1790.645264\n",
      "Train Epoch: 138 [45056/54000 (83%)] Loss: -1786.249878\n",
      "    epoch          : 138\n",
      "    loss           : -1788.4538850604363\n",
      "    ess            : 8.001185489150712\n",
      "    log_marginal   : 1788.4538850604363\n",
      "    val_loss       : -1797.5572204589844\n",
      "    val_ess        : 8.001185337702433\n",
      "    val_log_marginal: 1797.5572204589844\n",
      "Train Epoch: 139 [0/54000 (0%)] Loss: -1806.730347\n",
      "Train Epoch: 139 [11264/54000 (21%)] Loss: -1798.212891\n",
      "Train Epoch: 139 [22528/54000 (42%)] Loss: -1793.531128\n",
      "Train Epoch: 139 [33792/54000 (63%)] Loss: -1787.199097\n",
      "Train Epoch: 139 [45056/54000 (83%)] Loss: -1801.994751\n",
      "    epoch          : 139\n",
      "    loss           : -1795.1125039154629\n",
      "    ess            : 8.001185012313554\n",
      "    log_marginal   : 1795.1125039154629\n",
      "    val_loss       : -1797.9607543945312\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.9607543945312\n",
      "Train Epoch: 140 [0/54000 (0%)] Loss: -1805.071533\n",
      "Train Epoch: 140 [11264/54000 (21%)] Loss: -1811.606812\n",
      "Train Epoch: 140 [22528/54000 (42%)] Loss: -1774.754639\n",
      "Train Epoch: 140 [33792/54000 (63%)] Loss: -1789.945923\n",
      "Train Epoch: 140 [45056/54000 (83%)] Loss: -1795.339844\n",
      "    epoch          : 140\n",
      "    loss           : -1796.7273628666717\n",
      "    ess            : 8.001184616448745\n",
      "    log_marginal   : 1796.727361715065\n",
      "    val_loss       : -1800.5294291178386\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1800.5294291178386\n",
      "Train Epoch: 141 [0/54000 (0%)] Loss: -1805.373535\n",
      "Train Epoch: 141 [11264/54000 (21%)] Loss: -1794.199585\n",
      "Train Epoch: 141 [22528/54000 (42%)] Loss: -1789.032471\n",
      "Train Epoch: 141 [33792/54000 (63%)] Loss: -1778.298096\n",
      "Train Epoch: 141 [45056/54000 (83%)] Loss: -1796.195312\n",
      "    epoch          : 141\n",
      "    loss           : -1791.131323472509\n",
      "    ess            : 8.001185219242888\n",
      "    log_marginal   : 1791.131323472509\n",
      "    val_loss       : -1800.772969563802\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1800.7729797363281\n",
      "Train Epoch: 142 [0/54000 (0%)] Loss: -1805.866211\n",
      "Train Epoch: 142 [11264/54000 (21%)] Loss: -1803.557495\n",
      "Train Epoch: 142 [22528/54000 (42%)] Loss: -1801.350830\n",
      "Train Epoch: 142 [33792/54000 (63%)] Loss: -1791.767456\n",
      "Train Epoch: 142 [45056/54000 (83%)] Loss: -1799.469971\n",
      "    epoch          : 142\n",
      "    loss           : -1798.1907705630897\n",
      "    ess            : 8.001185093285903\n",
      "    log_marginal   : 1798.1907705630897\n",
      "    val_loss       : -1808.6921081542969\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1808.692118326823\n",
      "Train Epoch: 143 [0/54000 (0%)] Loss: -1809.637207\n",
      "Train Epoch: 143 [11264/54000 (21%)] Loss: -1784.020020\n",
      "Train Epoch: 143 [22528/54000 (42%)] Loss: -1773.376221\n",
      "Train Epoch: 143 [33792/54000 (63%)] Loss: -1782.112305\n",
      "Train Epoch: 143 [45056/54000 (83%)] Loss: -1779.279541\n",
      "    epoch          : 143\n",
      "    loss           : -1790.126208035451\n",
      "    ess            : 8.001185084288975\n",
      "    log_marginal   : 1790.126208035451\n",
      "    val_loss       : -1796.5337219238281\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1796.5337219238281\n",
      "Train Epoch: 144 [0/54000 (0%)] Loss: -1792.922363\n",
      "Train Epoch: 144 [11264/54000 (21%)] Loss: -1803.005249\n",
      "Train Epoch: 144 [22528/54000 (42%)] Loss: -1775.456421\n",
      "Train Epoch: 144 [33792/54000 (63%)] Loss: -1792.838013\n",
      "Train Epoch: 144 [45056/54000 (83%)] Loss: -1770.163086\n",
      "    epoch          : 144\n",
      "    loss           : -1786.1695717865566\n",
      "    ess            : 8.001185345199874\n",
      "    log_marginal   : 1786.1695706349499\n",
      "    val_loss       : -1782.9345703125\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1782.9345703125\n",
      "Train Epoch: 145 [0/54000 (0%)] Loss: -1786.420044\n",
      "Train Epoch: 145 [11264/54000 (21%)] Loss: -1783.669067\n",
      "Train Epoch: 145 [22528/54000 (42%)] Loss: -1793.382568\n",
      "Train Epoch: 145 [33792/54000 (63%)] Loss: -1781.881104\n",
      "Train Epoch: 145 [45056/54000 (83%)] Loss: -1792.239136\n",
      "    epoch          : 145\n",
      "    loss           : -1790.0288673256928\n",
      "    ess            : 8.001185453163004\n",
      "    log_marginal   : 1790.0288673256928\n",
      "    val_loss       : -1788.8523966471355\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1788.8523966471355\n",
      "Train Epoch: 146 [0/54000 (0%)] Loss: -1798.884277\n",
      "Train Epoch: 146 [11264/54000 (21%)] Loss: -1796.011230\n",
      "Train Epoch: 146 [22528/54000 (42%)] Loss: -1782.065308\n",
      "Train Epoch: 146 [33792/54000 (63%)] Loss: -1788.453247\n",
      "Train Epoch: 146 [45056/54000 (83%)] Loss: -1788.886475\n",
      "    epoch          : 146\n",
      "    loss           : -1792.08879693949\n",
      "    ess            : 8.001185255230597\n",
      "    log_marginal   : 1792.0887957878833\n",
      "    val_loss       : -1798.706522623698\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1798.706522623698\n",
      "Train Epoch: 147 [0/54000 (0%)] Loss: -1805.055542\n",
      "Train Epoch: 147 [11264/54000 (21%)] Loss: -1811.906250\n",
      "Train Epoch: 147 [22528/54000 (42%)] Loss: -1810.446289\n",
      "Train Epoch: 147 [33792/54000 (63%)] Loss: -1800.252686\n",
      "Train Epoch: 147 [45056/54000 (83%)] Loss: -1790.053345\n",
      "    epoch          : 147\n",
      "    loss           : -1801.8340246812352\n",
      "    ess            : 8.001184607451817\n",
      "    log_marginal   : 1801.8340246812352\n",
      "    val_loss       : -1804.5006306966145\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1804.5006306966145\n",
      "Train Epoch: 148 [0/54000 (0%)] Loss: -1810.279541\n",
      "Train Epoch: 148 [11264/54000 (21%)] Loss: -1786.500000\n",
      "Train Epoch: 148 [22528/54000 (42%)] Loss: -1796.842285\n",
      "Train Epoch: 148 [33792/54000 (63%)] Loss: -1786.675903\n",
      "Train Epoch: 148 [45056/54000 (83%)] Loss: -1792.517700\n",
      "    epoch          : 148\n",
      "    loss           : -1793.217065199366\n",
      "    ess            : 8.001185363193727\n",
      "    log_marginal   : 1793.217065199366\n",
      "    val_loss       : -1796.1287740071614\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1796.1287740071614\n",
      "Train Epoch: 149 [0/54000 (0%)] Loss: -1802.883545\n",
      "Train Epoch: 149 [11264/54000 (21%)] Loss: -1803.343506\n",
      "Train Epoch: 149 [22528/54000 (42%)] Loss: -1802.852905\n",
      "Train Epoch: 149 [33792/54000 (63%)] Loss: -1792.569824\n",
      "Train Epoch: 149 [45056/54000 (83%)] Loss: -1788.453857\n",
      "    epoch          : 149\n",
      "    loss           : -1799.4276687334168\n",
      "    ess            : 8.001184670430309\n",
      "    log_marginal   : 1799.4276687334168\n",
      "    val_loss       : -1800.1727803548176\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1800.1727803548176\n",
      "Train Epoch: 150 [0/54000 (0%)] Loss: -1808.027954\n",
      "Train Epoch: 150 [11264/54000 (21%)] Loss: -1784.678223\n",
      "Train Epoch: 150 [22528/54000 (42%)] Loss: -1790.935059\n",
      "Train Epoch: 150 [33792/54000 (63%)] Loss: -1787.304199\n",
      "Train Epoch: 150 [45056/54000 (83%)] Loss: -1749.553223\n",
      "    epoch          : 150\n",
      "    loss           : -1782.4826395286705\n",
      "    ess            : 8.001185093285903\n",
      "    log_marginal   : 1782.4826395286705\n",
      "    val_loss       : -1756.0640258789062\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1756.0640258789062\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch150.pth ...\n",
      "Train Epoch: 151 [0/54000 (0%)] Loss: -1759.765015\n",
      "Train Epoch: 151 [11264/54000 (21%)] Loss: -1787.686401\n",
      "Train Epoch: 151 [22528/54000 (42%)] Loss: -1777.623047\n",
      "Train Epoch: 151 [33792/54000 (63%)] Loss: -1780.164307\n",
      "Train Epoch: 151 [45056/54000 (83%)] Loss: -1788.105713\n",
      "    epoch          : 151\n",
      "    loss           : -1779.577387971698\n",
      "    ess            : 8.001185309212163\n",
      "    log_marginal   : 1779.577387971698\n",
      "    val_loss       : -1789.8901468912761\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1789.890157063802\n",
      "Train Epoch: 152 [0/54000 (0%)] Loss: -1792.158691\n",
      "Train Epoch: 152 [11264/54000 (21%)] Loss: -1798.286621\n",
      "Train Epoch: 152 [22528/54000 (42%)] Loss: -1793.425293\n",
      "Train Epoch: 152 [33792/54000 (63%)] Loss: -1786.369629\n",
      "Train Epoch: 152 [45056/54000 (83%)] Loss: -1797.236084\n",
      "    epoch          : 152\n",
      "    loss           : -1795.480427292158\n",
      "    ess            : 8.001184769396511\n",
      "    log_marginal   : 1795.480427292158\n",
      "    val_loss       : -1801.8774922688801\n",
      "    val_ess        : 8.001184066136679\n",
      "    val_log_marginal: 1801.8774922688801\n",
      "Train Epoch: 153 [0/54000 (0%)] Loss: -1802.718506\n",
      "Train Epoch: 153 [11264/54000 (21%)] Loss: -1790.233887\n",
      "Train Epoch: 153 [22528/54000 (42%)] Loss: -1794.802612\n",
      "Train Epoch: 153 [33792/54000 (63%)] Loss: -1796.124634\n",
      "Train Epoch: 153 [45056/54000 (83%)] Loss: -1797.674438\n",
      "    epoch          : 153\n",
      "    loss           : -1793.5260700729657\n",
      "    ess            : 8.001184733408802\n",
      "    log_marginal   : 1793.5260700729657\n",
      "    val_loss       : -1800.030253092448\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1800.030253092448\n",
      "Train Epoch: 154 [0/54000 (0%)] Loss: -1805.537476\n",
      "Train Epoch: 154 [11264/54000 (21%)] Loss: -1801.677490\n",
      "Train Epoch: 154 [22528/54000 (42%)] Loss: -1783.345581\n",
      "Train Epoch: 154 [33792/54000 (63%)] Loss: -1793.781128\n",
      "Train Epoch: 154 [45056/54000 (83%)] Loss: -1788.016357\n",
      "    epoch          : 154\n",
      "    loss           : -1792.9295354879127\n",
      "    ess            : 8.001184751402658\n",
      "    log_marginal   : 1792.9295354879127\n",
      "    val_loss       : -1787.9353434244792\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1787.9353332519531\n",
      "Train Epoch: 155 [0/54000 (0%)] Loss: -1783.288940\n",
      "Train Epoch: 155 [11264/54000 (21%)] Loss: -1746.062134\n",
      "Train Epoch: 155 [22528/54000 (42%)] Loss: -1782.381836\n",
      "Train Epoch: 155 [33792/54000 (63%)] Loss: -1784.345459\n",
      "Train Epoch: 155 [45056/54000 (83%)] Loss: -1754.620361\n",
      "    epoch          : 155\n",
      "    loss           : -1773.322460246536\n",
      "    ess            : 8.001184985322773\n",
      "    log_marginal   : 1773.322460246536\n",
      "    val_loss       : -1768.805399576823\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1768.805399576823\n",
      "Train Epoch: 156 [0/54000 (0%)] Loss: -1783.958496\n",
      "Train Epoch: 156 [11264/54000 (21%)] Loss: -1795.832031\n",
      "Train Epoch: 156 [22528/54000 (42%)] Loss: -1793.020508\n",
      "Train Epoch: 156 [33792/54000 (63%)] Loss: -1794.665039\n",
      "Train Epoch: 156 [45056/54000 (83%)] Loss: -1794.444336\n",
      "    epoch          : 156\n",
      "    loss           : -1790.7371261884582\n",
      "    ess            : 8.001184895353497\n",
      "    log_marginal   : 1790.7371261884582\n",
      "    val_loss       : -1799.1606140136719\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.1606140136719\n",
      "Train Epoch: 157 [0/54000 (0%)] Loss: -1814.661377\n",
      "Train Epoch: 157 [11264/54000 (21%)] Loss: -1799.712158\n",
      "Train Epoch: 157 [22528/54000 (42%)] Loss: -1792.259033\n",
      "Train Epoch: 157 [33792/54000 (63%)] Loss: -1789.897583\n",
      "Train Epoch: 157 [45056/54000 (83%)] Loss: -1789.839722\n",
      "    epoch          : 157\n",
      "    loss           : -1793.8604022331958\n",
      "    ess            : 8.001185039304337\n",
      "    log_marginal   : 1793.8604033848026\n",
      "    val_loss       : -1798.5342407226562\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1798.5342407226562\n",
      "Train Epoch: 158 [0/54000 (0%)] Loss: -1804.919189\n",
      "Train Epoch: 158 [11264/54000 (21%)] Loss: -1797.018677\n",
      "Train Epoch: 158 [22528/54000 (42%)] Loss: -1791.910400\n",
      "Train Epoch: 158 [33792/54000 (63%)] Loss: -1779.595459\n",
      "Train Epoch: 158 [45056/54000 (83%)] Loss: -1792.271240\n",
      "    epoch          : 158\n",
      "    loss           : -1791.8836635373673\n",
      "    ess            : 8.001185039304337\n",
      "    log_marginal   : 1791.8836635373673\n",
      "    val_loss       : -1794.3532206217449\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1794.3532206217449\n",
      "Train Epoch: 159 [0/54000 (0%)] Loss: -1792.907471\n",
      "Train Epoch: 159 [11264/54000 (21%)] Loss: -1796.426025\n",
      "Train Epoch: 159 [22528/54000 (42%)] Loss: -1791.418213\n",
      "Train Epoch: 159 [33792/54000 (63%)] Loss: -1801.749634\n",
      "Train Epoch: 159 [45056/54000 (83%)] Loss: -1794.242432\n",
      "    epoch          : 159\n",
      "    loss           : -1796.767809597951\n",
      "    ess            : 8.001184481494832\n",
      "    log_marginal   : 1796.767809597951\n",
      "    val_loss       : -1805.7204284667969\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1805.7204284667969\n",
      "Train Epoch: 160 [0/54000 (0%)] Loss: -1813.885986\n",
      "Train Epoch: 160 [11264/54000 (21%)] Loss: -1793.498779\n",
      "Train Epoch: 160 [22528/54000 (42%)] Loss: -1797.855957\n",
      "Train Epoch: 160 [33792/54000 (63%)] Loss: -1789.703369\n",
      "Train Epoch: 160 [45056/54000 (83%)] Loss: -1790.031738\n",
      "    epoch          : 160\n",
      "    loss           : -1795.0612102004718\n",
      "    ess            : 8.00118408563002\n",
      "    log_marginal   : 1795.0612102004718\n",
      "    val_loss       : -1801.5094706217449\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1801.5094706217449\n",
      "Train Epoch: 161 [0/54000 (0%)] Loss: -1808.200317\n",
      "Train Epoch: 161 [11264/54000 (21%)] Loss: -1789.837036\n",
      "Train Epoch: 161 [22528/54000 (42%)] Loss: -1787.104980\n",
      "Train Epoch: 161 [33792/54000 (63%)] Loss: -1784.102051\n",
      "Train Epoch: 161 [45056/54000 (83%)] Loss: -1798.224121\n",
      "    epoch          : 161\n",
      "    loss           : -1792.757030559036\n",
      "    ess            : 8.001185093285903\n",
      "    log_marginal   : 1792.7570294074292\n",
      "    val_loss       : -1801.3347880045574\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1801.3347880045574\n",
      "Train Epoch: 162 [0/54000 (0%)] Loss: -1800.169434\n",
      "Train Epoch: 162 [11264/54000 (21%)] Loss: -1795.158691\n",
      "Train Epoch: 162 [22528/54000 (42%)] Loss: -1792.753418\n",
      "Train Epoch: 162 [33792/54000 (63%)] Loss: -1784.125122\n",
      "Train Epoch: 162 [45056/54000 (83%)] Loss: -1793.119385\n",
      "    epoch          : 162\n",
      "    loss           : -1795.856019218013\n",
      "    ess            : 8.001184400522485\n",
      "    log_marginal   : 1795.856019218013\n",
      "    val_loss       : -1809.3948974609375\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1809.3949178059895\n",
      "Train Epoch: 163 [0/54000 (0%)] Loss: -1814.779785\n",
      "Train Epoch: 163 [11264/54000 (21%)] Loss: -1796.069336\n",
      "Train Epoch: 163 [22528/54000 (42%)] Loss: -1812.766968\n",
      "Train Epoch: 163 [33792/54000 (63%)] Loss: -1786.620117\n",
      "Train Epoch: 163 [45056/54000 (83%)] Loss: -1785.302246\n",
      "    epoch          : 163\n",
      "    loss           : -1789.7546824329304\n",
      "    ess            : 8.001184805384222\n",
      "    log_marginal   : 1789.7546824329304\n",
      "    val_loss       : -1782.7512613932292\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1782.7512613932292\n",
      "Train Epoch: 164 [0/54000 (0%)] Loss: -1784.045532\n",
      "Train Epoch: 164 [11264/54000 (21%)] Loss: -1743.629150\n",
      "Train Epoch: 164 [22528/54000 (42%)] Loss: -1772.650391\n",
      "Train Epoch: 164 [33792/54000 (63%)] Loss: -1775.336548\n",
      "Train Epoch: 164 [45056/54000 (83%)] Loss: -1776.827881\n",
      "    epoch          : 164\n",
      "    loss           : -1770.9412312057782\n",
      "    ess            : 8.001185453163004\n",
      "    log_marginal   : 1770.941232357385\n",
      "    val_loss       : -1791.8808898925781\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1791.8809204101562\n",
      "Train Epoch: 165 [0/54000 (0%)] Loss: -1791.930176\n",
      "Train Epoch: 165 [11264/54000 (21%)] Loss: -1780.749634\n",
      "Train Epoch: 165 [22528/54000 (42%)] Loss: -1791.516235\n",
      "Train Epoch: 165 [33792/54000 (63%)] Loss: -1795.526123\n",
      "Train Epoch: 165 [45056/54000 (83%)] Loss: -1794.825806\n",
      "    epoch          : 165\n",
      "    loss           : -1791.3223082344487\n",
      "    ess            : 8.001184472497904\n",
      "    log_marginal   : 1791.3223082344487\n",
      "    val_loss       : -1798.5581970214844\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1798.5582071940105\n",
      "Train Epoch: 166 [0/54000 (0%)] Loss: -1803.426392\n",
      "Train Epoch: 166 [11264/54000 (21%)] Loss: -1813.333496\n",
      "Train Epoch: 166 [22528/54000 (42%)] Loss: -1779.180176\n",
      "Train Epoch: 166 [33792/54000 (63%)] Loss: -1797.933228\n",
      "Train Epoch: 166 [45056/54000 (83%)] Loss: -1792.788574\n",
      "    epoch          : 166\n",
      "    loss           : -1795.541566093013\n",
      "    ess            : 8.001184895353497\n",
      "    log_marginal   : 1795.5415672446197\n",
      "    val_loss       : -1799.5053609212239\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1799.5053812662761\n",
      "Train Epoch: 167 [0/54000 (0%)] Loss: -1806.823853\n",
      "Train Epoch: 167 [11264/54000 (21%)] Loss: -1803.270630\n",
      "Train Epoch: 167 [22528/54000 (42%)] Loss: -1780.514404\n",
      "Train Epoch: 167 [33792/54000 (63%)] Loss: -1794.942383\n",
      "Train Epoch: 167 [45056/54000 (83%)] Loss: -1797.088379\n",
      "    epoch          : 167\n",
      "    loss           : -1793.2257022497788\n",
      "    ess            : 8.001184931341207\n",
      "    log_marginal   : 1793.2257022497788\n",
      "    val_loss       : -1798.9485270182292\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1798.9485270182292\n",
      "Train Epoch: 168 [0/54000 (0%)] Loss: -1797.792603\n",
      "Train Epoch: 168 [11264/54000 (21%)] Loss: -1805.484375\n",
      "Train Epoch: 168 [22528/54000 (42%)] Loss: -1799.769287\n",
      "Train Epoch: 168 [33792/54000 (63%)] Loss: -1804.140625\n",
      "Train Epoch: 168 [45056/54000 (83%)] Loss: -1792.250977\n",
      "    epoch          : 168\n",
      "    loss           : -1800.4399840156987\n",
      "    ess            : 8.001184886356569\n",
      "    log_marginal   : 1800.4399840156987\n",
      "    val_loss       : -1796.0332845052083\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1796.0332743326824\n",
      "Train Epoch: 169 [0/54000 (0%)] Loss: -1813.213623\n",
      "Train Epoch: 169 [11264/54000 (21%)] Loss: -1766.936035\n",
      "Train Epoch: 169 [22528/54000 (42%)] Loss: -1767.309326\n",
      "Train Epoch: 169 [33792/54000 (63%)] Loss: -1787.314453\n",
      "Train Epoch: 169 [45056/54000 (83%)] Loss: -1778.448608\n",
      "    epoch          : 169\n",
      "    loss           : -1775.9152728386646\n",
      "    ess            : 8.00118604696022\n",
      "    log_marginal   : 1775.9152728386646\n",
      "    val_loss       : -1793.6892598470051\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1793.6892598470051\n",
      "Train Epoch: 170 [0/54000 (0%)] Loss: -1794.286499\n",
      "Train Epoch: 170 [11264/54000 (21%)] Loss: -1791.340088\n",
      "Train Epoch: 170 [22528/54000 (42%)] Loss: -1790.953613\n",
      "Train Epoch: 170 [33792/54000 (63%)] Loss: -1788.415527\n",
      "Train Epoch: 170 [45056/54000 (83%)] Loss: -1785.982544\n",
      "    epoch          : 170\n",
      "    loss           : -1791.4844682801445\n",
      "    ess            : 8.001185489150712\n",
      "    log_marginal   : 1791.4844682801445\n",
      "    val_loss       : -1789.4046427408855\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1789.4046427408855\n",
      "Train Epoch: 171 [0/54000 (0%)] Loss: -1798.817383\n",
      "Train Epoch: 171 [11264/54000 (21%)] Loss: -1784.548828\n",
      "Train Epoch: 171 [22528/54000 (42%)] Loss: -1783.789185\n",
      "Train Epoch: 171 [33792/54000 (63%)] Loss: -1785.672607\n",
      "Train Epoch: 171 [45056/54000 (83%)] Loss: -1794.608765\n",
      "    epoch          : 171\n",
      "    loss           : -1792.8026790978774\n",
      "    ess            : 8.001185903009379\n",
      "    log_marginal   : 1792.8026802494842\n",
      "    val_loss       : -1807.6991780598958\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.699198404948\n",
      "Train Epoch: 172 [0/54000 (0%)] Loss: -1804.543701\n",
      "Train Epoch: 172 [11264/54000 (21%)] Loss: -1808.797363\n",
      "Train Epoch: 172 [22528/54000 (42%)] Loss: -1782.603760\n",
      "Train Epoch: 172 [33792/54000 (63%)] Loss: -1793.114990\n",
      "Train Epoch: 172 [45056/54000 (83%)] Loss: -1787.068848\n",
      "    epoch          : 172\n",
      "    loss           : -1792.1714396926593\n",
      "    ess            : 8.001185363193727\n",
      "    log_marginal   : 1792.1714396926593\n",
      "    val_loss       : -1795.0072326660156\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1795.0072123209636\n",
      "Train Epoch: 173 [0/54000 (0%)] Loss: -1795.802612\n",
      "Train Epoch: 173 [11264/54000 (21%)] Loss: -1807.099609\n",
      "Train Epoch: 173 [22528/54000 (42%)] Loss: -1794.760132\n",
      "Train Epoch: 173 [33792/54000 (63%)] Loss: -1796.546875\n",
      "Train Epoch: 173 [45056/54000 (83%)] Loss: -1790.966309\n",
      "    epoch          : 173\n",
      "    loss           : -1797.6369018554688\n",
      "    ess            : 8.001185111279758\n",
      "    log_marginal   : 1797.636900703862\n",
      "    val_loss       : -1802.3870544433594\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.3870340983074\n",
      "Train Epoch: 174 [0/54000 (0%)] Loss: -1806.155273\n",
      "Train Epoch: 174 [11264/54000 (21%)] Loss: -1780.747559\n",
      "Train Epoch: 174 [22528/54000 (42%)] Loss: -1776.645142\n",
      "Train Epoch: 174 [33792/54000 (63%)] Loss: -1774.412231\n",
      "Train Epoch: 174 [45056/54000 (83%)] Loss: -1777.476562\n",
      "    epoch          : 174\n",
      "    loss           : -1783.3588187739533\n",
      "    ess            : 8.001185759058538\n",
      "    log_marginal   : 1783.3588187739533\n",
      "    val_loss       : -1797.505615234375\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1797.5056254069011\n",
      "Train Epoch: 175 [0/54000 (0%)] Loss: -1807.236694\n",
      "Train Epoch: 175 [11264/54000 (21%)] Loss: -1800.678711\n",
      "Train Epoch: 175 [22528/54000 (42%)] Loss: -1797.886108\n",
      "Train Epoch: 175 [33792/54000 (63%)] Loss: -1793.358154\n",
      "Train Epoch: 175 [45056/54000 (83%)] Loss: -1795.558838\n",
      "    epoch          : 175\n",
      "    loss           : -1796.7132556843308\n",
      "    ess            : 8.001185354196801\n",
      "    log_marginal   : 1796.7132556843308\n",
      "    val_loss       : -1798.518086751302\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1798.518086751302\n",
      "Train Epoch: 176 [0/54000 (0%)] Loss: -1810.805420\n",
      "Train Epoch: 176 [11264/54000 (21%)] Loss: -1809.827393\n",
      "Train Epoch: 176 [22528/54000 (42%)] Loss: -1746.889404\n",
      "Train Epoch: 176 [33792/54000 (63%)] Loss: -1778.859375\n",
      "Train Epoch: 176 [45056/54000 (83%)] Loss: -1786.061279\n",
      "    epoch          : 176\n",
      "    loss           : -1781.2684118882664\n",
      "    ess            : 8.001185597113842\n",
      "    log_marginal   : 1781.2684118882664\n",
      "    val_loss       : -1763.1327412923176\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1763.1327514648438\n",
      "Train Epoch: 177 [0/54000 (0%)] Loss: -1776.982910\n",
      "Train Epoch: 177 [11264/54000 (21%)] Loss: -1791.959961\n",
      "Train Epoch: 177 [22528/54000 (42%)] Loss: -1784.870850\n",
      "Train Epoch: 177 [33792/54000 (63%)] Loss: -1787.431396\n",
      "Train Epoch: 177 [45056/54000 (83%)] Loss: -1794.792236\n",
      "    epoch          : 177\n",
      "    loss           : -1788.8680558114681\n",
      "    ess            : 8.00118560611077\n",
      "    log_marginal   : 1788.8680558114681\n",
      "    val_loss       : -1771.630594889323\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1771.630594889323\n",
      "Train Epoch: 178 [0/54000 (0%)] Loss: -1803.132568\n",
      "Train Epoch: 178 [11264/54000 (21%)] Loss: -1786.464600\n",
      "Train Epoch: 178 [22528/54000 (42%)] Loss: -1785.158203\n",
      "Train Epoch: 178 [33792/54000 (63%)] Loss: -1788.550415\n",
      "Train Epoch: 178 [45056/54000 (83%)] Loss: -1794.089844\n",
      "    epoch          : 178\n",
      "    loss           : -1791.0246271097435\n",
      "    ess            : 8.001185597113842\n",
      "    log_marginal   : 1791.0246271097435\n",
      "    val_loss       : -1791.9657592773438\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1791.9657592773438\n",
      "Train Epoch: 179 [0/54000 (0%)] Loss: -1804.183350\n",
      "Train Epoch: 179 [11264/54000 (21%)] Loss: -1796.572632\n",
      "Train Epoch: 179 [22528/54000 (42%)] Loss: -1782.487915\n",
      "Train Epoch: 179 [33792/54000 (63%)] Loss: -1793.957275\n",
      "Train Epoch: 179 [45056/54000 (83%)] Loss: -1796.699097\n",
      "    epoch          : 179\n",
      "    loss           : -1798.561759516878\n",
      "    ess            : 8.001185633101553\n",
      "    log_marginal   : 1798.5617618200913\n",
      "    val_loss       : -1805.2911580403645\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1805.2911580403645\n",
      "Train Epoch: 180 [0/54000 (0%)] Loss: -1820.060791\n",
      "Train Epoch: 180 [11264/54000 (21%)] Loss: -1740.446411\n",
      "Train Epoch: 180 [22528/54000 (42%)] Loss: -1761.351074\n",
      "Train Epoch: 180 [33792/54000 (63%)] Loss: -1765.064453\n",
      "Train Epoch: 180 [45056/54000 (83%)] Loss: -1769.580566\n",
      "    epoch          : 180\n",
      "    loss           : -1770.2632653578273\n",
      "    ess            : 8.001185894012451\n",
      "    log_marginal   : 1770.2632653578273\n",
      "    val_loss       : -1779.0014038085938\n",
      "    val_ess        : 8.00118605295817\n",
      "    val_log_marginal: 1779.0014038085938\n",
      "Train Epoch: 181 [0/54000 (0%)] Loss: -1783.924072\n",
      "Train Epoch: 181 [11264/54000 (21%)] Loss: -1780.066650\n",
      "Train Epoch: 181 [22528/54000 (42%)] Loss: -1787.935913\n",
      "Train Epoch: 181 [33792/54000 (63%)] Loss: -1792.944458\n",
      "Train Epoch: 181 [45056/54000 (83%)] Loss: -1792.904175\n",
      "    epoch          : 181\n",
      "    loss           : -1783.5235169608638\n",
      "    ess            : 8.001185759058538\n",
      "    log_marginal   : 1783.523515809257\n",
      "    val_loss       : -1802.3757019042969\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.3757019042969\n",
      "Train Epoch: 182 [0/54000 (0%)] Loss: -1798.220947\n",
      "Train Epoch: 182 [11264/54000 (21%)] Loss: -1795.713745\n",
      "Train Epoch: 182 [22528/54000 (42%)] Loss: -1791.396973\n",
      "Train Epoch: 182 [33792/54000 (63%)] Loss: -1797.212402\n",
      "Train Epoch: 182 [45056/54000 (83%)] Loss: -1777.736816\n",
      "    epoch          : 182\n",
      "    loss           : -1788.7546893425708\n",
      "    ess            : 8.00118583103396\n",
      "    log_marginal   : 1788.754688190964\n",
      "    val_loss       : -1797.6217142740886\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.6217142740886\n",
      "Train Epoch: 183 [0/54000 (0%)] Loss: -1806.278076\n",
      "Train Epoch: 183 [11264/54000 (21%)] Loss: -1807.823975\n",
      "Train Epoch: 183 [22528/54000 (42%)] Loss: -1784.824707\n",
      "Train Epoch: 183 [33792/54000 (63%)] Loss: -1787.280396\n",
      "Train Epoch: 183 [45056/54000 (83%)] Loss: -1796.513916\n",
      "    epoch          : 183\n",
      "    loss           : -1793.121826171875\n",
      "    ess            : 8.001185273224452\n",
      "    log_marginal   : 1793.121826171875\n",
      "    val_loss       : -1800.8976643880208\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1800.8976542154949\n",
      "Train Epoch: 184 [0/54000 (0%)] Loss: -1808.893433\n",
      "Train Epoch: 184 [11264/54000 (21%)] Loss: -1807.336060\n",
      "Train Epoch: 184 [22528/54000 (42%)] Loss: -1793.790039\n",
      "Train Epoch: 184 [33792/54000 (63%)] Loss: -1804.750000\n",
      "Train Epoch: 184 [45056/54000 (83%)] Loss: -1796.378540\n",
      "    epoch          : 184\n",
      "    loss           : -1799.3487387603184\n",
      "    ess            : 8.001184967328918\n",
      "    log_marginal   : 1799.3487387603184\n",
      "    val_loss       : -1800.9140116373699\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1800.9140116373699\n",
      "Train Epoch: 185 [0/54000 (0%)] Loss: -1811.727783\n",
      "Train Epoch: 185 [11264/54000 (21%)] Loss: -1792.402344\n",
      "Train Epoch: 185 [22528/54000 (42%)] Loss: -1788.386353\n",
      "Train Epoch: 185 [33792/54000 (63%)] Loss: -1783.399292\n",
      "Train Epoch: 185 [45056/54000 (83%)] Loss: -1785.839600\n",
      "    epoch          : 185\n",
      "    loss           : -1791.9670318027713\n",
      "    ess            : 8.001185264227525\n",
      "    log_marginal   : 1791.9670318027713\n",
      "    val_loss       : -1800.8330790201824\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1800.8330790201824\n",
      "Train Epoch: 186 [0/54000 (0%)] Loss: -1808.024658\n",
      "Train Epoch: 186 [11264/54000 (21%)] Loss: -1796.639648\n",
      "Train Epoch: 186 [22528/54000 (42%)] Loss: -1789.783936\n",
      "Train Epoch: 186 [33792/54000 (63%)] Loss: -1796.816406\n",
      "Train Epoch: 186 [45056/54000 (83%)] Loss: -1795.446777\n",
      "    epoch          : 186\n",
      "    loss           : -1793.8280374778892\n",
      "    ess            : 8.001185291218308\n",
      "    log_marginal   : 1793.8280374778892\n",
      "    val_loss       : -1802.1086527506511\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.1086527506511\n",
      "Train Epoch: 187 [0/54000 (0%)] Loss: -1804.251953\n",
      "Train Epoch: 187 [11264/54000 (21%)] Loss: -1803.900513\n",
      "Train Epoch: 187 [22528/54000 (42%)] Loss: -1802.094238\n",
      "Train Epoch: 187 [33792/54000 (63%)] Loss: -1789.948975\n",
      "Train Epoch: 187 [45056/54000 (83%)] Loss: -1797.931641\n",
      "    epoch          : 187\n",
      "    loss           : -1800.4212761645047\n",
      "    ess            : 8.001185723070828\n",
      "    log_marginal   : 1800.4212761645047\n",
      "    val_loss       : -1799.9780578613281\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1799.9780375162761\n",
      "Train Epoch: 188 [0/54000 (0%)] Loss: -1808.123901\n",
      "Train Epoch: 188 [11264/54000 (21%)] Loss: -1799.944092\n",
      "Train Epoch: 188 [22528/54000 (42%)] Loss: -1787.886841\n",
      "Train Epoch: 188 [33792/54000 (63%)] Loss: -1788.920654\n",
      "Train Epoch: 188 [45056/54000 (83%)] Loss: -1788.932373\n",
      "    epoch          : 188\n",
      "    loss           : -1799.9158440355984\n",
      "    ess            : 8.001185300215235\n",
      "    log_marginal   : 1799.9158428839917\n",
      "    val_loss       : -1802.3990580240886\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.3990783691406\n",
      "Train Epoch: 189 [0/54000 (0%)] Loss: -1810.363892\n",
      "Train Epoch: 189 [11264/54000 (21%)] Loss: -1808.049316\n",
      "Train Epoch: 189 [22528/54000 (42%)] Loss: -1796.467651\n",
      "Train Epoch: 189 [33792/54000 (63%)] Loss: -1799.296143\n",
      "Train Epoch: 189 [45056/54000 (83%)] Loss: -1791.463623\n",
      "    epoch          : 189\n",
      "    loss           : -1799.4214938181751\n",
      "    ess            : 8.001184895353497\n",
      "    log_marginal   : 1799.4214926665684\n",
      "    val_loss       : -1798.0512084960938\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1798.0512084960938\n",
      "Train Epoch: 190 [0/54000 (0%)] Loss: -1808.998169\n",
      "Train Epoch: 190 [11264/54000 (21%)] Loss: -1786.992676\n",
      "Train Epoch: 190 [22528/54000 (42%)] Loss: -1793.271973\n",
      "Train Epoch: 190 [33792/54000 (63%)] Loss: -1787.392090\n",
      "Train Epoch: 190 [45056/54000 (83%)] Loss: -1797.849731\n",
      "    epoch          : 190\n",
      "    loss           : -1796.5374168539947\n",
      "    ess            : 8.001184805384222\n",
      "    log_marginal   : 1796.5374168539947\n",
      "    val_loss       : -1802.587910970052\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.587910970052\n",
      "Train Epoch: 191 [0/54000 (0%)] Loss: -1812.006836\n",
      "Train Epoch: 191 [11264/54000 (21%)] Loss: -1792.441528\n",
      "Train Epoch: 191 [22528/54000 (42%)] Loss: -1790.548828\n",
      "Train Epoch: 191 [33792/54000 (63%)] Loss: -1792.845825\n",
      "Train Epoch: 191 [45056/54000 (83%)] Loss: -1793.223877\n",
      "    epoch          : 191\n",
      "    loss           : -1794.7993336803509\n",
      "    ess            : 8.001185399181438\n",
      "    log_marginal   : 1794.7993336803509\n",
      "    val_loss       : -1801.2059122721355\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1801.2059326171875\n",
      "Train Epoch: 192 [0/54000 (0%)] Loss: -1803.652222\n",
      "Train Epoch: 192 [11264/54000 (21%)] Loss: -1796.338623\n",
      "Train Epoch: 192 [22528/54000 (42%)] Loss: -1793.048340\n",
      "Train Epoch: 192 [33792/54000 (63%)] Loss: -1790.863525\n",
      "Train Epoch: 192 [45056/54000 (83%)] Loss: -1802.326660\n",
      "    epoch          : 192\n",
      "    loss           : -1800.3461614644752\n",
      "    ess            : 8.001184985322773\n",
      "    log_marginal   : 1800.3461614644752\n",
      "    val_loss       : -1806.7200622558594\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1806.7200622558594\n",
      "Train Epoch: 193 [0/54000 (0%)] Loss: -1818.951904\n",
      "Train Epoch: 193 [11264/54000 (21%)] Loss: -1778.772461\n",
      "Train Epoch: 193 [22528/54000 (42%)] Loss: -1771.329468\n",
      "Train Epoch: 193 [33792/54000 (63%)] Loss: -1781.391113\n",
      "Train Epoch: 193 [45056/54000 (83%)] Loss: -1788.725952\n",
      "    epoch          : 193\n",
      "    loss           : -1778.3130735001473\n",
      "    ess            : 8.001185453163004\n",
      "    log_marginal   : 1778.3130735001473\n",
      "    val_loss       : -1780.1356099446614\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1780.1356099446614\n",
      "Train Epoch: 194 [0/54000 (0%)] Loss: -1790.328003\n",
      "Train Epoch: 194 [11264/54000 (21%)] Loss: -1791.543213\n",
      "Train Epoch: 194 [22528/54000 (42%)] Loss: -1793.857788\n",
      "Train Epoch: 194 [33792/54000 (63%)] Loss: -1786.273071\n",
      "Train Epoch: 194 [45056/54000 (83%)] Loss: -1796.429199\n",
      "    epoch          : 194\n",
      "    loss           : -1792.9900835145195\n",
      "    ess            : 8.001185255230597\n",
      "    log_marginal   : 1792.9900823629127\n",
      "    val_loss       : -1792.6341349283855\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1792.6341349283855\n",
      "Train Epoch: 195 [0/54000 (0%)] Loss: -1812.812256\n",
      "Train Epoch: 195 [11264/54000 (21%)] Loss: -1813.659180\n",
      "Train Epoch: 195 [22528/54000 (42%)] Loss: -1798.491943\n",
      "Train Epoch: 195 [33792/54000 (63%)] Loss: -1784.444580\n",
      "Train Epoch: 195 [45056/54000 (83%)] Loss: -1797.989014\n",
      "    epoch          : 195\n",
      "    loss           : -1797.5917945717865\n",
      "    ess            : 8.001185255230597\n",
      "    log_marginal   : 1797.5917945717865\n",
      "    val_loss       : -1799.5394490559895\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1799.5394490559895\n",
      "Train Epoch: 196 [0/54000 (0%)] Loss: -1808.233643\n",
      "Train Epoch: 196 [11264/54000 (21%)] Loss: -1808.029297\n",
      "Train Epoch: 196 [22528/54000 (42%)] Loss: -1792.228760\n",
      "Train Epoch: 196 [33792/54000 (63%)] Loss: -1796.902588\n",
      "Train Epoch: 196 [45056/54000 (83%)] Loss: -1802.397949\n",
      "    epoch          : 196\n",
      "    loss           : -1798.2061030549823\n",
      "    ess            : 8.001185093285903\n",
      "    log_marginal   : 1798.206104206589\n",
      "    val_loss       : -1803.6496887207031\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1803.6496988932292\n",
      "Train Epoch: 197 [0/54000 (0%)] Loss: -1816.230469\n",
      "Train Epoch: 197 [11264/54000 (21%)] Loss: -1809.347168\n",
      "Train Epoch: 197 [22528/54000 (42%)] Loss: -1802.089722\n",
      "Train Epoch: 197 [33792/54000 (63%)] Loss: -1797.342285\n",
      "Train Epoch: 197 [45056/54000 (83%)] Loss: -1787.454102\n",
      "    epoch          : 197\n",
      "    loss           : -1801.1083961342865\n",
      "    ess            : 8.001184787390367\n",
      "    log_marginal   : 1801.1083949826798\n",
      "    val_loss       : -1803.5470784505208\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1803.5470784505208\n",
      "Train Epoch: 198 [0/54000 (0%)] Loss: -1809.758301\n",
      "Train Epoch: 198 [11264/54000 (21%)] Loss: -1791.386719\n",
      "Train Epoch: 198 [22528/54000 (42%)] Loss: -1783.303711\n",
      "Train Epoch: 198 [33792/54000 (63%)] Loss: -1791.210205\n",
      "Train Epoch: 198 [45056/54000 (83%)] Loss: -1782.740479\n",
      "    epoch          : 198\n",
      "    loss           : -1788.5859018001916\n",
      "    ess            : 8.001185507144568\n",
      "    log_marginal   : 1788.5859018001916\n",
      "    val_loss       : -1796.2761840820312\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1796.2761840820312\n",
      "Train Epoch: 199 [0/54000 (0%)] Loss: -1792.077148\n",
      "Train Epoch: 199 [11264/54000 (21%)] Loss: -1786.436768\n",
      "Train Epoch: 199 [22528/54000 (42%)] Loss: -1790.192871\n",
      "Train Epoch: 199 [33792/54000 (63%)] Loss: -1731.574097\n",
      "Train Epoch: 199 [45056/54000 (83%)] Loss: -1738.724854\n",
      "    epoch          : 199\n",
      "    loss           : -1762.7856330151828\n",
      "    ess            : 8.001184976325845\n",
      "    log_marginal   : 1762.7856330151828\n",
      "    val_loss       : -1772.4607747395833\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1772.4607747395833\n",
      "Train Epoch: 200 [0/54000 (0%)] Loss: -1772.875244\n",
      "Train Epoch: 200 [11264/54000 (21%)] Loss: -1778.230713\n",
      "Train Epoch: 200 [22528/54000 (42%)] Loss: -1784.362427\n",
      "Train Epoch: 200 [33792/54000 (63%)] Loss: -1780.232544\n",
      "Train Epoch: 200 [45056/54000 (83%)] Loss: -1787.644531\n",
      "    epoch          : 200\n",
      "    loss           : -1782.4569425762825\n",
      "    ess            : 8.001185507144568\n",
      "    log_marginal   : 1782.4569425762825\n",
      "    val_loss       : -1798.4397277832031\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1798.4397277832031\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch200.pth ...\n",
      "Train Epoch: 201 [0/54000 (0%)] Loss: -1808.169678\n",
      "Train Epoch: 201 [11264/54000 (21%)] Loss: -1803.148071\n",
      "Train Epoch: 201 [22528/54000 (42%)] Loss: -1761.984863\n",
      "Train Epoch: 201 [33792/54000 (63%)] Loss: -1793.019287\n",
      "Train Epoch: 201 [45056/54000 (83%)] Loss: -1779.701416\n",
      "    epoch          : 201\n",
      "    loss           : -1788.4705050486439\n",
      "    ess            : 8.00118531820909\n",
      "    log_marginal   : 1788.4705050486439\n",
      "    val_loss       : -1797.4878234863281\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.4878234863281\n",
      "Train Epoch: 202 [0/54000 (0%)] Loss: -1806.040039\n",
      "Train Epoch: 202 [11264/54000 (21%)] Loss: -1795.087646\n",
      "Train Epoch: 202 [22528/54000 (42%)] Loss: -1790.689697\n",
      "Train Epoch: 202 [33792/54000 (63%)] Loss: -1791.853394\n",
      "Train Epoch: 202 [45056/54000 (83%)] Loss: -1790.864258\n",
      "    epoch          : 202\n",
      "    loss           : -1791.5829076227153\n",
      "    ess            : 8.001185363193727\n",
      "    log_marginal   : 1791.5829076227153\n",
      "    val_loss       : -1792.6744079589844\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1792.6744079589844\n",
      "Train Epoch: 203 [0/54000 (0%)] Loss: -1804.705444\n",
      "Train Epoch: 203 [11264/54000 (21%)] Loss: -1796.228271\n",
      "Train Epoch: 203 [22528/54000 (42%)] Loss: -1796.886475\n",
      "Train Epoch: 203 [33792/54000 (63%)] Loss: -1800.284668\n",
      "Train Epoch: 203 [45056/54000 (83%)] Loss: -1800.399658\n",
      "    epoch          : 203\n",
      "    loss           : -1800.0116450471698\n",
      "    ess            : 8.001185489150712\n",
      "    log_marginal   : 1800.0116461987766\n",
      "    val_loss       : -1805.3991088867188\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.3991292317708\n",
      "Train Epoch: 204 [0/54000 (0%)] Loss: -1806.515747\n",
      "Train Epoch: 204 [11264/54000 (21%)] Loss: -1773.348145\n",
      "Train Epoch: 204 [22528/54000 (42%)] Loss: -1788.698364\n",
      "Train Epoch: 204 [33792/54000 (63%)] Loss: -1786.927246\n",
      "Train Epoch: 204 [45056/54000 (83%)] Loss: -1793.624023\n",
      "    epoch          : 204\n",
      "    loss           : -1790.3672554447967\n",
      "    ess            : 8.001185435169148\n",
      "    log_marginal   : 1790.3672554447967\n",
      "    val_loss       : -1799.2117919921875\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.2117919921875\n",
      "Train Epoch: 205 [0/54000 (0%)] Loss: -1808.809448\n",
      "Train Epoch: 205 [11264/54000 (21%)] Loss: -1796.814575\n",
      "Train Epoch: 205 [22528/54000 (42%)] Loss: -1802.427490\n",
      "Train Epoch: 205 [33792/54000 (63%)] Loss: -1788.733765\n",
      "Train Epoch: 205 [45056/54000 (83%)] Loss: -1792.137207\n",
      "    epoch          : 205\n",
      "    loss           : -1798.8244928323998\n",
      "    ess            : 8.001185300215235\n",
      "    log_marginal   : 1798.8244928323998\n",
      "    val_loss       : -1800.8014119466145\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1800.8013916015625\n",
      "Train Epoch: 206 [0/54000 (0%)] Loss: -1814.087158\n",
      "Train Epoch: 206 [11264/54000 (21%)] Loss: -1805.992798\n",
      "Train Epoch: 206 [22528/54000 (42%)] Loss: -1797.568848\n",
      "Train Epoch: 206 [33792/54000 (63%)] Loss: -1797.751831\n",
      "Train Epoch: 206 [45056/54000 (83%)] Loss: -1788.365479\n",
      "    epoch          : 206\n",
      "    loss           : -1801.9313699974205\n",
      "    ess            : 8.00118440951941\n",
      "    log_marginal   : 1801.9313699974205\n",
      "    val_loss       : -1805.4044189453125\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.4044189453125\n",
      "Train Epoch: 207 [0/54000 (0%)] Loss: -1803.057739\n",
      "Train Epoch: 207 [11264/54000 (21%)] Loss: -1800.660400\n",
      "Train Epoch: 207 [22528/54000 (42%)] Loss: -1795.570557\n",
      "Train Epoch: 207 [33792/54000 (63%)] Loss: -1792.575684\n",
      "Train Epoch: 207 [45056/54000 (83%)] Loss: -1789.125977\n",
      "    epoch          : 207\n",
      "    loss           : -1799.832490741082\n",
      "    ess            : 8.001184805384222\n",
      "    log_marginal   : 1799.8324918926887\n",
      "    val_loss       : -1800.0463460286458\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1800.0463460286458\n",
      "Train Epoch: 208 [0/54000 (0%)] Loss: -1815.153320\n",
      "Train Epoch: 208 [11264/54000 (21%)] Loss: -1799.761719\n",
      "Train Epoch: 208 [22528/54000 (42%)] Loss: -1796.176025\n",
      "Train Epoch: 208 [33792/54000 (63%)] Loss: -1796.478882\n",
      "Train Epoch: 208 [45056/54000 (83%)] Loss: -1791.592529\n",
      "    epoch          : 208\n",
      "    loss           : -1796.7706840083283\n",
      "    ess            : 8.001185201249033\n",
      "    log_marginal   : 1796.7706840083283\n",
      "    val_loss       : -1799.366231282552\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1799.366231282552\n",
      "Train Epoch: 209 [0/54000 (0%)] Loss: -1800.502075\n",
      "Train Epoch: 209 [11264/54000 (21%)] Loss: -1798.735840\n",
      "Train Epoch: 209 [22528/54000 (42%)] Loss: -1793.908447\n",
      "Train Epoch: 209 [33792/54000 (63%)] Loss: -1796.243164\n",
      "Train Epoch: 209 [45056/54000 (83%)] Loss: -1770.809814\n",
      "    epoch          : 209\n",
      "    loss           : -1787.3515809257076\n",
      "    ess            : 8.001184913347352\n",
      "    log_marginal   : 1787.3515809257076\n",
      "    val_loss       : -1749.881368001302\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1749.8813883463542\n",
      "Train Epoch: 210 [0/54000 (0%)] Loss: -1768.477295\n",
      "Train Epoch: 210 [11264/54000 (21%)] Loss: -1775.142822\n",
      "Train Epoch: 210 [22528/54000 (42%)] Loss: -1774.639282\n",
      "Train Epoch: 210 [33792/54000 (63%)] Loss: -1774.596680\n",
      "Train Epoch: 210 [45056/54000 (83%)] Loss: -1792.123047\n",
      "    epoch          : 210\n",
      "    loss           : -1778.4215456404777\n",
      "    ess            : 8.001185651095408\n",
      "    log_marginal   : 1778.4215456404777\n",
      "    val_loss       : -1764.1357116699219\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1764.1357116699219\n",
      "Train Epoch: 211 [0/54000 (0%)] Loss: -1803.249756\n",
      "Train Epoch: 211 [11264/54000 (21%)] Loss: -1800.398438\n",
      "Train Epoch: 211 [22528/54000 (42%)] Loss: -1794.010742\n",
      "Train Epoch: 211 [33792/54000 (63%)] Loss: -1797.129639\n",
      "Train Epoch: 211 [45056/54000 (83%)] Loss: -1789.230103\n",
      "    epoch          : 211\n",
      "    loss           : -1795.073854842276\n",
      "    ess            : 8.001184625445672\n",
      "    log_marginal   : 1795.073854842276\n",
      "    val_loss       : -1788.5757954915364\n",
      "    val_ess        : 8.001185894012451\n",
      "    val_log_marginal: 1788.5757954915364\n",
      "Train Epoch: 212 [0/54000 (0%)] Loss: -1810.682861\n",
      "Train Epoch: 212 [11264/54000 (21%)] Loss: -1798.677246\n",
      "Train Epoch: 212 [22528/54000 (42%)] Loss: -1790.269043\n",
      "Train Epoch: 212 [33792/54000 (63%)] Loss: -1793.779053\n",
      "Train Epoch: 212 [45056/54000 (83%)] Loss: -1783.538086\n",
      "    epoch          : 212\n",
      "    loss           : -1796.971215589991\n",
      "    ess            : 8.001184751402658\n",
      "    log_marginal   : 1796.9712144383843\n",
      "    val_loss       : -1797.3106079101562\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1797.3106079101562\n",
      "Train Epoch: 213 [0/54000 (0%)] Loss: -1806.986084\n",
      "Train Epoch: 213 [11264/54000 (21%)] Loss: -1785.124023\n",
      "Train Epoch: 213 [22528/54000 (42%)] Loss: -1791.715576\n",
      "Train Epoch: 213 [33792/54000 (63%)] Loss: -1794.291870\n",
      "Train Epoch: 213 [45056/54000 (83%)] Loss: -1793.979248\n",
      "    epoch          : 213\n",
      "    loss           : -1793.8583293410968\n",
      "    ess            : 8.001185201249033\n",
      "    log_marginal   : 1793.8583293410968\n",
      "    val_loss       : -1793.8125\n",
      "    val_ess        : 8.001185973485311\n",
      "    val_log_marginal: 1793.8125\n",
      "Train Epoch: 214 [0/54000 (0%)] Loss: -1805.779541\n",
      "Train Epoch: 214 [11264/54000 (21%)] Loss: -1807.469604\n",
      "Train Epoch: 214 [22528/54000 (42%)] Loss: -1802.616577\n",
      "Train Epoch: 214 [33792/54000 (63%)] Loss: -1805.861694\n",
      "Train Epoch: 214 [45056/54000 (83%)] Loss: -1789.331299\n",
      "    epoch          : 214\n",
      "    loss           : -1798.7819294479657\n",
      "    ess            : 8.00118430155628\n",
      "    log_marginal   : 1798.7819282963592\n",
      "    val_loss       : -1793.3231099446614\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1793.3231099446614\n",
      "Train Epoch: 215 [0/54000 (0%)] Loss: -1807.677368\n",
      "Train Epoch: 215 [11264/54000 (21%)] Loss: -1799.664795\n",
      "Train Epoch: 215 [22528/54000 (42%)] Loss: -1795.004272\n",
      "Train Epoch: 215 [33792/54000 (63%)] Loss: -1776.143066\n",
      "Train Epoch: 215 [45056/54000 (83%)] Loss: -1764.980225\n",
      "    epoch          : 215\n",
      "    loss           : -1787.7021300117924\n",
      "    ess            : 8.001184985322773\n",
      "    log_marginal   : 1787.7021311633991\n",
      "    val_loss       : -1775.3912556966145\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1775.3912455240886\n",
      "Train Epoch: 216 [0/54000 (0%)] Loss: -1779.833984\n",
      "Train Epoch: 216 [11264/54000 (21%)] Loss: -1774.759521\n",
      "Train Epoch: 216 [22528/54000 (42%)] Loss: -1797.465576\n",
      "Train Epoch: 216 [33792/54000 (63%)] Loss: -1786.520996\n",
      "Train Epoch: 216 [45056/54000 (83%)] Loss: -1791.724609\n",
      "    epoch          : 216\n",
      "    loss           : -1789.7330045880012\n",
      "    ess            : 8.001185345199874\n",
      "    log_marginal   : 1789.7330045880012\n",
      "    val_loss       : -1786.7573140462239\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1786.7573140462239\n",
      "Train Epoch: 217 [0/54000 (0%)] Loss: -1793.838379\n",
      "Train Epoch: 217 [11264/54000 (21%)] Loss: -1796.028076\n",
      "Train Epoch: 217 [22528/54000 (42%)] Loss: -1795.486572\n",
      "Train Epoch: 217 [33792/54000 (63%)] Loss: -1788.416748\n",
      "Train Epoch: 217 [45056/54000 (83%)] Loss: -1794.095947\n",
      "    epoch          : 217\n",
      "    loss           : -1794.5608082897259\n",
      "    ess            : 8.001184823378077\n",
      "    log_marginal   : 1794.5608082897259\n",
      "    val_loss       : -1797.957763671875\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.957743326823\n",
      "Train Epoch: 218 [0/54000 (0%)] Loss: -1806.363525\n",
      "Train Epoch: 218 [11264/54000 (21%)] Loss: -1803.930542\n",
      "Train Epoch: 218 [22528/54000 (42%)] Loss: -1774.663818\n",
      "Train Epoch: 218 [33792/54000 (63%)] Loss: -1778.610840\n",
      "Train Epoch: 218 [45056/54000 (83%)] Loss: -1782.877197\n",
      "    epoch          : 218\n",
      "    loss           : -1792.2978147110848\n",
      "    ess            : 8.001185435169148\n",
      "    log_marginal   : 1792.2978147110848\n",
      "    val_loss       : -1795.8916015625\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1795.8916015625\n",
      "Train Epoch: 219 [0/54000 (0%)] Loss: -1802.671631\n",
      "Train Epoch: 219 [11264/54000 (21%)] Loss: -1786.708130\n",
      "Train Epoch: 219 [22528/54000 (42%)] Loss: -1776.775269\n",
      "Train Epoch: 219 [33792/54000 (63%)] Loss: -1777.631470\n",
      "Train Epoch: 219 [45056/54000 (83%)] Loss: -1787.999878\n",
      "    epoch          : 219\n",
      "    loss           : -1788.691398188753\n",
      "    ess            : 8.001185624104625\n",
      "    log_marginal   : 1788.6913993403596\n",
      "    val_loss       : -1802.5782674153645\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1802.5782674153645\n",
      "Train Epoch: 220 [0/54000 (0%)] Loss: -1806.241943\n",
      "Train Epoch: 220 [11264/54000 (21%)] Loss: -1801.680664\n",
      "Train Epoch: 220 [22528/54000 (42%)] Loss: -1796.337769\n",
      "Train Epoch: 220 [33792/54000 (63%)] Loss: -1799.573242\n",
      "Train Epoch: 220 [45056/54000 (83%)] Loss: -1796.066040\n",
      "    epoch          : 220\n",
      "    loss           : -1796.7525415960347\n",
      "    ess            : 8.001185093285903\n",
      "    log_marginal   : 1796.7525427476414\n",
      "    val_loss       : -1779.3876546223958\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1779.3876444498699\n",
      "Train Epoch: 221 [0/54000 (0%)] Loss: -1797.695679\n",
      "Train Epoch: 221 [11264/54000 (21%)] Loss: -1768.326538\n",
      "Train Epoch: 221 [22528/54000 (42%)] Loss: -1787.750366\n",
      "Train Epoch: 221 [33792/54000 (63%)] Loss: -1772.131470\n",
      "Train Epoch: 221 [45056/54000 (83%)] Loss: -1771.921753\n",
      "    epoch          : 221\n",
      "    loss           : -1778.3086041144604\n",
      "    ess            : 8.001185543132278\n",
      "    log_marginal   : 1778.3086052660672\n",
      "    val_loss       : -1792.783955891927\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1792.783955891927\n",
      "Train Epoch: 222 [0/54000 (0%)] Loss: -1802.887329\n",
      "Train Epoch: 222 [11264/54000 (21%)] Loss: -1788.717651\n",
      "Train Epoch: 222 [22528/54000 (42%)] Loss: -1790.432373\n",
      "Train Epoch: 222 [33792/54000 (63%)] Loss: -1790.838135\n",
      "Train Epoch: 222 [45056/54000 (83%)] Loss: -1798.008301\n",
      "    epoch          : 222\n",
      "    loss           : -1793.9905913730838\n",
      "    ess            : 8.001184841371932\n",
      "    log_marginal   : 1793.9905913730838\n",
      "    val_loss       : -1801.052998860677\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1801.052998860677\n",
      "Train Epoch: 223 [0/54000 (0%)] Loss: -1821.127319\n",
      "Train Epoch: 223 [11264/54000 (21%)] Loss: -1815.866455\n",
      "Train Epoch: 223 [22528/54000 (42%)] Loss: -1792.693359\n",
      "Train Epoch: 223 [33792/54000 (63%)] Loss: -1782.345947\n",
      "Train Epoch: 223 [45056/54000 (83%)] Loss: -1795.299438\n",
      "    epoch          : 223\n",
      "    loss           : -1797.6252637179393\n",
      "    ess            : 8.001185075292048\n",
      "    log_marginal   : 1797.6252637179393\n",
      "    val_loss       : -1798.1726786295574\n",
      "    val_ess        : 8.001185337702433\n",
      "    val_log_marginal: 1798.1726786295574\n",
      "Train Epoch: 224 [0/54000 (0%)] Loss: -1802.992676\n",
      "Train Epoch: 224 [11264/54000 (21%)] Loss: -1798.816528\n",
      "Train Epoch: 224 [22528/54000 (42%)] Loss: -1796.057861\n",
      "Train Epoch: 224 [33792/54000 (63%)] Loss: -1794.544312\n",
      "Train Epoch: 224 [45056/54000 (83%)] Loss: -1785.656006\n",
      "    epoch          : 224\n",
      "    loss           : -1795.740272378022\n",
      "    ess            : 8.001184895353497\n",
      "    log_marginal   : 1795.7402735296284\n",
      "    val_loss       : -1800.5516662597656\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1800.5516662597656\n",
      "Train Epoch: 225 [0/54000 (0%)] Loss: -1803.000610\n",
      "Train Epoch: 225 [11264/54000 (21%)] Loss: -1793.613770\n",
      "Train Epoch: 225 [22528/54000 (42%)] Loss: -1792.454956\n",
      "Train Epoch: 225 [33792/54000 (63%)] Loss: -1790.831787\n",
      "Train Epoch: 225 [45056/54000 (83%)] Loss: -1795.411499\n",
      "    epoch          : 225\n",
      "    loss           : -1792.0160246075325\n",
      "    ess            : 8.00118513827054\n",
      "    log_marginal   : 1792.0160246075325\n",
      "    val_loss       : -1799.7400309244792\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1799.7400309244792\n",
      "Train Epoch: 226 [0/54000 (0%)] Loss: -1810.539062\n",
      "Train Epoch: 226 [11264/54000 (21%)] Loss: -1813.082520\n",
      "Train Epoch: 226 [22528/54000 (42%)] Loss: -1804.457764\n",
      "Train Epoch: 226 [33792/54000 (63%)] Loss: -1789.521973\n",
      "Train Epoch: 226 [45056/54000 (83%)] Loss: -1801.937866\n",
      "    epoch          : 226\n",
      "    loss           : -1799.9265666457843\n",
      "    ess            : 8.001184715414947\n",
      "    log_marginal   : 1799.9265666457843\n",
      "    val_loss       : -1798.8638712565105\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1798.8638712565105\n",
      "Train Epoch: 227 [0/54000 (0%)] Loss: -1815.667114\n",
      "Train Epoch: 227 [11264/54000 (21%)] Loss: -1803.199951\n",
      "Train Epoch: 227 [22528/54000 (42%)] Loss: -1797.069336\n",
      "Train Epoch: 227 [33792/54000 (63%)] Loss: -1784.644653\n",
      "Train Epoch: 227 [45056/54000 (83%)] Loss: -1798.699707\n",
      "    epoch          : 227\n",
      "    loss           : -1796.8583477668042\n",
      "    ess            : 8.001184625445672\n",
      "    log_marginal   : 1796.8583477668042\n",
      "    val_loss       : -1799.0357055664062\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1799.0357055664062\n",
      "Train Epoch: 228 [0/54000 (0%)] Loss: -1821.754883\n",
      "Train Epoch: 228 [11264/54000 (21%)] Loss: -1803.139038\n",
      "Train Epoch: 228 [22528/54000 (42%)] Loss: -1798.019287\n",
      "Train Epoch: 228 [33792/54000 (63%)] Loss: -1794.140625\n",
      "Train Epoch: 228 [45056/54000 (83%)] Loss: -1789.552734\n",
      "    epoch          : 228\n",
      "    loss           : -1799.0449115105396\n",
      "    ess            : 8.001185435169148\n",
      "    log_marginal   : 1799.0449115105396\n",
      "    val_loss       : -1797.7685852050781\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.7685852050781\n",
      "Train Epoch: 229 [0/54000 (0%)] Loss: -1805.829224\n",
      "Train Epoch: 229 [11264/54000 (21%)] Loss: -1800.747437\n",
      "Train Epoch: 229 [22528/54000 (42%)] Loss: -1798.420166\n",
      "Train Epoch: 229 [33792/54000 (63%)] Loss: -1794.382568\n",
      "Train Epoch: 229 [45056/54000 (83%)] Loss: -1803.250000\n",
      "    epoch          : 229\n",
      "    loss           : -1798.2152191737912\n",
      "    ess            : 8.001184715414947\n",
      "    log_marginal   : 1798.2152191737912\n",
      "    val_loss       : -1801.8720092773438\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1801.8719991048176\n",
      "Train Epoch: 230 [0/54000 (0%)] Loss: -1806.548340\n",
      "Train Epoch: 230 [11264/54000 (21%)] Loss: -1796.292236\n",
      "Train Epoch: 230 [22528/54000 (42%)] Loss: -1796.243530\n",
      "Train Epoch: 230 [33792/54000 (63%)] Loss: -1805.818604\n",
      "Train Epoch: 230 [45056/54000 (83%)] Loss: -1797.787476\n",
      "    epoch          : 230\n",
      "    loss           : -1800.6890086048054\n",
      "    ess            : 8.001184895353497\n",
      "    log_marginal   : 1800.6890086048054\n",
      "    val_loss       : -1802.4887186686199\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1802.4887186686199\n",
      "Train Epoch: 231 [0/54000 (0%)] Loss: -1811.492920\n",
      "Train Epoch: 231 [11264/54000 (21%)] Loss: -1793.450439\n",
      "Train Epoch: 231 [22528/54000 (42%)] Loss: -1795.481689\n",
      "Train Epoch: 231 [33792/54000 (63%)] Loss: -1791.347778\n",
      "Train Epoch: 231 [45056/54000 (83%)] Loss: -1770.085083\n",
      "    epoch          : 231\n",
      "    loss           : -1787.5832991690006\n",
      "    ess            : 8.00118506629512\n",
      "    log_marginal   : 1787.5832991690006\n",
      "    val_loss       : -1768.8440043131511\n",
      "    val_ess        : 8.00118581453959\n",
      "    val_log_marginal: 1768.8440043131511\n",
      "Train Epoch: 232 [0/54000 (0%)] Loss: -1787.138550\n",
      "Train Epoch: 232 [11264/54000 (21%)] Loss: -1772.442627\n",
      "Train Epoch: 232 [22528/54000 (42%)] Loss: -1768.285645\n",
      "Train Epoch: 232 [33792/54000 (63%)] Loss: -1784.946777\n",
      "Train Epoch: 232 [45056/54000 (83%)] Loss: -1795.695557\n",
      "    epoch          : 232\n",
      "    loss           : -1782.630797188237\n",
      "    ess            : 8.001185777052394\n",
      "    log_marginal   : 1782.630797188237\n",
      "    val_loss       : -1789.9328918457031\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1789.9328918457031\n",
      "Train Epoch: 233 [0/54000 (0%)] Loss: -1806.334595\n",
      "Train Epoch: 233 [11264/54000 (21%)] Loss: -1798.322998\n",
      "Train Epoch: 233 [22528/54000 (42%)] Loss: -1798.370850\n",
      "Train Epoch: 233 [33792/54000 (63%)] Loss: -1792.315063\n",
      "Train Epoch: 233 [45056/54000 (83%)] Loss: -1796.597656\n",
      "    epoch          : 233\n",
      "    loss           : -1795.9571567751327\n",
      "    ess            : 8.001184859365788\n",
      "    log_marginal   : 1795.9571567751327\n",
      "    val_loss       : -1805.1894836425781\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.1894836425781\n",
      "Train Epoch: 234 [0/54000 (0%)] Loss: -1818.206421\n",
      "Train Epoch: 234 [11264/54000 (21%)] Loss: -1789.447510\n",
      "Train Epoch: 234 [22528/54000 (42%)] Loss: -1780.323853\n",
      "Train Epoch: 234 [33792/54000 (63%)] Loss: -1788.338135\n",
      "Train Epoch: 234 [45056/54000 (83%)] Loss: -1799.030273\n",
      "    epoch          : 234\n",
      "    loss           : -1795.5563688458137\n",
      "    ess            : 8.001185336202946\n",
      "    log_marginal   : 1795.5563699974205\n",
      "    val_loss       : -1799.4240620930989\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1799.4240417480469\n",
      "Train Epoch: 235 [0/54000 (0%)] Loss: -1799.554443\n",
      "Train Epoch: 235 [11264/54000 (21%)] Loss: -1811.663330\n",
      "Train Epoch: 235 [22528/54000 (42%)] Loss: -1793.120117\n",
      "Train Epoch: 235 [33792/54000 (63%)] Loss: -1792.770020\n",
      "Train Epoch: 235 [45056/54000 (83%)] Loss: -1788.497314\n",
      "    epoch          : 235\n",
      "    loss           : -1796.5277548735996\n",
      "    ess            : 8.001185183255178\n",
      "    log_marginal   : 1796.5277548735996\n",
      "    val_loss       : -1797.3518473307292\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.3518473307292\n",
      "Train Epoch: 236 [0/54000 (0%)] Loss: -1794.227905\n",
      "Train Epoch: 236 [11264/54000 (21%)] Loss: -1799.323975\n",
      "Train Epoch: 236 [22528/54000 (42%)] Loss: -1793.125000\n",
      "Train Epoch: 236 [33792/54000 (63%)] Loss: -1803.533569\n",
      "Train Epoch: 236 [45056/54000 (83%)] Loss: -1790.234741\n",
      "    epoch          : 236\n",
      "    loss           : -1798.4084933298939\n",
      "    ess            : 8.00118553413535\n",
      "    log_marginal   : 1798.4084933298939\n",
      "    val_loss       : -1803.306172688802\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1803.3061828613281\n",
      "Train Epoch: 237 [0/54000 (0%)] Loss: -1816.245239\n",
      "Train Epoch: 237 [11264/54000 (21%)] Loss: -1803.533447\n",
      "Train Epoch: 237 [22528/54000 (42%)] Loss: -1798.647095\n",
      "Train Epoch: 237 [33792/54000 (63%)] Loss: -1795.942383\n",
      "Train Epoch: 237 [45056/54000 (83%)] Loss: -1795.146240\n",
      "    epoch          : 237\n",
      "    loss           : -1798.3692454212116\n",
      "    ess            : 8.001185003316627\n",
      "    log_marginal   : 1798.3692454212116\n",
      "    val_loss       : -1797.9607340494792\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1797.9607340494792\n",
      "Train Epoch: 238 [0/54000 (0%)] Loss: -1814.303711\n",
      "Train Epoch: 238 [11264/54000 (21%)] Loss: -1803.476196\n",
      "Train Epoch: 238 [22528/54000 (42%)] Loss: -1774.604980\n",
      "Train Epoch: 238 [33792/54000 (63%)] Loss: -1788.683838\n",
      "Train Epoch: 238 [45056/54000 (83%)] Loss: -1792.511963\n",
      "    epoch          : 238\n",
      "    loss           : -1796.1662010336822\n",
      "    ess            : 8.001184895353497\n",
      "    log_marginal   : 1796.1662010336822\n",
      "    val_loss       : -1801.6675211588542\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1801.6675211588542\n",
      "Train Epoch: 239 [0/54000 (0%)] Loss: -1804.987061\n",
      "Train Epoch: 239 [11264/54000 (21%)] Loss: -1810.119629\n",
      "Train Epoch: 239 [22528/54000 (42%)] Loss: -1802.692383\n",
      "Train Epoch: 239 [33792/54000 (63%)] Loss: -1804.380127\n",
      "Train Epoch: 239 [45056/54000 (83%)] Loss: -1801.626099\n",
      "    epoch          : 239\n",
      "    loss           : -1803.3015827682782\n",
      "    ess            : 8.001184787390367\n",
      "    log_marginal   : 1803.3015827682782\n",
      "    val_loss       : -1805.3011678059895\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.3011678059895\n",
      "Train Epoch: 240 [0/54000 (0%)] Loss: -1818.892456\n",
      "Train Epoch: 240 [11264/54000 (21%)] Loss: -1792.213135\n",
      "Train Epoch: 240 [22528/54000 (42%)] Loss: -1802.686768\n",
      "Train Epoch: 240 [33792/54000 (63%)] Loss: -1777.185059\n",
      "Train Epoch: 240 [45056/54000 (83%)] Loss: -1783.532715\n",
      "    epoch          : 240\n",
      "    loss           : -1792.6728354400059\n",
      "    ess            : 8.00118506629512\n",
      "    log_marginal   : 1792.6728354400059\n",
      "    val_loss       : -1770.4432474772136\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1770.4432474772136\n",
      "Train Epoch: 241 [0/54000 (0%)] Loss: -1781.857910\n",
      "Train Epoch: 241 [11264/54000 (21%)] Loss: -1732.951660\n",
      "Train Epoch: 241 [22528/54000 (42%)] Loss: -1738.785156\n",
      "Train Epoch: 241 [33792/54000 (63%)] Loss: -1759.057007\n",
      "Train Epoch: 241 [45056/54000 (83%)] Loss: -1784.295044\n",
      "    epoch          : 241\n",
      "    loss           : -1752.1893713609227\n",
      "    ess            : 8.001185588116916\n",
      "    log_marginal   : 1752.1893713609227\n",
      "    val_loss       : -1733.5198567708333\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1733.5198364257812\n",
      "Train Epoch: 242 [0/54000 (0%)] Loss: -1734.896973\n",
      "Train Epoch: 242 [11264/54000 (21%)] Loss: -1784.210205\n",
      "Train Epoch: 242 [22528/54000 (42%)] Loss: -1786.177734\n",
      "Train Epoch: 242 [33792/54000 (63%)] Loss: -1781.523438\n",
      "Train Epoch: 242 [45056/54000 (83%)] Loss: -1785.225220\n",
      "    epoch          : 242\n",
      "    loss           : -1778.737465912441\n",
      "    ess            : 8.001185651095408\n",
      "    log_marginal   : 1778.737465912441\n",
      "    val_loss       : -1772.3326924641926\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1772.3327128092449\n",
      "Train Epoch: 243 [0/54000 (0%)] Loss: -1805.269653\n",
      "Train Epoch: 243 [11264/54000 (21%)] Loss: -1796.425537\n",
      "Train Epoch: 243 [22528/54000 (42%)] Loss: -1788.993164\n",
      "Train Epoch: 243 [33792/54000 (63%)] Loss: -1792.437500\n",
      "Train Epoch: 243 [45056/54000 (83%)] Loss: -1794.900635\n",
      "    epoch          : 243\n",
      "    loss           : -1794.5016824974205\n",
      "    ess            : 8.001184805384222\n",
      "    log_marginal   : 1794.5016824974205\n",
      "    val_loss       : -1791.2102864583333\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1791.2102864583333\n",
      "Train Epoch: 244 [0/54000 (0%)] Loss: -1800.452881\n",
      "Train Epoch: 244 [11264/54000 (21%)] Loss: -1803.460938\n",
      "Train Epoch: 244 [22528/54000 (42%)] Loss: -1790.858887\n",
      "Train Epoch: 244 [33792/54000 (63%)] Loss: -1793.673462\n",
      "Train Epoch: 244 [45056/54000 (83%)] Loss: -1793.334961\n",
      "    epoch          : 244\n",
      "    loss           : -1795.012029683815\n",
      "    ess            : 8.001184895353497\n",
      "    log_marginal   : 1795.012029683815\n",
      "    val_loss       : -1799.118428548177\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.118428548177\n",
      "Train Epoch: 245 [0/54000 (0%)] Loss: -1813.754517\n",
      "Train Epoch: 245 [11264/54000 (21%)] Loss: -1811.097656\n",
      "Train Epoch: 245 [22528/54000 (42%)] Loss: -1785.813599\n",
      "Train Epoch: 245 [33792/54000 (63%)] Loss: -1759.930786\n",
      "Train Epoch: 245 [45056/54000 (83%)] Loss: -1735.623535\n",
      "    epoch          : 245\n",
      "    loss           : -1779.8921347564121\n",
      "    ess            : 8.001185201249033\n",
      "    log_marginal   : 1779.8921347564121\n",
      "    val_loss       : -1768.0747477213542\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1768.0747477213542\n",
      "Train Epoch: 246 [0/54000 (0%)] Loss: -1769.521240\n",
      "Train Epoch: 246 [11264/54000 (21%)] Loss: -1792.116699\n",
      "Train Epoch: 246 [22528/54000 (42%)] Loss: -1791.549683\n",
      "Train Epoch: 246 [33792/54000 (63%)] Loss: -1793.392334\n",
      "Train Epoch: 246 [45056/54000 (83%)] Loss: -1798.469727\n",
      "    epoch          : 246\n",
      "    loss           : -1791.491372162441\n",
      "    ess            : 8.001185480153785\n",
      "    log_marginal   : 1791.491372162441\n",
      "    val_loss       : -1793.9113667805989\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1793.9113667805989\n",
      "Train Epoch: 247 [0/54000 (0%)] Loss: -1806.732178\n",
      "Train Epoch: 247 [11264/54000 (21%)] Loss: -1809.850342\n",
      "Train Epoch: 247 [22528/54000 (42%)] Loss: -1781.404785\n",
      "Train Epoch: 247 [33792/54000 (63%)] Loss: -1785.437012\n",
      "Train Epoch: 247 [45056/54000 (83%)] Loss: -1780.906006\n",
      "    epoch          : 247\n",
      "    loss           : -1790.7534893683667\n",
      "    ess            : 8.001185084288975\n",
      "    log_marginal   : 1790.7534893683667\n",
      "    val_loss       : -1792.0164489746094\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1792.0164591471355\n",
      "Train Epoch: 248 [0/54000 (0%)] Loss: -1799.730103\n",
      "Train Epoch: 248 [11264/54000 (21%)] Loss: -1797.403687\n",
      "Train Epoch: 248 [22528/54000 (42%)] Loss: -1801.827515\n",
      "Train Epoch: 248 [33792/54000 (63%)] Loss: -1812.719482\n",
      "Train Epoch: 248 [45056/54000 (83%)] Loss: -1792.946777\n",
      "    epoch          : 248\n",
      "    loss           : -1800.549729833063\n",
      "    ess            : 8.001185021310482\n",
      "    log_marginal   : 1800.5497286814564\n",
      "    val_loss       : -1800.4969075520833\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1800.4969075520833\n",
      "Train Epoch: 249 [0/54000 (0%)] Loss: -1812.575439\n",
      "Train Epoch: 249 [11264/54000 (21%)] Loss: -1800.053955\n",
      "Train Epoch: 249 [22528/54000 (42%)] Loss: -1781.786011\n",
      "Train Epoch: 249 [33792/54000 (63%)] Loss: -1789.530762\n",
      "Train Epoch: 249 [45056/54000 (83%)] Loss: -1779.831055\n",
      "    epoch          : 249\n",
      "    loss           : -1789.5800458800118\n",
      "    ess            : 8.001185471156859\n",
      "    log_marginal   : 1789.5800458800118\n",
      "    val_loss       : -1787.4865824381511\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1787.4865824381511\n",
      "Train Epoch: 250 [0/54000 (0%)] Loss: -1795.061279\n",
      "Train Epoch: 250 [11264/54000 (21%)] Loss: -1773.975830\n",
      "Train Epoch: 250 [22528/54000 (42%)] Loss: -1775.731934\n",
      "Train Epoch: 250 [33792/54000 (63%)] Loss: -1772.055176\n",
      "Train Epoch: 250 [45056/54000 (83%)] Loss: -1767.915527\n",
      "    epoch          : 250\n",
      "    loss           : -1772.993971338812\n",
      "    ess            : 8.001185669089264\n",
      "    log_marginal   : 1772.993971338812\n",
      "    val_loss       : -1794.2750142415364\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1794.2750142415364\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch250.pth ...\n",
      "Train Epoch: 251 [0/54000 (0%)] Loss: -1797.820312\n",
      "Train Epoch: 251 [11264/54000 (21%)] Loss: -1783.461182\n",
      "Train Epoch: 251 [22528/54000 (42%)] Loss: -1792.984131\n",
      "Train Epoch: 251 [33792/54000 (63%)] Loss: -1786.459229\n",
      "Train Epoch: 251 [45056/54000 (83%)] Loss: -1789.754761\n",
      "    epoch          : 251\n",
      "    loss           : -1790.865148004496\n",
      "    ess            : 8.001185543132278\n",
      "    log_marginal   : 1790.865148004496\n",
      "    val_loss       : -1802.6489562988281\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.6489562988281\n",
      "Train Epoch: 252 [0/54000 (0%)] Loss: -1809.438843\n",
      "Train Epoch: 252 [11264/54000 (21%)] Loss: -1813.579346\n",
      "Train Epoch: 252 [22528/54000 (42%)] Loss: -1809.548828\n",
      "Train Epoch: 252 [33792/54000 (63%)] Loss: -1791.314453\n",
      "Train Epoch: 252 [45056/54000 (83%)] Loss: -1791.658081\n",
      "    epoch          : 252\n",
      "    loss           : -1801.3816470739976\n",
      "    ess            : 8.001185435169148\n",
      "    log_marginal   : 1801.3816470739976\n",
      "    val_loss       : -1800.6812642415364\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1800.6812642415364\n",
      "Train Epoch: 253 [0/54000 (0%)] Loss: -1799.685303\n",
      "Train Epoch: 253 [11264/54000 (21%)] Loss: -1786.876953\n",
      "Train Epoch: 253 [22528/54000 (42%)] Loss: -1796.823364\n",
      "Train Epoch: 253 [33792/54000 (63%)] Loss: -1799.625244\n",
      "Train Epoch: 253 [45056/54000 (83%)] Loss: -1806.779053\n",
      "    epoch          : 253\n",
      "    loss           : -1798.8206591336232\n",
      "    ess            : 8.001185633101553\n",
      "    log_marginal   : 1798.8206591336232\n",
      "    val_loss       : -1809.307840983073\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1809.307840983073\n",
      "Train Epoch: 254 [0/54000 (0%)] Loss: -1813.129639\n",
      "Train Epoch: 254 [11264/54000 (21%)] Loss: -1789.415771\n",
      "Train Epoch: 254 [22528/54000 (42%)] Loss: -1794.669800\n",
      "Train Epoch: 254 [33792/54000 (63%)] Loss: -1808.797363\n",
      "Train Epoch: 254 [45056/54000 (83%)] Loss: -1799.730957\n",
      "    epoch          : 254\n",
      "    loss           : -1799.5936498102153\n",
      "    ess            : 8.001185327206018\n",
      "    log_marginal   : 1799.5936498102153\n",
      "    val_loss       : -1805.9377339680989\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1805.9377136230469\n",
      "Train Epoch: 255 [0/54000 (0%)] Loss: -1818.931641\n",
      "Train Epoch: 255 [11264/54000 (21%)] Loss: -1810.911133\n",
      "Train Epoch: 255 [22528/54000 (42%)] Loss: -1791.139160\n",
      "Train Epoch: 255 [33792/54000 (63%)] Loss: -1794.961060\n",
      "Train Epoch: 255 [45056/54000 (83%)] Loss: -1791.358887\n",
      "    epoch          : 255\n",
      "    loss           : -1797.6291964548939\n",
      "    ess            : 8.001185381187582\n",
      "    log_marginal   : 1797.6291964548939\n",
      "    val_loss       : -1797.9669392903645\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1797.9669494628906\n",
      "Train Epoch: 256 [0/54000 (0%)] Loss: -1800.516357\n",
      "Train Epoch: 256 [11264/54000 (21%)] Loss: -1809.129395\n",
      "Train Epoch: 256 [22528/54000 (42%)] Loss: -1806.979736\n",
      "Train Epoch: 256 [33792/54000 (63%)] Loss: -1805.073853\n",
      "Train Epoch: 256 [45056/54000 (83%)] Loss: -1806.123291\n",
      "    epoch          : 256\n",
      "    loss           : -1800.6175007370282\n",
      "    ess            : 8.001185255230597\n",
      "    log_marginal   : 1800.6175007370282\n",
      "    val_loss       : -1806.9393107096355\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1806.9393107096355\n",
      "Train Epoch: 257 [0/54000 (0%)] Loss: -1808.917725\n",
      "Train Epoch: 257 [11264/54000 (21%)] Loss: -1802.963745\n",
      "Train Epoch: 257 [22528/54000 (42%)] Loss: -1787.543701\n",
      "Train Epoch: 257 [33792/54000 (63%)] Loss: -1802.232666\n",
      "Train Epoch: 257 [45056/54000 (83%)] Loss: -1786.275513\n",
      "    epoch          : 257\n",
      "    loss           : -1799.8152534916717\n",
      "    ess            : 8.001184823378077\n",
      "    log_marginal   : 1799.8152534916717\n",
      "    val_loss       : -1807.3223571777344\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1807.3223368326824\n",
      "Train Epoch: 258 [0/54000 (0%)] Loss: -1822.237305\n",
      "Train Epoch: 258 [11264/54000 (21%)] Loss: -1811.136841\n",
      "Train Epoch: 258 [22528/54000 (42%)] Loss: -1797.719971\n",
      "Train Epoch: 258 [33792/54000 (63%)] Loss: -1789.466797\n",
      "Train Epoch: 258 [45056/54000 (83%)] Loss: -1800.169067\n",
      "    epoch          : 258\n",
      "    loss           : -1799.0136258107311\n",
      "    ess            : 8.001185273224452\n",
      "    log_marginal   : 1799.0136269623379\n",
      "    val_loss       : -1799.2897542317708\n",
      "    val_ess        : 8.001185337702433\n",
      "    val_log_marginal: 1799.2897542317708\n",
      "Train Epoch: 259 [0/54000 (0%)] Loss: -1802.000732\n",
      "Train Epoch: 259 [11264/54000 (21%)] Loss: -1805.583130\n",
      "Train Epoch: 259 [22528/54000 (42%)] Loss: -1798.897095\n",
      "Train Epoch: 259 [33792/54000 (63%)] Loss: -1789.471680\n",
      "Train Epoch: 259 [45056/54000 (83%)] Loss: -1794.610474\n",
      "    epoch          : 259\n",
      "    loss           : -1796.7729434607163\n",
      "    ess            : 8.001185579119989\n",
      "    log_marginal   : 1796.7729434607163\n",
      "    val_loss       : -1799.1177571614583\n",
      "    val_ess        : 8.001185337702433\n",
      "    val_log_marginal: 1799.1177571614583\n",
      "Train Epoch: 260 [0/54000 (0%)] Loss: -1809.734253\n",
      "Train Epoch: 260 [11264/54000 (21%)] Loss: -1790.521484\n",
      "Train Epoch: 260 [22528/54000 (42%)] Loss: -1796.035400\n",
      "Train Epoch: 260 [33792/54000 (63%)] Loss: -1788.019287\n",
      "Train Epoch: 260 [45056/54000 (83%)] Loss: -1794.865479\n",
      "    epoch          : 260\n",
      "    loss           : -1793.7815459629276\n",
      "    ess            : 8.001185669089264\n",
      "    log_marginal   : 1793.7815459629276\n",
      "    val_loss       : -1801.1128845214844\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1801.1128845214844\n",
      "Train Epoch: 261 [0/54000 (0%)] Loss: -1817.206177\n",
      "Train Epoch: 261 [11264/54000 (21%)] Loss: -1806.931885\n",
      "Train Epoch: 261 [22528/54000 (42%)] Loss: -1799.510498\n",
      "Train Epoch: 261 [33792/54000 (63%)] Loss: -1782.544922\n",
      "Train Epoch: 261 [45056/54000 (83%)] Loss: -1783.181885\n",
      "    epoch          : 261\n",
      "    loss           : -1795.4020362710053\n",
      "    ess            : 8.001184949335062\n",
      "    log_marginal   : 1795.402037422612\n",
      "    val_loss       : -1797.4790649414062\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1797.4790547688801\n",
      "Train Epoch: 262 [0/54000 (0%)] Loss: -1808.496582\n",
      "Train Epoch: 262 [11264/54000 (21%)] Loss: -1802.312988\n",
      "Train Epoch: 262 [22528/54000 (42%)] Loss: -1784.870605\n",
      "Train Epoch: 262 [33792/54000 (63%)] Loss: -1774.466309\n",
      "Train Epoch: 262 [45056/54000 (83%)] Loss: -1770.771851\n",
      "    epoch          : 262\n",
      "    loss           : -1787.7152065061173\n",
      "    ess            : 8.001185354196801\n",
      "    log_marginal   : 1787.7152065061173\n",
      "    val_loss       : -1776.3214823404949\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1776.3214823404949\n",
      "Train Epoch: 263 [0/54000 (0%)] Loss: -1790.471924\n",
      "Train Epoch: 263 [11264/54000 (21%)] Loss: -1791.393799\n",
      "Train Epoch: 263 [22528/54000 (42%)] Loss: -1798.770264\n",
      "Train Epoch: 263 [33792/54000 (63%)] Loss: -1801.423096\n",
      "Train Epoch: 263 [45056/54000 (83%)] Loss: -1794.412842\n",
      "    epoch          : 263\n",
      "    loss           : -1793.9981367003243\n",
      "    ess            : 8.001185165261322\n",
      "    log_marginal   : 1793.9981367003243\n",
      "    val_loss       : -1789.0086873372395\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1789.0086771647136\n",
      "Train Epoch: 264 [0/54000 (0%)] Loss: -1801.372192\n",
      "Train Epoch: 264 [11264/54000 (21%)] Loss: -1782.801514\n",
      "Train Epoch: 264 [22528/54000 (42%)] Loss: -1793.303711\n",
      "Train Epoch: 264 [33792/54000 (63%)] Loss: -1789.065674\n",
      "Train Epoch: 264 [45056/54000 (83%)] Loss: -1794.653809\n",
      "    epoch          : 264\n",
      "    loss           : -1792.7604542858196\n",
      "    ess            : 8.001184949335062\n",
      "    log_marginal   : 1792.7604542858196\n",
      "    val_loss       : -1803.3262939453125\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1803.3262939453125\n",
      "Train Epoch: 265 [0/54000 (0%)] Loss: -1810.687012\n",
      "Train Epoch: 265 [11264/54000 (21%)] Loss: -1797.327148\n",
      "Train Epoch: 265 [22528/54000 (42%)] Loss: -1789.771118\n",
      "Train Epoch: 265 [33792/54000 (63%)] Loss: -1804.189819\n",
      "Train Epoch: 265 [45056/54000 (83%)] Loss: -1773.125977\n",
      "    epoch          : 265\n",
      "    loss           : -1796.915656323703\n",
      "    ess            : 8.001184463500977\n",
      "    log_marginal   : 1796.915656323703\n",
      "    val_loss       : -1800.7605285644531\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1800.7605285644531\n",
      "Train Epoch: 266 [0/54000 (0%)] Loss: -1806.258667\n",
      "Train Epoch: 266 [11264/54000 (21%)] Loss: -1786.548950\n",
      "Train Epoch: 266 [22528/54000 (42%)] Loss: -1797.053955\n",
      "Train Epoch: 266 [33792/54000 (63%)] Loss: -1791.086914\n",
      "Train Epoch: 266 [45056/54000 (83%)] Loss: -1782.762329\n",
      "    epoch          : 266\n",
      "    loss           : -1789.480183151533\n",
      "    ess            : 8.001185561126134\n",
      "    log_marginal   : 1789.480183151533\n",
      "    val_loss       : -1798.4042867024739\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1798.404296875\n",
      "Train Epoch: 267 [0/54000 (0%)] Loss: -1800.061157\n",
      "Train Epoch: 267 [11264/54000 (21%)] Loss: -1766.345703\n",
      "Train Epoch: 267 [22528/54000 (42%)] Loss: -1778.678955\n",
      "Train Epoch: 267 [33792/54000 (63%)] Loss: -1752.241211\n",
      "Train Epoch: 267 [45056/54000 (83%)] Loss: -1757.491943\n",
      "    epoch          : 267\n",
      "    loss           : -1775.556653292674\n",
      "    ess            : 8.001185525138423\n",
      "    log_marginal   : 1775.556653292674\n",
      "    val_loss       : -1789.1454467773438\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1789.1454467773438\n",
      "Train Epoch: 268 [0/54000 (0%)] Loss: -1798.526733\n",
      "Train Epoch: 268 [11264/54000 (21%)] Loss: -1797.688232\n",
      "Train Epoch: 268 [22528/54000 (42%)] Loss: -1789.851318\n",
      "Train Epoch: 268 [33792/54000 (63%)] Loss: -1787.078125\n",
      "Train Epoch: 268 [45056/54000 (83%)] Loss: -1785.126465\n",
      "    epoch          : 268\n",
      "    loss           : -1793.567515247273\n",
      "    ess            : 8.001185165261322\n",
      "    log_marginal   : 1793.567515247273\n",
      "    val_loss       : -1797.4732767740886\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1797.4732767740886\n",
      "Train Epoch: 269 [0/54000 (0%)] Loss: -1814.751953\n",
      "Train Epoch: 269 [11264/54000 (21%)] Loss: -1815.720459\n",
      "Train Epoch: 269 [22528/54000 (42%)] Loss: -1796.213135\n",
      "Train Epoch: 269 [33792/54000 (63%)] Loss: -1796.795288\n",
      "Train Epoch: 269 [45056/54000 (83%)] Loss: -1793.744873\n",
      "    epoch          : 269\n",
      "    loss           : -1801.1070280255012\n",
      "    ess            : 8.001185237236744\n",
      "    log_marginal   : 1801.1070280255012\n",
      "    val_loss       : -1802.6873270670574\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.6873270670574\n",
      "Train Epoch: 270 [0/54000 (0%)] Loss: -1804.695312\n",
      "Train Epoch: 270 [11264/54000 (21%)] Loss: -1804.095215\n",
      "Train Epoch: 270 [22528/54000 (42%)] Loss: -1791.633667\n",
      "Train Epoch: 270 [33792/54000 (63%)] Loss: -1793.856201\n",
      "Train Epoch: 270 [45056/54000 (83%)] Loss: -1793.179443\n",
      "    epoch          : 270\n",
      "    loss           : -1797.3717962301002\n",
      "    ess            : 8.001185219242888\n",
      "    log_marginal   : 1797.3717962301002\n",
      "    val_loss       : -1797.5235900878906\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.5235900878906\n",
      "Train Epoch: 271 [0/54000 (0%)] Loss: -1806.858398\n",
      "Train Epoch: 271 [11264/54000 (21%)] Loss: -1800.350342\n",
      "Train Epoch: 271 [22528/54000 (42%)] Loss: -1796.272949\n",
      "Train Epoch: 271 [33792/54000 (63%)] Loss: -1784.846802\n",
      "Train Epoch: 271 [45056/54000 (83%)] Loss: -1793.259033\n",
      "    epoch          : 271\n",
      "    loss           : -1795.165867067733\n",
      "    ess            : 8.001185111279758\n",
      "    log_marginal   : 1795.165867067733\n",
      "    val_loss       : -1793.4650777180989\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1793.4650777180989\n",
      "Train Epoch: 272 [0/54000 (0%)] Loss: -1803.366211\n",
      "Train Epoch: 272 [11264/54000 (21%)] Loss: -1789.932129\n",
      "Train Epoch: 272 [22528/54000 (42%)] Loss: -1788.656982\n",
      "Train Epoch: 272 [33792/54000 (63%)] Loss: -1779.494141\n",
      "Train Epoch: 272 [45056/54000 (83%)] Loss: -1797.966553\n",
      "    epoch          : 272\n",
      "    loss           : -1792.546622798128\n",
      "    ess            : 8.001185615107698\n",
      "    log_marginal   : 1792.546622798128\n",
      "    val_loss       : -1801.7391357421875\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1801.7391357421875\n",
      "Train Epoch: 273 [0/54000 (0%)] Loss: -1808.768799\n",
      "Train Epoch: 273 [11264/54000 (21%)] Loss: -1757.847778\n",
      "Train Epoch: 273 [22528/54000 (42%)] Loss: -1790.719971\n",
      "Train Epoch: 273 [33792/54000 (63%)] Loss: -1796.779785\n",
      "Train Epoch: 273 [45056/54000 (83%)] Loss: -1787.946899\n",
      "    epoch          : 273\n",
      "    loss           : -1789.144523188753\n",
      "    ess            : 8.001185471156859\n",
      "    log_marginal   : 1789.1445243403596\n",
      "    val_loss       : -1804.1443277994792\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.1443277994792\n",
      "Train Epoch: 274 [0/54000 (0%)] Loss: -1800.396973\n",
      "Train Epoch: 274 [11264/54000 (21%)] Loss: -1798.194092\n",
      "Train Epoch: 274 [22528/54000 (42%)] Loss: -1802.114258\n",
      "Train Epoch: 274 [33792/54000 (63%)] Loss: -1793.595093\n",
      "Train Epoch: 274 [45056/54000 (83%)] Loss: -1792.429932\n",
      "    epoch          : 274\n",
      "    loss           : -1797.5708238133843\n",
      "    ess            : 8.001185345199874\n",
      "    log_marginal   : 1797.5708238133843\n",
      "    val_loss       : -1813.5004984537761\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1813.5004984537761\n",
      "Train Epoch: 275 [0/54000 (0%)] Loss: -1807.011475\n",
      "Train Epoch: 275 [11264/54000 (21%)] Loss: -1808.512939\n",
      "Train Epoch: 275 [22528/54000 (42%)] Loss: -1791.279053\n",
      "Train Epoch: 275 [33792/54000 (63%)] Loss: -1794.830566\n",
      "Train Epoch: 275 [45056/54000 (83%)] Loss: -1795.454834\n",
      "    epoch          : 275\n",
      "    loss           : -1796.931726995504\n",
      "    ess            : 8.001185192252105\n",
      "    log_marginal   : 1796.931726995504\n",
      "    val_loss       : -1804.8560282389324\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1804.8560282389324\n",
      "Train Epoch: 276 [0/54000 (0%)] Loss: -1800.102539\n",
      "Train Epoch: 276 [11264/54000 (21%)] Loss: -1812.370850\n",
      "Train Epoch: 276 [22528/54000 (42%)] Loss: -1801.519775\n",
      "Train Epoch: 276 [33792/54000 (63%)] Loss: -1804.103271\n",
      "Train Epoch: 276 [45056/54000 (83%)] Loss: -1792.128174\n",
      "    epoch          : 276\n",
      "    loss           : -1801.8713125552772\n",
      "    ess            : 8.001184877359643\n",
      "    log_marginal   : 1801.8713125552772\n",
      "    val_loss       : -1803.7582499186199\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1803.7582499186199\n",
      "Train Epoch: 277 [0/54000 (0%)] Loss: -1807.407349\n",
      "Train Epoch: 277 [11264/54000 (21%)] Loss: -1805.979614\n",
      "Train Epoch: 277 [22528/54000 (42%)] Loss: -1793.084717\n",
      "Train Epoch: 277 [33792/54000 (63%)] Loss: -1792.721680\n",
      "Train Epoch: 277 [45056/54000 (83%)] Loss: -1792.225342\n",
      "    epoch          : 277\n",
      "    loss           : -1797.7862295474647\n",
      "    ess            : 8.001184679427237\n",
      "    log_marginal   : 1797.7862295474647\n",
      "    val_loss       : -1799.3055928548176\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1799.3055725097656\n",
      "Train Epoch: 278 [0/54000 (0%)] Loss: -1808.613647\n",
      "Train Epoch: 278 [11264/54000 (21%)] Loss: -1805.329712\n",
      "Train Epoch: 278 [22528/54000 (42%)] Loss: -1793.213135\n",
      "Train Epoch: 278 [33792/54000 (63%)] Loss: -1782.033081\n",
      "Train Epoch: 278 [45056/54000 (83%)] Loss: -1787.697510\n",
      "    epoch          : 278\n",
      "    loss           : -1789.9888662662147\n",
      "    ess            : 8.001185363193727\n",
      "    log_marginal   : 1789.9888662662147\n",
      "    val_loss       : -1772.2384948730469\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1772.2384948730469\n",
      "Train Epoch: 279 [0/54000 (0%)] Loss: -1788.868652\n",
      "Train Epoch: 279 [11264/54000 (21%)] Loss: -1764.593994\n",
      "Train Epoch: 279 [22528/54000 (42%)] Loss: -1743.752930\n",
      "Train Epoch: 279 [33792/54000 (63%)] Loss: -1773.912354\n",
      "Train Epoch: 279 [45056/54000 (83%)] Loss: -1764.827393\n",
      "    epoch          : 279\n",
      "    loss           : -1772.968266325177\n",
      "    ess            : 8.001185327206018\n",
      "    log_marginal   : 1772.968266325177\n",
      "    val_loss       : -1789.906758626302\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1789.906758626302\n",
      "Train Epoch: 280 [0/54000 (0%)] Loss: -1800.460327\n",
      "Train Epoch: 280 [11264/54000 (21%)] Loss: -1796.893555\n",
      "Train Epoch: 280 [22528/54000 (42%)] Loss: -1791.591553\n",
      "Train Epoch: 280 [33792/54000 (63%)] Loss: -1794.290771\n",
      "Train Epoch: 280 [45056/54000 (83%)] Loss: -1794.969971\n",
      "    epoch          : 280\n",
      "    loss           : -1794.6830778301887\n",
      "    ess            : 8.001184166602368\n",
      "    log_marginal   : 1794.6830778301887\n",
      "    val_loss       : -1805.2516682942708\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1805.2516581217449\n",
      "Train Epoch: 281 [0/54000 (0%)] Loss: -1807.683228\n",
      "Train Epoch: 281 [11264/54000 (21%)] Loss: -1806.527710\n",
      "Train Epoch: 281 [22528/54000 (42%)] Loss: -1794.198364\n",
      "Train Epoch: 281 [33792/54000 (63%)] Loss: -1793.052979\n",
      "Train Epoch: 281 [45056/54000 (83%)] Loss: -1788.058228\n",
      "    epoch          : 281\n",
      "    loss           : -1797.036252579599\n",
      "    ess            : 8.001184931341207\n",
      "    log_marginal   : 1797.0362537312058\n",
      "    val_loss       : -1795.0857340494792\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1795.0857543945312\n",
      "Train Epoch: 282 [0/54000 (0%)] Loss: -1801.618408\n",
      "Train Epoch: 282 [11264/54000 (21%)] Loss: -1809.447876\n",
      "Train Epoch: 282 [22528/54000 (42%)] Loss: -1807.697754\n",
      "Train Epoch: 282 [33792/54000 (63%)] Loss: -1808.132935\n",
      "Train Epoch: 282 [45056/54000 (83%)] Loss: -1793.609619\n",
      "    epoch          : 282\n",
      "    loss           : -1802.9655727170548\n",
      "    ess            : 8.001184679427237\n",
      "    log_marginal   : 1802.9655727170548\n",
      "    val_loss       : -1803.5906473795574\n",
      "    val_ess        : 8.001185337702433\n",
      "    val_log_marginal: 1803.5906473795574\n",
      "Train Epoch: 283 [0/54000 (0%)] Loss: -1812.923218\n",
      "Train Epoch: 283 [11264/54000 (21%)] Loss: -1808.032959\n",
      "Train Epoch: 283 [22528/54000 (42%)] Loss: -1797.374268\n",
      "Train Epoch: 283 [33792/54000 (63%)] Loss: -1790.763428\n",
      "Train Epoch: 283 [45056/54000 (83%)] Loss: -1759.661621\n",
      "    epoch          : 283\n",
      "    loss           : -1791.728598540684\n",
      "    ess            : 8.001185417175293\n",
      "    log_marginal   : 1791.7285973890773\n",
      "    val_loss       : -1771.5245361328125\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1771.5245361328125\n",
      "Train Epoch: 284 [0/54000 (0%)] Loss: -1781.437500\n",
      "Train Epoch: 284 [11264/54000 (21%)] Loss: -1774.329346\n",
      "Train Epoch: 284 [22528/54000 (42%)] Loss: -1783.898682\n",
      "Train Epoch: 284 [33792/54000 (63%)] Loss: -1781.820679\n",
      "Train Epoch: 284 [45056/54000 (83%)] Loss: -1787.950195\n",
      "    epoch          : 284\n",
      "    loss           : -1778.5669705372936\n",
      "    ess            : 8.00118575006161\n",
      "    log_marginal   : 1778.5669705372936\n",
      "    val_loss       : -1783.516866048177\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1783.5168558756511\n",
      "Train Epoch: 285 [0/54000 (0%)] Loss: -1797.980225\n",
      "Train Epoch: 285 [11264/54000 (21%)] Loss: -1793.221436\n",
      "Train Epoch: 285 [22528/54000 (42%)] Loss: -1791.981445\n",
      "Train Epoch: 285 [33792/54000 (63%)] Loss: -1796.472046\n",
      "Train Epoch: 285 [45056/54000 (83%)] Loss: -1787.370361\n",
      "    epoch          : 285\n",
      "    loss           : -1788.6383471219044\n",
      "    ess            : 8.001185525138423\n",
      "    log_marginal   : 1788.6383471219044\n",
      "    val_loss       : -1797.3012084960938\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.3012084960938\n",
      "Train Epoch: 286 [0/54000 (0%)] Loss: -1805.363525\n",
      "Train Epoch: 286 [11264/54000 (21%)] Loss: -1786.020264\n",
      "Train Epoch: 286 [22528/54000 (42%)] Loss: -1791.122803\n",
      "Train Epoch: 286 [33792/54000 (63%)] Loss: -1784.646240\n",
      "Train Epoch: 286 [45056/54000 (83%)] Loss: -1789.662354\n",
      "    epoch          : 286\n",
      "    loss           : -1790.3806255988354\n",
      "    ess            : 8.001185723070828\n",
      "    log_marginal   : 1790.3806255988354\n",
      "    val_loss       : -1797.2358805338542\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1797.2358805338542\n",
      "Train Epoch: 287 [0/54000 (0%)] Loss: -1808.077515\n",
      "Train Epoch: 287 [11264/54000 (21%)] Loss: -1780.195312\n",
      "Train Epoch: 287 [22528/54000 (42%)] Loss: -1781.835083\n",
      "Train Epoch: 287 [33792/54000 (63%)] Loss: -1787.695801\n",
      "Train Epoch: 287 [45056/54000 (83%)] Loss: -1786.386230\n",
      "    epoch          : 287\n",
      "    loss           : -1789.8366952572228\n",
      "    ess            : 8.001185840030885\n",
      "    log_marginal   : 1789.8366952572228\n",
      "    val_loss       : -1797.2903645833333\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1797.2903442382812\n",
      "Train Epoch: 288 [0/54000 (0%)] Loss: -1810.934570\n",
      "Train Epoch: 288 [11264/54000 (21%)] Loss: -1804.558838\n",
      "Train Epoch: 288 [22528/54000 (42%)] Loss: -1810.797363\n",
      "Train Epoch: 288 [33792/54000 (63%)] Loss: -1806.941772\n",
      "Train Epoch: 288 [45056/54000 (83%)] Loss: -1801.525146\n",
      "    epoch          : 288\n",
      "    loss           : -1804.1303192714474\n",
      "    ess            : 8.001185273224452\n",
      "    log_marginal   : 1804.1303192714474\n",
      "    val_loss       : -1806.0853474934895\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1806.0853474934895\n",
      "Train Epoch: 289 [0/54000 (0%)] Loss: -1812.035645\n",
      "Train Epoch: 289 [11264/54000 (21%)] Loss: -1802.735596\n",
      "Train Epoch: 289 [22528/54000 (42%)] Loss: -1785.269897\n",
      "Train Epoch: 289 [33792/54000 (63%)] Loss: -1795.819946\n",
      "Train Epoch: 289 [45056/54000 (83%)] Loss: -1780.973145\n",
      "    epoch          : 289\n",
      "    loss           : -1792.3894227225826\n",
      "    ess            : 8.001185615107698\n",
      "    log_marginal   : 1792.3894227225826\n",
      "    val_loss       : -1792.6636352539062\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1792.6636352539062\n",
      "Train Epoch: 290 [0/54000 (0%)] Loss: -1784.342896\n",
      "Train Epoch: 290 [11264/54000 (21%)] Loss: -1800.651733\n",
      "Train Epoch: 290 [22528/54000 (42%)] Loss: -1791.435059\n",
      "Train Epoch: 290 [33792/54000 (63%)] Loss: -1800.981689\n",
      "Train Epoch: 290 [45056/54000 (83%)] Loss: -1801.489502\n",
      "    epoch          : 290\n",
      "    loss           : -1793.1210281084168\n",
      "    ess            : 8.001185300215235\n",
      "    log_marginal   : 1793.1210281084168\n",
      "    val_loss       : -1803.8558858235676\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1803.8558858235676\n",
      "Train Epoch: 291 [0/54000 (0%)] Loss: -1820.170898\n",
      "Train Epoch: 291 [11264/54000 (21%)] Loss: -1798.880981\n",
      "Train Epoch: 291 [22528/54000 (42%)] Loss: -1791.678345\n",
      "Train Epoch: 291 [33792/54000 (63%)] Loss: -1788.072266\n",
      "Train Epoch: 291 [45056/54000 (83%)] Loss: -1789.604248\n",
      "    epoch          : 291\n",
      "    loss           : -1795.4498129790684\n",
      "    ess            : 8.001184985322773\n",
      "    log_marginal   : 1795.4498129790684\n",
      "    val_loss       : -1800.0038960774739\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1800.00390625\n",
      "Train Epoch: 292 [0/54000 (0%)] Loss: -1813.108887\n",
      "Train Epoch: 292 [11264/54000 (21%)] Loss: -1811.937988\n",
      "Train Epoch: 292 [22528/54000 (42%)] Loss: -1787.703857\n",
      "Train Epoch: 292 [33792/54000 (63%)] Loss: -1792.216675\n",
      "Train Epoch: 292 [45056/54000 (83%)] Loss: -1794.569580\n",
      "    epoch          : 292\n",
      "    loss           : -1795.802126326651\n",
      "    ess            : 8.001185345199874\n",
      "    log_marginal   : 1795.802126326651\n",
      "    val_loss       : -1796.5112202962239\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1796.511210123698\n",
      "Train Epoch: 293 [0/54000 (0%)] Loss: -1804.306641\n",
      "Train Epoch: 293 [11264/54000 (21%)] Loss: -1802.161377\n",
      "Train Epoch: 293 [22528/54000 (42%)] Loss: -1811.472412\n",
      "Train Epoch: 293 [33792/54000 (63%)] Loss: -1790.952148\n",
      "Train Epoch: 293 [45056/54000 (83%)] Loss: -1803.621094\n",
      "    epoch          : 293\n",
      "    loss           : -1800.6755589899028\n",
      "    ess            : 8.001185003316627\n",
      "    log_marginal   : 1800.6755589899028\n",
      "    val_loss       : -1803.279561360677\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1803.279561360677\n",
      "Train Epoch: 294 [0/54000 (0%)] Loss: -1816.143311\n",
      "Train Epoch: 294 [11264/54000 (21%)] Loss: -1799.496338\n",
      "Train Epoch: 294 [22528/54000 (42%)] Loss: -1775.117065\n",
      "Train Epoch: 294 [33792/54000 (63%)] Loss: -1758.846069\n",
      "Train Epoch: 294 [45056/54000 (83%)] Loss: -1770.575928\n",
      "    epoch          : 294\n",
      "    loss           : -1779.0033776625148\n",
      "    ess            : 8.00118583103396\n",
      "    log_marginal   : 1779.0033776625148\n",
      "    val_loss       : -1782.1271057128906\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1782.1271057128906\n",
      "Train Epoch: 295 [0/54000 (0%)] Loss: -1789.559937\n",
      "Train Epoch: 295 [11264/54000 (21%)] Loss: -1784.452393\n",
      "Train Epoch: 295 [22528/54000 (42%)] Loss: -1769.747559\n",
      "Train Epoch: 295 [33792/54000 (63%)] Loss: -1789.807617\n",
      "Train Epoch: 295 [45056/54000 (83%)] Loss: -1785.132690\n",
      "    epoch          : 295\n",
      "    loss           : -1786.5636562131485\n",
      "    ess            : 8.001185372190655\n",
      "    log_marginal   : 1786.5636562131485\n",
      "    val_loss       : -1790.4668070475261\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1790.4668070475261\n",
      "Train Epoch: 296 [0/54000 (0%)] Loss: -1809.524902\n",
      "Train Epoch: 296 [11264/54000 (21%)] Loss: -1772.343018\n",
      "Train Epoch: 296 [22528/54000 (42%)] Loss: -1786.515137\n",
      "Train Epoch: 296 [33792/54000 (63%)] Loss: -1774.711304\n",
      "Train Epoch: 296 [45056/54000 (83%)] Loss: -1784.631104\n",
      "    epoch          : 296\n",
      "    loss           : -1784.058966870578\n",
      "    ess            : 8.001185453163004\n",
      "    log_marginal   : 1784.0589680221844\n",
      "    val_loss       : -1791.857157389323\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1791.857157389323\n",
      "Train Epoch: 297 [0/54000 (0%)] Loss: -1802.784180\n",
      "Train Epoch: 297 [11264/54000 (21%)] Loss: -1796.245239\n",
      "Train Epoch: 297 [22528/54000 (42%)] Loss: -1786.189697\n",
      "Train Epoch: 297 [33792/54000 (63%)] Loss: -1791.697266\n",
      "Train Epoch: 297 [45056/54000 (83%)] Loss: -1799.261475\n",
      "    epoch          : 297\n",
      "    loss           : -1795.1503388026974\n",
      "    ess            : 8.00118513827054\n",
      "    log_marginal   : 1795.1503388026974\n",
      "    val_loss       : -1798.7962748209636\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1798.7962748209636\n",
      "Train Epoch: 298 [0/54000 (0%)] Loss: -1806.144043\n",
      "Train Epoch: 298 [11264/54000 (21%)] Loss: -1801.331055\n",
      "Train Epoch: 298 [22528/54000 (42%)] Loss: -1792.563232\n",
      "Train Epoch: 298 [33792/54000 (63%)] Loss: -1802.165649\n",
      "Train Epoch: 298 [45056/54000 (83%)] Loss: -1785.083740\n",
      "    epoch          : 298\n",
      "    loss           : -1797.0149536132812\n",
      "    ess            : 8.001185363193727\n",
      "    log_marginal   : 1797.0149536132812\n",
      "    val_loss       : -1802.576680501302\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1802.576680501302\n",
      "Train Epoch: 299 [0/54000 (0%)] Loss: -1817.807373\n",
      "Train Epoch: 299 [11264/54000 (21%)] Loss: -1805.381348\n",
      "Train Epoch: 299 [22528/54000 (42%)] Loss: -1800.091064\n",
      "Train Epoch: 299 [33792/54000 (63%)] Loss: -1796.953003\n",
      "Train Epoch: 299 [45056/54000 (83%)] Loss: -1799.821411\n",
      "    epoch          : 299\n",
      "    loss           : -1800.7294749133991\n",
      "    ess            : 8.001185471156859\n",
      "    log_marginal   : 1800.7294737617924\n",
      "    val_loss       : -1793.0098775227864\n",
      "    val_ess        : 8.001185337702433\n",
      "    val_log_marginal: 1793.0098775227864\n",
      "Train Epoch: 300 [0/54000 (0%)] Loss: -1804.451050\n",
      "Train Epoch: 300 [11264/54000 (21%)] Loss: -1754.388916\n",
      "Train Epoch: 300 [22528/54000 (42%)] Loss: -1766.542725\n",
      "Train Epoch: 300 [33792/54000 (63%)] Loss: -1770.441284\n",
      "Train Epoch: 300 [45056/54000 (83%)] Loss: -1794.215454\n",
      "    epoch          : 300\n",
      "    loss           : -1774.4318536722435\n",
      "    ess            : 8.001185615107698\n",
      "    log_marginal   : 1774.4318536722435\n",
      "    val_loss       : -1785.7875874837239\n",
      "    val_ess        : 8.00118581453959\n",
      "    val_log_marginal: 1785.7875874837239\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch300.pth ...\n",
      "Train Epoch: 301 [0/54000 (0%)] Loss: -1764.713257\n",
      "Train Epoch: 301 [11264/54000 (21%)] Loss: -1792.690674\n",
      "Train Epoch: 301 [22528/54000 (42%)] Loss: -1803.428467\n",
      "Train Epoch: 301 [33792/54000 (63%)] Loss: -1787.660645\n",
      "Train Epoch: 301 [45056/54000 (83%)] Loss: -1791.082642\n",
      "    epoch          : 301\n",
      "    loss           : -1790.3473533774322\n",
      "    ess            : 8.001185399181438\n",
      "    log_marginal   : 1790.3473533774322\n",
      "    val_loss       : -1804.4624837239583\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.4624837239583\n",
      "Train Epoch: 302 [0/54000 (0%)] Loss: -1815.919067\n",
      "Train Epoch: 302 [11264/54000 (21%)] Loss: -1781.945801\n",
      "Train Epoch: 302 [22528/54000 (42%)] Loss: -1786.045898\n",
      "Train Epoch: 302 [33792/54000 (63%)] Loss: -1792.776611\n",
      "Train Epoch: 302 [45056/54000 (83%)] Loss: -1791.538330\n",
      "    epoch          : 302\n",
      "    loss           : -1791.5011527583283\n",
      "    ess            : 8.001185615107698\n",
      "    log_marginal   : 1791.5011527583283\n",
      "    val_loss       : -1795.2122701009114\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1795.2122701009114\n",
      "Train Epoch: 303 [0/54000 (0%)] Loss: -1806.762329\n",
      "Train Epoch: 303 [11264/54000 (21%)] Loss: -1800.404785\n",
      "Train Epoch: 303 [22528/54000 (42%)] Loss: -1797.883911\n",
      "Train Epoch: 303 [33792/54000 (63%)] Loss: -1789.577881\n",
      "Train Epoch: 303 [45056/54000 (83%)] Loss: -1795.420654\n",
      "    epoch          : 303\n",
      "    loss           : -1796.7262792047466\n",
      "    ess            : 8.001185705076974\n",
      "    log_marginal   : 1796.726280356353\n",
      "    val_loss       : -1804.0216267903645\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.0216166178386\n",
      "Train Epoch: 304 [0/54000 (0%)] Loss: -1817.267578\n",
      "Train Epoch: 304 [11264/54000 (21%)] Loss: -1798.555908\n",
      "Train Epoch: 304 [22528/54000 (42%)] Loss: -1781.865479\n",
      "Train Epoch: 304 [33792/54000 (63%)] Loss: -1788.371460\n",
      "Train Epoch: 304 [45056/54000 (83%)] Loss: -1786.703857\n",
      "    epoch          : 304\n",
      "    loss           : -1793.7828737654777\n",
      "    ess            : 8.001185507144568\n",
      "    log_marginal   : 1793.7828749170844\n",
      "    val_loss       : -1801.5184631347656\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1801.5184834798176\n",
      "Train Epoch: 305 [0/54000 (0%)] Loss: -1809.871338\n",
      "Train Epoch: 305 [11264/54000 (21%)] Loss: -1805.509033\n",
      "Train Epoch: 305 [22528/54000 (42%)] Loss: -1799.329346\n",
      "Train Epoch: 305 [33792/54000 (63%)] Loss: -1795.094727\n",
      "Train Epoch: 305 [45056/54000 (83%)] Loss: -1802.707031\n",
      "    epoch          : 305\n",
      "    loss           : -1802.1440164817955\n",
      "    ess            : 8.001185156264395\n",
      "    log_marginal   : 1802.1440164817955\n",
      "    val_loss       : -1803.1893107096355\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1803.1893107096355\n",
      "Train Epoch: 306 [0/54000 (0%)] Loss: -1814.160156\n",
      "Train Epoch: 306 [11264/54000 (21%)] Loss: -1801.122559\n",
      "Train Epoch: 306 [22528/54000 (42%)] Loss: -1806.010498\n",
      "Train Epoch: 306 [33792/54000 (63%)] Loss: -1794.403076\n",
      "Train Epoch: 306 [45056/54000 (83%)] Loss: -1794.212524\n",
      "    epoch          : 306\n",
      "    loss           : -1800.7308050191627\n",
      "    ess            : 8.001184679427237\n",
      "    log_marginal   : 1800.7308050191627\n",
      "    val_loss       : -1805.010518391927\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1805.010518391927\n",
      "Train Epoch: 307 [0/54000 (0%)] Loss: -1806.546387\n",
      "Train Epoch: 307 [11264/54000 (21%)] Loss: -1805.885498\n",
      "Train Epoch: 307 [22528/54000 (42%)] Loss: -1794.115723\n",
      "Train Epoch: 307 [33792/54000 (63%)] Loss: -1794.119995\n",
      "Train Epoch: 307 [45056/54000 (83%)] Loss: -1802.726807\n",
      "    epoch          : 307\n",
      "    loss           : -1800.7381419055866\n",
      "    ess            : 8.001185057298192\n",
      "    log_marginal   : 1800.7381419055866\n",
      "    val_loss       : -1803.4746500651042\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1803.4746602376301\n",
      "Train Epoch: 308 [0/54000 (0%)] Loss: -1815.386230\n",
      "Train Epoch: 308 [11264/54000 (21%)] Loss: -1804.817993\n",
      "Train Epoch: 308 [22528/54000 (42%)] Loss: -1798.335815\n",
      "Train Epoch: 308 [33792/54000 (63%)] Loss: -1786.947998\n",
      "Train Epoch: 308 [45056/54000 (83%)] Loss: -1799.601807\n",
      "    epoch          : 308\n",
      "    loss           : -1797.9031659971993\n",
      "    ess            : 8.001185183255178\n",
      "    log_marginal   : 1797.9031659971993\n",
      "    val_loss       : -1803.8925882975261\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1803.892578125\n",
      "Train Epoch: 309 [0/54000 (0%)] Loss: -1808.678467\n",
      "Train Epoch: 309 [11264/54000 (21%)] Loss: -1806.413818\n",
      "Train Epoch: 309 [22528/54000 (42%)] Loss: -1790.788086\n",
      "Train Epoch: 309 [33792/54000 (63%)] Loss: -1798.532959\n",
      "Train Epoch: 309 [45056/54000 (83%)] Loss: -1802.146851\n",
      "    epoch          : 309\n",
      "    loss           : -1802.8423162496315\n",
      "    ess            : 8.001184787390367\n",
      "    log_marginal   : 1802.8423162496315\n",
      "    val_loss       : -1810.4451904296875\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1810.4452107747395\n",
      "Train Epoch: 310 [0/54000 (0%)] Loss: -1813.089600\n",
      "Train Epoch: 310 [11264/54000 (21%)] Loss: -1781.449341\n",
      "Train Epoch: 310 [22528/54000 (42%)] Loss: -1794.284668\n",
      "Train Epoch: 310 [33792/54000 (63%)] Loss: -1805.924194\n",
      "Train Epoch: 310 [45056/54000 (83%)] Loss: -1798.426758\n",
      "    epoch          : 310\n",
      "    loss           : -1799.835469947671\n",
      "    ess            : 8.001184472497904\n",
      "    log_marginal   : 1799.835469947671\n",
      "    val_loss       : -1804.3350321451824\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.3350524902344\n",
      "Train Epoch: 311 [0/54000 (0%)] Loss: -1808.986084\n",
      "Train Epoch: 311 [11264/54000 (21%)] Loss: -1799.437744\n",
      "Train Epoch: 311 [22528/54000 (42%)] Loss: -1802.200928\n",
      "Train Epoch: 311 [33792/54000 (63%)] Loss: -1790.390259\n",
      "Train Epoch: 311 [45056/54000 (83%)] Loss: -1802.743896\n",
      "    epoch          : 311\n",
      "    loss           : -1797.5017930516656\n",
      "    ess            : 8.001184454504049\n",
      "    log_marginal   : 1797.501791900059\n",
      "    val_loss       : -1792.3812052408855\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1792.3812052408855\n",
      "Train Epoch: 312 [0/54000 (0%)] Loss: -1799.641113\n",
      "Train Epoch: 312 [11264/54000 (21%)] Loss: -1792.108887\n",
      "Train Epoch: 312 [22528/54000 (42%)] Loss: -1777.864380\n",
      "Train Epoch: 312 [33792/54000 (63%)] Loss: -1766.528442\n",
      "Train Epoch: 312 [45056/54000 (83%)] Loss: -1784.612549\n",
      "    epoch          : 312\n",
      "    loss           : -1785.8015021558078\n",
      "    ess            : 8.001184859365788\n",
      "    log_marginal   : 1785.8015021558078\n",
      "    val_loss       : -1783.0956929524739\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1783.0956929524739\n",
      "Train Epoch: 313 [0/54000 (0%)] Loss: -1789.681396\n",
      "Train Epoch: 313 [11264/54000 (21%)] Loss: -1769.289185\n",
      "Train Epoch: 313 [22528/54000 (42%)] Loss: -1786.072998\n",
      "Train Epoch: 313 [33792/54000 (63%)] Loss: -1780.898682\n",
      "Train Epoch: 313 [45056/54000 (83%)] Loss: -1797.519287\n",
      "    epoch          : 313\n",
      "    loss           : -1784.6163249465656\n",
      "    ess            : 8.0011846344426\n",
      "    log_marginal   : 1784.6163249465656\n",
      "    val_loss       : -1784.1592915852864\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1784.1592814127605\n",
      "Train Epoch: 314 [0/54000 (0%)] Loss: -1801.511719\n",
      "Train Epoch: 314 [11264/54000 (21%)] Loss: -1801.515503\n",
      "Train Epoch: 314 [22528/54000 (42%)] Loss: -1794.113159\n",
      "Train Epoch: 314 [33792/54000 (63%)] Loss: -1793.986572\n",
      "Train Epoch: 314 [45056/54000 (83%)] Loss: -1798.393555\n",
      "    epoch          : 314\n",
      "    loss           : -1796.6997266085643\n",
      "    ess            : 8.001184391525557\n",
      "    log_marginal   : 1796.6997266085643\n",
      "    val_loss       : -1798.646952311198\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1798.646952311198\n",
      "Train Epoch: 315 [0/54000 (0%)] Loss: -1813.956055\n",
      "Train Epoch: 315 [11264/54000 (21%)] Loss: -1794.494141\n",
      "Train Epoch: 315 [22528/54000 (42%)] Loss: -1802.410889\n",
      "Train Epoch: 315 [33792/54000 (63%)] Loss: -1788.559814\n",
      "Train Epoch: 315 [45056/54000 (83%)] Loss: -1803.580078\n",
      "    epoch          : 315\n",
      "    loss           : -1800.5637679190006\n",
      "    ess            : 8.001184571464107\n",
      "    log_marginal   : 1800.5637679190006\n",
      "    val_loss       : -1803.791280110677\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1803.791280110677\n",
      "Train Epoch: 316 [0/54000 (0%)] Loss: -1812.355225\n",
      "Train Epoch: 316 [11264/54000 (21%)] Loss: -1804.630371\n",
      "Train Epoch: 316 [22528/54000 (42%)] Loss: -1798.226562\n",
      "Train Epoch: 316 [33792/54000 (63%)] Loss: -1802.329590\n",
      "Train Epoch: 316 [45056/54000 (83%)] Loss: -1805.708374\n",
      "    epoch          : 316\n",
      "    loss           : -1801.0062497696786\n",
      "    ess            : 8.001184391525557\n",
      "    log_marginal   : 1801.0062497696786\n",
      "    val_loss       : -1805.1053975423176\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1805.1053975423176\n",
      "Train Epoch: 317 [0/54000 (0%)] Loss: -1808.147461\n",
      "Train Epoch: 317 [11264/54000 (21%)] Loss: -1813.229736\n",
      "Train Epoch: 317 [22528/54000 (42%)] Loss: -1790.590820\n",
      "Train Epoch: 317 [33792/54000 (63%)] Loss: -1798.279663\n",
      "Train Epoch: 317 [45056/54000 (83%)] Loss: -1796.907593\n",
      "    epoch          : 317\n",
      "    loss           : -1802.755835191259\n",
      "    ess            : 8.00118408563002\n",
      "    log_marginal   : 1802.755835191259\n",
      "    val_loss       : -1802.0264180501301\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1802.0264180501301\n",
      "Train Epoch: 318 [0/54000 (0%)] Loss: -1811.283325\n",
      "Train Epoch: 318 [11264/54000 (21%)] Loss: -1807.499023\n",
      "Train Epoch: 318 [22528/54000 (42%)] Loss: -1803.629395\n",
      "Train Epoch: 318 [33792/54000 (63%)] Loss: -1796.998779\n",
      "Train Epoch: 318 [45056/54000 (83%)] Loss: -1789.545898\n",
      "    epoch          : 318\n",
      "    loss           : -1803.4403571362766\n",
      "    ess            : 8.001183662774428\n",
      "    log_marginal   : 1803.4403571362766\n",
      "    val_loss       : -1804.9801940917969\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1804.9801940917969\n",
      "Train Epoch: 319 [0/54000 (0%)] Loss: -1817.932861\n",
      "Train Epoch: 319 [11264/54000 (21%)] Loss: -1796.208984\n",
      "Train Epoch: 319 [22528/54000 (42%)] Loss: -1786.993652\n",
      "Train Epoch: 319 [33792/54000 (63%)] Loss: -1777.732666\n",
      "Train Epoch: 319 [45056/54000 (83%)] Loss: -1763.485107\n",
      "    epoch          : 319\n",
      "    loss           : -1782.1292471255897\n",
      "    ess            : 8.001184553470251\n",
      "    log_marginal   : 1782.1292471255897\n",
      "    val_loss       : -1777.4472147623699\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1777.4472147623699\n",
      "Train Epoch: 320 [0/54000 (0%)] Loss: -1790.363525\n",
      "Train Epoch: 320 [11264/54000 (21%)] Loss: -1802.450928\n",
      "Train Epoch: 320 [22528/54000 (42%)] Loss: -1789.317627\n",
      "Train Epoch: 320 [33792/54000 (63%)] Loss: -1793.585449\n",
      "Train Epoch: 320 [45056/54000 (83%)] Loss: -1793.047485\n",
      "    epoch          : 320\n",
      "    loss           : -1795.8617139132518\n",
      "    ess            : 8.00118459845489\n",
      "    log_marginal   : 1795.8617139132518\n",
      "    val_loss       : -1793.1741231282551\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1793.1741231282551\n",
      "Train Epoch: 321 [0/54000 (0%)] Loss: -1812.798096\n",
      "Train Epoch: 321 [11264/54000 (21%)] Loss: -1810.474121\n",
      "Train Epoch: 321 [22528/54000 (42%)] Loss: -1783.675415\n",
      "Train Epoch: 321 [33792/54000 (63%)] Loss: -1789.393799\n",
      "Train Epoch: 321 [45056/54000 (83%)] Loss: -1787.684204\n",
      "    epoch          : 321\n",
      "    loss           : -1796.7534813071197\n",
      "    ess            : 8.001183923685327\n",
      "    log_marginal   : 1796.7534813071197\n",
      "    val_loss       : -1799.1425069173176\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1799.1425069173176\n",
      "Train Epoch: 322 [0/54000 (0%)] Loss: -1806.736816\n",
      "Train Epoch: 322 [11264/54000 (21%)] Loss: -1790.720947\n",
      "Train Epoch: 322 [22528/54000 (42%)] Loss: -1791.786133\n",
      "Train Epoch: 322 [33792/54000 (63%)] Loss: -1788.120117\n",
      "Train Epoch: 322 [45056/54000 (83%)] Loss: -1795.095459\n",
      "    epoch          : 322\n",
      "    loss           : -1794.268152776754\n",
      "    ess            : 8.001184589457962\n",
      "    log_marginal   : 1794.268152776754\n",
      "    val_loss       : -1800.4494934082031\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1800.4494934082031\n",
      "Train Epoch: 323 [0/54000 (0%)] Loss: -1809.529907\n",
      "Train Epoch: 323 [11264/54000 (21%)] Loss: -1821.026367\n",
      "Train Epoch: 323 [22528/54000 (42%)] Loss: -1800.303711\n",
      "Train Epoch: 323 [33792/54000 (63%)] Loss: -1804.967041\n",
      "Train Epoch: 323 [45056/54000 (83%)] Loss: -1788.269165\n",
      "    epoch          : 323\n",
      "    loss           : -1797.8649994472287\n",
      "    ess            : 8.001184265568572\n",
      "    log_marginal   : 1797.8650005988354\n",
      "    val_loss       : -1796.8824869791667\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1796.8824971516926\n",
      "Train Epoch: 324 [0/54000 (0%)] Loss: -1802.039551\n",
      "Train Epoch: 324 [11264/54000 (21%)] Loss: -1808.309692\n",
      "Train Epoch: 324 [22528/54000 (42%)] Loss: -1793.989990\n",
      "Train Epoch: 324 [33792/54000 (63%)] Loss: -1796.058960\n",
      "Train Epoch: 324 [45056/54000 (83%)] Loss: -1805.638184\n",
      "    epoch          : 324\n",
      "    loss           : -1798.2754343860554\n",
      "    ess            : 8.001185093285903\n",
      "    log_marginal   : 1798.2754343860554\n",
      "    val_loss       : -1808.5052591959636\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1808.5052591959636\n",
      "Train Epoch: 325 [0/54000 (0%)] Loss: -1812.773926\n",
      "Train Epoch: 325 [11264/54000 (21%)] Loss: -1794.793213\n",
      "Train Epoch: 325 [22528/54000 (42%)] Loss: -1793.714600\n",
      "Train Epoch: 325 [33792/54000 (63%)] Loss: -1784.317627\n",
      "Train Epoch: 325 [45056/54000 (83%)] Loss: -1793.776611\n",
      "    epoch          : 325\n",
      "    loss           : -1798.411140873747\n",
      "    ess            : 8.001184166602368\n",
      "    log_marginal   : 1798.411140873747\n",
      "    val_loss       : -1804.2679239908855\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1804.2679443359375\n",
      "Train Epoch: 326 [0/54000 (0%)] Loss: -1820.728271\n",
      "Train Epoch: 326 [11264/54000 (21%)] Loss: -1807.361816\n",
      "Train Epoch: 326 [22528/54000 (42%)] Loss: -1786.375000\n",
      "Train Epoch: 326 [33792/54000 (63%)] Loss: -1805.271973\n",
      "Train Epoch: 326 [45056/54000 (83%)] Loss: -1807.193481\n",
      "    epoch          : 326\n",
      "    loss           : -1802.106730910967\n",
      "    ess            : 8.001183752743703\n",
      "    log_marginal   : 1802.106730910967\n",
      "    val_loss       : -1807.0852762858074\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.0852457682292\n",
      "Train Epoch: 327 [0/54000 (0%)] Loss: -1808.995117\n",
      "Train Epoch: 327 [11264/54000 (21%)] Loss: -1795.863770\n",
      "Train Epoch: 327 [22528/54000 (42%)] Loss: -1787.926514\n",
      "Train Epoch: 327 [33792/54000 (63%)] Loss: -1768.181396\n",
      "Train Epoch: 327 [45056/54000 (83%)] Loss: -1774.138916\n",
      "    epoch          : 327\n",
      "    loss           : -1781.6505633660083\n",
      "    ess            : 8.001185057298192\n",
      "    log_marginal   : 1781.6505633660083\n",
      "    val_loss       : -1793.3043619791667\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1793.3043619791667\n",
      "Train Epoch: 328 [0/54000 (0%)] Loss: -1803.441040\n",
      "Train Epoch: 328 [11264/54000 (21%)] Loss: -1791.243774\n",
      "Train Epoch: 328 [22528/54000 (42%)] Loss: -1789.238281\n",
      "Train Epoch: 328 [33792/54000 (63%)] Loss: -1801.462402\n",
      "Train Epoch: 328 [45056/54000 (83%)] Loss: -1794.925293\n",
      "    epoch          : 328\n",
      "    loss           : -1793.1913693985848\n",
      "    ess            : 8.001184724411875\n",
      "    log_marginal   : 1793.1913693985848\n",
      "    val_loss       : -1800.7068888346355\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1800.7068888346355\n",
      "Train Epoch: 329 [0/54000 (0%)] Loss: -1811.638306\n",
      "Train Epoch: 329 [11264/54000 (21%)] Loss: -1808.486816\n",
      "Train Epoch: 329 [22528/54000 (42%)] Loss: -1797.699463\n",
      "Train Epoch: 329 [33792/54000 (63%)] Loss: -1802.697876\n",
      "Train Epoch: 329 [45056/54000 (83%)] Loss: -1787.623413\n",
      "    epoch          : 329\n",
      "    loss           : -1797.4340912441037\n",
      "    ess            : 8.00118485036886\n",
      "    log_marginal   : 1797.4340912441037\n",
      "    val_loss       : -1805.9691975911458\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1805.9691975911458\n",
      "Train Epoch: 330 [0/54000 (0%)] Loss: -1816.180908\n",
      "Train Epoch: 330 [11264/54000 (21%)] Loss: -1813.077148\n",
      "Train Epoch: 330 [22528/54000 (42%)] Loss: -1798.812012\n",
      "Train Epoch: 330 [33792/54000 (63%)] Loss: -1795.032715\n",
      "Train Epoch: 330 [45056/54000 (83%)] Loss: -1797.825439\n",
      "    epoch          : 330\n",
      "    loss           : -1803.0457613963001\n",
      "    ess            : 8.001184679427237\n",
      "    log_marginal   : 1803.0457613963001\n",
      "    val_loss       : -1802.5062255859375\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1802.5062154134114\n",
      "Train Epoch: 331 [0/54000 (0%)] Loss: -1803.860962\n",
      "Train Epoch: 331 [11264/54000 (21%)] Loss: -1804.334229\n",
      "Train Epoch: 331 [22528/54000 (42%)] Loss: -1797.539795\n",
      "Train Epoch: 331 [33792/54000 (63%)] Loss: -1801.589355\n",
      "Train Epoch: 331 [45056/54000 (83%)] Loss: -1795.154785\n",
      "    epoch          : 331\n",
      "    loss           : -1801.954088894826\n",
      "    ess            : 8.001184679427237\n",
      "    log_marginal   : 1801.9540877432194\n",
      "    val_loss       : -1807.4596252441406\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1807.4596252441406\n",
      "Train Epoch: 332 [0/54000 (0%)] Loss: -1814.666504\n",
      "Train Epoch: 332 [11264/54000 (21%)] Loss: -1781.886353\n",
      "Train Epoch: 332 [22528/54000 (42%)] Loss: -1795.116455\n",
      "Train Epoch: 332 [33792/54000 (63%)] Loss: -1797.939209\n",
      "Train Epoch: 332 [45056/54000 (83%)] Loss: -1798.659668\n",
      "    epoch          : 332\n",
      "    loss           : -1795.3581704193691\n",
      "    ess            : 8.001184661433381\n",
      "    log_marginal   : 1795.3581704193691\n",
      "    val_loss       : -1798.7495829264324\n",
      "    val_ess        : 8.001183907190958\n",
      "    val_log_marginal: 1798.7495829264324\n",
      "Train Epoch: 333 [0/54000 (0%)] Loss: -1802.944580\n",
      "Train Epoch: 333 [11264/54000 (21%)] Loss: -1787.482666\n",
      "Train Epoch: 333 [22528/54000 (42%)] Loss: -1808.983887\n",
      "Train Epoch: 333 [33792/54000 (63%)] Loss: -1792.385254\n",
      "Train Epoch: 333 [45056/54000 (83%)] Loss: -1781.963867\n",
      "    epoch          : 333\n",
      "    loss           : -1792.5192180129718\n",
      "    ess            : 8.001184787390367\n",
      "    log_marginal   : 1792.5192191645783\n",
      "    val_loss       : -1787.093505859375\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1787.093505859375\n",
      "Train Epoch: 334 [0/54000 (0%)] Loss: -1793.098145\n",
      "Train Epoch: 334 [11264/54000 (21%)] Loss: -1788.571777\n",
      "Train Epoch: 334 [22528/54000 (42%)] Loss: -1787.103271\n",
      "Train Epoch: 334 [33792/54000 (63%)] Loss: -1787.604004\n",
      "Train Epoch: 334 [45056/54000 (83%)] Loss: -1792.170898\n",
      "    epoch          : 334\n",
      "    loss           : -1790.2820215765034\n",
      "    ess            : 8.00118521024596\n",
      "    log_marginal   : 1790.2820215765034\n",
      "    val_loss       : -1787.7864379882812\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1787.7864379882812\n",
      "Train Epoch: 335 [0/54000 (0%)] Loss: -1811.439941\n",
      "Train Epoch: 335 [11264/54000 (21%)] Loss: -1796.368896\n",
      "Train Epoch: 335 [22528/54000 (42%)] Loss: -1798.331909\n",
      "Train Epoch: 335 [33792/54000 (63%)] Loss: -1805.653809\n",
      "Train Epoch: 335 [45056/54000 (83%)] Loss: -1801.510620\n",
      "    epoch          : 335\n",
      "    loss           : -1798.875397304319\n",
      "    ess            : 8.001184769396511\n",
      "    log_marginal   : 1798.8753961527123\n",
      "    val_loss       : -1806.1389872233074\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1806.1389872233074\n",
      "Train Epoch: 336 [0/54000 (0%)] Loss: -1814.440430\n",
      "Train Epoch: 336 [11264/54000 (21%)] Loss: -1808.925049\n",
      "Train Epoch: 336 [22528/54000 (42%)] Loss: -1803.095215\n",
      "Train Epoch: 336 [33792/54000 (63%)] Loss: -1799.638428\n",
      "Train Epoch: 336 [45056/54000 (83%)] Loss: -1800.402832\n",
      "    epoch          : 336\n",
      "    loss           : -1797.91803899801\n",
      "    ess            : 8.001185003316627\n",
      "    log_marginal   : 1797.91803899801\n",
      "    val_loss       : -1777.7129007975261\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1777.7129007975261\n",
      "Train Epoch: 337 [0/54000 (0%)] Loss: -1788.384521\n",
      "Train Epoch: 337 [11264/54000 (21%)] Loss: -1786.186401\n",
      "Train Epoch: 337 [22528/54000 (42%)] Loss: -1757.294678\n",
      "Train Epoch: 337 [33792/54000 (63%)] Loss: -1787.844604\n",
      "Train Epoch: 337 [45056/54000 (83%)] Loss: -1780.195312\n",
      "    epoch          : 337\n",
      "    loss           : -1774.1500094431751\n",
      "    ess            : 8.001185489150712\n",
      "    log_marginal   : 1774.1500082915684\n",
      "    val_loss       : -1782.5838724772136\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1782.5838724772136\n",
      "Train Epoch: 338 [0/54000 (0%)] Loss: -1790.701416\n",
      "Train Epoch: 338 [11264/54000 (21%)] Loss: -1795.478516\n",
      "Train Epoch: 338 [22528/54000 (42%)] Loss: -1795.857300\n",
      "Train Epoch: 338 [33792/54000 (63%)] Loss: -1795.871582\n",
      "Train Epoch: 338 [45056/54000 (83%)] Loss: -1778.459961\n",
      "    epoch          : 338\n",
      "    loss           : -1791.194600807046\n",
      "    ess            : 8.00118506629512\n",
      "    log_marginal   : 1791.1946019586528\n",
      "    val_loss       : -1789.4883728027344\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1789.4883728027344\n",
      "Train Epoch: 339 [0/54000 (0%)] Loss: -1817.448730\n",
      "Train Epoch: 339 [11264/54000 (21%)] Loss: -1804.501221\n",
      "Train Epoch: 339 [22528/54000 (42%)] Loss: -1786.978271\n",
      "Train Epoch: 339 [33792/54000 (63%)] Loss: -1796.684570\n",
      "Train Epoch: 339 [45056/54000 (83%)] Loss: -1785.317627\n",
      "    epoch          : 339\n",
      "    loss           : -1796.6025229400059\n",
      "    ess            : 8.001185075292048\n",
      "    log_marginal   : 1796.6025229400059\n",
      "    val_loss       : -1798.3857727050781\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1798.3857930501301\n",
      "Train Epoch: 340 [0/54000 (0%)] Loss: -1809.107178\n",
      "Train Epoch: 340 [11264/54000 (21%)] Loss: -1811.247070\n",
      "Train Epoch: 340 [22528/54000 (42%)] Loss: -1812.961670\n",
      "Train Epoch: 340 [33792/54000 (63%)] Loss: -1801.551270\n",
      "Train Epoch: 340 [45056/54000 (83%)] Loss: -1797.487793\n",
      "    epoch          : 340\n",
      "    loss           : -1803.2207169442806\n",
      "    ess            : 8.001184967328918\n",
      "    log_marginal   : 1803.2207169442806\n",
      "    val_loss       : -1800.2226053873699\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1800.2225850423176\n",
      "Train Epoch: 341 [0/54000 (0%)] Loss: -1809.320923\n",
      "Train Epoch: 341 [11264/54000 (21%)] Loss: -1807.456543\n",
      "Train Epoch: 341 [22528/54000 (42%)] Loss: -1799.429688\n",
      "Train Epoch: 341 [33792/54000 (63%)] Loss: -1789.328125\n",
      "Train Epoch: 341 [45056/54000 (83%)] Loss: -1803.582397\n",
      "    epoch          : 341\n",
      "    loss           : -1798.9087109835643\n",
      "    ess            : 8.001185327206018\n",
      "    log_marginal   : 1798.908712135171\n",
      "    val_loss       : -1807.6415608723958\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1807.6415506998699\n",
      "Train Epoch: 342 [0/54000 (0%)] Loss: -1822.464600\n",
      "Train Epoch: 342 [11264/54000 (21%)] Loss: -1800.806885\n",
      "Train Epoch: 342 [22528/54000 (42%)] Loss: -1791.509766\n",
      "Train Epoch: 342 [33792/54000 (63%)] Loss: -1794.521729\n",
      "Train Epoch: 342 [45056/54000 (83%)] Loss: -1796.952393\n",
      "    epoch          : 342\n",
      "    loss           : -1797.5479436910377\n",
      "    ess            : 8.001184823378077\n",
      "    log_marginal   : 1797.5479436910377\n",
      "    val_loss       : -1799.2303975423176\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.2303975423176\n",
      "Train Epoch: 343 [0/54000 (0%)] Loss: -1818.870361\n",
      "Train Epoch: 343 [11264/54000 (21%)] Loss: -1801.996826\n",
      "Train Epoch: 343 [22528/54000 (42%)] Loss: -1794.252441\n",
      "Train Epoch: 343 [33792/54000 (63%)] Loss: -1792.181396\n",
      "Train Epoch: 343 [45056/54000 (83%)] Loss: -1793.855835\n",
      "    epoch          : 343\n",
      "    loss           : -1796.7571699034493\n",
      "    ess            : 8.001184760399585\n",
      "    log_marginal   : 1796.7571699034493\n",
      "    val_loss       : -1801.0845438639324\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1801.0845438639324\n",
      "Train Epoch: 344 [0/54000 (0%)] Loss: -1811.729126\n",
      "Train Epoch: 344 [11264/54000 (21%)] Loss: -1811.794189\n",
      "Train Epoch: 344 [22528/54000 (42%)] Loss: -1804.559204\n",
      "Train Epoch: 344 [33792/54000 (63%)] Loss: -1791.074707\n",
      "Train Epoch: 344 [45056/54000 (83%)] Loss: -1801.201172\n",
      "    epoch          : 344\n",
      "    loss           : -1801.7441602023143\n",
      "    ess            : 8.001184310553208\n",
      "    log_marginal   : 1801.7441602023143\n",
      "    val_loss       : -1787.1523844401042\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1787.1523844401042\n",
      "Train Epoch: 345 [0/54000 (0%)] Loss: -1796.337646\n",
      "Train Epoch: 345 [11264/54000 (21%)] Loss: -1754.868042\n",
      "Train Epoch: 345 [22528/54000 (42%)] Loss: -1779.350830\n",
      "Train Epoch: 345 [33792/54000 (63%)] Loss: -1780.465576\n",
      "Train Epoch: 345 [45056/54000 (83%)] Loss: -1800.280151\n",
      "    epoch          : 345\n",
      "    loss           : -1779.2388628113945\n",
      "    ess            : 8.001185372190655\n",
      "    log_marginal   : 1779.2388628113945\n",
      "    val_loss       : -1795.6113789876301\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1795.6113789876301\n",
      "Train Epoch: 346 [0/54000 (0%)] Loss: -1792.022705\n",
      "Train Epoch: 346 [11264/54000 (21%)] Loss: -1793.612305\n",
      "Train Epoch: 346 [22528/54000 (42%)] Loss: -1785.023438\n",
      "Train Epoch: 346 [33792/54000 (63%)] Loss: -1790.871826\n",
      "Train Epoch: 346 [45056/54000 (83%)] Loss: -1792.841309\n",
      "    epoch          : 346\n",
      "    loss           : -1789.084625819944\n",
      "    ess            : 8.001184481494832\n",
      "    log_marginal   : 1789.084625819944\n",
      "    val_loss       : -1800.0472310384114\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1800.0472310384114\n",
      "Train Epoch: 347 [0/54000 (0%)] Loss: -1800.504395\n",
      "Train Epoch: 347 [11264/54000 (21%)] Loss: -1794.975464\n",
      "Train Epoch: 347 [22528/54000 (42%)] Loss: -1789.875488\n",
      "Train Epoch: 347 [33792/54000 (63%)] Loss: -1789.241455\n",
      "Train Epoch: 347 [45056/54000 (83%)] Loss: -1784.898926\n",
      "    epoch          : 347\n",
      "    loss           : -1789.3212568175118\n",
      "    ess            : 8.001185039304337\n",
      "    log_marginal   : 1789.3212568175118\n",
      "    val_loss       : -1795.3757019042969\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1795.3757019042969\n",
      "Train Epoch: 348 [0/54000 (0%)] Loss: -1800.511230\n",
      "Train Epoch: 348 [11264/54000 (21%)] Loss: -1805.489136\n",
      "Train Epoch: 348 [22528/54000 (42%)] Loss: -1799.596436\n",
      "Train Epoch: 348 [33792/54000 (63%)] Loss: -1805.046021\n",
      "Train Epoch: 348 [45056/54000 (83%)] Loss: -1797.373413\n",
      "    epoch          : 348\n",
      "    loss           : -1797.7133489644752\n",
      "    ess            : 8.001184679427237\n",
      "    log_marginal   : 1797.7133489644752\n",
      "    val_loss       : -1795.7271830240886\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1795.7271728515625\n",
      "Train Epoch: 349 [0/54000 (0%)] Loss: -1804.486328\n",
      "Train Epoch: 349 [11264/54000 (21%)] Loss: -1805.315308\n",
      "Train Epoch: 349 [22528/54000 (42%)] Loss: -1778.883789\n",
      "Train Epoch: 349 [33792/54000 (63%)] Loss: -1790.096802\n",
      "Train Epoch: 349 [45056/54000 (83%)] Loss: -1790.248535\n",
      "    epoch          : 349\n",
      "    loss           : -1796.871073021079\n",
      "    ess            : 8.001184760399585\n",
      "    log_marginal   : 1796.871073021079\n",
      "    val_loss       : -1797.5033264160156\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1797.5033264160156\n",
      "Train Epoch: 350 [0/54000 (0%)] Loss: -1804.646729\n",
      "Train Epoch: 350 [11264/54000 (21%)] Loss: -1803.755493\n",
      "Train Epoch: 350 [22528/54000 (42%)] Loss: -1796.491211\n",
      "Train Epoch: 350 [33792/54000 (63%)] Loss: -1803.099365\n",
      "Train Epoch: 350 [45056/54000 (83%)] Loss: -1785.251587\n",
      "    epoch          : 350\n",
      "    loss           : -1802.7860683225235\n",
      "    ess            : 8.001184787390367\n",
      "    log_marginal   : 1802.7860683225235\n",
      "    val_loss       : -1799.7601420084636\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1799.7601420084636\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch350.pth ...\n",
      "Train Epoch: 351 [0/54000 (0%)] Loss: -1805.481079\n",
      "Train Epoch: 351 [11264/54000 (21%)] Loss: -1794.274536\n",
      "Train Epoch: 351 [22528/54000 (42%)] Loss: -1788.908691\n",
      "Train Epoch: 351 [33792/54000 (63%)] Loss: -1803.236084\n",
      "Train Epoch: 351 [45056/54000 (83%)] Loss: -1797.764038\n",
      "    epoch          : 351\n",
      "    loss           : -1799.232583099941\n",
      "    ess            : 8.001184823378077\n",
      "    log_marginal   : 1799.2325819483344\n",
      "    val_loss       : -1808.4439798990886\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1808.4439798990886\n",
      "Train Epoch: 352 [0/54000 (0%)] Loss: -1814.890869\n",
      "Train Epoch: 352 [11264/54000 (21%)] Loss: -1802.905640\n",
      "Train Epoch: 352 [22528/54000 (42%)] Loss: -1792.257690\n",
      "Train Epoch: 352 [33792/54000 (63%)] Loss: -1794.019287\n",
      "Train Epoch: 352 [45056/54000 (83%)] Loss: -1795.186035\n",
      "    epoch          : 352\n",
      "    loss           : -1798.7978538657135\n",
      "    ess            : 8.00118474240573\n",
      "    log_marginal   : 1798.7978527141067\n",
      "    val_loss       : -1801.033467610677\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.033467610677\n",
      "Train Epoch: 353 [0/54000 (0%)] Loss: -1810.427979\n",
      "Train Epoch: 353 [11264/54000 (21%)] Loss: -1805.020996\n",
      "Train Epoch: 353 [22528/54000 (42%)] Loss: -1803.552490\n",
      "Train Epoch: 353 [33792/54000 (63%)] Loss: -1800.731689\n",
      "Train Epoch: 353 [45056/54000 (83%)] Loss: -1806.481812\n",
      "    epoch          : 353\n",
      "    loss           : -1805.3106401551445\n",
      "    ess            : 8.001184391525557\n",
      "    log_marginal   : 1805.3106390035377\n",
      "    val_loss       : -1807.2074890136719\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.2074890136719\n",
      "Train Epoch: 354 [0/54000 (0%)] Loss: -1815.794189\n",
      "Train Epoch: 354 [11264/54000 (21%)] Loss: -1794.659912\n",
      "Train Epoch: 354 [22528/54000 (42%)] Loss: -1795.745361\n",
      "Train Epoch: 354 [33792/54000 (63%)] Loss: -1791.996460\n",
      "Train Epoch: 354 [45056/54000 (83%)] Loss: -1787.465820\n",
      "    epoch          : 354\n",
      "    loss           : -1798.671374051076\n",
      "    ess            : 8.001184913347352\n",
      "    log_marginal   : 1798.671374051076\n",
      "    val_loss       : -1806.0198160807292\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1806.0198160807292\n",
      "Train Epoch: 355 [0/54000 (0%)] Loss: -1812.672607\n",
      "Train Epoch: 355 [11264/54000 (21%)] Loss: -1798.697021\n",
      "Train Epoch: 355 [22528/54000 (42%)] Loss: -1792.516846\n",
      "Train Epoch: 355 [33792/54000 (63%)] Loss: -1772.591309\n",
      "Train Epoch: 355 [45056/54000 (83%)] Loss: -1789.048828\n",
      "    epoch          : 355\n",
      "    loss           : -1794.085651901533\n",
      "    ess            : 8.001184841371932\n",
      "    log_marginal   : 1794.085651901533\n",
      "    val_loss       : -1802.015889485677\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1802.015889485677\n",
      "Train Epoch: 356 [0/54000 (0%)] Loss: -1800.738037\n",
      "Train Epoch: 356 [11264/54000 (21%)] Loss: -1796.824707\n",
      "Train Epoch: 356 [22528/54000 (42%)] Loss: -1809.220093\n",
      "Train Epoch: 356 [33792/54000 (63%)] Loss: -1802.811768\n",
      "Train Epoch: 356 [45056/54000 (83%)] Loss: -1803.564575\n",
      "    epoch          : 356\n",
      "    loss           : -1802.8597665462853\n",
      "    ess            : 8.00118419359315\n",
      "    log_marginal   : 1802.8597665462853\n",
      "    val_loss       : -1807.4476318359375\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1807.4476318359375\n",
      "Train Epoch: 357 [0/54000 (0%)] Loss: -1812.649170\n",
      "Train Epoch: 357 [11264/54000 (21%)] Loss: -1819.658936\n",
      "Train Epoch: 357 [22528/54000 (42%)] Loss: -1792.926270\n",
      "Train Epoch: 357 [33792/54000 (63%)] Loss: -1791.777344\n",
      "Train Epoch: 357 [45056/54000 (83%)] Loss: -1797.774902\n",
      "    epoch          : 357\n",
      "    loss           : -1803.848020157724\n",
      "    ess            : 8.001184013654601\n",
      "    log_marginal   : 1803.8480213093308\n",
      "    val_loss       : -1804.317403157552\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1804.3174235026042\n",
      "Train Epoch: 358 [0/54000 (0%)] Loss: -1804.203979\n",
      "Train Epoch: 358 [11264/54000 (21%)] Loss: -1805.352417\n",
      "Train Epoch: 358 [22528/54000 (42%)] Loss: -1804.022339\n",
      "Train Epoch: 358 [33792/54000 (63%)] Loss: -1790.357056\n",
      "Train Epoch: 358 [45056/54000 (83%)] Loss: -1798.428223\n",
      "    epoch          : 358\n",
      "    loss           : -1800.5962052255306\n",
      "    ess            : 8.001184472497904\n",
      "    log_marginal   : 1800.5962052255306\n",
      "    val_loss       : -1808.777364095052\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1808.7773844401042\n",
      "Train Epoch: 359 [0/54000 (0%)] Loss: -1812.418457\n",
      "Train Epoch: 359 [11264/54000 (21%)] Loss: -1802.839844\n",
      "Train Epoch: 359 [22528/54000 (42%)] Loss: -1788.564941\n",
      "Train Epoch: 359 [33792/54000 (63%)] Loss: -1793.325195\n",
      "Train Epoch: 359 [45056/54000 (83%)] Loss: -1805.041504\n",
      "    epoch          : 359\n",
      "    loss           : -1800.5954774100826\n",
      "    ess            : 8.001183572805152\n",
      "    log_marginal   : 1800.5954762584759\n",
      "    val_loss       : -1802.9602762858074\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.9602762858074\n",
      "Train Epoch: 360 [0/54000 (0%)] Loss: -1817.976318\n",
      "Train Epoch: 360 [11264/54000 (21%)] Loss: -1797.567139\n",
      "Train Epoch: 360 [22528/54000 (42%)] Loss: -1795.549072\n",
      "Train Epoch: 360 [33792/54000 (63%)] Loss: -1763.425415\n",
      "Train Epoch: 360 [45056/54000 (83%)] Loss: -1774.839600\n",
      "    epoch          : 360\n",
      "    loss           : -1786.2624523234817\n",
      "    ess            : 8.001184841371932\n",
      "    log_marginal   : 1786.262451171875\n",
      "    val_loss       : -1784.2315165201824\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1784.2315165201824\n",
      "Train Epoch: 361 [0/54000 (0%)] Loss: -1787.274292\n",
      "Train Epoch: 361 [11264/54000 (21%)] Loss: -1791.367798\n",
      "Train Epoch: 361 [22528/54000 (42%)] Loss: -1795.991699\n",
      "Train Epoch: 361 [33792/54000 (63%)] Loss: -1792.341797\n",
      "Train Epoch: 361 [45056/54000 (83%)] Loss: -1800.463867\n",
      "    epoch          : 361\n",
      "    loss           : -1796.344300468013\n",
      "    ess            : 8.001185093285903\n",
      "    log_marginal   : 1796.344300468013\n",
      "    val_loss       : -1798.4534708658855\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1798.4534708658855\n",
      "Train Epoch: 362 [0/54000 (0%)] Loss: -1810.152588\n",
      "Train Epoch: 362 [11264/54000 (21%)] Loss: -1815.115234\n",
      "Train Epoch: 362 [22528/54000 (42%)] Loss: -1798.577148\n",
      "Train Epoch: 362 [33792/54000 (63%)] Loss: -1790.830200\n",
      "Train Epoch: 362 [45056/54000 (83%)] Loss: -1797.576782\n",
      "    epoch          : 362\n",
      "    loss           : -1798.6069946289062\n",
      "    ess            : 8.001184481494832\n",
      "    log_marginal   : 1798.6069934772995\n",
      "    val_loss       : -1799.2000020345051\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1799.1999816894531\n",
      "Train Epoch: 363 [0/54000 (0%)] Loss: -1813.149170\n",
      "Train Epoch: 363 [11264/54000 (21%)] Loss: -1809.120239\n",
      "Train Epoch: 363 [22528/54000 (42%)] Loss: -1793.023193\n",
      "Train Epoch: 363 [33792/54000 (63%)] Loss: -1796.998291\n",
      "Train Epoch: 363 [45056/54000 (83%)] Loss: -1796.098145\n",
      "    epoch          : 363\n",
      "    loss           : -1799.6988398713886\n",
      "    ess            : 8.001185147267467\n",
      "    log_marginal   : 1799.6988398713886\n",
      "    val_loss       : -1805.6574198404949\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1805.6573994954426\n",
      "Train Epoch: 364 [0/54000 (0%)] Loss: -1811.765381\n",
      "Train Epoch: 364 [11264/54000 (21%)] Loss: -1799.205322\n",
      "Train Epoch: 364 [22528/54000 (42%)] Loss: -1787.462769\n",
      "Train Epoch: 364 [33792/54000 (63%)] Loss: -1787.016113\n",
      "Train Epoch: 364 [45056/54000 (83%)] Loss: -1778.883789\n",
      "    epoch          : 364\n",
      "    loss           : -1794.591834878022\n",
      "    ess            : 8.001185237236744\n",
      "    log_marginal   : 1794.591834878022\n",
      "    val_loss       : -1799.9697875976562\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.9697875976562\n",
      "Train Epoch: 365 [0/54000 (0%)] Loss: -1811.949585\n",
      "Train Epoch: 365 [11264/54000 (21%)] Loss: -1807.822754\n",
      "Train Epoch: 365 [22528/54000 (42%)] Loss: -1809.732910\n",
      "Train Epoch: 365 [33792/54000 (63%)] Loss: -1801.997314\n",
      "Train Epoch: 365 [45056/54000 (83%)] Loss: -1793.974976\n",
      "    epoch          : 365\n",
      "    loss           : -1803.8096543797906\n",
      "    ess            : 8.001184805384222\n",
      "    log_marginal   : 1803.8096543797906\n",
      "    val_loss       : -1805.0448506673176\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.0448303222656\n",
      "Train Epoch: 366 [0/54000 (0%)] Loss: -1807.286377\n",
      "Train Epoch: 366 [11264/54000 (21%)] Loss: -1792.609009\n",
      "Train Epoch: 366 [22528/54000 (42%)] Loss: -1797.601440\n",
      "Train Epoch: 366 [33792/54000 (63%)] Loss: -1789.322266\n",
      "Train Epoch: 366 [45056/54000 (83%)] Loss: -1782.627441\n",
      "    epoch          : 366\n",
      "    loss           : -1794.943632305793\n",
      "    ess            : 8.001185435169148\n",
      "    log_marginal   : 1794.943632305793\n",
      "    val_loss       : -1801.579833984375\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.579833984375\n",
      "Train Epoch: 367 [0/54000 (0%)] Loss: -1815.759644\n",
      "Train Epoch: 367 [11264/54000 (21%)] Loss: -1797.891602\n",
      "Train Epoch: 367 [22528/54000 (42%)] Loss: -1789.530273\n",
      "Train Epoch: 367 [33792/54000 (63%)] Loss: -1783.523560\n",
      "Train Epoch: 367 [45056/54000 (83%)] Loss: -1784.720703\n",
      "    epoch          : 367\n",
      "    loss           : -1794.2940443506782\n",
      "    ess            : 8.001184787390367\n",
      "    log_marginal   : 1794.2940443506782\n",
      "    val_loss       : -1798.3016866048176\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1798.3016866048176\n",
      "Train Epoch: 368 [0/54000 (0%)] Loss: -1799.001831\n",
      "Train Epoch: 368 [11264/54000 (21%)] Loss: -1796.241455\n",
      "Train Epoch: 368 [22528/54000 (42%)] Loss: -1798.635376\n",
      "Train Epoch: 368 [33792/54000 (63%)] Loss: -1804.655884\n",
      "Train Epoch: 368 [45056/54000 (83%)] Loss: -1796.894409\n",
      "    epoch          : 368\n",
      "    loss           : -1797.0564655807782\n",
      "    ess            : 8.001184589457962\n",
      "    log_marginal   : 1797.0564655807782\n",
      "    val_loss       : -1805.2443542480469\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.2443542480469\n",
      "Train Epoch: 369 [0/54000 (0%)] Loss: -1812.613647\n",
      "Train Epoch: 369 [11264/54000 (21%)] Loss: -1807.533936\n",
      "Train Epoch: 369 [22528/54000 (42%)] Loss: -1798.662842\n",
      "Train Epoch: 369 [33792/54000 (63%)] Loss: -1799.736572\n",
      "Train Epoch: 369 [45056/54000 (83%)] Loss: -1807.564941\n",
      "    epoch          : 369\n",
      "    loss           : -1804.5526157595077\n",
      "    ess            : 8.001183536817443\n",
      "    log_marginal   : 1804.5526157595077\n",
      "    val_loss       : -1801.3025919596355\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1801.3025817871094\n",
      "Train Epoch: 370 [0/54000 (0%)] Loss: -1813.266479\n",
      "Train Epoch: 370 [11264/54000 (21%)] Loss: -1793.213013\n",
      "Train Epoch: 370 [22528/54000 (42%)] Loss: -1801.370361\n",
      "Train Epoch: 370 [33792/54000 (63%)] Loss: -1781.880737\n",
      "Train Epoch: 370 [45056/54000 (83%)] Loss: -1793.460205\n",
      "    epoch          : 370\n",
      "    loss           : -1797.2931679779629\n",
      "    ess            : 8.001184076633093\n",
      "    log_marginal   : 1797.2931679779629\n",
      "    val_loss       : -1797.4866333007812\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1797.4866333007812\n",
      "Train Epoch: 371 [0/54000 (0%)] Loss: -1802.723267\n",
      "Train Epoch: 371 [11264/54000 (21%)] Loss: -1799.985596\n",
      "Train Epoch: 371 [22528/54000 (42%)] Loss: -1803.188965\n",
      "Train Epoch: 371 [33792/54000 (63%)] Loss: -1793.756592\n",
      "Train Epoch: 371 [45056/54000 (83%)] Loss: -1790.773193\n",
      "    epoch          : 371\n",
      "    loss           : -1799.5276523806015\n",
      "    ess            : 8.001183671771356\n",
      "    log_marginal   : 1799.5276523806015\n",
      "    val_loss       : -1799.6236368815105\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1799.6236368815105\n",
      "Train Epoch: 372 [0/54000 (0%)] Loss: -1799.958984\n",
      "Train Epoch: 372 [11264/54000 (21%)] Loss: -1798.739380\n",
      "Train Epoch: 372 [22528/54000 (42%)] Loss: -1802.041748\n",
      "Train Epoch: 372 [33792/54000 (63%)] Loss: -1797.582031\n",
      "Train Epoch: 372 [45056/54000 (83%)] Loss: -1789.360718\n",
      "    epoch          : 372\n",
      "    loss           : -1797.9222584850384\n",
      "    ess            : 8.001183878700688\n",
      "    log_marginal   : 1797.9222584850384\n",
      "    val_loss       : -1795.97900390625\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1795.97900390625\n",
      "Train Epoch: 373 [0/54000 (0%)] Loss: -1804.739502\n",
      "Train Epoch: 373 [11264/54000 (21%)] Loss: -1779.170776\n",
      "Train Epoch: 373 [22528/54000 (42%)] Loss: -1789.788208\n",
      "Train Epoch: 373 [33792/54000 (63%)] Loss: -1798.907471\n",
      "Train Epoch: 373 [45056/54000 (83%)] Loss: -1796.350464\n",
      "    epoch          : 373\n",
      "    loss           : -1795.5081648916569\n",
      "    ess            : 8.001183707759065\n",
      "    log_marginal   : 1795.5081648916569\n",
      "    val_loss       : -1806.2565104166667\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1806.2565104166667\n",
      "Train Epoch: 374 [0/54000 (0%)] Loss: -1810.929932\n",
      "Train Epoch: 374 [11264/54000 (21%)] Loss: -1806.510254\n",
      "Train Epoch: 374 [22528/54000 (42%)] Loss: -1732.136719\n",
      "Train Epoch: 374 [33792/54000 (63%)] Loss: -1780.367676\n",
      "Train Epoch: 374 [45056/54000 (83%)] Loss: -1778.317383\n",
      "    epoch          : 374\n",
      "    loss           : -1780.7043675836528\n",
      "    ess            : 8.001183707759065\n",
      "    log_marginal   : 1780.7043675836528\n",
      "    val_loss       : -1750.0636494954426\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1750.0636393229167\n",
      "Train Epoch: 375 [0/54000 (0%)] Loss: -1764.790771\n",
      "Train Epoch: 375 [11264/54000 (21%)] Loss: -1792.668701\n",
      "Train Epoch: 375 [22528/54000 (42%)] Loss: -1791.082764\n",
      "Train Epoch: 375 [33792/54000 (63%)] Loss: -1799.260986\n",
      "Train Epoch: 375 [45056/54000 (83%)] Loss: -1802.594971\n",
      "    epoch          : 375\n",
      "    loss           : -1792.3748894457547\n",
      "    ess            : 8.001184670430309\n",
      "    log_marginal   : 1792.3748894457547\n",
      "    val_loss       : -1779.2454630533855\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1779.2454630533855\n",
      "Train Epoch: 376 [0/54000 (0%)] Loss: -1821.256348\n",
      "Train Epoch: 376 [11264/54000 (21%)] Loss: -1810.111450\n",
      "Train Epoch: 376 [22528/54000 (42%)] Loss: -1801.699463\n",
      "Train Epoch: 376 [33792/54000 (63%)] Loss: -1800.194336\n",
      "Train Epoch: 376 [45056/54000 (83%)] Loss: -1792.892822\n",
      "    epoch          : 376\n",
      "    loss           : -1802.559744205115\n",
      "    ess            : 8.00118358180208\n",
      "    log_marginal   : 1802.559744205115\n",
      "    val_loss       : -1795.8477274576824\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1795.8477274576824\n",
      "Train Epoch: 377 [0/54000 (0%)] Loss: -1812.339600\n",
      "Train Epoch: 377 [11264/54000 (21%)] Loss: -1807.255859\n",
      "Train Epoch: 377 [22528/54000 (42%)] Loss: -1804.277100\n",
      "Train Epoch: 377 [33792/54000 (63%)] Loss: -1793.261230\n",
      "Train Epoch: 377 [45056/54000 (83%)] Loss: -1790.188965\n",
      "    epoch          : 377\n",
      "    loss           : -1797.5655655770931\n",
      "    ess            : 8.001184139611587\n",
      "    log_marginal   : 1797.5655655770931\n",
      "    val_loss       : -1786.3858032226562\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1786.3858133951824\n",
      "Train Epoch: 378 [0/54000 (0%)] Loss: -1792.922363\n",
      "Train Epoch: 378 [11264/54000 (21%)] Loss: -1785.425049\n",
      "Train Epoch: 378 [22528/54000 (42%)] Loss: -1797.824463\n",
      "Train Epoch: 378 [33792/54000 (63%)] Loss: -1789.286743\n",
      "Train Epoch: 378 [45056/54000 (83%)] Loss: -1772.985840\n",
      "    epoch          : 378\n",
      "    loss           : -1788.85463268352\n",
      "    ess            : 8.001184967328918\n",
      "    log_marginal   : 1788.8546315319134\n",
      "    val_loss       : -1796.1781819661458\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1796.1781819661458\n",
      "Train Epoch: 379 [0/54000 (0%)] Loss: -1803.528442\n",
      "Train Epoch: 379 [11264/54000 (21%)] Loss: -1792.791382\n",
      "Train Epoch: 379 [22528/54000 (42%)] Loss: -1793.892944\n",
      "Train Epoch: 379 [33792/54000 (63%)] Loss: -1798.610962\n",
      "Train Epoch: 379 [45056/54000 (83%)] Loss: -1796.795654\n",
      "    epoch          : 379\n",
      "    loss           : -1793.7141297538326\n",
      "    ess            : 8.001184121617731\n",
      "    log_marginal   : 1793.7141262990124\n",
      "    val_loss       : -1807.888671875\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1807.888671875\n",
      "Train Epoch: 380 [0/54000 (0%)] Loss: -1816.447266\n",
      "Train Epoch: 380 [11264/54000 (21%)] Loss: -1817.322021\n",
      "Train Epoch: 380 [22528/54000 (42%)] Loss: -1801.279297\n",
      "Train Epoch: 380 [33792/54000 (63%)] Loss: -1802.181885\n",
      "Train Epoch: 380 [45056/54000 (83%)] Loss: -1795.258789\n",
      "    epoch          : 380\n",
      "    loss           : -1801.2743081146816\n",
      "    ess            : 8.001184589457962\n",
      "    log_marginal   : 1801.2743092662884\n",
      "    val_loss       : -1797.8552144368489\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1797.855204264323\n",
      "Train Epoch: 381 [0/54000 (0%)] Loss: -1799.759766\n",
      "Train Epoch: 381 [11264/54000 (21%)] Loss: -1800.947754\n",
      "Train Epoch: 381 [22528/54000 (42%)] Loss: -1791.952393\n",
      "Train Epoch: 381 [33792/54000 (63%)] Loss: -1799.857056\n",
      "Train Epoch: 381 [45056/54000 (83%)] Loss: -1797.234375\n",
      "    epoch          : 381\n",
      "    loss           : -1797.6850275003685\n",
      "    ess            : 8.001183752743703\n",
      "    log_marginal   : 1797.6850275003685\n",
      "    val_loss       : -1801.7247517903645\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1801.7247517903645\n",
      "Train Epoch: 382 [0/54000 (0%)] Loss: -1808.866333\n",
      "Train Epoch: 382 [11264/54000 (21%)] Loss: -1812.136963\n",
      "Train Epoch: 382 [22528/54000 (42%)] Loss: -1804.545654\n",
      "Train Epoch: 382 [33792/54000 (63%)] Loss: -1798.765137\n",
      "Train Epoch: 382 [45056/54000 (83%)] Loss: -1802.267700\n",
      "    epoch          : 382\n",
      "    loss           : -1803.5740379477447\n",
      "    ess            : 8.001183347881964\n",
      "    log_marginal   : 1803.5740379477447\n",
      "    val_loss       : -1798.3291117350261\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1798.3291117350261\n",
      "Train Epoch: 383 [0/54000 (0%)] Loss: -1810.219238\n",
      "Train Epoch: 383 [11264/54000 (21%)] Loss: -1780.776123\n",
      "Train Epoch: 383 [22528/54000 (42%)] Loss: -1796.638916\n",
      "Train Epoch: 383 [33792/54000 (63%)] Loss: -1777.410645\n",
      "Train Epoch: 383 [45056/54000 (83%)] Loss: -1794.419800\n",
      "    epoch          : 383\n",
      "    loss           : -1791.980870660746\n",
      "    ess            : 8.00118521024596\n",
      "    log_marginal   : 1791.9808695091392\n",
      "    val_loss       : -1804.2146809895833\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1804.2146809895833\n",
      "Train Epoch: 384 [0/54000 (0%)] Loss: -1822.422974\n",
      "Train Epoch: 384 [11264/54000 (21%)] Loss: -1802.023315\n",
      "Train Epoch: 384 [22528/54000 (42%)] Loss: -1799.265015\n",
      "Train Epoch: 384 [33792/54000 (63%)] Loss: -1796.717773\n",
      "Train Epoch: 384 [45056/54000 (83%)] Loss: -1790.800537\n",
      "    epoch          : 384\n",
      "    loss           : -1802.0655068451504\n",
      "    ess            : 8.001183860706833\n",
      "    log_marginal   : 1802.0655091483638\n",
      "    val_loss       : -1811.0675964355469\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1811.0675862630208\n",
      "Train Epoch: 385 [0/54000 (0%)] Loss: -1817.488281\n",
      "Train Epoch: 385 [11264/54000 (21%)] Loss: -1822.300293\n",
      "Train Epoch: 385 [22528/54000 (42%)] Loss: -1785.955811\n",
      "Train Epoch: 385 [33792/54000 (63%)] Loss: -1801.677490\n",
      "Train Epoch: 385 [45056/54000 (83%)] Loss: -1798.057739\n",
      "    epoch          : 385\n",
      "    loss           : -1803.2537853312942\n",
      "    ess            : 8.00118383371605\n",
      "    log_marginal   : 1803.2537853312942\n",
      "    val_loss       : -1807.6790568033855\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.6790568033855\n",
      "Train Epoch: 386 [0/54000 (0%)] Loss: -1813.977051\n",
      "Train Epoch: 386 [11264/54000 (21%)] Loss: -1807.896484\n",
      "Train Epoch: 386 [22528/54000 (42%)] Loss: -1792.404785\n",
      "Train Epoch: 386 [33792/54000 (63%)] Loss: -1794.912842\n",
      "Train Epoch: 386 [45056/54000 (83%)] Loss: -1799.952393\n",
      "    epoch          : 386\n",
      "    loss           : -1797.9079532263413\n",
      "    ess            : 8.001184859365788\n",
      "    log_marginal   : 1797.9079520747346\n",
      "    val_loss       : -1801.1018676757812\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1801.1018676757812\n",
      "Train Epoch: 387 [0/54000 (0%)] Loss: -1805.659546\n",
      "Train Epoch: 387 [11264/54000 (21%)] Loss: -1802.621094\n",
      "Train Epoch: 387 [22528/54000 (42%)] Loss: -1785.468750\n",
      "Train Epoch: 387 [33792/54000 (63%)] Loss: -1760.661133\n",
      "Train Epoch: 387 [45056/54000 (83%)] Loss: -1764.898926\n",
      "    epoch          : 387\n",
      "    loss           : -1784.1200043300412\n",
      "    ess            : 8.001184823378077\n",
      "    log_marginal   : 1784.1200043300412\n",
      "    val_loss       : -1780.8845621744792\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1780.8845723470051\n",
      "Train Epoch: 388 [0/54000 (0%)] Loss: -1786.520142\n",
      "Train Epoch: 388 [11264/54000 (21%)] Loss: -1805.030273\n",
      "Train Epoch: 388 [22528/54000 (42%)] Loss: -1804.115845\n",
      "Train Epoch: 388 [33792/54000 (63%)] Loss: -1798.263184\n",
      "Train Epoch: 388 [45056/54000 (83%)] Loss: -1800.640625\n",
      "    epoch          : 388\n",
      "    loss           : -1797.945089088296\n",
      "    ess            : 8.001185327206018\n",
      "    log_marginal   : 1797.945089088296\n",
      "    val_loss       : -1805.033467610677\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.033467610677\n",
      "Train Epoch: 389 [0/54000 (0%)] Loss: -1818.223022\n",
      "Train Epoch: 389 [11264/54000 (21%)] Loss: -1796.218262\n",
      "Train Epoch: 389 [22528/54000 (42%)] Loss: -1789.334229\n",
      "Train Epoch: 389 [33792/54000 (63%)] Loss: -1789.243652\n",
      "Train Epoch: 389 [45056/54000 (83%)] Loss: -1784.075439\n",
      "    epoch          : 389\n",
      "    loss           : -1796.3644547372494\n",
      "    ess            : 8.001183932682252\n",
      "    log_marginal   : 1796.3644535856427\n",
      "    val_loss       : -1801.412353515625\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.4123433430989\n",
      "Train Epoch: 390 [0/54000 (0%)] Loss: -1796.065186\n",
      "Train Epoch: 390 [11264/54000 (21%)] Loss: -1813.916748\n",
      "Train Epoch: 390 [22528/54000 (42%)] Loss: -1803.971802\n",
      "Train Epoch: 390 [33792/54000 (63%)] Loss: -1798.235840\n",
      "Train Epoch: 390 [45056/54000 (83%)] Loss: -1804.440674\n",
      "    epoch          : 390\n",
      "    loss           : -1802.554401901533\n",
      "    ess            : 8.001184238577789\n",
      "    log_marginal   : 1802.554401901533\n",
      "    val_loss       : -1805.468037923177\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.468037923177\n",
      "Train Epoch: 391 [0/54000 (0%)] Loss: -1812.877319\n",
      "Train Epoch: 391 [11264/54000 (21%)] Loss: -1809.606323\n",
      "Train Epoch: 391 [22528/54000 (42%)] Loss: -1771.704590\n",
      "Train Epoch: 391 [33792/54000 (63%)] Loss: -1794.732910\n",
      "Train Epoch: 391 [45056/54000 (83%)] Loss: -1779.218140\n",
      "    epoch          : 391\n",
      "    loss           : -1790.0895109356575\n",
      "    ess            : 8.001184841371932\n",
      "    log_marginal   : 1790.0895109356575\n",
      "    val_loss       : -1788.8201599121094\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1788.8201599121094\n",
      "Train Epoch: 392 [0/54000 (0%)] Loss: -1804.164062\n",
      "Train Epoch: 392 [11264/54000 (21%)] Loss: -1799.088623\n",
      "Train Epoch: 392 [22528/54000 (42%)] Loss: -1795.660645\n",
      "Train Epoch: 392 [33792/54000 (63%)] Loss: -1788.816406\n",
      "Train Epoch: 392 [45056/54000 (83%)] Loss: -1771.871582\n",
      "    epoch          : 392\n",
      "    loss           : -1789.2056055608784\n",
      "    ess            : 8.001184913347352\n",
      "    log_marginal   : 1789.2056055608784\n",
      "    val_loss       : -1803.6446634928386\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1803.6446634928386\n",
      "Train Epoch: 393 [0/54000 (0%)] Loss: -1806.140503\n",
      "Train Epoch: 393 [11264/54000 (21%)] Loss: -1801.756958\n",
      "Train Epoch: 393 [22528/54000 (42%)] Loss: -1777.714722\n",
      "Train Epoch: 393 [33792/54000 (63%)] Loss: -1800.099487\n",
      "Train Epoch: 393 [45056/54000 (83%)] Loss: -1796.815918\n",
      "    epoch          : 393\n",
      "    loss           : -1793.470401404039\n",
      "    ess            : 8.001185003316627\n",
      "    log_marginal   : 1793.470401404039\n",
      "    val_loss       : -1797.7433166503906\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.7432963053386\n",
      "Train Epoch: 394 [0/54000 (0%)] Loss: -1811.265259\n",
      "Train Epoch: 394 [11264/54000 (21%)] Loss: -1810.677002\n",
      "Train Epoch: 394 [22528/54000 (42%)] Loss: -1801.216553\n",
      "Train Epoch: 394 [33792/54000 (63%)] Loss: -1799.023682\n",
      "Train Epoch: 394 [45056/54000 (83%)] Loss: -1808.506470\n",
      "    epoch          : 394\n",
      "    loss           : -1797.4209433501621\n",
      "    ess            : 8.001185435169148\n",
      "    log_marginal   : 1797.4209445017689\n",
      "    val_loss       : -1805.2688903808594\n",
      "    val_ess        : 8.001185337702433\n",
      "    val_log_marginal: 1805.2688903808594\n",
      "Train Epoch: 395 [0/54000 (0%)] Loss: -1818.106445\n",
      "Train Epoch: 395 [11264/54000 (21%)] Loss: -1795.980103\n",
      "Train Epoch: 395 [22528/54000 (42%)] Loss: -1809.356445\n",
      "Train Epoch: 395 [33792/54000 (63%)] Loss: -1794.588257\n",
      "Train Epoch: 395 [45056/54000 (83%)] Loss: -1797.125977\n",
      "    epoch          : 395\n",
      "    loss           : -1802.5711347471993\n",
      "    ess            : 8.001183887697616\n",
      "    log_marginal   : 1802.5711347471993\n",
      "    val_loss       : -1803.0108642578125\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1803.0108642578125\n",
      "Train Epoch: 396 [0/54000 (0%)] Loss: -1806.198608\n",
      "Train Epoch: 396 [11264/54000 (21%)] Loss: -1810.594482\n",
      "Train Epoch: 396 [22528/54000 (42%)] Loss: -1806.047485\n",
      "Train Epoch: 396 [33792/54000 (63%)] Loss: -1792.044312\n",
      "Train Epoch: 396 [45056/54000 (83%)] Loss: -1799.868164\n",
      "    epoch          : 396\n",
      "    loss           : -1803.5494868440448\n",
      "    ess            : 8.00118404964231\n",
      "    log_marginal   : 1803.549485692438\n",
      "    val_loss       : -1807.9335021972656\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1807.9335021972656\n",
      "Train Epoch: 397 [0/54000 (0%)] Loss: -1812.881836\n",
      "Train Epoch: 397 [11264/54000 (21%)] Loss: -1801.840332\n",
      "Train Epoch: 397 [22528/54000 (42%)] Loss: -1803.428955\n",
      "Train Epoch: 397 [33792/54000 (63%)] Loss: -1800.444702\n",
      "Train Epoch: 397 [45056/54000 (83%)] Loss: -1804.453003\n",
      "    epoch          : 397\n",
      "    loss           : -1805.9672448500148\n",
      "    ess            : 8.001183275906545\n",
      "    log_marginal   : 1805.9672448500148\n",
      "    val_loss       : -1804.7376098632812\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1804.7376098632812\n",
      "Train Epoch: 398 [0/54000 (0%)] Loss: -1810.559570\n",
      "Train Epoch: 398 [11264/54000 (21%)] Loss: -1812.532471\n",
      "Train Epoch: 398 [22528/54000 (42%)] Loss: -1788.080566\n",
      "Train Epoch: 398 [33792/54000 (63%)] Loss: -1800.927734\n",
      "Train Epoch: 398 [45056/54000 (83%)] Loss: -1801.129883\n",
      "    epoch          : 398\n",
      "    loss           : -1801.051952434036\n",
      "    ess            : 8.00118304198643\n",
      "    log_marginal   : 1801.051952434036\n",
      "    val_loss       : -1805.5250447591145\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1805.5250549316406\n",
      "Train Epoch: 399 [0/54000 (0%)] Loss: -1823.271973\n",
      "Train Epoch: 399 [11264/54000 (21%)] Loss: -1775.834961\n",
      "Train Epoch: 399 [22528/54000 (42%)] Loss: -1790.489746\n",
      "Train Epoch: 399 [33792/54000 (63%)] Loss: -1792.882202\n",
      "Train Epoch: 399 [45056/54000 (83%)] Loss: -1797.672241\n",
      "    epoch          : 399\n",
      "    loss           : -1796.1142278707252\n",
      "    ess            : 8.00118438252863\n",
      "    log_marginal   : 1796.1142278707252\n",
      "    val_loss       : -1804.1247049967449\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.1247049967449\n",
      "Train Epoch: 400 [0/54000 (0%)] Loss: -1816.704102\n",
      "Train Epoch: 400 [11264/54000 (21%)] Loss: -1810.903931\n",
      "Train Epoch: 400 [22528/54000 (42%)] Loss: -1809.992554\n",
      "Train Epoch: 400 [33792/54000 (63%)] Loss: -1812.132935\n",
      "Train Epoch: 400 [45056/54000 (83%)] Loss: -1808.967651\n",
      "    epoch          : 400\n",
      "    loss           : -1808.120323325103\n",
      "    ess            : 8.0011832939004\n",
      "    log_marginal   : 1808.1203244767098\n",
      "    val_loss       : -1800.0579528808594\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1800.0579528808594\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch400.pth ...\n",
      "Train Epoch: 401 [0/54000 (0%)] Loss: -1815.704590\n",
      "Train Epoch: 401 [11264/54000 (21%)] Loss: -1785.362793\n",
      "Train Epoch: 401 [22528/54000 (42%)] Loss: -1794.636963\n",
      "Train Epoch: 401 [33792/54000 (63%)] Loss: -1790.063232\n",
      "Train Epoch: 401 [45056/54000 (83%)] Loss: -1793.911865\n",
      "    epoch          : 401\n",
      "    loss           : -1796.072766573924\n",
      "    ess            : 8.00118438252863\n",
      "    log_marginal   : 1796.0727677255306\n",
      "    val_loss       : -1800.7063090006511\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1800.706298828125\n",
      "Train Epoch: 402 [0/54000 (0%)] Loss: -1803.770752\n",
      "Train Epoch: 402 [11264/54000 (21%)] Loss: -1797.322632\n",
      "Train Epoch: 402 [22528/54000 (42%)] Loss: -1786.150513\n",
      "Train Epoch: 402 [33792/54000 (63%)] Loss: -1789.695068\n",
      "Train Epoch: 402 [45056/54000 (83%)] Loss: -1788.008667\n",
      "    epoch          : 402\n",
      "    loss           : -1793.803581957547\n",
      "    ess            : 8.001184076633093\n",
      "    log_marginal   : 1793.803581957547\n",
      "    val_loss       : -1800.423319498698\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1800.423319498698\n",
      "Train Epoch: 403 [0/54000 (0%)] Loss: -1817.083862\n",
      "Train Epoch: 403 [11264/54000 (21%)] Loss: -1810.966187\n",
      "Train Epoch: 403 [22528/54000 (42%)] Loss: -1803.137085\n",
      "Train Epoch: 403 [33792/54000 (63%)] Loss: -1794.458984\n",
      "Train Epoch: 403 [45056/54000 (83%)] Loss: -1798.316895\n",
      "    epoch          : 403\n",
      "    loss           : -1802.225718372273\n",
      "    ess            : 8.00118361778979\n",
      "    log_marginal   : 1802.225718372273\n",
      "    val_loss       : -1798.7322489420574\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1798.7322489420574\n",
      "Train Epoch: 404 [0/54000 (0%)] Loss: -1817.373657\n",
      "Train Epoch: 404 [11264/54000 (21%)] Loss: -1809.166016\n",
      "Train Epoch: 404 [22528/54000 (42%)] Loss: -1811.390625\n",
      "Train Epoch: 404 [33792/54000 (63%)] Loss: -1787.164551\n",
      "Train Epoch: 404 [45056/54000 (83%)] Loss: -1798.382568\n",
      "    epoch          : 404\n",
      "    loss           : -1801.9163403780956\n",
      "    ess            : 8.001183635783645\n",
      "    log_marginal   : 1801.9163403780956\n",
      "    val_loss       : -1799.0064595540364\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1799.0064595540364\n",
      "Train Epoch: 405 [0/54000 (0%)] Loss: -1804.350464\n",
      "Train Epoch: 405 [11264/54000 (21%)] Loss: -1810.270020\n",
      "Train Epoch: 405 [22528/54000 (42%)] Loss: -1799.205078\n",
      "Train Epoch: 405 [33792/54000 (63%)] Loss: -1806.790283\n",
      "Train Epoch: 405 [45056/54000 (83%)] Loss: -1812.085205\n",
      "    epoch          : 405\n",
      "    loss           : -1806.0983449108196\n",
      "    ess            : 8.00118282606017\n",
      "    log_marginal   : 1806.0983437592129\n",
      "    val_loss       : -1812.762939453125\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1812.762919108073\n",
      "Train Epoch: 406 [0/54000 (0%)] Loss: -1822.932495\n",
      "Train Epoch: 406 [11264/54000 (21%)] Loss: -1802.626709\n",
      "Train Epoch: 406 [22528/54000 (42%)] Loss: -1803.746338\n",
      "Train Epoch: 406 [33792/54000 (63%)] Loss: -1774.494629\n",
      "Train Epoch: 406 [45056/54000 (83%)] Loss: -1784.552490\n",
      "    epoch          : 406\n",
      "    loss           : -1794.4998330170254\n",
      "    ess            : 8.00118361778979\n",
      "    log_marginal   : 1794.4998318654186\n",
      "    val_loss       : -1798.3521525065105\n",
      "    val_ess        : 8.001184701919556\n",
      "    val_log_marginal: 1798.3521525065105\n",
      "Train Epoch: 407 [0/54000 (0%)] Loss: -1798.130859\n",
      "Train Epoch: 407 [11264/54000 (21%)] Loss: -1800.763062\n",
      "Train Epoch: 407 [22528/54000 (42%)] Loss: -1795.457031\n",
      "Train Epoch: 407 [33792/54000 (63%)] Loss: -1796.877319\n",
      "Train Epoch: 407 [45056/54000 (83%)] Loss: -1801.477539\n",
      "    epoch          : 407\n",
      "    loss           : -1798.7247061099647\n",
      "    ess            : 8.001183824719122\n",
      "    log_marginal   : 1798.724704958358\n",
      "    val_loss       : -1810.7024841308594\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1810.7024841308594\n",
      "Train Epoch: 408 [0/54000 (0%)] Loss: -1815.689453\n",
      "Train Epoch: 408 [11264/54000 (21%)] Loss: -1806.489380\n",
      "Train Epoch: 408 [22528/54000 (42%)] Loss: -1794.010742\n",
      "Train Epoch: 408 [33792/54000 (63%)] Loss: -1794.951538\n",
      "Train Epoch: 408 [45056/54000 (83%)] Loss: -1790.138184\n",
      "    epoch          : 408\n",
      "    loss           : -1800.284005794885\n",
      "    ess            : 8.00118340186353\n",
      "    log_marginal   : 1800.284005794885\n",
      "    val_loss       : -1805.8279520670574\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1805.8279520670574\n",
      "Train Epoch: 409 [0/54000 (0%)] Loss: -1810.727417\n",
      "Train Epoch: 409 [11264/54000 (21%)] Loss: -1797.877563\n",
      "Train Epoch: 409 [22528/54000 (42%)] Loss: -1800.745850\n",
      "Train Epoch: 409 [33792/54000 (63%)] Loss: -1798.083984\n",
      "Train Epoch: 409 [45056/54000 (83%)] Loss: -1799.754883\n",
      "    epoch          : 409\n",
      "    loss           : -1800.9335258052033\n",
      "    ess            : 8.001183608792863\n",
      "    log_marginal   : 1800.93352695681\n",
      "    val_loss       : -1811.3316141764324\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1811.3316141764324\n",
      "Train Epoch: 410 [0/54000 (0%)] Loss: -1818.233643\n",
      "Train Epoch: 410 [11264/54000 (21%)] Loss: -1782.962646\n",
      "Train Epoch: 410 [22528/54000 (42%)] Loss: -1789.189819\n",
      "Train Epoch: 410 [33792/54000 (63%)] Loss: -1800.648926\n",
      "Train Epoch: 410 [45056/54000 (83%)] Loss: -1796.438599\n",
      "    epoch          : 410\n",
      "    loss           : -1794.0706648916569\n",
      "    ess            : 8.001184391525557\n",
      "    log_marginal   : 1794.0706648916569\n",
      "    val_loss       : -1804.8947347005208\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.894755045573\n",
      "Train Epoch: 411 [0/54000 (0%)] Loss: -1814.935059\n",
      "Train Epoch: 411 [11264/54000 (21%)] Loss: -1813.015747\n",
      "Train Epoch: 411 [22528/54000 (42%)] Loss: -1789.140137\n",
      "Train Epoch: 411 [33792/54000 (63%)] Loss: -1785.211914\n",
      "Train Epoch: 411 [45056/54000 (83%)] Loss: -1785.886597\n",
      "    epoch          : 411\n",
      "    loss           : -1789.7002125866009\n",
      "    ess            : 8.001184013654601\n",
      "    log_marginal   : 1789.7002125866009\n",
      "    val_loss       : -1756.7163594563801\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1756.7163594563801\n",
      "Train Epoch: 412 [0/54000 (0%)] Loss: -1760.594360\n",
      "Train Epoch: 412 [11264/54000 (21%)] Loss: -1791.699707\n",
      "Train Epoch: 412 [22528/54000 (42%)] Loss: -1790.855225\n",
      "Train Epoch: 412 [33792/54000 (63%)] Loss: -1781.249268\n",
      "Train Epoch: 412 [45056/54000 (83%)] Loss: -1787.289551\n",
      "    epoch          : 412\n",
      "    loss           : -1789.571938568691\n",
      "    ess            : 8.001184481494832\n",
      "    log_marginal   : 1789.571938568691\n",
      "    val_loss       : -1785.4532267252605\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1785.4532165527344\n",
      "Train Epoch: 413 [0/54000 (0%)] Loss: -1804.213867\n",
      "Train Epoch: 413 [11264/54000 (21%)] Loss: -1803.924805\n",
      "Train Epoch: 413 [22528/54000 (42%)] Loss: -1806.678955\n",
      "Train Epoch: 413 [33792/54000 (63%)] Loss: -1791.936035\n",
      "Train Epoch: 413 [45056/54000 (83%)] Loss: -1799.276123\n",
      "    epoch          : 413\n",
      "    loss           : -1799.4744446952388\n",
      "    ess            : 8.00118376174063\n",
      "    log_marginal   : 1799.4744446952388\n",
      "    val_loss       : -1799.5156555175781\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1799.5156555175781\n",
      "Train Epoch: 414 [0/54000 (0%)] Loss: -1809.917725\n",
      "Train Epoch: 414 [11264/54000 (21%)] Loss: -1818.090088\n",
      "Train Epoch: 414 [22528/54000 (42%)] Loss: -1796.340332\n",
      "Train Epoch: 414 [33792/54000 (63%)] Loss: -1808.351074\n",
      "Train Epoch: 414 [45056/54000 (83%)] Loss: -1798.083740\n",
      "    epoch          : 414\n",
      "    loss           : -1806.864803674086\n",
      "    ess            : 8.001183779734486\n",
      "    log_marginal   : 1806.864803674086\n",
      "    val_loss       : -1807.807840983073\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1807.807840983073\n",
      "Train Epoch: 415 [0/54000 (0%)] Loss: -1811.788330\n",
      "Train Epoch: 415 [11264/54000 (21%)] Loss: -1807.830078\n",
      "Train Epoch: 415 [22528/54000 (42%)] Loss: -1791.888672\n",
      "Train Epoch: 415 [33792/54000 (63%)] Loss: -1796.100830\n",
      "Train Epoch: 415 [45056/54000 (83%)] Loss: -1793.026611\n",
      "    epoch          : 415\n",
      "    loss           : -1797.9990579857017\n",
      "    ess            : 8.00118419359315\n",
      "    log_marginal   : 1797.9990556824882\n",
      "    val_loss       : -1800.8370768229167\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1800.8370768229167\n",
      "Train Epoch: 416 [0/54000 (0%)] Loss: -1816.707275\n",
      "Train Epoch: 416 [11264/54000 (21%)] Loss: -1779.701050\n",
      "Train Epoch: 416 [22528/54000 (42%)] Loss: -1798.033691\n",
      "Train Epoch: 416 [33792/54000 (63%)] Loss: -1790.066162\n",
      "Train Epoch: 416 [45056/54000 (83%)] Loss: -1796.605713\n",
      "    epoch          : 416\n",
      "    loss           : -1791.688174841539\n",
      "    ess            : 8.001184265568572\n",
      "    log_marginal   : 1791.6881736899322\n",
      "    val_loss       : -1803.7533569335938\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1803.7533569335938\n",
      "Train Epoch: 417 [0/54000 (0%)] Loss: -1816.000977\n",
      "Train Epoch: 417 [11264/54000 (21%)] Loss: -1800.639648\n",
      "Train Epoch: 417 [22528/54000 (42%)] Loss: -1806.075195\n",
      "Train Epoch: 417 [33792/54000 (63%)] Loss: -1798.950195\n",
      "Train Epoch: 417 [45056/54000 (83%)] Loss: -1791.100098\n",
      "    epoch          : 417\n",
      "    loss           : -1798.0225070017689\n",
      "    ess            : 8.00118368976521\n",
      "    log_marginal   : 1798.0225070017689\n",
      "    val_loss       : -1790.146993001302\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1790.1469828287761\n",
      "Train Epoch: 418 [0/54000 (0%)] Loss: -1789.124146\n",
      "Train Epoch: 418 [11264/54000 (21%)] Loss: -1771.585205\n",
      "Train Epoch: 418 [22528/54000 (42%)] Loss: -1800.663330\n",
      "Train Epoch: 418 [33792/54000 (63%)] Loss: -1796.893921\n",
      "Train Epoch: 418 [45056/54000 (83%)] Loss: -1810.348267\n",
      "    epoch          : 418\n",
      "    loss           : -1797.986249815743\n",
      "    ess            : 8.001184643439526\n",
      "    log_marginal   : 1797.9862486641362\n",
      "    val_loss       : -1804.5231526692708\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1804.5231526692708\n",
      "Train Epoch: 419 [0/54000 (0%)] Loss: -1815.114380\n",
      "Train Epoch: 419 [11264/54000 (21%)] Loss: -1798.550049\n",
      "Train Epoch: 419 [22528/54000 (42%)] Loss: -1796.775024\n",
      "Train Epoch: 419 [33792/54000 (63%)] Loss: -1799.358398\n",
      "Train Epoch: 419 [45056/54000 (83%)] Loss: -1811.245361\n",
      "    epoch          : 419\n",
      "    loss           : -1801.8159029978626\n",
      "    ess            : 8.00118304198643\n",
      "    log_marginal   : 1801.8159041494694\n",
      "    val_loss       : -1807.9848734537761\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.98486328125\n",
      "Train Epoch: 420 [0/54000 (0%)] Loss: -1807.500977\n",
      "Train Epoch: 420 [11264/54000 (21%)] Loss: -1797.605713\n",
      "Train Epoch: 420 [22528/54000 (42%)] Loss: -1788.682007\n",
      "Train Epoch: 420 [33792/54000 (63%)] Loss: -1790.362427\n",
      "Train Epoch: 420 [45056/54000 (83%)] Loss: -1796.759399\n",
      "    epoch          : 420\n",
      "    loss           : -1798.5197972711528\n",
      "    ess            : 8.001184463500977\n",
      "    log_marginal   : 1798.5197984227593\n",
      "    val_loss       : -1805.6095581054688\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1805.6095581054688\n",
      "Train Epoch: 421 [0/54000 (0%)] Loss: -1810.938477\n",
      "Train Epoch: 421 [11264/54000 (21%)] Loss: -1789.859619\n",
      "Train Epoch: 421 [22528/54000 (42%)] Loss: -1779.743164\n",
      "Train Epoch: 421 [33792/54000 (63%)] Loss: -1783.799316\n",
      "Train Epoch: 421 [45056/54000 (83%)] Loss: -1803.874268\n",
      "    epoch          : 421\n",
      "    loss           : -1795.883342239092\n",
      "    ess            : 8.001184544473324\n",
      "    log_marginal   : 1795.883342239092\n",
      "    val_loss       : -1801.5027567545574\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.5027567545574\n",
      "Train Epoch: 422 [0/54000 (0%)] Loss: -1811.047729\n",
      "Train Epoch: 422 [11264/54000 (21%)] Loss: -1806.424072\n",
      "Train Epoch: 422 [22528/54000 (42%)] Loss: -1804.786621\n",
      "Train Epoch: 422 [33792/54000 (63%)] Loss: -1803.453369\n",
      "Train Epoch: 422 [45056/54000 (83%)] Loss: -1803.400513\n",
      "    epoch          : 422\n",
      "    loss           : -1807.732024570681\n",
      "    ess            : 8.001183500829733\n",
      "    log_marginal   : 1807.732024570681\n",
      "    val_loss       : -1805.3141988118489\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1805.3141988118489\n",
      "Train Epoch: 423 [0/54000 (0%)] Loss: -1816.336548\n",
      "Train Epoch: 423 [11264/54000 (21%)] Loss: -1801.638672\n",
      "Train Epoch: 423 [22528/54000 (42%)] Loss: -1777.101807\n",
      "Train Epoch: 423 [33792/54000 (63%)] Loss: -1749.397949\n",
      "Train Epoch: 423 [45056/54000 (83%)] Loss: -1774.921631\n",
      "    epoch          : 423\n",
      "    loss           : -1783.0446869472287\n",
      "    ess            : 8.001184625445672\n",
      "    log_marginal   : 1783.0446869472287\n",
      "    val_loss       : -1767.1809590657551\n",
      "    val_ess        : 8.001185973485311\n",
      "    val_log_marginal: 1767.1809590657551\n",
      "Train Epoch: 424 [0/54000 (0%)] Loss: -1784.173584\n",
      "Train Epoch: 424 [11264/54000 (21%)] Loss: -1790.914062\n",
      "Train Epoch: 424 [22528/54000 (42%)] Loss: -1798.137573\n",
      "Train Epoch: 424 [33792/54000 (63%)] Loss: -1786.679932\n",
      "Train Epoch: 424 [45056/54000 (83%)] Loss: -1790.108887\n",
      "    epoch          : 424\n",
      "    loss           : -1788.3925263026974\n",
      "    ess            : 8.001185435169148\n",
      "    log_marginal   : 1788.3925274543042\n",
      "    val_loss       : -1776.0595703125\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1776.0595703125\n",
      "Train Epoch: 425 [0/54000 (0%)] Loss: -1812.847168\n",
      "Train Epoch: 425 [11264/54000 (21%)] Loss: -1808.197998\n",
      "Train Epoch: 425 [22528/54000 (42%)] Loss: -1800.914795\n",
      "Train Epoch: 425 [33792/54000 (63%)] Loss: -1793.385742\n",
      "Train Epoch: 425 [45056/54000 (83%)] Loss: -1802.231812\n",
      "    epoch          : 425\n",
      "    loss           : -1798.3292512713738\n",
      "    ess            : 8.001184931341207\n",
      "    log_marginal   : 1798.3292524229805\n",
      "    val_loss       : -1796.6053466796875\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1796.6053466796875\n",
      "Train Epoch: 426 [0/54000 (0%)] Loss: -1805.433594\n",
      "Train Epoch: 426 [11264/54000 (21%)] Loss: -1792.294189\n",
      "Train Epoch: 426 [22528/54000 (42%)] Loss: -1786.369263\n",
      "Train Epoch: 426 [33792/54000 (63%)] Loss: -1799.542236\n",
      "Train Epoch: 426 [45056/54000 (83%)] Loss: -1788.126465\n",
      "    epoch          : 426\n",
      "    loss           : -1797.8718918134582\n",
      "    ess            : 8.001185399181438\n",
      "    log_marginal   : 1797.8718918134582\n",
      "    val_loss       : -1803.6603698730469\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1803.6603698730469\n",
      "Train Epoch: 427 [0/54000 (0%)] Loss: -1810.214600\n",
      "Train Epoch: 427 [11264/54000 (21%)] Loss: -1803.791626\n",
      "Train Epoch: 427 [22528/54000 (42%)] Loss: -1793.646729\n",
      "Train Epoch: 427 [33792/54000 (63%)] Loss: -1799.214722\n",
      "Train Epoch: 427 [45056/54000 (83%)] Loss: -1797.218994\n",
      "    epoch          : 427\n",
      "    loss           : -1799.6723724941037\n",
      "    ess            : 8.001184733408802\n",
      "    log_marginal   : 1799.6723736457104\n",
      "    val_loss       : -1802.2757161458333\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1802.2757161458333\n",
      "Train Epoch: 428 [0/54000 (0%)] Loss: -1821.747925\n",
      "Train Epoch: 428 [11264/54000 (21%)] Loss: -1815.822510\n",
      "Train Epoch: 428 [22528/54000 (42%)] Loss: -1794.396484\n",
      "Train Epoch: 428 [33792/54000 (63%)] Loss: -1803.090454\n",
      "Train Epoch: 428 [45056/54000 (83%)] Loss: -1806.151123\n",
      "    epoch          : 428\n",
      "    loss           : -1806.1545225899174\n",
      "    ess            : 8.001184094626948\n",
      "    log_marginal   : 1806.1545225899174\n",
      "    val_loss       : -1808.8531799316406\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1808.8531799316406\n",
      "Train Epoch: 429 [0/54000 (0%)] Loss: -1827.369141\n",
      "Train Epoch: 429 [11264/54000 (21%)] Loss: -1810.621826\n",
      "Train Epoch: 429 [22528/54000 (42%)] Loss: -1812.065918\n",
      "Train Epoch: 429 [33792/54000 (63%)] Loss: -1804.884033\n",
      "Train Epoch: 429 [45056/54000 (83%)] Loss: -1792.655884\n",
      "    epoch          : 429\n",
      "    loss           : -1806.4047298791274\n",
      "    ess            : 8.001184265568572\n",
      "    log_marginal   : 1806.4047298791274\n",
      "    val_loss       : -1807.1381225585938\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1807.1381225585938\n",
      "Train Epoch: 430 [0/54000 (0%)] Loss: -1821.925171\n",
      "Train Epoch: 430 [11264/54000 (21%)] Loss: -1801.462280\n",
      "Train Epoch: 430 [22528/54000 (42%)] Loss: -1800.502319\n",
      "Train Epoch: 430 [33792/54000 (63%)] Loss: -1792.125244\n",
      "Train Epoch: 430 [45056/54000 (83%)] Loss: -1804.674561\n",
      "    epoch          : 430\n",
      "    loss           : -1800.260617813974\n",
      "    ess            : 8.001184517482542\n",
      "    log_marginal   : 1800.260617813974\n",
      "    val_loss       : -1797.7399800618489\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1797.7399597167969\n",
      "Train Epoch: 431 [0/54000 (0%)] Loss: -1805.459595\n",
      "Train Epoch: 431 [11264/54000 (21%)] Loss: -1807.678711\n",
      "Train Epoch: 431 [22528/54000 (42%)] Loss: -1781.549316\n",
      "Train Epoch: 431 [33792/54000 (63%)] Loss: -1781.217041\n",
      "Train Epoch: 431 [45056/54000 (83%)] Loss: -1797.000000\n",
      "    epoch          : 431\n",
      "    loss           : -1798.9189464641067\n",
      "    ess            : 8.00118474240573\n",
      "    log_marginal   : 1798.9189464641067\n",
      "    val_loss       : -1807.927714029948\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1807.927734375\n",
      "Train Epoch: 432 [0/54000 (0%)] Loss: -1822.084229\n",
      "Train Epoch: 432 [11264/54000 (21%)] Loss: -1803.297607\n",
      "Train Epoch: 432 [22528/54000 (42%)] Loss: -1785.753784\n",
      "Train Epoch: 432 [33792/54000 (63%)] Loss: -1801.183594\n",
      "Train Epoch: 432 [45056/54000 (83%)] Loss: -1793.733887\n",
      "    epoch          : 432\n",
      "    loss           : -1800.7817348264298\n",
      "    ess            : 8.001184715414947\n",
      "    log_marginal   : 1800.7817348264298\n",
      "    val_loss       : -1803.2928975423176\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1803.2928975423176\n",
      "Train Epoch: 433 [0/54000 (0%)] Loss: -1812.074341\n",
      "Train Epoch: 433 [11264/54000 (21%)] Loss: -1799.886475\n",
      "Train Epoch: 433 [22528/54000 (42%)] Loss: -1785.154053\n",
      "Train Epoch: 433 [33792/54000 (63%)] Loss: -1796.838867\n",
      "Train Epoch: 433 [45056/54000 (83%)] Loss: -1778.700562\n",
      "    epoch          : 433\n",
      "    loss           : -1795.3504224093456\n",
      "    ess            : 8.001184247574717\n",
      "    log_marginal   : 1795.3504224093456\n",
      "    val_loss       : -1801.5221048990886\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.5221048990886\n",
      "Train Epoch: 434 [0/54000 (0%)] Loss: -1810.230713\n",
      "Train Epoch: 434 [11264/54000 (21%)] Loss: -1785.301025\n",
      "Train Epoch: 434 [22528/54000 (42%)] Loss: -1785.680908\n",
      "Train Epoch: 434 [33792/54000 (63%)] Loss: -1811.046143\n",
      "Train Epoch: 434 [45056/54000 (83%)] Loss: -1798.825439\n",
      "    epoch          : 434\n",
      "    loss           : -1796.2704387160968\n",
      "    ess            : 8.001184319550136\n",
      "    log_marginal   : 1796.2704387160968\n",
      "    val_loss       : -1801.1574401855469\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.157450358073\n",
      "Train Epoch: 435 [0/54000 (0%)] Loss: -1816.011108\n",
      "Train Epoch: 435 [11264/54000 (21%)] Loss: -1811.829834\n",
      "Train Epoch: 435 [22528/54000 (42%)] Loss: -1788.837524\n",
      "Train Epoch: 435 [33792/54000 (63%)] Loss: -1786.298828\n",
      "Train Epoch: 435 [45056/54000 (83%)] Loss: -1792.423340\n",
      "    epoch          : 435\n",
      "    loss           : -1796.5114769125885\n",
      "    ess            : 8.001184859365788\n",
      "    log_marginal   : 1796.5114769125885\n",
      "    val_loss       : -1800.2156168619792\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1800.2156270345051\n",
      "Train Epoch: 436 [0/54000 (0%)] Loss: -1810.701782\n",
      "Train Epoch: 436 [11264/54000 (21%)] Loss: -1810.090332\n",
      "Train Epoch: 436 [22528/54000 (42%)] Loss: -1786.375732\n",
      "Train Epoch: 436 [33792/54000 (63%)] Loss: -1781.701660\n",
      "Train Epoch: 436 [45056/54000 (83%)] Loss: -1763.110352\n",
      "    epoch          : 436\n",
      "    loss           : -1786.2942124852593\n",
      "    ess            : 8.001184355537847\n",
      "    log_marginal   : 1786.294213636866\n",
      "    val_loss       : -1775.8769632975261\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1775.8769632975261\n",
      "Train Epoch: 437 [0/54000 (0%)] Loss: -1789.953491\n",
      "Train Epoch: 437 [11264/54000 (21%)] Loss: -1769.573975\n",
      "Train Epoch: 437 [22528/54000 (42%)] Loss: -1767.147095\n",
      "Train Epoch: 437 [33792/54000 (63%)] Loss: -1779.479492\n",
      "Train Epoch: 437 [45056/54000 (83%)] Loss: -1777.249390\n",
      "    epoch          : 437\n",
      "    loss           : -1778.4343722361439\n",
      "    ess            : 8.001184697421092\n",
      "    log_marginal   : 1778.4343722361439\n",
      "    val_loss       : -1799.1869812011719\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.1869812011719\n",
      "Train Epoch: 438 [0/54000 (0%)] Loss: -1801.242065\n",
      "Train Epoch: 438 [11264/54000 (21%)] Loss: -1789.333008\n",
      "Train Epoch: 438 [22528/54000 (42%)] Loss: -1800.073975\n",
      "Train Epoch: 438 [33792/54000 (63%)] Loss: -1792.276001\n",
      "Train Epoch: 438 [45056/54000 (83%)] Loss: -1784.961426\n",
      "    epoch          : 438\n",
      "    loss           : -1794.5522357292896\n",
      "    ess            : 8.001183671771356\n",
      "    log_marginal   : 1794.5522357292896\n",
      "    val_loss       : -1806.4840087890625\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1806.4840087890625\n",
      "Train Epoch: 439 [0/54000 (0%)] Loss: -1810.031738\n",
      "Train Epoch: 439 [11264/54000 (21%)] Loss: -1809.165039\n",
      "Train Epoch: 439 [22528/54000 (42%)] Loss: -1797.865479\n",
      "Train Epoch: 439 [33792/54000 (63%)] Loss: -1806.058594\n",
      "Train Epoch: 439 [45056/54000 (83%)] Loss: -1800.485229\n",
      "    epoch          : 439\n",
      "    loss           : -1803.9009733380012\n",
      "    ess            : 8.001184022651529\n",
      "    log_marginal   : 1803.9009733380012\n",
      "    val_loss       : -1804.92822265625\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.928243001302\n",
      "Train Epoch: 440 [0/54000 (0%)] Loss: -1811.578125\n",
      "Train Epoch: 440 [11264/54000 (21%)] Loss: -1808.195068\n",
      "Train Epoch: 440 [22528/54000 (42%)] Loss: -1783.091797\n",
      "Train Epoch: 440 [33792/54000 (63%)] Loss: -1788.235352\n",
      "Train Epoch: 440 [45056/54000 (83%)] Loss: -1799.523926\n",
      "    epoch          : 440\n",
      "    loss           : -1799.7799417747642\n",
      "    ess            : 8.001184211587006\n",
      "    log_marginal   : 1799.7799417747642\n",
      "    val_loss       : -1802.0443929036458\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.0444234212239\n",
      "Train Epoch: 441 [0/54000 (0%)] Loss: -1805.850830\n",
      "Train Epoch: 441 [11264/54000 (21%)] Loss: -1805.016602\n",
      "Train Epoch: 441 [22528/54000 (42%)] Loss: -1804.215332\n",
      "Train Epoch: 441 [33792/54000 (63%)] Loss: -1807.415161\n",
      "Train Epoch: 441 [45056/54000 (83%)] Loss: -1796.917358\n",
      "    epoch          : 441\n",
      "    loss           : -1806.3353329064712\n",
      "    ess            : 8.001183311894255\n",
      "    log_marginal   : 1806.3353329064712\n",
      "    val_loss       : -1808.1094462076824\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1808.1094462076824\n",
      "Train Epoch: 442 [0/54000 (0%)] Loss: -1819.386475\n",
      "Train Epoch: 442 [11264/54000 (21%)] Loss: -1810.367188\n",
      "Train Epoch: 442 [22528/54000 (42%)] Loss: -1784.062744\n",
      "Train Epoch: 442 [33792/54000 (63%)] Loss: -1794.228516\n",
      "Train Epoch: 442 [45056/54000 (83%)] Loss: -1787.065674\n",
      "    epoch          : 442\n",
      "    loss           : -1792.5509413233344\n",
      "    ess            : 8.001184157605442\n",
      "    log_marginal   : 1792.5509413233344\n",
      "    val_loss       : -1785.4838358561199\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1785.4838358561199\n",
      "Train Epoch: 443 [0/54000 (0%)] Loss: -1788.570068\n",
      "Train Epoch: 443 [11264/54000 (21%)] Loss: -1785.445801\n",
      "Train Epoch: 443 [22528/54000 (42%)] Loss: -1802.687378\n",
      "Train Epoch: 443 [33792/54000 (63%)] Loss: -1791.629883\n",
      "Train Epoch: 443 [45056/54000 (83%)] Loss: -1783.900146\n",
      "    epoch          : 443\n",
      "    loss           : -1793.875818792379\n",
      "    ess            : 8.001184787390367\n",
      "    log_marginal   : 1793.8758199439858\n",
      "    val_loss       : -1796.2064819335938\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1796.2064819335938\n",
      "Train Epoch: 444 [0/54000 (0%)] Loss: -1806.477905\n",
      "Train Epoch: 444 [11264/54000 (21%)] Loss: -1807.938721\n",
      "Train Epoch: 444 [22528/54000 (42%)] Loss: -1793.705078\n",
      "Train Epoch: 444 [33792/54000 (63%)] Loss: -1788.962891\n",
      "Train Epoch: 444 [45056/54000 (83%)] Loss: -1790.999023\n",
      "    epoch          : 444\n",
      "    loss           : -1797.844913122789\n",
      "    ess            : 8.0011832939004\n",
      "    log_marginal   : 1797.844913122789\n",
      "    val_loss       : -1799.6695556640625\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1799.6695556640625\n",
      "Train Epoch: 445 [0/54000 (0%)] Loss: -1805.717163\n",
      "Train Epoch: 445 [11264/54000 (21%)] Loss: -1804.564697\n",
      "Train Epoch: 445 [22528/54000 (42%)] Loss: -1811.527832\n",
      "Train Epoch: 445 [33792/54000 (63%)] Loss: -1788.102783\n",
      "Train Epoch: 445 [45056/54000 (83%)] Loss: -1803.217041\n",
      "    epoch          : 445\n",
      "    loss           : -1799.792422888414\n",
      "    ess            : 8.001183023992574\n",
      "    log_marginal   : 1799.7924240400207\n",
      "    val_loss       : -1801.4395751953125\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1801.4395751953125\n",
      "Train Epoch: 446 [0/54000 (0%)] Loss: -1803.464600\n",
      "Train Epoch: 446 [11264/54000 (21%)] Loss: -1810.768555\n",
      "Train Epoch: 446 [22528/54000 (42%)] Loss: -1807.017578\n",
      "Train Epoch: 446 [33792/54000 (63%)] Loss: -1803.242188\n",
      "Train Epoch: 446 [45056/54000 (83%)] Loss: -1796.176758\n",
      "    epoch          : 446\n",
      "    loss           : -1807.1194204654334\n",
      "    ess            : 8.001182844054025\n",
      "    log_marginal   : 1807.1194204654334\n",
      "    val_loss       : -1809.5079650878906\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1809.5079650878906\n",
      "Train Epoch: 447 [0/54000 (0%)] Loss: -1821.677612\n",
      "Train Epoch: 447 [11264/54000 (21%)] Loss: -1800.520752\n",
      "Train Epoch: 447 [22528/54000 (42%)] Loss: -1782.293945\n",
      "Train Epoch: 447 [33792/54000 (63%)] Loss: -1807.634277\n",
      "Train Epoch: 447 [45056/54000 (83%)] Loss: -1800.704712\n",
      "    epoch          : 447\n",
      "    loss           : -1798.8054636829304\n",
      "    ess            : 8.001183086971068\n",
      "    log_marginal   : 1798.8054636829304\n",
      "    val_loss       : -1798.1604817708333\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1798.1604817708333\n",
      "Train Epoch: 448 [0/54000 (0%)] Loss: -1805.054199\n",
      "Train Epoch: 448 [11264/54000 (21%)] Loss: -1809.099121\n",
      "Train Epoch: 448 [22528/54000 (42%)] Loss: -1803.114990\n",
      "Train Epoch: 448 [33792/54000 (63%)] Loss: -1794.768311\n",
      "Train Epoch: 448 [45056/54000 (83%)] Loss: -1792.200439\n",
      "    epoch          : 448\n",
      "    loss           : -1801.8690876510907\n",
      "    ess            : 8.001183500829733\n",
      "    log_marginal   : 1801.8690899543042\n",
      "    val_loss       : -1797.7930806477864\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1797.7930603027344\n",
      "Train Epoch: 449 [0/54000 (0%)] Loss: -1793.957520\n",
      "Train Epoch: 449 [11264/54000 (21%)] Loss: -1785.398315\n",
      "Train Epoch: 449 [22528/54000 (42%)] Loss: -1804.828003\n",
      "Train Epoch: 449 [33792/54000 (63%)] Loss: -1791.537964\n",
      "Train Epoch: 449 [45056/54000 (83%)] Loss: -1799.075317\n",
      "    epoch          : 449\n",
      "    loss           : -1798.5071226875737\n",
      "    ess            : 8.001183635783645\n",
      "    log_marginal   : 1798.5071226875737\n",
      "    val_loss       : -1811.6927998860676\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1811.6927998860676\n",
      "Train Epoch: 450 [0/54000 (0%)] Loss: -1813.135010\n",
      "Train Epoch: 450 [11264/54000 (21%)] Loss: -1792.576904\n",
      "Train Epoch: 450 [22528/54000 (42%)] Loss: -1792.227173\n",
      "Train Epoch: 450 [33792/54000 (63%)] Loss: -1807.019775\n",
      "Train Epoch: 450 [45056/54000 (83%)] Loss: -1803.983398\n",
      "    epoch          : 450\n",
      "    loss           : -1804.1281369767098\n",
      "    ess            : 8.001182331229156\n",
      "    log_marginal   : 1804.1281369767098\n",
      "    val_loss       : -1809.494160970052\n",
      "    val_ess        : 8.001182794570923\n",
      "    val_log_marginal: 1809.4941711425781\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch450.pth ...\n",
      "Train Epoch: 451 [0/54000 (0%)] Loss: -1818.499023\n",
      "Train Epoch: 451 [11264/54000 (21%)] Loss: -1808.370483\n",
      "Train Epoch: 451 [22528/54000 (42%)] Loss: -1809.061768\n",
      "Train Epoch: 451 [33792/54000 (63%)] Loss: -1793.921753\n",
      "Train Epoch: 451 [45056/54000 (83%)] Loss: -1801.628052\n",
      "    epoch          : 451\n",
      "    loss           : -1806.912051794664\n",
      "    ess            : 8.00118282606017\n",
      "    log_marginal   : 1806.912051794664\n",
      "    val_loss       : -1809.6488647460938\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1809.6488647460938\n",
      "Train Epoch: 452 [0/54000 (0%)] Loss: -1811.821899\n",
      "Train Epoch: 452 [11264/54000 (21%)] Loss: -1804.842773\n",
      "Train Epoch: 452 [22528/54000 (42%)] Loss: -1801.542603\n",
      "Train Epoch: 452 [33792/54000 (63%)] Loss: -1778.338867\n",
      "Train Epoch: 452 [45056/54000 (83%)] Loss: -1791.616211\n",
      "    epoch          : 452\n",
      "    loss           : -1795.074839466023\n",
      "    ess            : 8.001182502170778\n",
      "    log_marginal   : 1795.074839466023\n",
      "    val_loss       : -1776.6635843912761\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1776.6635843912761\n",
      "Train Epoch: 453 [0/54000 (0%)] Loss: -1790.910522\n",
      "Train Epoch: 453 [11264/54000 (21%)] Loss: -1777.400635\n",
      "Train Epoch: 453 [22528/54000 (42%)] Loss: -1782.969116\n",
      "Train Epoch: 453 [33792/54000 (63%)] Loss: -1787.024658\n",
      "Train Epoch: 453 [45056/54000 (83%)] Loss: -1762.082275\n",
      "    epoch          : 453\n",
      "    loss           : -1786.6581017836086\n",
      "    ess            : 8.001184643439526\n",
      "    log_marginal   : 1786.6581017836086\n",
      "    val_loss       : -1772.4625752766926\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1772.4625651041667\n",
      "Train Epoch: 454 [0/54000 (0%)] Loss: -1806.164062\n",
      "Train Epoch: 454 [11264/54000 (21%)] Loss: -1801.208496\n",
      "Train Epoch: 454 [22528/54000 (42%)] Loss: -1789.721313\n",
      "Train Epoch: 454 [33792/54000 (63%)] Loss: -1793.516968\n",
      "Train Epoch: 454 [45056/54000 (83%)] Loss: -1782.299316\n",
      "    epoch          : 454\n",
      "    loss           : -1793.8113357256043\n",
      "    ess            : 8.001184508485615\n",
      "    log_marginal   : 1793.8113357256043\n",
      "    val_loss       : -1792.4296976725261\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1792.4296976725261\n",
      "Train Epoch: 455 [0/54000 (0%)] Loss: -1810.238892\n",
      "Train Epoch: 455 [11264/54000 (21%)] Loss: -1791.919678\n",
      "Train Epoch: 455 [22528/54000 (42%)] Loss: -1796.947754\n",
      "Train Epoch: 455 [33792/54000 (63%)] Loss: -1792.459106\n",
      "Train Epoch: 455 [45056/54000 (83%)] Loss: -1795.812134\n",
      "    epoch          : 455\n",
      "    loss           : -1795.823808778007\n",
      "    ess            : 8.001184292559353\n",
      "    log_marginal   : 1795.823808778007\n",
      "    val_loss       : -1798.5458475748699\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1798.5458475748699\n",
      "Train Epoch: 456 [0/54000 (0%)] Loss: -1805.229248\n",
      "Train Epoch: 456 [11264/54000 (21%)] Loss: -1812.618408\n",
      "Train Epoch: 456 [22528/54000 (42%)] Loss: -1807.423096\n",
      "Train Epoch: 456 [33792/54000 (63%)] Loss: -1804.145630\n",
      "Train Epoch: 456 [45056/54000 (83%)] Loss: -1802.273926\n",
      "    epoch          : 456\n",
      "    loss           : -1802.4489734577683\n",
      "    ess            : 8.001183383869675\n",
      "    log_marginal   : 1802.4489723061615\n",
      "    val_loss       : -1804.7719624837239\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1804.7719624837239\n",
      "Train Epoch: 457 [0/54000 (0%)] Loss: -1818.644043\n",
      "Train Epoch: 457 [11264/54000 (21%)] Loss: -1808.248413\n",
      "Train Epoch: 457 [22528/54000 (42%)] Loss: -1795.035400\n",
      "Train Epoch: 457 [33792/54000 (63%)] Loss: -1786.702148\n",
      "Train Epoch: 457 [45056/54000 (83%)] Loss: -1802.774414\n",
      "    epoch          : 457\n",
      "    loss           : -1798.4153879993366\n",
      "    ess            : 8.001184103623876\n",
      "    log_marginal   : 1798.4153879993366\n",
      "    val_loss       : -1804.9258321126301\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.9258321126301\n",
      "Train Epoch: 458 [0/54000 (0%)] Loss: -1815.174805\n",
      "Train Epoch: 458 [11264/54000 (21%)] Loss: -1802.430420\n",
      "Train Epoch: 458 [22528/54000 (42%)] Loss: -1800.777100\n",
      "Train Epoch: 458 [33792/54000 (63%)] Loss: -1798.649658\n",
      "Train Epoch: 458 [45056/54000 (83%)] Loss: -1794.888184\n",
      "    epoch          : 458\n",
      "    loss           : -1803.4943939784787\n",
      "    ess            : 8.00118440951941\n",
      "    log_marginal   : 1803.4943939784787\n",
      "    val_loss       : -1802.9492797851562\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1802.9492797851562\n",
      "Train Epoch: 459 [0/54000 (0%)] Loss: -1814.001221\n",
      "Train Epoch: 459 [11264/54000 (21%)] Loss: -1805.313477\n",
      "Train Epoch: 459 [22528/54000 (42%)] Loss: -1800.627441\n",
      "Train Epoch: 459 [33792/54000 (63%)] Loss: -1801.437134\n",
      "Train Epoch: 459 [45056/54000 (83%)] Loss: -1796.781250\n",
      "    epoch          : 459\n",
      "    loss           : -1799.2400455114976\n",
      "    ess            : 8.001183347881964\n",
      "    log_marginal   : 1799.2400455114976\n",
      "    val_loss       : -1805.3938090006511\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1805.3938090006511\n",
      "Train Epoch: 460 [0/54000 (0%)] Loss: -1804.908325\n",
      "Train Epoch: 460 [11264/54000 (21%)] Loss: -1798.384033\n",
      "Train Epoch: 460 [22528/54000 (42%)] Loss: -1809.557495\n",
      "Train Epoch: 460 [33792/54000 (63%)] Loss: -1789.103394\n",
      "Train Epoch: 460 [45056/54000 (83%)] Loss: -1802.635498\n",
      "    epoch          : 460\n",
      "    loss           : -1800.0964102115272\n",
      "    ess            : 8.001183491832805\n",
      "    log_marginal   : 1800.0964102115272\n",
      "    val_loss       : -1801.7027079264324\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.7027079264324\n",
      "Train Epoch: 461 [0/54000 (0%)] Loss: -1812.815552\n",
      "Train Epoch: 461 [11264/54000 (21%)] Loss: -1813.538086\n",
      "Train Epoch: 461 [22528/54000 (42%)] Loss: -1806.626221\n",
      "Train Epoch: 461 [33792/54000 (63%)] Loss: -1798.382568\n",
      "Train Epoch: 461 [45056/54000 (83%)] Loss: -1797.377319\n",
      "    epoch          : 461\n",
      "    loss           : -1803.3604863004864\n",
      "    ess            : 8.001183491832805\n",
      "    log_marginal   : 1803.3604851488797\n",
      "    val_loss       : -1781.1910705566406\n",
      "    val_ess        : 8.001184145609537\n",
      "    val_log_marginal: 1781.1910807291667\n",
      "Train Epoch: 462 [0/54000 (0%)] Loss: -1786.835938\n",
      "Train Epoch: 462 [11264/54000 (21%)] Loss: -1768.800049\n",
      "Train Epoch: 462 [22528/54000 (42%)] Loss: -1756.405029\n",
      "Train Epoch: 462 [33792/54000 (63%)] Loss: -1761.694824\n",
      "Train Epoch: 462 [45056/54000 (83%)] Loss: -1778.182739\n",
      "    epoch          : 462\n",
      "    loss           : -1768.9209790499706\n",
      "    ess            : 8.00118528222138\n",
      "    log_marginal   : 1768.9209778983638\n",
      "    val_loss       : -1793.3053385416667\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1793.3053181966145\n",
      "Train Epoch: 463 [0/54000 (0%)] Loss: -1809.770996\n",
      "Train Epoch: 463 [11264/54000 (21%)] Loss: -1790.071533\n",
      "Train Epoch: 463 [22528/54000 (42%)] Loss: -1790.745361\n",
      "Train Epoch: 463 [33792/54000 (63%)] Loss: -1790.271729\n",
      "Train Epoch: 463 [45056/54000 (83%)] Loss: -1780.844727\n",
      "    epoch          : 463\n",
      "    loss           : -1789.212704064711\n",
      "    ess            : 8.0011849943197\n",
      "    log_marginal   : 1789.212704064711\n",
      "    val_loss       : -1800.4524943033855\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1800.4524943033855\n",
      "Train Epoch: 464 [0/54000 (0%)] Loss: -1813.273560\n",
      "Train Epoch: 464 [11264/54000 (21%)] Loss: -1810.341919\n",
      "Train Epoch: 464 [22528/54000 (42%)] Loss: -1794.809814\n",
      "Train Epoch: 464 [33792/54000 (63%)] Loss: -1799.339111\n",
      "Train Epoch: 464 [45056/54000 (83%)] Loss: -1789.150635\n",
      "    epoch          : 464\n",
      "    loss           : -1801.031229271079\n",
      "    ess            : 8.001184679427237\n",
      "    log_marginal   : 1801.031229271079\n",
      "    val_loss       : -1804.9286600748699\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1804.9286600748699\n",
      "Train Epoch: 465 [0/54000 (0%)] Loss: -1801.743530\n",
      "Train Epoch: 465 [11264/54000 (21%)] Loss: -1792.242432\n",
      "Train Epoch: 465 [22528/54000 (42%)] Loss: -1800.540527\n",
      "Train Epoch: 465 [33792/54000 (63%)] Loss: -1794.681030\n",
      "Train Epoch: 465 [45056/54000 (83%)] Loss: -1795.849365\n",
      "    epoch          : 465\n",
      "    loss           : -1796.5400333044663\n",
      "    ess            : 8.001184589457962\n",
      "    log_marginal   : 1796.5400333044663\n",
      "    val_loss       : -1799.9598795572917\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1799.9598795572917\n",
      "Train Epoch: 466 [0/54000 (0%)] Loss: -1810.469482\n",
      "Train Epoch: 466 [11264/54000 (21%)] Loss: -1807.386719\n",
      "Train Epoch: 466 [22528/54000 (42%)] Loss: -1805.961792\n",
      "Train Epoch: 466 [33792/54000 (63%)] Loss: -1804.597412\n",
      "Train Epoch: 466 [45056/54000 (83%)] Loss: -1804.189697\n",
      "    epoch          : 466\n",
      "    loss           : -1801.7680318580483\n",
      "    ess            : 8.001184103623876\n",
      "    log_marginal   : 1801.7680307064416\n",
      "    val_loss       : -1801.9850056966145\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1801.9850056966145\n",
      "Train Epoch: 467 [0/54000 (0%)] Loss: -1807.235474\n",
      "Train Epoch: 467 [11264/54000 (21%)] Loss: -1804.370361\n",
      "Train Epoch: 467 [22528/54000 (42%)] Loss: -1780.694580\n",
      "Train Epoch: 467 [33792/54000 (63%)] Loss: -1789.829468\n",
      "Train Epoch: 467 [45056/54000 (83%)] Loss: -1782.714844\n",
      "    epoch          : 467\n",
      "    loss           : -1793.144325112397\n",
      "    ess            : 8.001184769396511\n",
      "    log_marginal   : 1793.1443262640034\n",
      "    val_loss       : -1803.1688944498699\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1803.1688944498699\n",
      "Train Epoch: 468 [0/54000 (0%)] Loss: -1812.248901\n",
      "Train Epoch: 468 [11264/54000 (21%)] Loss: -1791.790771\n",
      "Train Epoch: 468 [22528/54000 (42%)] Loss: -1794.180908\n",
      "Train Epoch: 468 [33792/54000 (63%)] Loss: -1794.766602\n",
      "Train Epoch: 468 [45056/54000 (83%)] Loss: -1797.068115\n",
      "    epoch          : 468\n",
      "    loss           : -1797.2570996554393\n",
      "    ess            : 8.001183716755992\n",
      "    log_marginal   : 1797.2570996554393\n",
      "    val_loss       : -1802.8366394042969\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1802.8366190592449\n",
      "Train Epoch: 469 [0/54000 (0%)] Loss: -1808.477051\n",
      "Train Epoch: 469 [11264/54000 (21%)] Loss: -1803.571411\n",
      "Train Epoch: 469 [22528/54000 (42%)] Loss: -1795.973633\n",
      "Train Epoch: 469 [33792/54000 (63%)] Loss: -1796.427612\n",
      "Train Epoch: 469 [45056/54000 (83%)] Loss: -1807.761597\n",
      "    epoch          : 469\n",
      "    loss           : -1801.9330778301887\n",
      "    ess            : 8.001184607451817\n",
      "    log_marginal   : 1801.9330778301887\n",
      "    val_loss       : -1810.8130493164062\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1810.8130493164062\n",
      "Train Epoch: 470 [0/54000 (0%)] Loss: -1817.498779\n",
      "Train Epoch: 470 [11264/54000 (21%)] Loss: -1792.311768\n",
      "Train Epoch: 470 [22528/54000 (42%)] Loss: -1778.844360\n",
      "Train Epoch: 470 [33792/54000 (63%)] Loss: -1799.507324\n",
      "Train Epoch: 470 [45056/54000 (83%)] Loss: -1804.949585\n",
      "    epoch          : 470\n",
      "    loss           : -1800.4107585403156\n",
      "    ess            : 8.001184832375005\n",
      "    log_marginal   : 1800.4107585403156\n",
      "    val_loss       : -1809.5522054036458\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1809.5522054036458\n",
      "Train Epoch: 471 [0/54000 (0%)] Loss: -1820.272949\n",
      "Train Epoch: 471 [11264/54000 (21%)] Loss: -1820.225586\n",
      "Train Epoch: 471 [22528/54000 (42%)] Loss: -1803.443359\n",
      "Train Epoch: 471 [33792/54000 (63%)] Loss: -1784.776489\n",
      "Train Epoch: 471 [45056/54000 (83%)] Loss: -1764.445557\n",
      "    epoch          : 471\n",
      "    loss           : -1794.4477239644752\n",
      "    ess            : 8.001184589457962\n",
      "    log_marginal   : 1794.4477239644752\n",
      "    val_loss       : -1792.18701171875\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1792.18701171875\n",
      "Train Epoch: 472 [0/54000 (0%)] Loss: -1804.837158\n",
      "Train Epoch: 472 [11264/54000 (21%)] Loss: -1800.816406\n",
      "Train Epoch: 472 [22528/54000 (42%)] Loss: -1793.670776\n",
      "Train Epoch: 472 [33792/54000 (63%)] Loss: -1794.714355\n",
      "Train Epoch: 472 [45056/54000 (83%)] Loss: -1771.968140\n",
      "    epoch          : 472\n",
      "    loss           : -1789.9891011939858\n",
      "    ess            : 8.001184859365788\n",
      "    log_marginal   : 1789.9891011939858\n",
      "    val_loss       : -1799.0083618164062\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1799.0083414713542\n",
      "Train Epoch: 473 [0/54000 (0%)] Loss: -1808.883301\n",
      "Train Epoch: 473 [11264/54000 (21%)] Loss: -1792.097900\n",
      "Train Epoch: 473 [22528/54000 (42%)] Loss: -1793.949097\n",
      "Train Epoch: 473 [33792/54000 (63%)] Loss: -1786.509033\n",
      "Train Epoch: 473 [45056/54000 (83%)] Loss: -1781.939087\n",
      "    epoch          : 473\n",
      "    loss           : -1789.0635375976562\n",
      "    ess            : 8.001185435169148\n",
      "    log_marginal   : 1789.0635375976562\n",
      "    val_loss       : -1795.1504720052083\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1795.1504720052083\n",
      "Train Epoch: 474 [0/54000 (0%)] Loss: -1805.493652\n",
      "Train Epoch: 474 [11264/54000 (21%)] Loss: -1796.019287\n",
      "Train Epoch: 474 [22528/54000 (42%)] Loss: -1811.801758\n",
      "Train Epoch: 474 [33792/54000 (63%)] Loss: -1791.064697\n",
      "Train Epoch: 474 [45056/54000 (83%)] Loss: -1791.764160\n",
      "    epoch          : 474\n",
      "    loss           : -1799.506567613134\n",
      "    ess            : 8.001184427513266\n",
      "    log_marginal   : 1799.5065687647407\n",
      "    val_loss       : -1809.3724975585938\n",
      "    val_ess        : 8.00118350982666\n",
      "    val_log_marginal: 1809.3724975585938\n",
      "Train Epoch: 475 [0/54000 (0%)] Loss: -1813.335083\n",
      "Train Epoch: 475 [11264/54000 (21%)] Loss: -1794.933350\n",
      "Train Epoch: 475 [22528/54000 (42%)] Loss: -1793.030396\n",
      "Train Epoch: 475 [33792/54000 (63%)] Loss: -1804.502197\n",
      "Train Epoch: 475 [45056/54000 (83%)] Loss: -1795.216797\n",
      "    epoch          : 475\n",
      "    loss           : -1801.6192500276386\n",
      "    ess            : 8.001184373531702\n",
      "    log_marginal   : 1801.6192511792453\n",
      "    val_loss       : -1808.3369547526042\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1808.3369445800781\n",
      "Train Epoch: 476 [0/54000 (0%)] Loss: -1817.954346\n",
      "Train Epoch: 476 [11264/54000 (21%)] Loss: -1810.827759\n",
      "Train Epoch: 476 [22528/54000 (42%)] Loss: -1806.771729\n",
      "Train Epoch: 476 [33792/54000 (63%)] Loss: -1798.642578\n",
      "Train Epoch: 476 [45056/54000 (83%)] Loss: -1804.488403\n",
      "    epoch          : 476\n",
      "    loss           : -1802.1298689932194\n",
      "    ess            : 8.001183851709905\n",
      "    log_marginal   : 1802.1298689932194\n",
      "    val_loss       : -1802.1445821126301\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1802.1446024576824\n",
      "Train Epoch: 477 [0/54000 (0%)] Loss: -1821.077881\n",
      "Train Epoch: 477 [11264/54000 (21%)] Loss: -1806.661377\n",
      "Train Epoch: 477 [22528/54000 (42%)] Loss: -1804.734985\n",
      "Train Epoch: 477 [33792/54000 (63%)] Loss: -1806.839600\n",
      "Train Epoch: 477 [45056/54000 (83%)] Loss: -1800.595947\n",
      "    epoch          : 477\n",
      "    loss           : -1807.0213634562942\n",
      "    ess            : 8.001184211587006\n",
      "    log_marginal   : 1807.0213634562942\n",
      "    val_loss       : -1803.6241149902344\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1803.6241353352864\n",
      "Train Epoch: 478 [0/54000 (0%)] Loss: -1804.143799\n",
      "Train Epoch: 478 [11264/54000 (21%)] Loss: -1809.430054\n",
      "Train Epoch: 478 [22528/54000 (42%)] Loss: -1796.246948\n",
      "Train Epoch: 478 [33792/54000 (63%)] Loss: -1803.836670\n",
      "Train Epoch: 478 [45056/54000 (83%)] Loss: -1801.064331\n",
      "    epoch          : 478\n",
      "    loss           : -1803.4703115787147\n",
      "    ess            : 8.001183500829733\n",
      "    log_marginal   : 1803.4703115787147\n",
      "    val_loss       : -1802.5916239420574\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1802.5916239420574\n",
      "Train Epoch: 479 [0/54000 (0%)] Loss: -1805.632324\n",
      "Train Epoch: 479 [11264/54000 (21%)] Loss: -1794.017944\n",
      "Train Epoch: 479 [22528/54000 (42%)] Loss: -1781.737671\n",
      "Train Epoch: 479 [33792/54000 (63%)] Loss: -1792.132080\n",
      "Train Epoch: 479 [45056/54000 (83%)] Loss: -1788.307739\n",
      "    epoch          : 479\n",
      "    loss           : -1792.0516576227153\n",
      "    ess            : 8.00118430155628\n",
      "    log_marginal   : 1792.0516587743218\n",
      "    val_loss       : -1801.425272623698\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.425272623698\n",
      "Train Epoch: 480 [0/54000 (0%)] Loss: -1809.411743\n",
      "Train Epoch: 480 [11264/54000 (21%)] Loss: -1805.776855\n",
      "Train Epoch: 480 [22528/54000 (42%)] Loss: -1800.813110\n",
      "Train Epoch: 480 [33792/54000 (63%)] Loss: -1804.266479\n",
      "Train Epoch: 480 [45056/54000 (83%)] Loss: -1805.204102\n",
      "    epoch          : 480\n",
      "    loss           : -1803.6311680056015\n",
      "    ess            : 8.00118368976521\n",
      "    log_marginal   : 1803.6311680056015\n",
      "    val_loss       : -1796.6781107584636\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1796.6781107584636\n",
      "Train Epoch: 481 [0/54000 (0%)] Loss: -1800.283203\n",
      "Train Epoch: 481 [11264/54000 (21%)] Loss: -1795.247559\n",
      "Train Epoch: 481 [22528/54000 (42%)] Loss: -1807.936035\n",
      "Train Epoch: 481 [33792/54000 (63%)] Loss: -1798.826538\n",
      "Train Epoch: 481 [45056/54000 (83%)] Loss: -1790.588379\n",
      "    epoch          : 481\n",
      "    loss           : -1800.0960704875442\n",
      "    ess            : 8.00118361778979\n",
      "    log_marginal   : 1800.0960693359375\n",
      "    val_loss       : -1802.9116109212239\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1802.9116109212239\n",
      "Train Epoch: 482 [0/54000 (0%)] Loss: -1817.428101\n",
      "Train Epoch: 482 [11264/54000 (21%)] Loss: -1815.934326\n",
      "Train Epoch: 482 [22528/54000 (42%)] Loss: -1788.879272\n",
      "Train Epoch: 482 [33792/54000 (63%)] Loss: -1792.386108\n",
      "Train Epoch: 482 [45056/54000 (83%)] Loss: -1757.248047\n",
      "    epoch          : 482\n",
      "    loss           : -1788.607190402049\n",
      "    ess            : 8.001184391525557\n",
      "    log_marginal   : 1788.607190402049\n",
      "    val_loss       : -1784.8318888346355\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1784.8318888346355\n",
      "Train Epoch: 483 [0/54000 (0%)] Loss: -1797.895752\n",
      "Train Epoch: 483 [11264/54000 (21%)] Loss: -1795.508057\n",
      "Train Epoch: 483 [22528/54000 (42%)] Loss: -1786.505371\n",
      "Train Epoch: 483 [33792/54000 (63%)] Loss: -1810.127686\n",
      "Train Epoch: 483 [45056/54000 (83%)] Loss: -1801.601318\n",
      "    epoch          : 483\n",
      "    loss           : -1800.3644904370578\n",
      "    ess            : 8.001184337543991\n",
      "    log_marginal   : 1800.364489285451\n",
      "    val_loss       : -1803.9495544433594\n",
      "    val_ess        : 8.001185019810995\n",
      "    val_log_marginal: 1803.9495544433594\n",
      "Train Epoch: 484 [0/54000 (0%)] Loss: -1810.122803\n",
      "Train Epoch: 484 [11264/54000 (21%)] Loss: -1804.621582\n",
      "Train Epoch: 484 [22528/54000 (42%)] Loss: -1790.237915\n",
      "Train Epoch: 484 [33792/54000 (63%)] Loss: -1792.333374\n",
      "Train Epoch: 484 [45056/54000 (83%)] Loss: -1796.022339\n",
      "    epoch          : 484\n",
      "    loss           : -1795.8061845887382\n",
      "    ess            : 8.001183635783645\n",
      "    log_marginal   : 1795.806185740345\n",
      "    val_loss       : -1802.6962788899739\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1802.6962788899739\n",
      "Train Epoch: 485 [0/54000 (0%)] Loss: -1809.121338\n",
      "Train Epoch: 485 [11264/54000 (21%)] Loss: -1803.283936\n",
      "Train Epoch: 485 [22528/54000 (42%)] Loss: -1813.211426\n",
      "Train Epoch: 485 [33792/54000 (63%)] Loss: -1807.189941\n",
      "Train Epoch: 485 [45056/54000 (83%)] Loss: -1794.906250\n",
      "    epoch          : 485\n",
      "    loss           : -1804.275514998526\n",
      "    ess            : 8.001182799069387\n",
      "    log_marginal   : 1804.275514998526\n",
      "    val_loss       : -1804.5468648274739\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1804.5468648274739\n",
      "Train Epoch: 486 [0/54000 (0%)] Loss: -1817.125610\n",
      "Train Epoch: 486 [11264/54000 (21%)] Loss: -1795.101562\n",
      "Train Epoch: 486 [22528/54000 (42%)] Loss: -1797.645874\n",
      "Train Epoch: 486 [33792/54000 (63%)] Loss: -1806.405640\n",
      "Train Epoch: 486 [45056/54000 (83%)] Loss: -1801.388916\n",
      "    epoch          : 486\n",
      "    loss           : -1798.4252572689416\n",
      "    ess            : 8.00118383371605\n",
      "    log_marginal   : 1798.4252572689416\n",
      "    val_loss       : -1799.808349609375\n",
      "    val_ess        : 8.00118605295817\n",
      "    val_log_marginal: 1799.808369954427\n",
      "Train Epoch: 487 [0/54000 (0%)] Loss: -1813.392578\n",
      "Train Epoch: 487 [11264/54000 (21%)] Loss: -1793.034302\n",
      "Train Epoch: 487 [22528/54000 (42%)] Loss: -1785.663574\n",
      "Train Epoch: 487 [33792/54000 (63%)] Loss: -1785.543457\n",
      "Train Epoch: 487 [45056/54000 (83%)] Loss: -1789.676880\n",
      "    epoch          : 487\n",
      "    loss           : -1792.323738529997\n",
      "    ess            : 8.001184004657674\n",
      "    log_marginal   : 1792.323738529997\n",
      "    val_loss       : -1804.096903483073\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1804.096903483073\n",
      "Train Epoch: 488 [0/54000 (0%)] Loss: -1816.490234\n",
      "Train Epoch: 488 [11264/54000 (21%)] Loss: -1803.302246\n",
      "Train Epoch: 488 [22528/54000 (42%)] Loss: -1798.623535\n",
      "Train Epoch: 488 [33792/54000 (63%)] Loss: -1793.550293\n",
      "Train Epoch: 488 [45056/54000 (83%)] Loss: -1779.072266\n",
      "    epoch          : 488\n",
      "    loss           : -1797.128662109375\n",
      "    ess            : 8.001184157605442\n",
      "    log_marginal   : 1797.128662109375\n",
      "    val_loss       : -1801.8666687011719\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1801.8666585286458\n",
      "Train Epoch: 489 [0/54000 (0%)] Loss: -1798.737305\n",
      "Train Epoch: 489 [11264/54000 (21%)] Loss: -1802.162476\n",
      "Train Epoch: 489 [22528/54000 (42%)] Loss: -1790.635376\n",
      "Train Epoch: 489 [33792/54000 (63%)] Loss: -1790.719727\n",
      "Train Epoch: 489 [45056/54000 (83%)] Loss: -1779.427979\n",
      "    epoch          : 489\n",
      "    loss           : -1790.157446519384\n",
      "    ess            : 8.001185417175293\n",
      "    log_marginal   : 1790.157446519384\n",
      "    val_loss       : -1793.5488891601562\n",
      "    val_ess        : 8.001184701919556\n",
      "    val_log_marginal: 1793.5488891601562\n",
      "Train Epoch: 490 [0/54000 (0%)] Loss: -1801.893311\n",
      "Train Epoch: 490 [11264/54000 (21%)] Loss: -1787.880371\n",
      "Train Epoch: 490 [22528/54000 (42%)] Loss: -1776.873413\n",
      "Train Epoch: 490 [33792/54000 (63%)] Loss: -1768.849121\n",
      "Train Epoch: 490 [45056/54000 (83%)] Loss: -1785.043457\n",
      "    epoch          : 490\n",
      "    loss           : -1782.475266942438\n",
      "    ess            : 8.00118521024596\n",
      "    log_marginal   : 1782.4752680940448\n",
      "    val_loss       : -1781.3775126139324\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1781.3775024414062\n",
      "Train Epoch: 491 [0/54000 (0%)] Loss: -1782.538696\n",
      "Train Epoch: 491 [11264/54000 (21%)] Loss: -1799.241333\n",
      "Train Epoch: 491 [22528/54000 (42%)] Loss: -1791.869873\n",
      "Train Epoch: 491 [33792/54000 (63%)] Loss: -1793.443359\n",
      "Train Epoch: 491 [45056/54000 (83%)] Loss: -1786.284668\n",
      "    epoch          : 491\n",
      "    loss           : -1792.1253212982754\n",
      "    ess            : 8.001185201249033\n",
      "    log_marginal   : 1792.125322449882\n",
      "    val_loss       : -1795.4107055664062\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1795.4107055664062\n",
      "Train Epoch: 492 [0/54000 (0%)] Loss: -1799.676025\n",
      "Train Epoch: 492 [11264/54000 (21%)] Loss: -1806.097168\n",
      "Train Epoch: 492 [22528/54000 (42%)] Loss: -1798.923950\n",
      "Train Epoch: 492 [33792/54000 (63%)] Loss: -1800.754883\n",
      "Train Epoch: 492 [45056/54000 (83%)] Loss: -1794.454712\n",
      "    epoch          : 492\n",
      "    loss           : -1799.95975019347\n",
      "    ess            : 8.001185309212163\n",
      "    log_marginal   : 1799.95975019347\n",
      "    val_loss       : -1811.8317260742188\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1811.8317362467449\n",
      "Train Epoch: 493 [0/54000 (0%)] Loss: -1813.557373\n",
      "Train Epoch: 493 [11264/54000 (21%)] Loss: -1801.836548\n",
      "Train Epoch: 493 [22528/54000 (42%)] Loss: -1791.234375\n",
      "Train Epoch: 493 [33792/54000 (63%)] Loss: -1808.216431\n",
      "Train Epoch: 493 [45056/54000 (83%)] Loss: -1796.603760\n",
      "    epoch          : 493\n",
      "    loss           : -1801.2438826650944\n",
      "    ess            : 8.001184571464107\n",
      "    log_marginal   : 1801.2438815134876\n",
      "    val_loss       : -1804.0714721679688\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1804.0714619954426\n",
      "Train Epoch: 494 [0/54000 (0%)] Loss: -1809.509277\n",
      "Train Epoch: 494 [11264/54000 (21%)] Loss: -1804.795654\n",
      "Train Epoch: 494 [22528/54000 (42%)] Loss: -1799.893555\n",
      "Train Epoch: 494 [33792/54000 (63%)] Loss: -1798.270752\n",
      "Train Epoch: 494 [45056/54000 (83%)] Loss: -1791.916504\n",
      "    epoch          : 494\n",
      "    loss           : -1800.711574338517\n",
      "    ess            : 8.001184553470251\n",
      "    log_marginal   : 1800.711574338517\n",
      "    val_loss       : -1804.0988057454426\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1804.0988057454426\n",
      "Train Epoch: 495 [0/54000 (0%)] Loss: -1805.187012\n",
      "Train Epoch: 495 [11264/54000 (21%)] Loss: -1805.395020\n",
      "Train Epoch: 495 [22528/54000 (42%)] Loss: -1799.268555\n",
      "Train Epoch: 495 [33792/54000 (63%)] Loss: -1807.099609\n",
      "Train Epoch: 495 [45056/54000 (83%)] Loss: -1805.359131\n",
      "    epoch          : 495\n",
      "    loss           : -1801.4413417600235\n",
      "    ess            : 8.001183905691471\n",
      "    log_marginal   : 1801.4413429116303\n",
      "    val_loss       : -1783.2425944010417\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1783.2425842285156\n",
      "Train Epoch: 496 [0/54000 (0%)] Loss: -1796.665283\n",
      "Train Epoch: 496 [11264/54000 (21%)] Loss: -1773.280273\n",
      "Train Epoch: 496 [22528/54000 (42%)] Loss: -1797.833740\n",
      "Train Epoch: 496 [33792/54000 (63%)] Loss: -1764.072021\n",
      "Train Epoch: 496 [45056/54000 (83%)] Loss: -1792.260742\n",
      "    epoch          : 496\n",
      "    loss           : -1783.3295599019752\n",
      "    ess            : 8.001184733408802\n",
      "    log_marginal   : 1783.3295599019752\n",
      "    val_loss       : -1777.7030537923176\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1777.7030537923176\n",
      "Train Epoch: 497 [0/54000 (0%)] Loss: -1789.576538\n",
      "Train Epoch: 497 [11264/54000 (21%)] Loss: -1787.617676\n",
      "Train Epoch: 497 [22528/54000 (42%)] Loss: -1790.150391\n",
      "Train Epoch: 497 [33792/54000 (63%)] Loss: -1789.669922\n",
      "Train Epoch: 497 [45056/54000 (83%)] Loss: -1781.903442\n",
      "    epoch          : 497\n",
      "    loss           : -1785.4719387990124\n",
      "    ess            : 8.001184841371932\n",
      "    log_marginal   : 1785.4719387990124\n",
      "    val_loss       : -1791.0601094563801\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1791.0601094563801\n",
      "Train Epoch: 498 [0/54000 (0%)] Loss: -1812.946655\n",
      "Train Epoch: 498 [11264/54000 (21%)] Loss: -1805.894531\n",
      "Train Epoch: 498 [22528/54000 (42%)] Loss: -1789.667480\n",
      "Train Epoch: 498 [33792/54000 (63%)] Loss: -1786.077393\n",
      "Train Epoch: 498 [45056/54000 (83%)] Loss: -1787.478516\n",
      "    epoch          : 498\n",
      "    loss           : -1795.5926743993218\n",
      "    ess            : 8.001184391525557\n",
      "    log_marginal   : 1795.5926743993218\n",
      "    val_loss       : -1797.2479553222656\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1797.2479654947917\n",
      "Train Epoch: 499 [0/54000 (0%)] Loss: -1821.255371\n",
      "Train Epoch: 499 [11264/54000 (21%)] Loss: -1808.088989\n",
      "Train Epoch: 499 [22528/54000 (42%)] Loss: -1807.333374\n",
      "Train Epoch: 499 [33792/54000 (63%)] Loss: -1800.926514\n",
      "Train Epoch: 499 [45056/54000 (83%)] Loss: -1810.344482\n",
      "    epoch          : 499\n",
      "    loss           : -1802.31019908977\n",
      "    ess            : 8.001184175599295\n",
      "    log_marginal   : 1802.3102002413768\n",
      "    val_loss       : -1807.1698201497395\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.1698303222656\n",
      "Train Epoch: 500 [0/54000 (0%)] Loss: -1819.233521\n",
      "Train Epoch: 500 [11264/54000 (21%)] Loss: -1789.010010\n",
      "Train Epoch: 500 [22528/54000 (42%)] Loss: -1793.083252\n",
      "Train Epoch: 500 [33792/54000 (63%)] Loss: -1796.694580\n",
      "Train Epoch: 500 [45056/54000 (83%)] Loss: -1791.471436\n",
      "    epoch          : 500\n",
      "    loss           : -1792.5859340451798\n",
      "    ess            : 8.001184859365788\n",
      "    log_marginal   : 1792.5859340451798\n",
      "    val_loss       : -1800.1946614583333\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1800.1946818033855\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch500.pth ...\n",
      "Train Epoch: 501 [0/54000 (0%)] Loss: -1817.224976\n",
      "Train Epoch: 501 [11264/54000 (21%)] Loss: -1801.361084\n",
      "Train Epoch: 501 [22528/54000 (42%)] Loss: -1784.229126\n",
      "Train Epoch: 501 [33792/54000 (63%)] Loss: -1792.085571\n",
      "Train Epoch: 501 [45056/54000 (83%)] Loss: -1793.880127\n",
      "    epoch          : 501\n",
      "    loss           : -1795.4152279260024\n",
      "    ess            : 8.001183716755992\n",
      "    log_marginal   : 1795.4152267743957\n",
      "    val_loss       : -1799.1061604817708\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1799.1061604817708\n",
      "Train Epoch: 502 [0/54000 (0%)] Loss: -1807.448730\n",
      "Train Epoch: 502 [11264/54000 (21%)] Loss: -1812.758057\n",
      "Train Epoch: 502 [22528/54000 (42%)] Loss: -1807.084961\n",
      "Train Epoch: 502 [33792/54000 (63%)] Loss: -1799.765503\n",
      "Train Epoch: 502 [45056/54000 (83%)] Loss: -1798.807861\n",
      "    epoch          : 502\n",
      "    loss           : -1802.7236097803657\n",
      "    ess            : 8.00118358180208\n",
      "    log_marginal   : 1802.7236097803657\n",
      "    val_loss       : -1807.4563090006511\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.4563090006511\n",
      "Train Epoch: 503 [0/54000 (0%)] Loss: -1816.166382\n",
      "Train Epoch: 503 [11264/54000 (21%)] Loss: -1795.485718\n",
      "Train Epoch: 503 [22528/54000 (42%)] Loss: -1803.963745\n",
      "Train Epoch: 503 [33792/54000 (63%)] Loss: -1804.395508\n",
      "Train Epoch: 503 [45056/54000 (83%)] Loss: -1798.804077\n",
      "    epoch          : 503\n",
      "    loss           : -1802.1934756872788\n",
      "    ess            : 8.001182952017155\n",
      "    log_marginal   : 1802.1934756872788\n",
      "    val_loss       : -1812.3034057617188\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1812.3034057617188\n",
      "Train Epoch: 504 [0/54000 (0%)] Loss: -1821.502563\n",
      "Train Epoch: 504 [11264/54000 (21%)] Loss: -1799.974609\n",
      "Train Epoch: 504 [22528/54000 (42%)] Loss: -1808.405396\n",
      "Train Epoch: 504 [33792/54000 (63%)] Loss: -1809.615601\n",
      "Train Epoch: 504 [45056/54000 (83%)] Loss: -1807.920898\n",
      "    epoch          : 504\n",
      "    loss           : -1806.4224531065743\n",
      "    ess            : 8.001183671771356\n",
      "    log_marginal   : 1806.4224531065743\n",
      "    val_loss       : -1811.6212972005208\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1811.6212972005208\n",
      "Train Epoch: 505 [0/54000 (0%)] Loss: -1820.201660\n",
      "Train Epoch: 505 [11264/54000 (21%)] Loss: -1819.316040\n",
      "Train Epoch: 505 [22528/54000 (42%)] Loss: -1805.550293\n",
      "Train Epoch: 505 [33792/54000 (63%)] Loss: -1801.125977\n",
      "Train Epoch: 505 [45056/54000 (83%)] Loss: -1801.425537\n",
      "    epoch          : 505\n",
      "    loss           : -1805.9144160432636\n",
      "    ess            : 8.00118350982666\n",
      "    log_marginal   : 1805.9144160432636\n",
      "    val_loss       : -1796.3966573079426\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1796.3966573079426\n",
      "Train Epoch: 506 [0/54000 (0%)] Loss: -1801.744385\n",
      "Train Epoch: 506 [11264/54000 (21%)] Loss: -1792.524658\n",
      "Train Epoch: 506 [22528/54000 (42%)] Loss: -1804.805176\n",
      "Train Epoch: 506 [33792/54000 (63%)] Loss: -1799.718750\n",
      "Train Epoch: 506 [45056/54000 (83%)] Loss: -1809.450684\n",
      "    epoch          : 506\n",
      "    loss           : -1800.094523879717\n",
      "    ess            : 8.00118336587582\n",
      "    log_marginal   : 1800.0945215765034\n",
      "    val_loss       : -1811.2383524576824\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1811.2383422851562\n",
      "Train Epoch: 507 [0/54000 (0%)] Loss: -1816.176147\n",
      "Train Epoch: 507 [11264/54000 (21%)] Loss: -1808.577393\n",
      "Train Epoch: 507 [22528/54000 (42%)] Loss: -1788.802979\n",
      "Train Epoch: 507 [33792/54000 (63%)] Loss: -1802.674072\n",
      "Train Epoch: 507 [45056/54000 (83%)] Loss: -1794.263672\n",
      "    epoch          : 507\n",
      "    loss           : -1801.8365420935288\n",
      "    ess            : 8.00118304198643\n",
      "    log_marginal   : 1801.836540941922\n",
      "    val_loss       : -1802.9425557454426\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1802.9425557454426\n",
      "Train Epoch: 508 [0/54000 (0%)] Loss: -1808.523682\n",
      "Train Epoch: 508 [11264/54000 (21%)] Loss: -1813.726318\n",
      "Train Epoch: 508 [22528/54000 (42%)] Loss: -1819.633667\n",
      "Train Epoch: 508 [33792/54000 (63%)] Loss: -1817.148560\n",
      "Train Epoch: 508 [45056/54000 (83%)] Loss: -1805.630127\n",
      "    epoch          : 508\n",
      "    loss           : -1811.4187322652565\n",
      "    ess            : 8.001182250256809\n",
      "    log_marginal   : 1811.4187322652565\n",
      "    val_loss       : -1815.9739074707031\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1815.9739074707031\n",
      "Train Epoch: 509 [0/54000 (0%)] Loss: -1816.197876\n",
      "Train Epoch: 509 [11264/54000 (21%)] Loss: -1808.724976\n",
      "Train Epoch: 509 [22528/54000 (42%)] Loss: -1789.361084\n",
      "Train Epoch: 509 [33792/54000 (63%)] Loss: -1807.198608\n",
      "Train Epoch: 509 [45056/54000 (83%)] Loss: -1797.428955\n",
      "    epoch          : 509\n",
      "    loss           : -1801.9317592404923\n",
      "    ess            : 8.001182457186141\n",
      "    log_marginal   : 1801.9317580888855\n",
      "    val_loss       : -1803.9473978678386\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1803.9473978678386\n",
      "Train Epoch: 510 [0/54000 (0%)] Loss: -1808.179688\n",
      "Train Epoch: 510 [11264/54000 (21%)] Loss: -1808.603394\n",
      "Train Epoch: 510 [22528/54000 (42%)] Loss: -1797.586426\n",
      "Train Epoch: 510 [33792/54000 (63%)] Loss: -1801.935303\n",
      "Train Epoch: 510 [45056/54000 (83%)] Loss: -1806.987305\n",
      "    epoch          : 510\n",
      "    loss           : -1806.5774248231132\n",
      "    ess            : 8.001181962355128\n",
      "    log_marginal   : 1806.5774248231132\n",
      "    val_loss       : -1808.7483317057292\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1808.7483317057292\n",
      "Train Epoch: 511 [0/54000 (0%)] Loss: -1819.882812\n",
      "Train Epoch: 511 [11264/54000 (21%)] Loss: -1802.170532\n",
      "Train Epoch: 511 [22528/54000 (42%)] Loss: -1798.563599\n",
      "Train Epoch: 511 [33792/54000 (63%)] Loss: -1800.234863\n",
      "Train Epoch: 511 [45056/54000 (83%)] Loss: -1804.613037\n",
      "    epoch          : 511\n",
      "    loss           : -1803.583831211306\n",
      "    ess            : 8.001182052324403\n",
      "    log_marginal   : 1803.5838323629127\n",
      "    val_loss       : -1805.2013142903645\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1805.2013142903645\n",
      "Train Epoch: 512 [0/54000 (0%)] Loss: -1803.554932\n",
      "Train Epoch: 512 [11264/54000 (21%)] Loss: -1792.831543\n",
      "Train Epoch: 512 [22528/54000 (42%)] Loss: -1794.549805\n",
      "Train Epoch: 512 [33792/54000 (63%)] Loss: -1789.385010\n",
      "Train Epoch: 512 [45056/54000 (83%)] Loss: -1788.121582\n",
      "    epoch          : 512\n",
      "    loss           : -1794.332146410672\n",
      "    ess            : 8.001182655118546\n",
      "    log_marginal   : 1794.332146410672\n",
      "    val_loss       : -1804.8389078776042\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1804.8389078776042\n",
      "Train Epoch: 513 [0/54000 (0%)] Loss: -1824.610107\n",
      "Train Epoch: 513 [11264/54000 (21%)] Loss: -1800.741211\n",
      "Train Epoch: 513 [22528/54000 (42%)] Loss: -1794.815796\n",
      "Train Epoch: 513 [33792/54000 (63%)] Loss: -1807.039673\n",
      "Train Epoch: 513 [45056/54000 (83%)] Loss: -1798.398071\n",
      "    epoch          : 513\n",
      "    loss           : -1799.1490420935288\n",
      "    ess            : 8.001181800410432\n",
      "    log_marginal   : 1799.1490420935288\n",
      "    val_loss       : -1803.6341959635417\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1803.6341959635417\n",
      "Train Epoch: 514 [0/54000 (0%)] Loss: -1815.454956\n",
      "Train Epoch: 514 [11264/54000 (21%)] Loss: -1811.507812\n",
      "Train Epoch: 514 [22528/54000 (42%)] Loss: -1791.127441\n",
      "Train Epoch: 514 [33792/54000 (63%)] Loss: -1768.885986\n",
      "Train Epoch: 514 [45056/54000 (83%)] Loss: -1763.495605\n",
      "    epoch          : 514\n",
      "    loss           : -1787.0837909050708\n",
      "    ess            : 8.001182808066314\n",
      "    log_marginal   : 1787.0837909050708\n",
      "    val_loss       : -1789.6702067057292\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1789.6702067057292\n",
      "Train Epoch: 515 [0/54000 (0%)] Loss: -1793.739014\n",
      "Train Epoch: 515 [11264/54000 (21%)] Loss: -1804.716797\n",
      "Train Epoch: 515 [22528/54000 (42%)] Loss: -1801.126953\n",
      "Train Epoch: 515 [33792/54000 (63%)] Loss: -1804.547485\n",
      "Train Epoch: 515 [45056/54000 (83%)] Loss: -1799.733643\n",
      "    epoch          : 515\n",
      "    loss           : -1800.8466647166126\n",
      "    ess            : 8.001183788731414\n",
      "    log_marginal   : 1800.8466647166126\n",
      "    val_loss       : -1804.7913818359375\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1804.7914021809895\n",
      "Train Epoch: 516 [0/54000 (0%)] Loss: -1824.038330\n",
      "Train Epoch: 516 [11264/54000 (21%)] Loss: -1819.734619\n",
      "Train Epoch: 516 [22528/54000 (42%)] Loss: -1793.521484\n",
      "Train Epoch: 516 [33792/54000 (63%)] Loss: -1784.503174\n",
      "Train Epoch: 516 [45056/54000 (83%)] Loss: -1794.271851\n",
      "    epoch          : 516\n",
      "    loss           : -1800.5423457307636\n",
      "    ess            : 8.001183113961849\n",
      "    log_marginal   : 1800.5423457307636\n",
      "    val_loss       : -1800.7825622558594\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1800.7825622558594\n",
      "Train Epoch: 517 [0/54000 (0%)] Loss: -1807.303101\n",
      "Train Epoch: 517 [11264/54000 (21%)] Loss: -1818.297852\n",
      "Train Epoch: 517 [22528/54000 (42%)] Loss: -1815.091187\n",
      "Train Epoch: 517 [33792/54000 (63%)] Loss: -1801.957153\n",
      "Train Epoch: 517 [45056/54000 (83%)] Loss: -1795.610962\n",
      "    epoch          : 517\n",
      "    loss           : -1804.8859851765183\n",
      "    ess            : 8.001183203931125\n",
      "    log_marginal   : 1804.8859828733048\n",
      "    val_loss       : -1804.536641438802\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1804.536641438802\n",
      "Train Epoch: 518 [0/54000 (0%)] Loss: -1815.162354\n",
      "Train Epoch: 518 [11264/54000 (21%)] Loss: -1792.345459\n",
      "Train Epoch: 518 [22528/54000 (42%)] Loss: -1796.920166\n",
      "Train Epoch: 518 [33792/54000 (63%)] Loss: -1799.319092\n",
      "Train Epoch: 518 [45056/54000 (83%)] Loss: -1803.922974\n",
      "    epoch          : 518\n",
      "    loss           : -1798.9876950821786\n",
      "    ess            : 8.001183176940343\n",
      "    log_marginal   : 1798.9876950821786\n",
      "    val_loss       : -1812.5892028808594\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1812.5892232259114\n",
      "Train Epoch: 519 [0/54000 (0%)] Loss: -1811.217651\n",
      "Train Epoch: 519 [11264/54000 (21%)] Loss: -1812.394287\n",
      "Train Epoch: 519 [22528/54000 (42%)] Loss: -1803.974365\n",
      "Train Epoch: 519 [33792/54000 (63%)] Loss: -1792.913818\n",
      "Train Epoch: 519 [45056/54000 (83%)] Loss: -1778.457275\n",
      "    epoch          : 519\n",
      "    loss           : -1798.4789682064416\n",
      "    ess            : 8.00118318593727\n",
      "    log_marginal   : 1798.4789682064416\n",
      "    val_loss       : -1793.7782389322917\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1793.7782389322917\n",
      "Train Epoch: 520 [0/54000 (0%)] Loss: -1811.621826\n",
      "Train Epoch: 520 [11264/54000 (21%)] Loss: -1801.112305\n",
      "Train Epoch: 520 [22528/54000 (42%)] Loss: -1786.636230\n",
      "Train Epoch: 520 [33792/54000 (63%)] Loss: -1792.039551\n",
      "Train Epoch: 520 [45056/54000 (83%)] Loss: -1774.161621\n",
      "    epoch          : 520\n",
      "    loss           : -1790.7214758531102\n",
      "    ess            : 8.001184949335062\n",
      "    log_marginal   : 1790.721477004717\n",
      "    val_loss       : -1796.3184814453125\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1796.3184611002605\n",
      "Train Epoch: 521 [0/54000 (0%)] Loss: -1795.937866\n",
      "Train Epoch: 521 [11264/54000 (21%)] Loss: -1793.837402\n",
      "Train Epoch: 521 [22528/54000 (42%)] Loss: -1785.468750\n",
      "Train Epoch: 521 [33792/54000 (63%)] Loss: -1800.447998\n",
      "Train Epoch: 521 [45056/54000 (83%)] Loss: -1802.761719\n",
      "    epoch          : 521\n",
      "    loss           : -1793.8807868237766\n",
      "    ess            : 8.001183923685327\n",
      "    log_marginal   : 1793.8807856721698\n",
      "    val_loss       : -1801.6800130208333\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1801.6800130208333\n",
      "Train Epoch: 522 [0/54000 (0%)] Loss: -1818.997437\n",
      "Train Epoch: 522 [11264/54000 (21%)] Loss: -1815.159424\n",
      "Train Epoch: 522 [22528/54000 (42%)] Loss: -1809.549927\n",
      "Train Epoch: 522 [33792/54000 (63%)] Loss: -1798.246948\n",
      "Train Epoch: 522 [45056/54000 (83%)] Loss: -1815.782959\n",
      "    epoch          : 522\n",
      "    loss           : -1806.795116496536\n",
      "    ess            : 8.00118376174063\n",
      "    log_marginal   : 1806.795116496536\n",
      "    val_loss       : -1805.7039693196614\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1805.7039693196614\n",
      "Train Epoch: 523 [0/54000 (0%)] Loss: -1813.835571\n",
      "Train Epoch: 523 [11264/54000 (21%)] Loss: -1795.286621\n",
      "Train Epoch: 523 [22528/54000 (42%)] Loss: -1792.211426\n",
      "Train Epoch: 523 [33792/54000 (63%)] Loss: -1801.321777\n",
      "Train Epoch: 523 [45056/54000 (83%)] Loss: -1798.459717\n",
      "    epoch          : 523\n",
      "    loss           : -1795.7256757628243\n",
      "    ess            : 8.001184175599295\n",
      "    log_marginal   : 1795.7256757628243\n",
      "    val_loss       : -1801.308573404948\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1801.308573404948\n",
      "Train Epoch: 524 [0/54000 (0%)] Loss: -1817.261719\n",
      "Train Epoch: 524 [11264/54000 (21%)] Loss: -1787.452148\n",
      "Train Epoch: 524 [22528/54000 (42%)] Loss: -1804.330688\n",
      "Train Epoch: 524 [33792/54000 (63%)] Loss: -1797.895996\n",
      "Train Epoch: 524 [45056/54000 (83%)] Loss: -1788.945801\n",
      "    epoch          : 524\n",
      "    loss           : -1797.2602723319576\n",
      "    ess            : 8.00118397766689\n",
      "    log_marginal   : 1797.2602711803509\n",
      "    val_loss       : -1804.1778055826824\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1804.1778055826824\n",
      "Train Epoch: 525 [0/54000 (0%)] Loss: -1817.516357\n",
      "Train Epoch: 525 [11264/54000 (21%)] Loss: -1817.590332\n",
      "Train Epoch: 525 [22528/54000 (42%)] Loss: -1810.258179\n",
      "Train Epoch: 525 [33792/54000 (63%)] Loss: -1797.546631\n",
      "Train Epoch: 525 [45056/54000 (83%)] Loss: -1804.368774\n",
      "    epoch          : 525\n",
      "    loss           : -1808.1415140403892\n",
      "    ess            : 8.00118318593727\n",
      "    log_marginal   : 1808.1415128887825\n",
      "    val_loss       : -1805.4624735514324\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1805.4624633789062\n",
      "Train Epoch: 526 [0/54000 (0%)] Loss: -1824.391113\n",
      "Train Epoch: 526 [11264/54000 (21%)] Loss: -1808.940430\n",
      "Train Epoch: 526 [22528/54000 (42%)] Loss: -1794.693726\n",
      "Train Epoch: 526 [33792/54000 (63%)] Loss: -1778.138428\n",
      "Train Epoch: 526 [45056/54000 (83%)] Loss: -1793.535400\n",
      "    epoch          : 526\n",
      "    loss           : -1794.8460555166569\n",
      "    ess            : 8.001183932682252\n",
      "    log_marginal   : 1794.8460555166569\n",
      "    val_loss       : -1773.1252543131511\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1773.1252543131511\n",
      "Train Epoch: 527 [0/54000 (0%)] Loss: -1779.757202\n",
      "Train Epoch: 527 [11264/54000 (21%)] Loss: -1784.303833\n",
      "Train Epoch: 527 [22528/54000 (42%)] Loss: -1769.827148\n",
      "Train Epoch: 527 [33792/54000 (63%)] Loss: -1763.099121\n",
      "Train Epoch: 527 [45056/54000 (83%)] Loss: -1783.925049\n",
      "    epoch          : 527\n",
      "    loss           : -1779.6437377929688\n",
      "    ess            : 8.00118564209848\n",
      "    log_marginal   : 1779.6437377929688\n",
      "    val_loss       : -1784.5491333007812\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1784.5491231282551\n",
      "Train Epoch: 528 [0/54000 (0%)] Loss: -1790.703369\n",
      "Train Epoch: 528 [11264/54000 (21%)] Loss: -1781.853149\n",
      "Train Epoch: 528 [22528/54000 (42%)] Loss: -1775.185425\n",
      "Train Epoch: 528 [33792/54000 (63%)] Loss: -1783.018921\n",
      "Train Epoch: 528 [45056/54000 (83%)] Loss: -1797.525269\n",
      "    epoch          : 528\n",
      "    loss           : -1787.0535796543338\n",
      "    ess            : 8.001184796387294\n",
      "    log_marginal   : 1787.0535808059406\n",
      "    val_loss       : -1798.4167175292969\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1798.4167378743489\n",
      "Train Epoch: 529 [0/54000 (0%)] Loss: -1812.957764\n",
      "Train Epoch: 529 [11264/54000 (21%)] Loss: -1812.464355\n",
      "Train Epoch: 529 [22528/54000 (42%)] Loss: -1801.244019\n",
      "Train Epoch: 529 [33792/54000 (63%)] Loss: -1792.108521\n",
      "Train Epoch: 529 [45056/54000 (83%)] Loss: -1786.064453\n",
      "    epoch          : 529\n",
      "    loss           : -1801.8798194741303\n",
      "    ess            : 8.00118470641802\n",
      "    log_marginal   : 1801.8798194741303\n",
      "    val_loss       : -1806.0424092610676\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1806.0424092610676\n",
      "Train Epoch: 530 [0/54000 (0%)] Loss: -1815.728271\n",
      "Train Epoch: 530 [11264/54000 (21%)] Loss: -1810.881836\n",
      "Train Epoch: 530 [22528/54000 (42%)] Loss: -1798.387451\n",
      "Train Epoch: 530 [33792/54000 (63%)] Loss: -1806.596313\n",
      "Train Epoch: 530 [45056/54000 (83%)] Loss: -1792.244873\n",
      "    epoch          : 530\n",
      "    loss           : -1806.2150061265477\n",
      "    ess            : 8.001184625445672\n",
      "    log_marginal   : 1806.2150061265477\n",
      "    val_loss       : -1808.578857421875\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1808.578857421875\n",
      "Train Epoch: 531 [0/54000 (0%)] Loss: -1810.957031\n",
      "Train Epoch: 531 [11264/54000 (21%)] Loss: -1798.126099\n",
      "Train Epoch: 531 [22528/54000 (42%)] Loss: -1783.749512\n",
      "Train Epoch: 531 [33792/54000 (63%)] Loss: -1793.244385\n",
      "Train Epoch: 531 [45056/54000 (83%)] Loss: -1799.135986\n",
      "    epoch          : 531\n",
      "    loss           : -1797.1391854915978\n",
      "    ess            : 8.001184769396511\n",
      "    log_marginal   : 1797.1391866432045\n",
      "    val_loss       : -1803.4338277180989\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1803.4338277180989\n",
      "Train Epoch: 532 [0/54000 (0%)] Loss: -1808.625122\n",
      "Train Epoch: 532 [11264/54000 (21%)] Loss: -1811.998169\n",
      "Train Epoch: 532 [22528/54000 (42%)] Loss: -1809.770142\n",
      "Train Epoch: 532 [33792/54000 (63%)] Loss: -1808.885498\n",
      "Train Epoch: 532 [45056/54000 (83%)] Loss: -1809.989868\n",
      "    epoch          : 532\n",
      "    loss           : -1807.516920557562\n",
      "    ess            : 8.001184211587006\n",
      "    log_marginal   : 1807.5169194059552\n",
      "    val_loss       : -1811.782958984375\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1811.782958984375\n",
      "Train Epoch: 533 [0/54000 (0%)] Loss: -1824.908691\n",
      "Train Epoch: 533 [11264/54000 (21%)] Loss: -1817.031006\n",
      "Train Epoch: 533 [22528/54000 (42%)] Loss: -1802.535156\n",
      "Train Epoch: 533 [33792/54000 (63%)] Loss: -1809.171265\n",
      "Train Epoch: 533 [45056/54000 (83%)] Loss: -1799.562012\n",
      "    epoch          : 533\n",
      "    loss           : -1807.9800115621315\n",
      "    ess            : 8.00118372575292\n",
      "    log_marginal   : 1807.9800115621315\n",
      "    val_loss       : -1806.9714253743489\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1806.9714253743489\n",
      "Train Epoch: 534 [0/54000 (0%)] Loss: -1819.090576\n",
      "Train Epoch: 534 [11264/54000 (21%)] Loss: -1809.979980\n",
      "Train Epoch: 534 [22528/54000 (42%)] Loss: -1791.622314\n",
      "Train Epoch: 534 [33792/54000 (63%)] Loss: -1809.543579\n",
      "Train Epoch: 534 [45056/54000 (83%)] Loss: -1802.947021\n",
      "    epoch          : 534\n",
      "    loss           : -1803.6737290868218\n",
      "    ess            : 8.00118419359315\n",
      "    log_marginal   : 1803.6737290868218\n",
      "    val_loss       : -1807.7080179850261\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.7080179850261\n",
      "Train Epoch: 535 [0/54000 (0%)] Loss: -1819.488892\n",
      "Train Epoch: 535 [11264/54000 (21%)] Loss: -1810.496582\n",
      "Train Epoch: 535 [22528/54000 (42%)] Loss: -1806.606934\n",
      "Train Epoch: 535 [33792/54000 (63%)] Loss: -1812.814209\n",
      "Train Epoch: 535 [45056/54000 (83%)] Loss: -1801.340820\n",
      "    epoch          : 535\n",
      "    loss           : -1808.038148124263\n",
      "    ess            : 8.001183851709905\n",
      "    log_marginal   : 1808.038148124263\n",
      "    val_loss       : -1812.0834147135417\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1812.0834147135417\n",
      "Train Epoch: 536 [0/54000 (0%)] Loss: -1814.504028\n",
      "Train Epoch: 536 [11264/54000 (21%)] Loss: -1804.690674\n",
      "Train Epoch: 536 [22528/54000 (42%)] Loss: -1814.048950\n",
      "Train Epoch: 536 [33792/54000 (63%)] Loss: -1805.671143\n",
      "Train Epoch: 536 [45056/54000 (83%)] Loss: -1798.678833\n",
      "    epoch          : 536\n",
      "    loss           : -1807.956844689711\n",
      "    ess            : 8.001183059980285\n",
      "    log_marginal   : 1807.956844689711\n",
      "    val_loss       : -1808.3330586751301\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1808.3330586751301\n",
      "Train Epoch: 537 [0/54000 (0%)] Loss: -1810.687500\n",
      "Train Epoch: 537 [11264/54000 (21%)] Loss: -1811.706421\n",
      "Train Epoch: 537 [22528/54000 (42%)] Loss: -1793.070557\n",
      "Train Epoch: 537 [33792/54000 (63%)] Loss: -1802.012329\n",
      "Train Epoch: 537 [45056/54000 (83%)] Loss: -1776.141235\n",
      "    epoch          : 537\n",
      "    loss           : -1800.1411720131928\n",
      "    ess            : 8.0011836537775\n",
      "    log_marginal   : 1800.1411731647995\n",
      "    val_loss       : -1797.6292114257812\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1797.6292114257812\n",
      "Train Epoch: 538 [0/54000 (0%)] Loss: -1808.841431\n",
      "Train Epoch: 538 [11264/54000 (21%)] Loss: -1793.411865\n",
      "Train Epoch: 538 [22528/54000 (42%)] Loss: -1798.706543\n",
      "Train Epoch: 538 [33792/54000 (63%)] Loss: -1797.941040\n",
      "Train Epoch: 538 [45056/54000 (83%)] Loss: -1803.135010\n",
      "    epoch          : 538\n",
      "    loss           : -1800.8038214917453\n",
      "    ess            : 8.001183518823588\n",
      "    log_marginal   : 1800.8038214917453\n",
      "    val_loss       : -1808.3962504069011\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1808.3962504069011\n",
      "Train Epoch: 539 [0/54000 (0%)] Loss: -1820.431030\n",
      "Train Epoch: 539 [11264/54000 (21%)] Loss: -1817.136475\n",
      "Train Epoch: 539 [22528/54000 (42%)] Loss: -1811.225830\n",
      "Train Epoch: 539 [33792/54000 (63%)] Loss: -1797.770386\n",
      "Train Epoch: 539 [45056/54000 (83%)] Loss: -1786.333374\n",
      "    epoch          : 539\n",
      "    loss           : -1800.9301596587559\n",
      "    ess            : 8.001182979007936\n",
      "    log_marginal   : 1800.9301596587559\n",
      "    val_loss       : -1793.974141438802\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1793.974141438802\n",
      "Train Epoch: 540 [0/54000 (0%)] Loss: -1799.281006\n",
      "Train Epoch: 540 [11264/54000 (21%)] Loss: -1796.769287\n",
      "Train Epoch: 540 [22528/54000 (42%)] Loss: -1790.300049\n",
      "Train Epoch: 540 [33792/54000 (63%)] Loss: -1808.316895\n",
      "Train Epoch: 540 [45056/54000 (83%)] Loss: -1805.513550\n",
      "    epoch          : 540\n",
      "    loss           : -1800.5065031231575\n",
      "    ess            : 8.00118474240573\n",
      "    log_marginal   : 1800.5065042747642\n",
      "    val_loss       : -1812.8170369466145\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1812.8170369466145\n",
      "Train Epoch: 541 [0/54000 (0%)] Loss: -1829.425781\n",
      "Train Epoch: 541 [11264/54000 (21%)] Loss: -1802.901855\n",
      "Train Epoch: 541 [22528/54000 (42%)] Loss: -1805.260010\n",
      "Train Epoch: 541 [33792/54000 (63%)] Loss: -1816.075439\n",
      "Train Epoch: 541 [45056/54000 (83%)] Loss: -1799.482544\n",
      "    epoch          : 541\n",
      "    loss           : -1805.6482947007664\n",
      "    ess            : 8.001183347881964\n",
      "    log_marginal   : 1805.6482935491597\n",
      "    val_loss       : -1804.1416117350261\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1804.1416117350261\n",
      "Train Epoch: 542 [0/54000 (0%)] Loss: -1810.003174\n",
      "Train Epoch: 542 [11264/54000 (21%)] Loss: -1802.049316\n",
      "Train Epoch: 542 [22528/54000 (42%)] Loss: -1798.024780\n",
      "Train Epoch: 542 [33792/54000 (63%)] Loss: -1795.035278\n",
      "Train Epoch: 542 [45056/54000 (83%)] Loss: -1801.946289\n",
      "    epoch          : 542\n",
      "    loss           : -1799.3659771613354\n",
      "    ess            : 8.001183608792863\n",
      "    log_marginal   : 1799.3659771613354\n",
      "    val_loss       : -1807.5298868815105\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1807.5298665364583\n",
      "Train Epoch: 543 [0/54000 (0%)] Loss: -1807.833252\n",
      "Train Epoch: 543 [11264/54000 (21%)] Loss: -1803.337158\n",
      "Train Epoch: 543 [22528/54000 (42%)] Loss: -1797.882568\n",
      "Train Epoch: 543 [33792/54000 (63%)] Loss: -1797.617065\n",
      "Train Epoch: 543 [45056/54000 (83%)] Loss: -1799.423340\n",
      "    epoch          : 543\n",
      "    loss           : -1803.1060284308667\n",
      "    ess            : 8.001182556152344\n",
      "    log_marginal   : 1803.10602727926\n",
      "    val_loss       : -1811.5108642578125\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1811.5108642578125\n",
      "Train Epoch: 544 [0/54000 (0%)] Loss: -1822.286743\n",
      "Train Epoch: 544 [11264/54000 (21%)] Loss: -1812.587402\n",
      "Train Epoch: 544 [22528/54000 (42%)] Loss: -1799.196533\n",
      "Train Epoch: 544 [33792/54000 (63%)] Loss: -1798.150513\n",
      "Train Epoch: 544 [45056/54000 (83%)] Loss: -1800.571045\n",
      "    epoch          : 544\n",
      "    loss           : -1807.458318746315\n",
      "    ess            : 8.001182988004864\n",
      "    log_marginal   : 1807.458318746315\n",
      "    val_loss       : -1810.6305135091145\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1810.6305135091145\n",
      "Train Epoch: 545 [0/54000 (0%)] Loss: -1820.083862\n",
      "Train Epoch: 545 [11264/54000 (21%)] Loss: -1805.712646\n",
      "Train Epoch: 545 [22528/54000 (42%)] Loss: -1812.671143\n",
      "Train Epoch: 545 [33792/54000 (63%)] Loss: -1815.209717\n",
      "Train Epoch: 545 [45056/54000 (83%)] Loss: -1801.103027\n",
      "    epoch          : 545\n",
      "    loss           : -1808.624735130454\n",
      "    ess            : 8.001181224607071\n",
      "    log_marginal   : 1808.6247362820607\n",
      "    val_loss       : -1810.657694498698\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1810.65771484375\n",
      "Train Epoch: 546 [0/54000 (0%)] Loss: -1816.924561\n",
      "Train Epoch: 546 [11264/54000 (21%)] Loss: -1757.226807\n",
      "Train Epoch: 546 [22528/54000 (42%)] Loss: -1801.834961\n",
      "Train Epoch: 546 [33792/54000 (63%)] Loss: -1792.114990\n",
      "Train Epoch: 546 [45056/54000 (83%)] Loss: -1789.138550\n",
      "    epoch          : 546\n",
      "    loss           : -1789.1095684699292\n",
      "    ess            : 8.001183338885037\n",
      "    log_marginal   : 1789.1095684699292\n",
      "    val_loss       : -1802.308329264323\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1802.3083089192708\n",
      "Train Epoch: 547 [0/54000 (0%)] Loss: -1797.614746\n",
      "Train Epoch: 547 [11264/54000 (21%)] Loss: -1804.857300\n",
      "Train Epoch: 547 [22528/54000 (42%)] Loss: -1797.889282\n",
      "Train Epoch: 547 [33792/54000 (63%)] Loss: -1794.189941\n",
      "Train Epoch: 547 [45056/54000 (83%)] Loss: -1791.856201\n",
      "    epoch          : 547\n",
      "    loss           : -1796.6597819778156\n",
      "    ess            : 8.001182367216867\n",
      "    log_marginal   : 1796.6597819778156\n",
      "    val_loss       : -1800.2848714192708\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1800.2848612467449\n",
      "Train Epoch: 548 [0/54000 (0%)] Loss: -1813.723145\n",
      "Train Epoch: 548 [11264/54000 (21%)] Loss: -1800.180298\n",
      "Train Epoch: 548 [22528/54000 (42%)] Loss: -1797.951904\n",
      "Train Epoch: 548 [33792/54000 (63%)] Loss: -1797.242310\n",
      "Train Epoch: 548 [45056/54000 (83%)] Loss: -1780.832764\n",
      "    epoch          : 548\n",
      "    loss           : -1797.8812014021964\n",
      "    ess            : 8.001182052324403\n",
      "    log_marginal   : 1797.8812025538032\n",
      "    val_loss       : -1788.7347513834636\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1788.7347412109375\n",
      "Train Epoch: 549 [0/54000 (0%)] Loss: -1806.271729\n",
      "Train Epoch: 549 [11264/54000 (21%)] Loss: -1800.567383\n",
      "Train Epoch: 549 [22528/54000 (42%)] Loss: -1791.189941\n",
      "Train Epoch: 549 [33792/54000 (63%)] Loss: -1800.952148\n",
      "Train Epoch: 549 [45056/54000 (83%)] Loss: -1784.781006\n",
      "    epoch          : 549\n",
      "    loss           : -1792.5209995485702\n",
      "    ess            : 8.001184481494832\n",
      "    log_marginal   : 1792.5209995485702\n",
      "    val_loss       : -1796.157958984375\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1796.157958984375\n",
      "Train Epoch: 550 [0/54000 (0%)] Loss: -1811.596558\n",
      "Train Epoch: 550 [11264/54000 (21%)] Loss: -1807.468506\n",
      "Train Epoch: 550 [22528/54000 (42%)] Loss: -1792.570557\n",
      "Train Epoch: 550 [33792/54000 (63%)] Loss: -1790.132324\n",
      "Train Epoch: 550 [45056/54000 (83%)] Loss: -1792.725342\n",
      "    epoch          : 550\n",
      "    loss           : -1793.5079748765477\n",
      "    ess            : 8.001184967328918\n",
      "    log_marginal   : 1793.5079748765477\n",
      "    val_loss       : -1792.7689107259114\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1792.7689107259114\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch550.pth ...\n",
      "Train Epoch: 551 [0/54000 (0%)] Loss: -1810.751953\n",
      "Train Epoch: 551 [11264/54000 (21%)] Loss: -1807.142334\n",
      "Train Epoch: 551 [22528/54000 (42%)] Loss: -1796.183594\n",
      "Train Epoch: 551 [33792/54000 (63%)] Loss: -1794.845947\n",
      "Train Epoch: 551 [45056/54000 (83%)] Loss: -1790.503662\n",
      "    epoch          : 551\n",
      "    loss           : -1798.2443686431309\n",
      "    ess            : 8.001184805384222\n",
      "    log_marginal   : 1798.2443674915241\n",
      "    val_loss       : -1795.7991231282551\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1795.7991231282551\n",
      "Train Epoch: 552 [0/54000 (0%)] Loss: -1815.474243\n",
      "Train Epoch: 552 [11264/54000 (21%)] Loss: -1801.227173\n",
      "Train Epoch: 552 [22528/54000 (42%)] Loss: -1789.986694\n",
      "Train Epoch: 552 [33792/54000 (63%)] Loss: -1797.326904\n",
      "Train Epoch: 552 [45056/54000 (83%)] Loss: -1792.539795\n",
      "    epoch          : 552\n",
      "    loss           : -1800.2741918024028\n",
      "    ess            : 8.001184454504049\n",
      "    log_marginal   : 1800.2741918024028\n",
      "    val_loss       : -1807.1836547851562\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1807.1836446126301\n",
      "Train Epoch: 553 [0/54000 (0%)] Loss: -1807.132080\n",
      "Train Epoch: 553 [11264/54000 (21%)] Loss: -1818.960693\n",
      "Train Epoch: 553 [22528/54000 (42%)] Loss: -1793.832275\n",
      "Train Epoch: 553 [33792/54000 (63%)] Loss: -1791.049072\n",
      "Train Epoch: 553 [45056/54000 (83%)] Loss: -1798.305664\n",
      "    epoch          : 553\n",
      "    loss           : -1804.5404547925266\n",
      "    ess            : 8.001183815722195\n",
      "    log_marginal   : 1804.5404536409198\n",
      "    val_loss       : -1807.2713623046875\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1807.2713623046875\n",
      "Train Epoch: 554 [0/54000 (0%)] Loss: -1818.699219\n",
      "Train Epoch: 554 [11264/54000 (21%)] Loss: -1821.542847\n",
      "Train Epoch: 554 [22528/54000 (42%)] Loss: -1810.664307\n",
      "Train Epoch: 554 [33792/54000 (63%)] Loss: -1810.625732\n",
      "Train Epoch: 554 [45056/54000 (83%)] Loss: -1812.174194\n",
      "    epoch          : 554\n",
      "    loss           : -1810.0782931345814\n",
      "    ess            : 8.00118397766689\n",
      "    log_marginal   : 1810.0782931345814\n",
      "    val_loss       : -1813.3693135579426\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1813.3693237304688\n",
      "Train Epoch: 555 [0/54000 (0%)] Loss: -1832.509399\n",
      "Train Epoch: 555 [11264/54000 (21%)] Loss: -1803.247559\n",
      "Train Epoch: 555 [22528/54000 (42%)] Loss: -1793.282959\n",
      "Train Epoch: 555 [33792/54000 (63%)] Loss: -1796.958496\n",
      "Train Epoch: 555 [45056/54000 (83%)] Loss: -1799.032959\n",
      "    epoch          : 555\n",
      "    loss           : -1801.5644346992924\n",
      "    ess            : 8.001183410860458\n",
      "    log_marginal   : 1801.5644346992924\n",
      "    val_loss       : -1799.8409525553386\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.8409525553386\n",
      "Train Epoch: 556 [0/54000 (0%)] Loss: -1807.932495\n",
      "Train Epoch: 556 [11264/54000 (21%)] Loss: -1809.360352\n",
      "Train Epoch: 556 [22528/54000 (42%)] Loss: -1811.252686\n",
      "Train Epoch: 556 [33792/54000 (63%)] Loss: -1808.720703\n",
      "Train Epoch: 556 [45056/54000 (83%)] Loss: -1805.877686\n",
      "    epoch          : 556\n",
      "    loss           : -1808.2216981132076\n",
      "    ess            : 8.001182988004864\n",
      "    log_marginal   : 1808.2216969616009\n",
      "    val_loss       : -1798.2635091145833\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1798.2635091145833\n",
      "Train Epoch: 557 [0/54000 (0%)] Loss: -1810.791748\n",
      "Train Epoch: 557 [11264/54000 (21%)] Loss: -1798.404785\n",
      "Train Epoch: 557 [22528/54000 (42%)] Loss: -1798.674561\n",
      "Train Epoch: 557 [33792/54000 (63%)] Loss: -1802.772705\n",
      "Train Epoch: 557 [45056/54000 (83%)] Loss: -1799.413696\n",
      "    epoch          : 557\n",
      "    loss           : -1799.8444375092129\n",
      "    ess            : 8.00118325791269\n",
      "    log_marginal   : 1799.8444375092129\n",
      "    val_loss       : -1809.7180074055989\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1809.7180074055989\n",
      "Train Epoch: 558 [0/54000 (0%)] Loss: -1818.015015\n",
      "Train Epoch: 558 [11264/54000 (21%)] Loss: -1805.719238\n",
      "Train Epoch: 558 [22528/54000 (42%)] Loss: -1789.729004\n",
      "Train Epoch: 558 [33792/54000 (63%)] Loss: -1792.003418\n",
      "Train Epoch: 558 [45056/54000 (83%)] Loss: -1793.959229\n",
      "    epoch          : 558\n",
      "    loss           : -1800.6231263358638\n",
      "    ess            : 8.00118318593727\n",
      "    log_marginal   : 1800.6231263358638\n",
      "    val_loss       : -1804.5200907389324\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1804.5200907389324\n",
      "Train Epoch: 559 [0/54000 (0%)] Loss: -1808.767578\n",
      "Train Epoch: 559 [11264/54000 (21%)] Loss: -1822.499512\n",
      "Train Epoch: 559 [22528/54000 (42%)] Loss: -1799.758423\n",
      "Train Epoch: 559 [33792/54000 (63%)] Loss: -1806.689697\n",
      "Train Epoch: 559 [45056/54000 (83%)] Loss: -1800.020264\n",
      "    epoch          : 559\n",
      "    loss           : -1808.913007628243\n",
      "    ess            : 8.001182988004864\n",
      "    log_marginal   : 1808.913007628243\n",
      "    val_loss       : -1809.467793782552\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1809.4678141276042\n",
      "Train Epoch: 560 [0/54000 (0%)] Loss: -1818.072021\n",
      "Train Epoch: 560 [11264/54000 (21%)] Loss: -1813.653320\n",
      "Train Epoch: 560 [22528/54000 (42%)] Loss: -1804.794067\n",
      "Train Epoch: 560 [33792/54000 (63%)] Loss: -1804.795898\n",
      "Train Epoch: 560 [45056/54000 (83%)] Loss: -1796.996216\n",
      "    epoch          : 560\n",
      "    loss           : -1807.5366383678509\n",
      "    ess            : 8.001182151290605\n",
      "    log_marginal   : 1807.5366395194576\n",
      "    val_loss       : -1804.2968037923176\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1804.2968037923176\n",
      "Train Epoch: 561 [0/54000 (0%)] Loss: -1813.858643\n",
      "Train Epoch: 561 [11264/54000 (21%)] Loss: -1789.250244\n",
      "Train Epoch: 561 [22528/54000 (42%)] Loss: -1803.625244\n",
      "Train Epoch: 561 [33792/54000 (63%)] Loss: -1804.118408\n",
      "Train Epoch: 561 [45056/54000 (83%)] Loss: -1788.379883\n",
      "    epoch          : 561\n",
      "    loss           : -1799.3467936965656\n",
      "    ess            : 8.0011836537775\n",
      "    log_marginal   : 1799.346794848172\n",
      "    val_loss       : -1797.1243489583333\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1797.1243591308594\n",
      "Train Epoch: 562 [0/54000 (0%)] Loss: -1807.681885\n",
      "Train Epoch: 562 [11264/54000 (21%)] Loss: -1810.501831\n",
      "Train Epoch: 562 [22528/54000 (42%)] Loss: -1808.086060\n",
      "Train Epoch: 562 [33792/54000 (63%)] Loss: -1798.538086\n",
      "Train Epoch: 562 [45056/54000 (83%)] Loss: -1805.125488\n",
      "    epoch          : 562\n",
      "    loss           : -1801.6242318783166\n",
      "    ess            : 8.001182943020227\n",
      "    log_marginal   : 1801.6242318783166\n",
      "    val_loss       : -1800.9585571289062\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1800.9585571289062\n",
      "Train Epoch: 563 [0/54000 (0%)] Loss: -1813.603027\n",
      "Train Epoch: 563 [11264/54000 (21%)] Loss: -1817.612793\n",
      "Train Epoch: 563 [22528/54000 (42%)] Loss: -1802.846313\n",
      "Train Epoch: 563 [33792/54000 (63%)] Loss: -1804.847290\n",
      "Train Epoch: 563 [45056/54000 (83%)] Loss: -1809.961914\n",
      "    epoch          : 563\n",
      "    loss           : -1805.0821210753243\n",
      "    ess            : 8.001182376213794\n",
      "    log_marginal   : 1805.082122226931\n",
      "    val_loss       : -1803.2322794596355\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1803.2322998046875\n",
      "Train Epoch: 564 [0/54000 (0%)] Loss: -1820.711670\n",
      "Train Epoch: 564 [11264/54000 (21%)] Loss: -1796.563232\n",
      "Train Epoch: 564 [22528/54000 (42%)] Loss: -1799.146729\n",
      "Train Epoch: 564 [33792/54000 (63%)] Loss: -1798.706543\n",
      "Train Epoch: 564 [45056/54000 (83%)] Loss: -1809.056152\n",
      "    epoch          : 564\n",
      "    loss           : -1801.9334682248673\n",
      "    ess            : 8.001183635783645\n",
      "    log_marginal   : 1801.9334670732605\n",
      "    val_loss       : -1811.4979044596355\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1811.4979044596355\n",
      "Train Epoch: 565 [0/54000 (0%)] Loss: -1816.607056\n",
      "Train Epoch: 565 [11264/54000 (21%)] Loss: -1807.347656\n",
      "Train Epoch: 565 [22528/54000 (42%)] Loss: -1797.308838\n",
      "Train Epoch: 565 [33792/54000 (63%)] Loss: -1801.210938\n",
      "Train Epoch: 565 [45056/54000 (83%)] Loss: -1802.527832\n",
      "    epoch          : 565\n",
      "    loss           : -1805.5713662201504\n",
      "    ess            : 8.001182682109329\n",
      "    log_marginal   : 1805.5713662201504\n",
      "    val_loss       : -1808.4105326334636\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1808.4105326334636\n",
      "Train Epoch: 566 [0/54000 (0%)] Loss: -1824.355225\n",
      "Train Epoch: 566 [11264/54000 (21%)] Loss: -1814.706665\n",
      "Train Epoch: 566 [22528/54000 (42%)] Loss: -1803.367676\n",
      "Train Epoch: 566 [33792/54000 (63%)] Loss: -1800.543701\n",
      "Train Epoch: 566 [45056/54000 (83%)] Loss: -1804.172363\n",
      "    epoch          : 566\n",
      "    loss           : -1804.3763439250442\n",
      "    ess            : 8.001182394207648\n",
      "    log_marginal   : 1804.3763427734375\n",
      "    val_loss       : -1799.0841166178386\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.0841166178386\n",
      "Train Epoch: 567 [0/54000 (0%)] Loss: -1809.140381\n",
      "Train Epoch: 567 [11264/54000 (21%)] Loss: -1795.949463\n",
      "Train Epoch: 567 [22528/54000 (42%)] Loss: -1795.308350\n",
      "Train Epoch: 567 [33792/54000 (63%)] Loss: -1777.769775\n",
      "Train Epoch: 567 [45056/54000 (83%)] Loss: -1791.881836\n",
      "    epoch          : 567\n",
      "    loss           : -1792.6032288749263\n",
      "    ess            : 8.001183203931125\n",
      "    log_marginal   : 1792.6032288749263\n",
      "    val_loss       : -1801.9470316569011\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1801.9470316569011\n",
      "Train Epoch: 568 [0/54000 (0%)] Loss: -1805.458618\n",
      "Train Epoch: 568 [11264/54000 (21%)] Loss: -1811.805054\n",
      "Train Epoch: 568 [22528/54000 (42%)] Loss: -1799.257080\n",
      "Train Epoch: 568 [33792/54000 (63%)] Loss: -1801.080078\n",
      "Train Epoch: 568 [45056/54000 (83%)] Loss: -1794.947754\n",
      "    epoch          : 568\n",
      "    loss           : -1797.6911459868809\n",
      "    ess            : 8.001182520164633\n",
      "    log_marginal   : 1797.6911459868809\n",
      "    val_loss       : -1778.2732442220051\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1778.2732340494792\n",
      "Train Epoch: 569 [0/54000 (0%)] Loss: -1792.528076\n",
      "Train Epoch: 569 [11264/54000 (21%)] Loss: -1792.991455\n",
      "Train Epoch: 569 [22528/54000 (42%)] Loss: -1782.829224\n",
      "Train Epoch: 569 [33792/54000 (63%)] Loss: -1800.619507\n",
      "Train Epoch: 569 [45056/54000 (83%)] Loss: -1795.560669\n",
      "    epoch          : 569\n",
      "    loss           : -1788.4605793503094\n",
      "    ess            : 8.001184733408802\n",
      "    log_marginal   : 1788.4605793503094\n",
      "    val_loss       : -1793.8773193359375\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1793.8773396809895\n",
      "Train Epoch: 570 [0/54000 (0%)] Loss: -1819.771973\n",
      "Train Epoch: 570 [11264/54000 (21%)] Loss: -1813.293457\n",
      "Train Epoch: 570 [22528/54000 (42%)] Loss: -1790.503052\n",
      "Train Epoch: 570 [33792/54000 (63%)] Loss: -1796.213989\n",
      "Train Epoch: 570 [45056/54000 (83%)] Loss: -1785.774902\n",
      "    epoch          : 570\n",
      "    loss           : -1796.672838894826\n",
      "    ess            : 8.001183446848168\n",
      "    log_marginal   : 1796.672838894826\n",
      "    val_loss       : -1800.7546284993489\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1800.7546284993489\n",
      "Train Epoch: 571 [0/54000 (0%)] Loss: -1820.560303\n",
      "Train Epoch: 571 [11264/54000 (21%)] Loss: -1810.376465\n",
      "Train Epoch: 571 [22528/54000 (42%)] Loss: -1815.541504\n",
      "Train Epoch: 571 [33792/54000 (63%)] Loss: -1806.210205\n",
      "Train Epoch: 571 [45056/54000 (83%)] Loss: -1809.182617\n",
      "    epoch          : 571\n",
      "    loss           : -1810.0662415702388\n",
      "    ess            : 8.001182880041734\n",
      "    log_marginal   : 1810.0662415702388\n",
      "    val_loss       : -1811.2566324869792\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1811.2566324869792\n",
      "Train Epoch: 572 [0/54000 (0%)] Loss: -1819.509033\n",
      "Train Epoch: 572 [11264/54000 (21%)] Loss: -1797.472168\n",
      "Train Epoch: 572 [22528/54000 (42%)] Loss: -1800.986816\n",
      "Train Epoch: 572 [33792/54000 (63%)] Loss: -1805.177246\n",
      "Train Epoch: 572 [45056/54000 (83%)] Loss: -1792.291992\n",
      "    epoch          : 572\n",
      "    loss           : -1798.4632199845223\n",
      "    ess            : 8.001183887697616\n",
      "    log_marginal   : 1798.463221136129\n",
      "    val_loss       : -1799.042460123698\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1799.0424906412761\n",
      "Train Epoch: 573 [0/54000 (0%)] Loss: -1804.264160\n",
      "Train Epoch: 573 [11264/54000 (21%)] Loss: -1801.268066\n",
      "Train Epoch: 573 [22528/54000 (42%)] Loss: -1803.074463\n",
      "Train Epoch: 573 [33792/54000 (63%)] Loss: -1801.823853\n",
      "Train Epoch: 573 [45056/54000 (83%)] Loss: -1808.093506\n",
      "    epoch          : 573\n",
      "    loss           : -1801.5379339254127\n",
      "    ess            : 8.00118383371605\n",
      "    log_marginal   : 1801.5379339254127\n",
      "    val_loss       : -1805.4357503255208\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1805.4357503255208\n",
      "Train Epoch: 574 [0/54000 (0%)] Loss: -1819.511841\n",
      "Train Epoch: 574 [11264/54000 (21%)] Loss: -1795.680908\n",
      "Train Epoch: 574 [22528/54000 (42%)] Loss: -1792.914795\n",
      "Train Epoch: 574 [33792/54000 (63%)] Loss: -1798.968018\n",
      "Train Epoch: 574 [45056/54000 (83%)] Loss: -1803.739746\n",
      "    epoch          : 574\n",
      "    loss           : -1800.8637729860702\n",
      "    ess            : 8.001183023992574\n",
      "    log_marginal   : 1800.8637706828567\n",
      "    val_loss       : -1809.73291015625\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1809.73291015625\n",
      "Train Epoch: 575 [0/54000 (0%)] Loss: -1814.283203\n",
      "Train Epoch: 575 [11264/54000 (21%)] Loss: -1809.997437\n",
      "Train Epoch: 575 [22528/54000 (42%)] Loss: -1803.816650\n",
      "Train Epoch: 575 [33792/54000 (63%)] Loss: -1811.231201\n",
      "Train Epoch: 575 [45056/54000 (83%)] Loss: -1811.950073\n",
      "    epoch          : 575\n",
      "    loss           : -1807.634550274543\n",
      "    ess            : 8.001182790072459\n",
      "    log_marginal   : 1807.6345491229363\n",
      "    val_loss       : -1811.0934651692708\n",
      "    val_ess        : 8.001183350880941\n",
      "    val_log_marginal: 1811.0934448242188\n",
      "Train Epoch: 576 [0/54000 (0%)] Loss: -1811.013672\n",
      "Train Epoch: 576 [11264/54000 (21%)] Loss: -1814.757690\n",
      "Train Epoch: 576 [22528/54000 (42%)] Loss: -1803.895874\n",
      "Train Epoch: 576 [33792/54000 (63%)] Loss: -1805.830566\n",
      "Train Epoch: 576 [45056/54000 (83%)] Loss: -1799.923096\n",
      "    epoch          : 576\n",
      "    loss           : -1809.2553849130306\n",
      "    ess            : 8.001182187278316\n",
      "    log_marginal   : 1809.255383761424\n",
      "    val_loss       : -1808.2878926595051\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1808.2878926595051\n",
      "Train Epoch: 577 [0/54000 (0%)] Loss: -1811.751709\n",
      "Train Epoch: 577 [11264/54000 (21%)] Loss: -1801.587158\n",
      "Train Epoch: 577 [22528/54000 (42%)] Loss: -1793.082764\n",
      "Train Epoch: 577 [33792/54000 (63%)] Loss: -1793.844482\n",
      "Train Epoch: 577 [45056/54000 (83%)] Loss: -1794.032959\n",
      "    epoch          : 577\n",
      "    loss           : -1799.421137971698\n",
      "    ess            : 8.001182475179997\n",
      "    log_marginal   : 1799.421137971698\n",
      "    val_loss       : -1807.3911234537761\n",
      "    val_ess        : 8.001182953516642\n",
      "    val_log_marginal: 1807.3911234537761\n",
      "Train Epoch: 578 [0/54000 (0%)] Loss: -1813.266846\n",
      "Train Epoch: 578 [11264/54000 (21%)] Loss: -1806.705566\n",
      "Train Epoch: 578 [22528/54000 (42%)] Loss: -1808.238281\n",
      "Train Epoch: 578 [33792/54000 (63%)] Loss: -1745.745483\n",
      "Train Epoch: 578 [45056/54000 (83%)] Loss: -1791.153320\n",
      "    epoch          : 578\n",
      "    loss           : -1792.628090912441\n",
      "    ess            : 8.001182322232228\n",
      "    log_marginal   : 1792.628090912441\n",
      "    val_loss       : -1797.5412292480469\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1797.5412292480469\n",
      "Train Epoch: 579 [0/54000 (0%)] Loss: -1798.314819\n",
      "Train Epoch: 579 [11264/54000 (21%)] Loss: -1805.341187\n",
      "Train Epoch: 579 [22528/54000 (42%)] Loss: -1818.816406\n",
      "Train Epoch: 579 [33792/54000 (63%)] Loss: -1805.619629\n",
      "Train Epoch: 579 [45056/54000 (83%)] Loss: -1802.794922\n",
      "    epoch          : 579\n",
      "    loss           : -1802.387599729142\n",
      "    ess            : 8.001183203931125\n",
      "    log_marginal   : 1802.387599729142\n",
      "    val_loss       : -1801.8646647135417\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.8646647135417\n",
      "Train Epoch: 580 [0/54000 (0%)] Loss: -1817.660767\n",
      "Train Epoch: 580 [11264/54000 (21%)] Loss: -1803.315552\n",
      "Train Epoch: 580 [22528/54000 (42%)] Loss: -1772.449341\n",
      "Train Epoch: 580 [33792/54000 (63%)] Loss: -1780.823120\n",
      "Train Epoch: 580 [45056/54000 (83%)] Loss: -1790.094727\n",
      "    epoch          : 580\n",
      "    loss           : -1789.6301327111587\n",
      "    ess            : 8.001184112620804\n",
      "    log_marginal   : 1789.6301327111587\n",
      "    val_loss       : -1794.6661173502605\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1794.6661071777344\n",
      "Train Epoch: 581 [0/54000 (0%)] Loss: -1805.318604\n",
      "Train Epoch: 581 [11264/54000 (21%)] Loss: -1814.267578\n",
      "Train Epoch: 581 [22528/54000 (42%)] Loss: -1800.511963\n",
      "Train Epoch: 581 [33792/54000 (63%)] Loss: -1797.939697\n",
      "Train Epoch: 581 [45056/54000 (83%)] Loss: -1793.042725\n",
      "    epoch          : 581\n",
      "    loss           : -1799.7322756209464\n",
      "    ess            : 8.001182961014083\n",
      "    log_marginal   : 1799.7322756209464\n",
      "    val_loss       : -1801.2857055664062\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1801.2857259114583\n",
      "Train Epoch: 582 [0/54000 (0%)] Loss: -1808.607910\n",
      "Train Epoch: 582 [11264/54000 (21%)] Loss: -1817.237183\n",
      "Train Epoch: 582 [22528/54000 (42%)] Loss: -1799.615967\n",
      "Train Epoch: 582 [33792/54000 (63%)] Loss: -1808.716553\n",
      "Train Epoch: 582 [45056/54000 (83%)] Loss: -1800.892578\n",
      "    epoch          : 582\n",
      "    loss           : -1807.4813393646816\n",
      "    ess            : 8.00118325791269\n",
      "    log_marginal   : 1807.4813393646816\n",
      "    val_loss       : -1808.0944315592449\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1808.0944315592449\n",
      "Train Epoch: 583 [0/54000 (0%)] Loss: -1821.084717\n",
      "Train Epoch: 583 [11264/54000 (21%)] Loss: -1811.204224\n",
      "Train Epoch: 583 [22528/54000 (42%)] Loss: -1792.056519\n",
      "Train Epoch: 583 [33792/54000 (63%)] Loss: -1789.880615\n",
      "Train Epoch: 583 [45056/54000 (83%)] Loss: -1797.697754\n",
      "    epoch          : 583\n",
      "    loss           : -1801.211090663694\n",
      "    ess            : 8.001182412201503\n",
      "    log_marginal   : 1801.2110895120873\n",
      "    val_loss       : -1801.1341145833333\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.1341247558594\n",
      "Train Epoch: 584 [0/54000 (0%)] Loss: -1810.220703\n",
      "Train Epoch: 584 [11264/54000 (21%)] Loss: -1802.793457\n",
      "Train Epoch: 584 [22528/54000 (42%)] Loss: -1808.460327\n",
      "Train Epoch: 584 [33792/54000 (63%)] Loss: -1796.821533\n",
      "Train Epoch: 584 [45056/54000 (83%)] Loss: -1795.837646\n",
      "    epoch          : 584\n",
      "    loss           : -1803.5062762566333\n",
      "    ess            : 8.00118314994956\n",
      "    log_marginal   : 1803.5062751050266\n",
      "    val_loss       : -1802.4750162760417\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1802.4750162760417\n",
      "Train Epoch: 585 [0/54000 (0%)] Loss: -1818.402710\n",
      "Train Epoch: 585 [11264/54000 (21%)] Loss: -1812.813965\n",
      "Train Epoch: 585 [22528/54000 (42%)] Loss: -1812.503418\n",
      "Train Epoch: 585 [33792/54000 (63%)] Loss: -1815.237793\n",
      "Train Epoch: 585 [45056/54000 (83%)] Loss: -1805.805542\n",
      "    epoch          : 585\n",
      "    loss           : -1811.1929079451652\n",
      "    ess            : 8.001181800410432\n",
      "    log_marginal   : 1811.1929079451652\n",
      "    val_loss       : -1803.6541239420574\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1803.6541239420574\n",
      "Train Epoch: 586 [0/54000 (0%)] Loss: -1807.590088\n",
      "Train Epoch: 586 [11264/54000 (21%)] Loss: -1798.167236\n",
      "Train Epoch: 586 [22528/54000 (42%)] Loss: -1789.986572\n",
      "Train Epoch: 586 [33792/54000 (63%)] Loss: -1764.265015\n",
      "Train Epoch: 586 [45056/54000 (83%)] Loss: -1771.741211\n",
      "    epoch          : 586\n",
      "    loss           : -1786.7669067382812\n",
      "    ess            : 8.001183059980285\n",
      "    log_marginal   : 1786.7669067382812\n",
      "    val_loss       : -1802.6725565592449\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1802.6725463867188\n",
      "Train Epoch: 587 [0/54000 (0%)] Loss: -1803.768066\n",
      "Train Epoch: 587 [11264/54000 (21%)] Loss: -1799.726807\n",
      "Train Epoch: 587 [22528/54000 (42%)] Loss: -1810.925049\n",
      "Train Epoch: 587 [33792/54000 (63%)] Loss: -1794.191772\n",
      "Train Epoch: 587 [45056/54000 (83%)] Loss: -1810.838013\n",
      "    epoch          : 587\n",
      "    loss           : -1799.8449188808224\n",
      "    ess            : 8.001182439192286\n",
      "    log_marginal   : 1799.8449177292157\n",
      "    val_loss       : -1809.3888854980469\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1809.3888854980469\n",
      "Train Epoch: 588 [0/54000 (0%)] Loss: -1820.762695\n",
      "Train Epoch: 588 [11264/54000 (21%)] Loss: -1807.760498\n",
      "Train Epoch: 588 [22528/54000 (42%)] Loss: -1811.246948\n",
      "Train Epoch: 588 [33792/54000 (63%)] Loss: -1791.655640\n",
      "Train Epoch: 588 [45056/54000 (83%)] Loss: -1806.605469\n",
      "    epoch          : 588\n",
      "    loss           : -1805.744132563753\n",
      "    ess            : 8.001182700103184\n",
      "    log_marginal   : 1805.744132563753\n",
      "    val_loss       : -1809.3678690592449\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1809.3678588867188\n",
      "Train Epoch: 589 [0/54000 (0%)] Loss: -1817.985229\n",
      "Train Epoch: 589 [11264/54000 (21%)] Loss: -1810.597656\n",
      "Train Epoch: 589 [22528/54000 (42%)] Loss: -1797.296509\n",
      "Train Epoch: 589 [33792/54000 (63%)] Loss: -1801.844971\n",
      "Train Epoch: 589 [45056/54000 (83%)] Loss: -1809.854248\n",
      "    epoch          : 589\n",
      "    loss           : -1806.2115029389004\n",
      "    ess            : 8.001182034330547\n",
      "    log_marginal   : 1806.211500635687\n",
      "    val_loss       : -1810.1401062011719\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1810.1401062011719\n",
      "Train Epoch: 590 [0/54000 (0%)] Loss: -1816.219482\n",
      "Train Epoch: 590 [11264/54000 (21%)] Loss: -1810.330811\n",
      "Train Epoch: 590 [22528/54000 (42%)] Loss: -1814.692627\n",
      "Train Epoch: 590 [33792/54000 (63%)] Loss: -1761.381104\n",
      "Train Epoch: 590 [45056/54000 (83%)] Loss: -1793.598511\n",
      "    epoch          : 590\n",
      "    loss           : -1800.0656001252948\n",
      "    ess            : 8.001182961014083\n",
      "    log_marginal   : 1800.0656001252948\n",
      "    val_loss       : -1800.4994405110676\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1800.4994405110676\n",
      "Train Epoch: 591 [0/54000 (0%)] Loss: -1802.802368\n",
      "Train Epoch: 591 [11264/54000 (21%)] Loss: -1806.146240\n",
      "Train Epoch: 591 [22528/54000 (42%)] Loss: -1808.435059\n",
      "Train Epoch: 591 [33792/54000 (63%)] Loss: -1806.584473\n",
      "Train Epoch: 591 [45056/54000 (83%)] Loss: -1800.290039\n",
      "    epoch          : 591\n",
      "    loss           : -1805.4885806677476\n",
      "    ess            : 8.001182376213794\n",
      "    log_marginal   : 1805.4885806677476\n",
      "    val_loss       : -1819.163594563802\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1819.163594563802\n",
      "Train Epoch: 592 [0/54000 (0%)] Loss: -1819.607666\n",
      "Train Epoch: 592 [11264/54000 (21%)] Loss: -1809.487427\n",
      "Train Epoch: 592 [22528/54000 (42%)] Loss: -1791.602783\n",
      "Train Epoch: 592 [33792/54000 (63%)] Loss: -1786.643921\n",
      "Train Epoch: 592 [45056/54000 (83%)] Loss: -1797.467041\n",
      "    epoch          : 592\n",
      "    loss           : -1798.9795774063973\n",
      "    ess            : 8.00118252916156\n",
      "    log_marginal   : 1798.9795762547906\n",
      "    val_loss       : -1800.8037923177083\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1800.8037923177083\n",
      "Train Epoch: 593 [0/54000 (0%)] Loss: -1811.693604\n",
      "Train Epoch: 593 [11264/54000 (21%)] Loss: -1809.271973\n",
      "Train Epoch: 593 [22528/54000 (42%)] Loss: -1796.613525\n",
      "Train Epoch: 593 [33792/54000 (63%)] Loss: -1782.178833\n",
      "Train Epoch: 593 [45056/54000 (83%)] Loss: -1778.078247\n",
      "    epoch          : 593\n",
      "    loss           : -1790.4175495651532\n",
      "    ess            : 8.001183554811298\n",
      "    log_marginal   : 1790.41755071676\n",
      "    val_loss       : -1795.3084615071614\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1795.3084615071614\n",
      "Train Epoch: 594 [0/54000 (0%)] Loss: -1795.606079\n",
      "Train Epoch: 594 [11264/54000 (21%)] Loss: -1791.420532\n",
      "Train Epoch: 594 [22528/54000 (42%)] Loss: -1777.120117\n",
      "Train Epoch: 594 [33792/54000 (63%)] Loss: -1782.948242\n",
      "Train Epoch: 594 [45056/54000 (83%)] Loss: -1785.986084\n",
      "    epoch          : 594\n",
      "    loss           : -1784.8473038583431\n",
      "    ess            : 8.001185021310482\n",
      "    log_marginal   : 1784.8473038583431\n",
      "    val_loss       : -1795.3393249511719\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1795.3393249511719\n",
      "Train Epoch: 595 [0/54000 (0%)] Loss: -1795.371826\n",
      "Train Epoch: 595 [11264/54000 (21%)] Loss: -1784.434448\n",
      "Train Epoch: 595 [22528/54000 (42%)] Loss: -1786.358032\n",
      "Train Epoch: 595 [33792/54000 (63%)] Loss: -1804.170410\n",
      "Train Epoch: 595 [45056/54000 (83%)] Loss: -1783.490723\n",
      "    epoch          : 595\n",
      "    loss           : -1794.5658546303803\n",
      "    ess            : 8.001183851709905\n",
      "    log_marginal   : 1794.5658534787735\n",
      "    val_loss       : -1796.266825358073\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1796.266825358073\n",
      "Train Epoch: 596 [0/54000 (0%)] Loss: -1812.909424\n",
      "Train Epoch: 596 [11264/54000 (21%)] Loss: -1804.872803\n",
      "Train Epoch: 596 [22528/54000 (42%)] Loss: -1802.843018\n",
      "Train Epoch: 596 [33792/54000 (63%)] Loss: -1789.301025\n",
      "Train Epoch: 596 [45056/54000 (83%)] Loss: -1805.069214\n",
      "    epoch          : 596\n",
      "    loss           : -1805.905732928582\n",
      "    ess            : 8.001183671771356\n",
      "    log_marginal   : 1805.9057317769752\n",
      "    val_loss       : -1806.8907267252605\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1806.8907267252605\n",
      "Train Epoch: 597 [0/54000 (0%)] Loss: -1808.225586\n",
      "Train Epoch: 597 [11264/54000 (21%)] Loss: -1807.222168\n",
      "Train Epoch: 597 [22528/54000 (42%)] Loss: -1783.503174\n",
      "Train Epoch: 597 [33792/54000 (63%)] Loss: -1786.350464\n",
      "Train Epoch: 597 [45056/54000 (83%)] Loss: -1803.880371\n",
      "    epoch          : 597\n",
      "    loss           : -1798.9890309459759\n",
      "    ess            : 8.001184526479468\n",
      "    log_marginal   : 1798.9890309459759\n",
      "    val_loss       : -1804.3358459472656\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1804.3358459472656\n",
      "Train Epoch: 598 [0/54000 (0%)] Loss: -1814.425415\n",
      "Train Epoch: 598 [11264/54000 (21%)] Loss: -1799.070312\n",
      "Train Epoch: 598 [22528/54000 (42%)] Loss: -1802.660889\n",
      "Train Epoch: 598 [33792/54000 (63%)] Loss: -1807.744873\n",
      "Train Epoch: 598 [45056/54000 (83%)] Loss: -1791.751709\n",
      "    epoch          : 598\n",
      "    loss           : -1803.3259818598908\n",
      "    ess            : 8.001183788731414\n",
      "    log_marginal   : 1803.3259818598908\n",
      "    val_loss       : -1807.7591654459636\n",
      "    val_ess        : 8.001184145609537\n",
      "    val_log_marginal: 1807.7591654459636\n",
      "Train Epoch: 599 [0/54000 (0%)] Loss: -1816.102051\n",
      "Train Epoch: 599 [11264/54000 (21%)] Loss: -1815.427979\n",
      "Train Epoch: 599 [22528/54000 (42%)] Loss: -1806.842041\n",
      "Train Epoch: 599 [33792/54000 (63%)] Loss: -1799.860352\n",
      "Train Epoch: 599 [45056/54000 (83%)] Loss: -1797.427490\n",
      "    epoch          : 599\n",
      "    loss           : -1806.2888298754422\n",
      "    ess            : 8.00118314994956\n",
      "    log_marginal   : 1806.2888298754422\n",
      "    val_loss       : -1795.769775390625\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1795.769775390625\n",
      "Train Epoch: 600 [0/54000 (0%)] Loss: -1805.598633\n",
      "Train Epoch: 600 [11264/54000 (21%)] Loss: -1803.051514\n",
      "Train Epoch: 600 [22528/54000 (42%)] Loss: -1799.505127\n",
      "Train Epoch: 600 [33792/54000 (63%)] Loss: -1799.069336\n",
      "Train Epoch: 600 [45056/54000 (83%)] Loss: -1807.077271\n",
      "    epoch          : 600\n",
      "    loss           : -1801.6821277546433\n",
      "    ess            : 8.001182547155416\n",
      "    log_marginal   : 1801.6821254514298\n",
      "    val_loss       : -1811.3731486002605\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1811.3731486002605\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch600.pth ...\n",
      "Train Epoch: 601 [0/54000 (0%)] Loss: -1806.120483\n",
      "Train Epoch: 601 [11264/54000 (21%)] Loss: -1808.876221\n",
      "Train Epoch: 601 [22528/54000 (42%)] Loss: -1797.700562\n",
      "Train Epoch: 601 [33792/54000 (63%)] Loss: -1804.218506\n",
      "Train Epoch: 601 [45056/54000 (83%)] Loss: -1800.372803\n",
      "    epoch          : 601\n",
      "    loss           : -1805.9174378593013\n",
      "    ess            : 8.001181764422723\n",
      "    log_marginal   : 1805.9174378593013\n",
      "    val_loss       : -1810.153055826823\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1810.153055826823\n",
      "Train Epoch: 602 [0/54000 (0%)] Loss: -1820.005371\n",
      "Train Epoch: 602 [11264/54000 (21%)] Loss: -1808.785645\n",
      "Train Epoch: 602 [22528/54000 (42%)] Loss: -1807.443848\n",
      "Train Epoch: 602 [33792/54000 (63%)] Loss: -1806.722412\n",
      "Train Epoch: 602 [45056/54000 (83%)] Loss: -1799.155640\n",
      "    epoch          : 602\n",
      "    loss           : -1808.919048957105\n",
      "    ess            : 8.001181926367417\n",
      "    log_marginal   : 1808.919048957105\n",
      "    val_loss       : -1809.9617207845051\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1809.9617309570312\n",
      "Train Epoch: 603 [0/54000 (0%)] Loss: -1820.222168\n",
      "Train Epoch: 603 [11264/54000 (21%)] Loss: -1805.319092\n",
      "Train Epoch: 603 [22528/54000 (42%)] Loss: -1809.216431\n",
      "Train Epoch: 603 [33792/54000 (63%)] Loss: -1808.816284\n",
      "Train Epoch: 603 [45056/54000 (83%)] Loss: -1793.850708\n",
      "    epoch          : 603\n",
      "    loss           : -1807.663677863355\n",
      "    ess            : 8.001180891720754\n",
      "    log_marginal   : 1807.663677863355\n",
      "    val_loss       : -1797.765848795573\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1797.765869140625\n",
      "Train Epoch: 604 [0/54000 (0%)] Loss: -1810.318726\n",
      "Train Epoch: 604 [11264/54000 (21%)] Loss: -1808.150757\n",
      "Train Epoch: 604 [22528/54000 (42%)] Loss: -1796.208496\n",
      "Train Epoch: 604 [33792/54000 (63%)] Loss: -1814.842773\n",
      "Train Epoch: 604 [45056/54000 (83%)] Loss: -1798.082275\n",
      "    epoch          : 604\n",
      "    loss           : -1803.4814913767689\n",
      "    ess            : 8.001182106305968\n",
      "    log_marginal   : 1803.4814913767689\n",
      "    val_loss       : -1799.2409261067708\n",
      "    val_ess        : 8.001182715098063\n",
      "    val_log_marginal: 1799.2409261067708\n",
      "Train Epoch: 605 [0/54000 (0%)] Loss: -1813.202393\n",
      "Train Epoch: 605 [11264/54000 (21%)] Loss: -1806.272217\n",
      "Train Epoch: 605 [22528/54000 (42%)] Loss: -1800.387695\n",
      "Train Epoch: 605 [33792/54000 (63%)] Loss: -1800.066650\n",
      "Train Epoch: 605 [45056/54000 (83%)] Loss: -1802.706543\n",
      "    epoch          : 605\n",
      "    loss           : -1801.3537413399174\n",
      "    ess            : 8.001181971352056\n",
      "    log_marginal   : 1801.3537401883107\n",
      "    val_loss       : -1803.452880859375\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1803.452880859375\n",
      "Train Epoch: 606 [0/54000 (0%)] Loss: -1811.498901\n",
      "Train Epoch: 606 [11264/54000 (21%)] Loss: -1816.667969\n",
      "Train Epoch: 606 [22528/54000 (42%)] Loss: -1806.459717\n",
      "Train Epoch: 606 [33792/54000 (63%)] Loss: -1801.333740\n",
      "Train Epoch: 606 [45056/54000 (83%)] Loss: -1799.937134\n",
      "    epoch          : 606\n",
      "    loss           : -1807.4153131448998\n",
      "    ess            : 8.00118271809704\n",
      "    log_marginal   : 1807.415311993293\n",
      "    val_loss       : -1804.0568542480469\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1804.0568440755208\n",
      "Train Epoch: 607 [0/54000 (0%)] Loss: -1809.193115\n",
      "Train Epoch: 607 [11264/54000 (21%)] Loss: -1796.725464\n",
      "Train Epoch: 607 [22528/54000 (42%)] Loss: -1772.277954\n",
      "Train Epoch: 607 [33792/54000 (63%)] Loss: -1779.469604\n",
      "Train Epoch: 607 [45056/54000 (83%)] Loss: -1790.305420\n",
      "    epoch          : 607\n",
      "    loss           : -1784.745964770047\n",
      "    ess            : 8.001183671771356\n",
      "    log_marginal   : 1784.745964770047\n",
      "    val_loss       : -1779.9441019694011\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1779.9441019694011\n",
      "Train Epoch: 608 [0/54000 (0%)] Loss: -1781.819336\n",
      "Train Epoch: 608 [11264/54000 (21%)] Loss: -1784.528076\n",
      "Train Epoch: 608 [22528/54000 (42%)] Loss: -1801.820312\n",
      "Train Epoch: 608 [33792/54000 (63%)] Loss: -1786.789062\n",
      "Train Epoch: 608 [45056/54000 (83%)] Loss: -1794.392212\n",
      "    epoch          : 608\n",
      "    loss           : -1790.2965364276238\n",
      "    ess            : 8.001184787390367\n",
      "    log_marginal   : 1790.2965364276238\n",
      "    val_loss       : -1804.1918131510417\n",
      "    val_ess        : 8.001183271408081\n",
      "    val_log_marginal: 1804.1918029785156\n",
      "Train Epoch: 609 [0/54000 (0%)] Loss: -1811.855225\n",
      "Train Epoch: 609 [11264/54000 (21%)] Loss: -1800.559082\n",
      "Train Epoch: 609 [22528/54000 (42%)] Loss: -1804.042358\n",
      "Train Epoch: 609 [33792/54000 (63%)] Loss: -1800.047485\n",
      "Train Epoch: 609 [45056/54000 (83%)] Loss: -1804.504639\n",
      "    epoch          : 609\n",
      "    loss           : -1799.7629567272259\n",
      "    ess            : 8.001182979007936\n",
      "    log_marginal   : 1799.7629567272259\n",
      "    val_loss       : -1810.917012532552\n",
      "    val_ess        : 8.001181761423746\n",
      "    val_log_marginal: 1810.917012532552\n",
      "Train Epoch: 610 [0/54000 (0%)] Loss: -1818.640015\n",
      "Train Epoch: 610 [11264/54000 (21%)] Loss: -1822.030884\n",
      "Train Epoch: 610 [22528/54000 (42%)] Loss: -1806.071411\n",
      "Train Epoch: 610 [33792/54000 (63%)] Loss: -1806.882324\n",
      "Train Epoch: 610 [45056/54000 (83%)] Loss: -1803.720703\n",
      "    epoch          : 610\n",
      "    loss           : -1807.1206411685584\n",
      "    ess            : 8.001182340226084\n",
      "    log_marginal   : 1807.1206411685584\n",
      "    val_loss       : -1802.5514119466145\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1802.5513916015625\n",
      "Train Epoch: 611 [0/54000 (0%)] Loss: -1812.051147\n",
      "Train Epoch: 611 [11264/54000 (21%)] Loss: -1806.855347\n",
      "Train Epoch: 611 [22528/54000 (42%)] Loss: -1801.749512\n",
      "Train Epoch: 611 [33792/54000 (63%)] Loss: -1813.512939\n",
      "Train Epoch: 611 [45056/54000 (83%)] Loss: -1807.546387\n",
      "    epoch          : 611\n",
      "    loss           : -1805.7776281968602\n",
      "    ess            : 8.001183815722195\n",
      "    log_marginal   : 1805.7776281968602\n",
      "    val_loss       : -1813.5789998372395\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1813.5790201822917\n",
      "Train Epoch: 612 [0/54000 (0%)] Loss: -1821.696533\n",
      "Train Epoch: 612 [11264/54000 (21%)] Loss: -1804.074219\n",
      "Train Epoch: 612 [22528/54000 (42%)] Loss: -1802.932861\n",
      "Train Epoch: 612 [33792/54000 (63%)] Loss: -1793.315186\n",
      "Train Epoch: 612 [45056/54000 (83%)] Loss: -1797.269775\n",
      "    epoch          : 612\n",
      "    loss           : -1801.7709845776828\n",
      "    ess            : 8.001182844054025\n",
      "    log_marginal   : 1801.7709845776828\n",
      "    val_loss       : -1808.2805277506511\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1808.2805074055989\n",
      "Train Epoch: 613 [0/54000 (0%)] Loss: -1817.677368\n",
      "Train Epoch: 613 [11264/54000 (21%)] Loss: -1822.748535\n",
      "Train Epoch: 613 [22528/54000 (42%)] Loss: -1815.249512\n",
      "Train Epoch: 613 [33792/54000 (63%)] Loss: -1796.328369\n",
      "Train Epoch: 613 [45056/54000 (83%)] Loss: -1798.333008\n",
      "    epoch          : 613\n",
      "    loss           : -1808.7290062094635\n",
      "    ess            : 8.001182799069387\n",
      "    log_marginal   : 1808.7290062094635\n",
      "    val_loss       : -1807.3010660807292\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1807.3010660807292\n",
      "Train Epoch: 614 [0/54000 (0%)] Loss: -1823.213257\n",
      "Train Epoch: 614 [11264/54000 (21%)] Loss: -1809.165771\n",
      "Train Epoch: 614 [22528/54000 (42%)] Loss: -1799.866699\n",
      "Train Epoch: 614 [33792/54000 (63%)] Loss: -1790.236816\n",
      "Train Epoch: 614 [45056/54000 (83%)] Loss: -1797.134033\n",
      "    epoch          : 614\n",
      "    loss           : -1800.9545829341096\n",
      "    ess            : 8.001182916029444\n",
      "    log_marginal   : 1800.9545840857163\n",
      "    val_loss       : -1800.4750569661458\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1800.4750569661458\n",
      "Train Epoch: 615 [0/54000 (0%)] Loss: -1807.592285\n",
      "Train Epoch: 615 [11264/54000 (21%)] Loss: -1814.529297\n",
      "Train Epoch: 615 [22528/54000 (42%)] Loss: -1798.409058\n",
      "Train Epoch: 615 [33792/54000 (63%)] Loss: -1802.537842\n",
      "Train Epoch: 615 [45056/54000 (83%)] Loss: -1794.775146\n",
      "    epoch          : 615\n",
      "    loss           : -1803.7103386645047\n",
      "    ess            : 8.001182691106257\n",
      "    log_marginal   : 1803.7103398161114\n",
      "    val_loss       : -1815.1178487141926\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1815.1178487141926\n",
      "Train Epoch: 616 [0/54000 (0%)] Loss: -1827.776367\n",
      "Train Epoch: 616 [11264/54000 (21%)] Loss: -1809.411377\n",
      "Train Epoch: 616 [22528/54000 (42%)] Loss: -1791.396729\n",
      "Train Epoch: 616 [33792/54000 (63%)] Loss: -1797.784912\n",
      "Train Epoch: 616 [45056/54000 (83%)] Loss: -1790.075928\n",
      "    epoch          : 616\n",
      "    loss           : -1797.4373364718456\n",
      "    ess            : 8.001183707759065\n",
      "    log_marginal   : 1797.4373364718456\n",
      "    val_loss       : -1797.2869873046875\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1797.2869873046875\n",
      "Train Epoch: 617 [0/54000 (0%)] Loss: -1794.474854\n",
      "Train Epoch: 617 [11264/54000 (21%)] Loss: -1809.875977\n",
      "Train Epoch: 617 [22528/54000 (42%)] Loss: -1812.993042\n",
      "Train Epoch: 617 [33792/54000 (63%)] Loss: -1807.947998\n",
      "Train Epoch: 617 [45056/54000 (83%)] Loss: -1798.559204\n",
      "    epoch          : 617\n",
      "    loss           : -1806.266609623747\n",
      "    ess            : 8.001182286244518\n",
      "    log_marginal   : 1806.266609623747\n",
      "    val_loss       : -1804.855000813802\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1804.8550109863281\n",
      "Train Epoch: 618 [0/54000 (0%)] Loss: -1812.697266\n",
      "Train Epoch: 618 [11264/54000 (21%)] Loss: -1802.058838\n",
      "Train Epoch: 618 [22528/54000 (42%)] Loss: -1803.419434\n",
      "Train Epoch: 618 [33792/54000 (63%)] Loss: -1793.248901\n",
      "Train Epoch: 618 [45056/54000 (83%)] Loss: -1805.562256\n",
      "    epoch          : 618\n",
      "    loss           : -1805.1100198997642\n",
      "    ess            : 8.001182475179997\n",
      "    log_marginal   : 1805.1100198997642\n",
      "    val_loss       : -1810.3592936197917\n",
      "    val_ess        : 8.001182079315186\n",
      "    val_log_marginal: 1810.3592936197917\n",
      "Train Epoch: 619 [0/54000 (0%)] Loss: -1812.802002\n",
      "Train Epoch: 619 [11264/54000 (21%)] Loss: -1805.012695\n",
      "Train Epoch: 619 [22528/54000 (42%)] Loss: -1813.880127\n",
      "Train Epoch: 619 [33792/54000 (63%)] Loss: -1810.033691\n",
      "Train Epoch: 619 [45056/54000 (83%)] Loss: -1794.024658\n",
      "    epoch          : 619\n",
      "    loss           : -1802.3217635244694\n",
      "    ess            : 8.00118368976521\n",
      "    log_marginal   : 1802.3217635244694\n",
      "    val_loss       : -1808.3584696451824\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1808.3584696451824\n",
      "Train Epoch: 620 [0/54000 (0%)] Loss: -1821.987427\n",
      "Train Epoch: 620 [11264/54000 (21%)] Loss: -1803.732788\n",
      "Train Epoch: 620 [22528/54000 (42%)] Loss: -1787.180908\n",
      "Train Epoch: 620 [33792/54000 (63%)] Loss: -1792.747314\n",
      "Train Epoch: 620 [45056/54000 (83%)] Loss: -1783.636963\n",
      "    epoch          : 620\n",
      "    loss           : -1798.917447072155\n",
      "    ess            : 8.001182853050953\n",
      "    log_marginal   : 1798.917447072155\n",
      "    val_loss       : -1803.32421875\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1803.3242289225261\n",
      "Train Epoch: 621 [0/54000 (0%)] Loss: -1808.828369\n",
      "Train Epoch: 621 [11264/54000 (21%)] Loss: -1803.972290\n",
      "Train Epoch: 621 [22528/54000 (42%)] Loss: -1804.927856\n",
      "Train Epoch: 621 [33792/54000 (63%)] Loss: -1804.203857\n",
      "Train Epoch: 621 [45056/54000 (83%)] Loss: -1800.381592\n",
      "    epoch          : 621\n",
      "    loss           : -1805.369707215507\n",
      "    ess            : 8.001181458527187\n",
      "    log_marginal   : 1805.369707215507\n",
      "    val_loss       : -1802.9314371744792\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1802.9314473470051\n",
      "Train Epoch: 622 [0/54000 (0%)] Loss: -1811.916748\n",
      "Train Epoch: 622 [11264/54000 (21%)] Loss: -1812.213623\n",
      "Train Epoch: 622 [22528/54000 (42%)] Loss: -1801.480469\n",
      "Train Epoch: 622 [33792/54000 (63%)] Loss: -1797.630615\n",
      "Train Epoch: 622 [45056/54000 (83%)] Loss: -1781.423584\n",
      "    epoch          : 622\n",
      "    loss           : -1798.6410084850384\n",
      "    ess            : 8.001182088312113\n",
      "    log_marginal   : 1798.6410073334316\n",
      "    val_loss       : -1771.2714742024739\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1771.2714742024739\n",
      "Train Epoch: 623 [0/54000 (0%)] Loss: -1786.986572\n",
      "Train Epoch: 623 [11264/54000 (21%)] Loss: -1775.527954\n",
      "Train Epoch: 623 [22528/54000 (42%)] Loss: -1796.361694\n",
      "Train Epoch: 623 [33792/54000 (63%)] Loss: -1805.180176\n",
      "Train Epoch: 623 [45056/54000 (83%)] Loss: -1797.182861\n",
      "    epoch          : 623\n",
      "    loss           : -1791.1898642485996\n",
      "    ess            : 8.001184553470251\n",
      "    log_marginal   : 1791.1898654002064\n",
      "    val_loss       : -1796.5275065104167\n",
      "    val_ess        : 8.001182158788046\n",
      "    val_log_marginal: 1796.5275166829426\n",
      "Train Epoch: 624 [0/54000 (0%)] Loss: -1806.850830\n",
      "Train Epoch: 624 [11264/54000 (21%)] Loss: -1801.765137\n",
      "Train Epoch: 624 [22528/54000 (42%)] Loss: -1792.667480\n",
      "Train Epoch: 624 [33792/54000 (63%)] Loss: -1786.345947\n",
      "Train Epoch: 624 [45056/54000 (83%)] Loss: -1786.052368\n",
      "    epoch          : 624\n",
      "    loss           : -1793.4797086895637\n",
      "    ess            : 8.001183320891183\n",
      "    log_marginal   : 1793.4797086895637\n",
      "    val_loss       : -1802.7028910319011\n",
      "    val_ess        : 8.00118374824524\n",
      "    val_log_marginal: 1802.702880859375\n",
      "Train Epoch: 625 [0/54000 (0%)] Loss: -1809.707520\n",
      "Train Epoch: 625 [11264/54000 (21%)] Loss: -1814.114746\n",
      "Train Epoch: 625 [22528/54000 (42%)] Loss: -1796.728760\n",
      "Train Epoch: 625 [33792/54000 (63%)] Loss: -1806.113647\n",
      "Train Epoch: 625 [45056/54000 (83%)] Loss: -1803.777588\n",
      "    epoch          : 625\n",
      "    loss           : -1804.9176301776238\n",
      "    ess            : 8.001183455845094\n",
      "    log_marginal   : 1804.9176301776238\n",
      "    val_loss       : -1806.9591471354167\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1806.9591471354167\n",
      "Train Epoch: 626 [0/54000 (0%)] Loss: -1812.202026\n",
      "Train Epoch: 626 [11264/54000 (21%)] Loss: -1805.628418\n",
      "Train Epoch: 626 [22528/54000 (42%)] Loss: -1789.726562\n",
      "Train Epoch: 626 [33792/54000 (63%)] Loss: -1795.668945\n",
      "Train Epoch: 626 [45056/54000 (83%)] Loss: -1796.920044\n",
      "    epoch          : 626\n",
      "    loss           : -1802.2433655936763\n",
      "    ess            : 8.001183707759065\n",
      "    log_marginal   : 1802.2433655936763\n",
      "    val_loss       : -1802.4954630533855\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1802.4954630533855\n",
      "Train Epoch: 627 [0/54000 (0%)] Loss: -1815.840332\n",
      "Train Epoch: 627 [11264/54000 (21%)] Loss: -1809.211426\n",
      "Train Epoch: 627 [22528/54000 (42%)] Loss: -1800.273193\n",
      "Train Epoch: 627 [33792/54000 (63%)] Loss: -1798.304443\n",
      "Train Epoch: 627 [45056/54000 (83%)] Loss: -1789.700684\n",
      "    epoch          : 627\n",
      "    loss           : -1801.3272992979805\n",
      "    ess            : 8.001183176940343\n",
      "    log_marginal   : 1801.3272992979805\n",
      "    val_loss       : -1806.9183451334636\n",
      "    val_ess        : 8.001183907190958\n",
      "    val_log_marginal: 1806.9183451334636\n",
      "Train Epoch: 628 [0/54000 (0%)] Loss: -1816.430298\n",
      "Train Epoch: 628 [11264/54000 (21%)] Loss: -1802.706543\n",
      "Train Epoch: 628 [22528/54000 (42%)] Loss: -1794.455688\n",
      "Train Epoch: 628 [33792/54000 (63%)] Loss: -1795.576416\n",
      "Train Epoch: 628 [45056/54000 (83%)] Loss: -1791.504761\n",
      "    epoch          : 628\n",
      "    loss           : -1798.4820786961968\n",
      "    ess            : 8.001182916029444\n",
      "    log_marginal   : 1798.4820786961968\n",
      "    val_loss       : -1799.9237162272136\n",
      "    val_ess        : 8.001183907190958\n",
      "    val_log_marginal: 1799.9237162272136\n",
      "Train Epoch: 629 [0/54000 (0%)] Loss: -1815.361084\n",
      "Train Epoch: 629 [11264/54000 (21%)] Loss: -1809.568115\n",
      "Train Epoch: 629 [22528/54000 (42%)] Loss: -1814.549072\n",
      "Train Epoch: 629 [33792/54000 (63%)] Loss: -1812.118286\n",
      "Train Epoch: 629 [45056/54000 (83%)] Loss: -1797.879150\n",
      "    epoch          : 629\n",
      "    loss           : -1807.7059694686027\n",
      "    ess            : 8.001183239918834\n",
      "    log_marginal   : 1807.7059694686027\n",
      "    val_loss       : -1801.5903727213542\n",
      "    val_ess        : 8.001184542973837\n",
      "    val_log_marginal: 1801.5903625488281\n",
      "Train Epoch: 630 [0/54000 (0%)] Loss: -1813.958130\n",
      "Train Epoch: 630 [11264/54000 (21%)] Loss: -1807.187622\n",
      "Train Epoch: 630 [22528/54000 (42%)] Loss: -1807.507568\n",
      "Train Epoch: 630 [33792/54000 (63%)] Loss: -1807.661621\n",
      "Train Epoch: 630 [45056/54000 (83%)] Loss: -1796.489624\n",
      "    epoch          : 630\n",
      "    loss           : -1804.365938006707\n",
      "    ess            : 8.001181602478027\n",
      "    log_marginal   : 1804.365938006707\n",
      "    val_loss       : -1810.5052185058594\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1810.5052083333333\n",
      "Train Epoch: 631 [0/54000 (0%)] Loss: -1820.686768\n",
      "Train Epoch: 631 [11264/54000 (21%)] Loss: -1810.043701\n",
      "Train Epoch: 631 [22528/54000 (42%)] Loss: -1813.826904\n",
      "Train Epoch: 631 [33792/54000 (63%)] Loss: -1804.138794\n",
      "Train Epoch: 631 [45056/54000 (83%)] Loss: -1802.087524\n",
      "    epoch          : 631\n",
      "    loss           : -1807.4803743182488\n",
      "    ess            : 8.001182412201503\n",
      "    log_marginal   : 1807.4803743182488\n",
      "    val_loss       : -1802.0466003417969\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1802.0466003417969\n",
      "Train Epoch: 632 [0/54000 (0%)] Loss: -1821.306641\n",
      "Train Epoch: 632 [11264/54000 (21%)] Loss: -1752.127319\n",
      "Train Epoch: 632 [22528/54000 (42%)] Loss: -1793.958374\n",
      "Train Epoch: 632 [33792/54000 (63%)] Loss: -1780.630371\n",
      "Train Epoch: 632 [45056/54000 (83%)] Loss: -1776.747070\n",
      "    epoch          : 632\n",
      "    loss           : -1781.4800541715802\n",
      "    ess            : 8.001184400522485\n",
      "    log_marginal   : 1781.4800541715802\n",
      "    val_loss       : -1799.861307779948\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1799.861307779948\n",
      "Train Epoch: 633 [0/54000 (0%)] Loss: -1797.393799\n",
      "Train Epoch: 633 [11264/54000 (21%)] Loss: -1787.729858\n",
      "Train Epoch: 633 [22528/54000 (42%)] Loss: -1794.237671\n",
      "Train Epoch: 633 [33792/54000 (63%)] Loss: -1782.421387\n",
      "Train Epoch: 633 [45056/54000 (83%)] Loss: -1800.094482\n",
      "    epoch          : 633\n",
      "    loss           : -1794.801674896816\n",
      "    ess            : 8.001182655118546\n",
      "    log_marginal   : 1794.801674896816\n",
      "    val_loss       : -1807.7176411946614\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1807.7176411946614\n",
      "Train Epoch: 634 [0/54000 (0%)] Loss: -1815.854492\n",
      "Train Epoch: 634 [11264/54000 (21%)] Loss: -1819.731689\n",
      "Train Epoch: 634 [22528/54000 (42%)] Loss: -1809.587646\n",
      "Train Epoch: 634 [33792/54000 (63%)] Loss: -1800.265381\n",
      "Train Epoch: 634 [45056/54000 (83%)] Loss: -1802.475342\n",
      "    epoch          : 634\n",
      "    loss           : -1805.897398750737\n",
      "    ess            : 8.001183383869675\n",
      "    log_marginal   : 1805.8973999023438\n",
      "    val_loss       : -1802.5296427408855\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1802.5296325683594\n",
      "Train Epoch: 635 [0/54000 (0%)] Loss: -1810.510498\n",
      "Train Epoch: 635 [11264/54000 (21%)] Loss: -1815.560425\n",
      "Train Epoch: 635 [22528/54000 (42%)] Loss: -1805.609375\n",
      "Train Epoch: 635 [33792/54000 (63%)] Loss: -1812.076660\n",
      "Train Epoch: 635 [45056/54000 (83%)] Loss: -1808.732910\n",
      "    epoch          : 635\n",
      "    loss           : -1806.6315906452683\n",
      "    ess            : 8.00118397766689\n",
      "    log_marginal   : 1806.6315906452683\n",
      "    val_loss       : -1813.4210815429688\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1813.4210815429688\n",
      "Train Epoch: 636 [0/54000 (0%)] Loss: -1819.755615\n",
      "Train Epoch: 636 [11264/54000 (21%)] Loss: -1805.355835\n",
      "Train Epoch: 636 [22528/54000 (42%)] Loss: -1792.697754\n",
      "Train Epoch: 636 [33792/54000 (63%)] Loss: -1793.522461\n",
      "Train Epoch: 636 [45056/54000 (83%)] Loss: -1808.145630\n",
      "    epoch          : 636\n",
      "    loss           : -1801.1306636018573\n",
      "    ess            : 8.001182772078604\n",
      "    log_marginal   : 1801.1306624502506\n",
      "    val_loss       : -1803.1612040201824\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1803.1612040201824\n",
      "Train Epoch: 637 [0/54000 (0%)] Loss: -1819.436523\n",
      "Train Epoch: 637 [11264/54000 (21%)] Loss: -1809.060181\n",
      "Train Epoch: 637 [22528/54000 (42%)] Loss: -1805.861816\n",
      "Train Epoch: 637 [33792/54000 (63%)] Loss: -1799.032959\n",
      "Train Epoch: 637 [45056/54000 (83%)] Loss: -1810.326660\n",
      "    epoch          : 637\n",
      "    loss           : -1806.2458726415093\n",
      "    ess            : 8.001183698762137\n",
      "    log_marginal   : 1806.2458726415093\n",
      "    val_loss       : -1811.281982421875\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1811.281962076823\n",
      "Train Epoch: 638 [0/54000 (0%)] Loss: -1820.731567\n",
      "Train Epoch: 638 [11264/54000 (21%)] Loss: -1805.737549\n",
      "Train Epoch: 638 [22528/54000 (42%)] Loss: -1803.429565\n",
      "Train Epoch: 638 [33792/54000 (63%)] Loss: -1809.636719\n",
      "Train Epoch: 638 [45056/54000 (83%)] Loss: -1809.043335\n",
      "    epoch          : 638\n",
      "    loss           : -1804.0551101396668\n",
      "    ess            : 8.001182196275243\n",
      "    log_marginal   : 1804.0551101396668\n",
      "    val_loss       : -1809.0256652832031\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1809.025655110677\n",
      "Train Epoch: 639 [0/54000 (0%)] Loss: -1813.772461\n",
      "Train Epoch: 639 [11264/54000 (21%)] Loss: -1792.792969\n",
      "Train Epoch: 639 [22528/54000 (42%)] Loss: -1794.071777\n",
      "Train Epoch: 639 [33792/54000 (63%)] Loss: -1795.604492\n",
      "Train Epoch: 639 [45056/54000 (83%)] Loss: -1799.498291\n",
      "    epoch          : 639\n",
      "    loss           : -1798.840158138635\n",
      "    ess            : 8.00118358180208\n",
      "    log_marginal   : 1798.840158138635\n",
      "    val_loss       : -1805.6785888671875\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.6785888671875\n",
      "Train Epoch: 640 [0/54000 (0%)] Loss: -1810.754639\n",
      "Train Epoch: 640 [11264/54000 (21%)] Loss: -1806.503174\n",
      "Train Epoch: 640 [22528/54000 (42%)] Loss: -1791.589355\n",
      "Train Epoch: 640 [33792/54000 (63%)] Loss: -1792.017334\n",
      "Train Epoch: 640 [45056/54000 (83%)] Loss: -1793.331421\n",
      "    epoch          : 640\n",
      "    loss           : -1797.4679461785083\n",
      "    ess            : 8.001183734749848\n",
      "    log_marginal   : 1797.4679461785083\n",
      "    val_loss       : -1800.3476155598958\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1800.3476155598958\n",
      "Train Epoch: 641 [0/54000 (0%)] Loss: -1810.971924\n",
      "Train Epoch: 641 [11264/54000 (21%)] Loss: -1817.072754\n",
      "Train Epoch: 641 [22528/54000 (42%)] Loss: -1812.258545\n",
      "Train Epoch: 641 [33792/54000 (63%)] Loss: -1791.565308\n",
      "Train Epoch: 641 [45056/54000 (83%)] Loss: -1780.997803\n",
      "    epoch          : 641\n",
      "    loss           : -1802.1190220095077\n",
      "    ess            : 8.001182619130836\n",
      "    log_marginal   : 1802.1190220095077\n",
      "    val_loss       : -1793.4519348144531\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1793.4519551595051\n",
      "Train Epoch: 642 [0/54000 (0%)] Loss: -1804.239868\n",
      "Train Epoch: 642 [11264/54000 (21%)] Loss: -1777.280884\n",
      "Train Epoch: 642 [22528/54000 (42%)] Loss: -1792.289917\n",
      "Train Epoch: 642 [33792/54000 (63%)] Loss: -1778.930420\n",
      "Train Epoch: 642 [45056/54000 (83%)] Loss: -1779.396484\n",
      "    epoch          : 642\n",
      "    loss           : -1786.8978052679097\n",
      "    ess            : 8.00118513827054\n",
      "    log_marginal   : 1786.8978052679097\n",
      "    val_loss       : -1791.7948506673176\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1791.7948404947917\n",
      "Train Epoch: 643 [0/54000 (0%)] Loss: -1805.297974\n",
      "Train Epoch: 643 [11264/54000 (21%)] Loss: -1809.059814\n",
      "Train Epoch: 643 [22528/54000 (42%)] Loss: -1793.681885\n",
      "Train Epoch: 643 [33792/54000 (63%)] Loss: -1788.250488\n",
      "Train Epoch: 643 [45056/54000 (83%)] Loss: -1794.219727\n",
      "    epoch          : 643\n",
      "    loss           : -1797.4050396613354\n",
      "    ess            : 8.001184058639238\n",
      "    log_marginal   : 1797.405041964549\n",
      "    val_loss       : -1802.6796264648438\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1802.6796264648438\n",
      "Train Epoch: 644 [0/54000 (0%)] Loss: -1809.900635\n",
      "Train Epoch: 644 [11264/54000 (21%)] Loss: -1821.300537\n",
      "Train Epoch: 644 [22528/54000 (42%)] Loss: -1811.996216\n",
      "Train Epoch: 644 [33792/54000 (63%)] Loss: -1806.980103\n",
      "Train Epoch: 644 [45056/54000 (83%)] Loss: -1809.659424\n",
      "    epoch          : 644\n",
      "    loss           : -1810.5028847748379\n",
      "    ess            : 8.001183275906545\n",
      "    log_marginal   : 1810.5028847748379\n",
      "    val_loss       : -1813.6253967285156\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1813.6254170735676\n",
      "Train Epoch: 645 [0/54000 (0%)] Loss: -1826.045410\n",
      "Train Epoch: 645 [11264/54000 (21%)] Loss: -1808.238159\n",
      "Train Epoch: 645 [22528/54000 (42%)] Loss: -1793.363281\n",
      "Train Epoch: 645 [33792/54000 (63%)] Loss: -1798.160522\n",
      "Train Epoch: 645 [45056/54000 (83%)] Loss: -1801.123657\n",
      "    epoch          : 645\n",
      "    loss           : -1803.7598473890773\n",
      "    ess            : 8.001182772078604\n",
      "    log_marginal   : 1803.759848540684\n",
      "    val_loss       : -1805.6138712565105\n",
      "    val_ess        : 8.001184066136679\n",
      "    val_log_marginal: 1805.6138712565105\n",
      "Train Epoch: 646 [0/54000 (0%)] Loss: -1815.483154\n",
      "Train Epoch: 646 [11264/54000 (21%)] Loss: -1812.250122\n",
      "Train Epoch: 646 [22528/54000 (42%)] Loss: -1810.250977\n",
      "Train Epoch: 646 [33792/54000 (63%)] Loss: -1790.536011\n",
      "Train Epoch: 646 [45056/54000 (83%)] Loss: -1799.059326\n",
      "    epoch          : 646\n",
      "    loss           : -1801.0181539283608\n",
      "    ess            : 8.00118397766689\n",
      "    log_marginal   : 1801.0181539283608\n",
      "    val_loss       : -1801.0928548177083\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1801.0928548177083\n",
      "Train Epoch: 647 [0/54000 (0%)] Loss: -1797.290039\n",
      "Train Epoch: 647 [11264/54000 (21%)] Loss: -1810.694702\n",
      "Train Epoch: 647 [22528/54000 (42%)] Loss: -1806.083740\n",
      "Train Epoch: 647 [33792/54000 (63%)] Loss: -1801.226807\n",
      "Train Epoch: 647 [45056/54000 (83%)] Loss: -1798.162109\n",
      "    epoch          : 647\n",
      "    loss           : -1805.2326061320755\n",
      "    ess            : 8.001183527820515\n",
      "    log_marginal   : 1805.2326061320755\n",
      "    val_loss       : -1812.0087076822917\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1812.0087076822917\n",
      "Train Epoch: 648 [0/54000 (0%)] Loss: -1818.502197\n",
      "Train Epoch: 648 [11264/54000 (21%)] Loss: -1817.707886\n",
      "Train Epoch: 648 [22528/54000 (42%)] Loss: -1791.458496\n",
      "Train Epoch: 648 [33792/54000 (63%)] Loss: -1808.048584\n",
      "Train Epoch: 648 [45056/54000 (83%)] Loss: -1798.068115\n",
      "    epoch          : 648\n",
      "    loss           : -1804.7033104086822\n",
      "    ess            : 8.001183428854313\n",
      "    log_marginal   : 1804.7033092570755\n",
      "    val_loss       : -1802.9332275390625\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1802.9332478841145\n",
      "Train Epoch: 649 [0/54000 (0%)] Loss: -1810.123779\n",
      "Train Epoch: 649 [11264/54000 (21%)] Loss: -1809.899902\n",
      "Train Epoch: 649 [22528/54000 (42%)] Loss: -1810.380615\n",
      "Train Epoch: 649 [33792/54000 (63%)] Loss: -1800.670410\n",
      "Train Epoch: 649 [45056/54000 (83%)] Loss: -1805.403320\n",
      "    epoch          : 649\n",
      "    loss           : -1808.1443723282723\n",
      "    ess            : 8.00118361778979\n",
      "    log_marginal   : 1808.1443723282723\n",
      "    val_loss       : -1809.4782816569011\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1809.4782816569011\n",
      "Train Epoch: 650 [0/54000 (0%)] Loss: -1808.843994\n",
      "Train Epoch: 650 [11264/54000 (21%)] Loss: -1810.827271\n",
      "Train Epoch: 650 [22528/54000 (42%)] Loss: -1794.203857\n",
      "Train Epoch: 650 [33792/54000 (63%)] Loss: -1802.997314\n",
      "Train Epoch: 650 [45056/54000 (83%)] Loss: -1800.711426\n",
      "    epoch          : 650\n",
      "    loss           : -1805.8047681124706\n",
      "    ess            : 8.001183086971068\n",
      "    log_marginal   : 1805.8047681124706\n",
      "    val_loss       : -1804.5305786132812\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1804.5305786132812\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch650.pth ...\n",
      "Train Epoch: 651 [0/54000 (0%)] Loss: -1811.695068\n",
      "Train Epoch: 651 [11264/54000 (21%)] Loss: -1815.681152\n",
      "Train Epoch: 651 [22528/54000 (42%)] Loss: -1809.470459\n",
      "Train Epoch: 651 [33792/54000 (63%)] Loss: -1807.046875\n",
      "Train Epoch: 651 [45056/54000 (83%)] Loss: -1799.404907\n",
      "    epoch          : 651\n",
      "    loss           : -1807.0193942088001\n",
      "    ess            : 8.001182304238373\n",
      "    log_marginal   : 1807.0193953604069\n",
      "    val_loss       : -1809.5887451171875\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1809.5887552897136\n",
      "Train Epoch: 652 [0/54000 (0%)] Loss: -1823.104980\n",
      "Train Epoch: 652 [11264/54000 (21%)] Loss: -1809.285645\n",
      "Train Epoch: 652 [22528/54000 (42%)] Loss: -1796.180298\n",
      "Train Epoch: 652 [33792/54000 (63%)] Loss: -1794.267822\n",
      "Train Epoch: 652 [45056/54000 (83%)] Loss: -1800.142578\n",
      "    epoch          : 652\n",
      "    loss           : -1803.3081308040978\n",
      "    ess            : 8.001182088312113\n",
      "    log_marginal   : 1803.3081308040978\n",
      "    val_loss       : -1807.8327026367188\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1807.8327128092449\n",
      "Train Epoch: 653 [0/54000 (0%)] Loss: -1810.715820\n",
      "Train Epoch: 653 [11264/54000 (21%)] Loss: -1808.509033\n",
      "Train Epoch: 653 [22528/54000 (42%)] Loss: -1800.707031\n",
      "Train Epoch: 653 [33792/54000 (63%)] Loss: -1799.662109\n",
      "Train Epoch: 653 [45056/54000 (83%)] Loss: -1800.837158\n",
      "    epoch          : 653\n",
      "    loss           : -1802.164989543411\n",
      "    ess            : 8.001182907032517\n",
      "    log_marginal   : 1802.164989543411\n",
      "    val_loss       : -1804.7345682779949\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1804.7345682779949\n",
      "Train Epoch: 654 [0/54000 (0%)] Loss: -1811.654907\n",
      "Train Epoch: 654 [11264/54000 (21%)] Loss: -1806.980957\n",
      "Train Epoch: 654 [22528/54000 (42%)] Loss: -1800.443115\n",
      "Train Epoch: 654 [33792/54000 (63%)] Loss: -1787.921387\n",
      "Train Epoch: 654 [45056/54000 (83%)] Loss: -1772.949219\n",
      "    epoch          : 654\n",
      "    loss           : -1797.9245259986733\n",
      "    ess            : 8.001182862047878\n",
      "    log_marginal   : 1797.92452715028\n",
      "    val_loss       : -1792.9957885742188\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1792.9957987467449\n",
      "Train Epoch: 655 [0/54000 (0%)] Loss: -1806.257080\n",
      "Train Epoch: 655 [11264/54000 (21%)] Loss: -1803.958984\n",
      "Train Epoch: 655 [22528/54000 (42%)] Loss: -1798.099121\n",
      "Train Epoch: 655 [33792/54000 (63%)] Loss: -1784.534424\n",
      "Train Epoch: 655 [45056/54000 (83%)] Loss: -1792.223389\n",
      "    epoch          : 655\n",
      "    loss           : -1796.5967775740714\n",
      "    ess            : 8.001184094626948\n",
      "    log_marginal   : 1796.5967775740714\n",
      "    val_loss       : -1802.6858622233074\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.6858723958333\n",
      "Train Epoch: 656 [0/54000 (0%)] Loss: -1808.659424\n",
      "Train Epoch: 656 [11264/54000 (21%)] Loss: -1806.587402\n",
      "Train Epoch: 656 [22528/54000 (42%)] Loss: -1805.841187\n",
      "Train Epoch: 656 [33792/54000 (63%)] Loss: -1805.594727\n",
      "Train Epoch: 656 [45056/54000 (83%)] Loss: -1797.431641\n",
      "    epoch          : 656\n",
      "    loss           : -1802.1922515293338\n",
      "    ess            : 8.001182916029444\n",
      "    log_marginal   : 1802.1922515293338\n",
      "    val_loss       : -1805.2170918782551\n",
      "    val_ess        : 8.001182794570923\n",
      "    val_log_marginal: 1805.2170715332031\n",
      "Train Epoch: 657 [0/54000 (0%)] Loss: -1802.668091\n",
      "Train Epoch: 657 [11264/54000 (21%)] Loss: -1792.564941\n",
      "Train Epoch: 657 [22528/54000 (42%)] Loss: -1787.956055\n",
      "Train Epoch: 657 [33792/54000 (63%)] Loss: -1795.699707\n",
      "Train Epoch: 657 [45056/54000 (83%)] Loss: -1793.926758\n",
      "    epoch          : 657\n",
      "    loss           : -1799.4107435694282\n",
      "    ess            : 8.001183716755992\n",
      "    log_marginal   : 1799.4107435694282\n",
      "    val_loss       : -1805.8971048990886\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1805.8971048990886\n",
      "Train Epoch: 658 [0/54000 (0%)] Loss: -1815.059204\n",
      "Train Epoch: 658 [11264/54000 (21%)] Loss: -1813.258545\n",
      "Train Epoch: 658 [22528/54000 (42%)] Loss: -1807.520752\n",
      "Train Epoch: 658 [33792/54000 (63%)] Loss: -1797.546387\n",
      "Train Epoch: 658 [45056/54000 (83%)] Loss: -1793.168335\n",
      "    epoch          : 658\n",
      "    loss           : -1803.776048192438\n",
      "    ess            : 8.001183455845094\n",
      "    log_marginal   : 1803.776048192438\n",
      "    val_loss       : -1802.634541829427\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1802.6345520019531\n",
      "Train Epoch: 659 [0/54000 (0%)] Loss: -1813.583496\n",
      "Train Epoch: 659 [11264/54000 (21%)] Loss: -1814.707031\n",
      "Train Epoch: 659 [22528/54000 (42%)] Loss: -1814.708252\n",
      "Train Epoch: 659 [33792/54000 (63%)] Loss: -1800.799805\n",
      "Train Epoch: 659 [45056/54000 (83%)] Loss: -1813.118896\n",
      "    epoch          : 659\n",
      "    loss           : -1807.0950052513267\n",
      "    ess            : 8.001183347881964\n",
      "    log_marginal   : 1807.0950052513267\n",
      "    val_loss       : -1809.8309631347656\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1809.8309631347656\n",
      "Train Epoch: 660 [0/54000 (0%)] Loss: -1821.265259\n",
      "Train Epoch: 660 [11264/54000 (21%)] Loss: -1809.022949\n",
      "Train Epoch: 660 [22528/54000 (42%)] Loss: -1799.062866\n",
      "Train Epoch: 660 [33792/54000 (63%)] Loss: -1801.660400\n",
      "Train Epoch: 660 [45056/54000 (83%)] Loss: -1800.814453\n",
      "    epoch          : 660\n",
      "    loss           : -1806.0449149653596\n",
      "    ess            : 8.001182322232228\n",
      "    log_marginal   : 1806.044913813753\n",
      "    val_loss       : -1807.9533996582031\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1807.9533996582031\n",
      "Train Epoch: 661 [0/54000 (0%)] Loss: -1821.451172\n",
      "Train Epoch: 661 [11264/54000 (21%)] Loss: -1816.636108\n",
      "Train Epoch: 661 [22528/54000 (42%)] Loss: -1792.165405\n",
      "Train Epoch: 661 [33792/54000 (63%)] Loss: -1809.484863\n",
      "Train Epoch: 661 [45056/54000 (83%)] Loss: -1808.400879\n",
      "    epoch          : 661\n",
      "    loss           : -1804.5732675228478\n",
      "    ess            : 8.001182223266026\n",
      "    log_marginal   : 1804.5732675228478\n",
      "    val_loss       : -1807.1280415852864\n",
      "    val_ess        : 8.001182874043783\n",
      "    val_log_marginal: 1807.1280212402344\n",
      "Train Epoch: 662 [0/54000 (0%)] Loss: -1818.518799\n",
      "Train Epoch: 662 [11264/54000 (21%)] Loss: -1819.346313\n",
      "Train Epoch: 662 [22528/54000 (42%)] Loss: -1814.010254\n",
      "Train Epoch: 662 [33792/54000 (63%)] Loss: -1800.099976\n",
      "Train Epoch: 662 [45056/54000 (83%)] Loss: -1792.857666\n",
      "    epoch          : 662\n",
      "    loss           : -1806.6211409658756\n",
      "    ess            : 8.001181728435013\n",
      "    log_marginal   : 1806.6211409658756\n",
      "    val_loss       : -1804.3329366048176\n",
      "    val_ess        : 8.001184701919556\n",
      "    val_log_marginal: 1804.3329366048176\n",
      "Train Epoch: 663 [0/54000 (0%)] Loss: -1816.044678\n",
      "Train Epoch: 663 [11264/54000 (21%)] Loss: -1802.312744\n",
      "Train Epoch: 663 [22528/54000 (42%)] Loss: -1791.787354\n",
      "Train Epoch: 663 [33792/54000 (63%)] Loss: -1805.963501\n",
      "Train Epoch: 663 [45056/54000 (83%)] Loss: -1796.210815\n",
      "    epoch          : 663\n",
      "    loss           : -1798.7892559699292\n",
      "    ess            : 8.001182556152344\n",
      "    log_marginal   : 1798.7892559699292\n",
      "    val_loss       : -1796.3549499511719\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1796.3549296061199\n",
      "Train Epoch: 664 [0/54000 (0%)] Loss: -1802.731323\n",
      "Train Epoch: 664 [11264/54000 (21%)] Loss: -1793.093506\n",
      "Train Epoch: 664 [22528/54000 (42%)] Loss: -1796.210449\n",
      "Train Epoch: 664 [33792/54000 (63%)] Loss: -1800.758667\n",
      "Train Epoch: 664 [45056/54000 (83%)] Loss: -1794.576172\n",
      "    epoch          : 664\n",
      "    loss           : -1798.03864446676\n",
      "    ess            : 8.001182925026372\n",
      "    log_marginal   : 1798.03864446676\n",
      "    val_loss       : -1802.6483968098958\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1802.6483968098958\n",
      "Train Epoch: 665 [0/54000 (0%)] Loss: -1811.113647\n",
      "Train Epoch: 665 [11264/54000 (21%)] Loss: -1795.915039\n",
      "Train Epoch: 665 [22528/54000 (42%)] Loss: -1794.008545\n",
      "Train Epoch: 665 [33792/54000 (63%)] Loss: -1799.168579\n",
      "Train Epoch: 665 [45056/54000 (83%)] Loss: -1811.373047\n",
      "    epoch          : 665\n",
      "    loss           : -1797.80510898806\n",
      "    ess            : 8.001182862047878\n",
      "    log_marginal   : 1797.8051078364533\n",
      "    val_loss       : -1806.3296305338542\n",
      "    val_ess        : 8.00118350982666\n",
      "    val_log_marginal: 1806.3296305338542\n",
      "Train Epoch: 666 [0/54000 (0%)] Loss: -1824.129883\n",
      "Train Epoch: 666 [11264/54000 (21%)] Loss: -1812.608887\n",
      "Train Epoch: 666 [22528/54000 (42%)] Loss: -1806.737061\n",
      "Train Epoch: 666 [33792/54000 (63%)] Loss: -1807.975342\n",
      "Train Epoch: 666 [45056/54000 (83%)] Loss: -1796.149658\n",
      "    epoch          : 666\n",
      "    loss           : -1804.6926522884728\n",
      "    ess            : 8.001181566490317\n",
      "    log_marginal   : 1804.6926534400795\n",
      "    val_loss       : -1807.2778015136719\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1807.2778218587239\n",
      "Train Epoch: 667 [0/54000 (0%)] Loss: -1816.705078\n",
      "Train Epoch: 667 [11264/54000 (21%)] Loss: -1821.099121\n",
      "Train Epoch: 667 [22528/54000 (42%)] Loss: -1779.777588\n",
      "Train Epoch: 667 [33792/54000 (63%)] Loss: -1795.613770\n",
      "Train Epoch: 667 [45056/54000 (83%)] Loss: -1787.502319\n",
      "    epoch          : 667\n",
      "    loss           : -1800.7031180903596\n",
      "    ess            : 8.001182088312113\n",
      "    log_marginal   : 1800.7031180903596\n",
      "    val_loss       : -1803.3774312337239\n",
      "    val_ess        : 8.001184145609537\n",
      "    val_log_marginal: 1803.3774108886719\n",
      "Train Epoch: 668 [0/54000 (0%)] Loss: -1818.841431\n",
      "Train Epoch: 668 [11264/54000 (21%)] Loss: -1814.145264\n",
      "Train Epoch: 668 [22528/54000 (42%)] Loss: -1799.148682\n",
      "Train Epoch: 668 [33792/54000 (63%)] Loss: -1798.057739\n",
      "Train Epoch: 668 [45056/54000 (83%)] Loss: -1798.632812\n",
      "    epoch          : 668\n",
      "    loss           : -1805.2197634139152\n",
      "    ess            : 8.001182844054025\n",
      "    log_marginal   : 1805.2197634139152\n",
      "    val_loss       : -1800.8536682128906\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1800.8536580403645\n",
      "Train Epoch: 669 [0/54000 (0%)] Loss: -1812.753296\n",
      "Train Epoch: 669 [11264/54000 (21%)] Loss: -1806.583740\n",
      "Train Epoch: 669 [22528/54000 (42%)] Loss: -1807.788574\n",
      "Train Epoch: 669 [33792/54000 (63%)] Loss: -1799.734741\n",
      "Train Epoch: 669 [45056/54000 (83%)] Loss: -1806.741333\n",
      "    epoch          : 669\n",
      "    loss           : -1805.290018333579\n",
      "    ess            : 8.001183455845094\n",
      "    log_marginal   : 1805.290018333579\n",
      "    val_loss       : -1811.2064819335938\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1811.2064819335938\n",
      "Train Epoch: 670 [0/54000 (0%)] Loss: -1807.573608\n",
      "Train Epoch: 670 [11264/54000 (21%)] Loss: -1796.151611\n",
      "Train Epoch: 670 [22528/54000 (42%)] Loss: -1804.945312\n",
      "Train Epoch: 670 [33792/54000 (63%)] Loss: -1795.627441\n",
      "Train Epoch: 670 [45056/54000 (83%)] Loss: -1766.884155\n",
      "    epoch          : 670\n",
      "    loss           : -1793.5067898732311\n",
      "    ess            : 8.001183113961849\n",
      "    log_marginal   : 1793.5067898732311\n",
      "    val_loss       : -1782.9883321126301\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1782.9883321126301\n",
      "Train Epoch: 671 [0/54000 (0%)] Loss: -1791.940918\n",
      "Train Epoch: 671 [11264/54000 (21%)] Loss: -1798.252686\n",
      "Train Epoch: 671 [22528/54000 (42%)] Loss: -1784.235840\n",
      "Train Epoch: 671 [33792/54000 (63%)] Loss: -1781.907593\n",
      "Train Epoch: 671 [45056/54000 (83%)] Loss: -1790.451660\n",
      "    epoch          : 671\n",
      "    loss           : -1788.2457943322524\n",
      "    ess            : 8.001184913347352\n",
      "    log_marginal   : 1788.2457943322524\n",
      "    val_loss       : -1805.478271484375\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1805.478271484375\n",
      "Train Epoch: 672 [0/54000 (0%)] Loss: -1813.232178\n",
      "Train Epoch: 672 [11264/54000 (21%)] Loss: -1791.726440\n",
      "Train Epoch: 672 [22528/54000 (42%)] Loss: -1801.816406\n",
      "Train Epoch: 672 [33792/54000 (63%)] Loss: -1802.378906\n",
      "Train Epoch: 672 [45056/54000 (83%)] Loss: -1800.880005\n",
      "    epoch          : 672\n",
      "    loss           : -1797.2952028670402\n",
      "    ess            : 8.001183482835877\n",
      "    log_marginal   : 1797.2952028670402\n",
      "    val_loss       : -1807.4717102050781\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.4717102050781\n",
      "Train Epoch: 673 [0/54000 (0%)] Loss: -1815.231812\n",
      "Train Epoch: 673 [11264/54000 (21%)] Loss: -1814.247070\n",
      "Train Epoch: 673 [22528/54000 (42%)] Loss: -1802.957275\n",
      "Train Epoch: 673 [33792/54000 (63%)] Loss: -1796.536743\n",
      "Train Epoch: 673 [45056/54000 (83%)] Loss: -1805.178955\n",
      "    epoch          : 673\n",
      "    loss           : -1807.844262464991\n",
      "    ess            : 8.001183104964921\n",
      "    log_marginal   : 1807.844262464991\n",
      "    val_loss       : -1811.6365254720051\n",
      "    val_ess        : 8.001184066136679\n",
      "    val_log_marginal: 1811.6365051269531\n",
      "Train Epoch: 674 [0/54000 (0%)] Loss: -1823.109009\n",
      "Train Epoch: 674 [11264/54000 (21%)] Loss: -1814.725220\n",
      "Train Epoch: 674 [22528/54000 (42%)] Loss: -1807.049316\n",
      "Train Epoch: 674 [33792/54000 (63%)] Loss: -1808.801758\n",
      "Train Epoch: 674 [45056/54000 (83%)] Loss: -1808.706055\n",
      "    epoch          : 674\n",
      "    loss           : -1809.733575784935\n",
      "    ess            : 8.001182700103184\n",
      "    log_marginal   : 1809.733575784935\n",
      "    val_loss       : -1813.8933410644531\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1813.8933410644531\n",
      "Train Epoch: 675 [0/54000 (0%)] Loss: -1817.336182\n",
      "Train Epoch: 675 [11264/54000 (21%)] Loss: -1799.665039\n",
      "Train Epoch: 675 [22528/54000 (42%)] Loss: -1799.250977\n",
      "Train Epoch: 675 [33792/54000 (63%)] Loss: -1812.694702\n",
      "Train Epoch: 675 [45056/54000 (83%)] Loss: -1798.664673\n",
      "    epoch          : 675\n",
      "    loss           : -1803.3071980026532\n",
      "    ess            : 8.001182862047878\n",
      "    log_marginal   : 1803.3071980026532\n",
      "    val_loss       : -1798.0205790201824\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1798.0205790201824\n",
      "Train Epoch: 676 [0/54000 (0%)] Loss: -1798.776611\n",
      "Train Epoch: 676 [11264/54000 (21%)] Loss: -1796.139648\n",
      "Train Epoch: 676 [22528/54000 (42%)] Loss: -1801.078003\n",
      "Train Epoch: 676 [33792/54000 (63%)] Loss: -1791.033203\n",
      "Train Epoch: 676 [45056/54000 (83%)] Loss: -1786.547363\n",
      "    epoch          : 676\n",
      "    loss           : -1798.1304252192658\n",
      "    ess            : 8.00118397766689\n",
      "    log_marginal   : 1798.1304252192658\n",
      "    val_loss       : -1802.5312805175781\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1802.5312805175781\n",
      "Train Epoch: 677 [0/54000 (0%)] Loss: -1809.345215\n",
      "Train Epoch: 677 [11264/54000 (21%)] Loss: -1802.904663\n",
      "Train Epoch: 677 [22528/54000 (42%)] Loss: -1801.910156\n",
      "Train Epoch: 677 [33792/54000 (63%)] Loss: -1792.956299\n",
      "Train Epoch: 677 [45056/54000 (83%)] Loss: -1785.196289\n",
      "    epoch          : 677\n",
      "    loss           : -1797.1849019752358\n",
      "    ess            : 8.001183113961849\n",
      "    log_marginal   : 1797.1849019752358\n",
      "    val_loss       : -1806.4086507161458\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1806.408671061198\n",
      "Train Epoch: 678 [0/54000 (0%)] Loss: -1812.846191\n",
      "Train Epoch: 678 [11264/54000 (21%)] Loss: -1816.925781\n",
      "Train Epoch: 678 [22528/54000 (42%)] Loss: -1805.316284\n",
      "Train Epoch: 678 [33792/54000 (63%)] Loss: -1807.148926\n",
      "Train Epoch: 678 [45056/54000 (83%)] Loss: -1803.439453\n",
      "    epoch          : 678\n",
      "    loss           : -1807.2446484835643\n",
      "    ess            : 8.00118227724759\n",
      "    log_marginal   : 1807.2446484835643\n",
      "    val_loss       : -1804.0765380859375\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1804.0765075683594\n",
      "Train Epoch: 679 [0/54000 (0%)] Loss: -1815.563110\n",
      "Train Epoch: 679 [11264/54000 (21%)] Loss: -1798.548584\n",
      "Train Epoch: 679 [22528/54000 (42%)] Loss: -1802.369385\n",
      "Train Epoch: 679 [33792/54000 (63%)] Loss: -1812.041016\n",
      "Train Epoch: 679 [45056/54000 (83%)] Loss: -1807.228394\n",
      "    epoch          : 679\n",
      "    loss           : -1805.5591419147995\n",
      "    ess            : 8.001182322232228\n",
      "    log_marginal   : 1805.5591419147995\n",
      "    val_loss       : -1809.2279154459636\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1809.2279154459636\n",
      "Train Epoch: 680 [0/54000 (0%)] Loss: -1820.487793\n",
      "Train Epoch: 680 [11264/54000 (21%)] Loss: -1797.406860\n",
      "Train Epoch: 680 [22528/54000 (42%)] Loss: -1798.953369\n",
      "Train Epoch: 680 [33792/54000 (63%)] Loss: -1802.093628\n",
      "Train Epoch: 680 [45056/54000 (83%)] Loss: -1788.876709\n",
      "    epoch          : 680\n",
      "    loss           : -1797.1489361457104\n",
      "    ess            : 8.001183698762137\n",
      "    log_marginal   : 1797.1489361457104\n",
      "    val_loss       : -1803.1265157063801\n",
      "    val_ess        : 8.001184701919556\n",
      "    val_log_marginal: 1803.1265157063801\n",
      "Train Epoch: 681 [0/54000 (0%)] Loss: -1811.507935\n",
      "Train Epoch: 681 [11264/54000 (21%)] Loss: -1816.853882\n",
      "Train Epoch: 681 [22528/54000 (42%)] Loss: -1794.413330\n",
      "Train Epoch: 681 [33792/54000 (63%)] Loss: -1803.549194\n",
      "Train Epoch: 681 [45056/54000 (83%)] Loss: -1800.647217\n",
      "    epoch          : 681\n",
      "    loss           : -1801.2160103276092\n",
      "    ess            : 8.001183140952632\n",
      "    log_marginal   : 1801.2160103276092\n",
      "    val_loss       : -1788.2926839192708\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1788.2926839192708\n",
      "Train Epoch: 682 [0/54000 (0%)] Loss: -1791.957153\n",
      "Train Epoch: 682 [11264/54000 (21%)] Loss: -1760.351196\n",
      "Train Epoch: 682 [22528/54000 (42%)] Loss: -1775.035645\n",
      "Train Epoch: 682 [33792/54000 (63%)] Loss: -1789.863770\n",
      "Train Epoch: 682 [45056/54000 (83%)] Loss: -1793.954834\n",
      "    epoch          : 682\n",
      "    loss           : -1780.2835532134434\n",
      "    ess            : 8.001185273224452\n",
      "    log_marginal   : 1780.2835532134434\n",
      "    val_loss       : -1805.9695536295574\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1805.9695536295574\n",
      "Train Epoch: 683 [0/54000 (0%)] Loss: -1804.806885\n",
      "Train Epoch: 683 [11264/54000 (21%)] Loss: -1797.610840\n",
      "Train Epoch: 683 [22528/54000 (42%)] Loss: -1800.704224\n",
      "Train Epoch: 683 [33792/54000 (63%)] Loss: -1793.458008\n",
      "Train Epoch: 683 [45056/54000 (83%)] Loss: -1792.356445\n",
      "    epoch          : 683\n",
      "    loss           : -1794.8620490308078\n",
      "    ess            : 8.00118376174063\n",
      "    log_marginal   : 1794.8620501824146\n",
      "    val_loss       : -1808.9613749186199\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1808.9613749186199\n",
      "Train Epoch: 684 [0/54000 (0%)] Loss: -1812.460449\n",
      "Train Epoch: 684 [11264/54000 (21%)] Loss: -1805.930664\n",
      "Train Epoch: 684 [22528/54000 (42%)] Loss: -1812.477295\n",
      "Train Epoch: 684 [33792/54000 (63%)] Loss: -1801.576172\n",
      "Train Epoch: 684 [45056/54000 (83%)] Loss: -1809.610840\n",
      "    epoch          : 684\n",
      "    loss           : -1804.7300173201652\n",
      "    ess            : 8.001184319550136\n",
      "    log_marginal   : 1804.7300173201652\n",
      "    val_loss       : -1810.7606302897136\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1810.7606302897136\n",
      "Train Epoch: 685 [0/54000 (0%)] Loss: -1819.852051\n",
      "Train Epoch: 685 [11264/54000 (21%)] Loss: -1815.661621\n",
      "Train Epoch: 685 [22528/54000 (42%)] Loss: -1797.716064\n",
      "Train Epoch: 685 [33792/54000 (63%)] Loss: -1793.066895\n",
      "Train Epoch: 685 [45056/54000 (83%)] Loss: -1786.059937\n",
      "    epoch          : 685\n",
      "    loss           : -1803.4586354381634\n",
      "    ess            : 8.001183905691471\n",
      "    log_marginal   : 1803.45863658977\n",
      "    val_loss       : -1806.8092549641926\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1806.8092346191406\n",
      "Train Epoch: 686 [0/54000 (0%)] Loss: -1813.479004\n",
      "Train Epoch: 686 [11264/54000 (21%)] Loss: -1819.236938\n",
      "Train Epoch: 686 [22528/54000 (42%)] Loss: -1814.628784\n",
      "Train Epoch: 686 [33792/54000 (63%)] Loss: -1806.844849\n",
      "Train Epoch: 686 [45056/54000 (83%)] Loss: -1804.475098\n",
      "    epoch          : 686\n",
      "    loss           : -1808.176240741082\n",
      "    ess            : 8.001183347881964\n",
      "    log_marginal   : 1808.1762418926887\n",
      "    val_loss       : -1804.3609517415364\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1804.3609517415364\n",
      "Train Epoch: 687 [0/54000 (0%)] Loss: -1809.950439\n",
      "Train Epoch: 687 [11264/54000 (21%)] Loss: -1815.987305\n",
      "Train Epoch: 687 [22528/54000 (42%)] Loss: -1805.292236\n",
      "Train Epoch: 687 [33792/54000 (63%)] Loss: -1795.679443\n",
      "Train Epoch: 687 [45056/54000 (83%)] Loss: -1801.459595\n",
      "    epoch          : 687\n",
      "    loss           : -1804.462224996315\n",
      "    ess            : 8.001182790072459\n",
      "    log_marginal   : 1804.462224996315\n",
      "    val_loss       : -1807.3439331054688\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1807.3439331054688\n",
      "Train Epoch: 688 [0/54000 (0%)] Loss: -1816.754883\n",
      "Train Epoch: 688 [11264/54000 (21%)] Loss: -1803.147705\n",
      "Train Epoch: 688 [22528/54000 (42%)] Loss: -1777.725586\n",
      "Train Epoch: 688 [33792/54000 (63%)] Loss: -1800.906860\n",
      "Train Epoch: 688 [45056/54000 (83%)] Loss: -1793.401855\n",
      "    epoch          : 688\n",
      "    loss           : -1801.4652721477005\n",
      "    ess            : 8.00118372575292\n",
      "    log_marginal   : 1801.4652732993072\n",
      "    val_loss       : -1803.0223897298176\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1803.0223897298176\n",
      "Train Epoch: 689 [0/54000 (0%)] Loss: -1817.449219\n",
      "Train Epoch: 689 [11264/54000 (21%)] Loss: -1813.692017\n",
      "Train Epoch: 689 [22528/54000 (42%)] Loss: -1795.982422\n",
      "Train Epoch: 689 [33792/54000 (63%)] Loss: -1782.883545\n",
      "Train Epoch: 689 [45056/54000 (83%)] Loss: -1797.579468\n",
      "    epoch          : 689\n",
      "    loss           : -1800.0419438200177\n",
      "    ess            : 8.00118354581437\n",
      "    log_marginal   : 1800.0419438200177\n",
      "    val_loss       : -1799.2779947916667\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1799.2780049641926\n",
      "Train Epoch: 690 [0/54000 (0%)] Loss: -1791.861816\n",
      "Train Epoch: 690 [11264/54000 (21%)] Loss: -1807.179077\n",
      "Train Epoch: 690 [22528/54000 (42%)] Loss: -1797.554565\n",
      "Train Epoch: 690 [33792/54000 (63%)] Loss: -1790.938843\n",
      "Train Epoch: 690 [45056/54000 (83%)] Loss: -1788.163818\n",
      "    epoch          : 690\n",
      "    loss           : -1799.185829018647\n",
      "    ess            : 8.001183383869675\n",
      "    log_marginal   : 1799.185829018647\n",
      "    val_loss       : -1801.7886657714844\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1801.7886759440105\n",
      "Train Epoch: 691 [0/54000 (0%)] Loss: -1813.061035\n",
      "Train Epoch: 691 [11264/54000 (21%)] Loss: -1816.850464\n",
      "Train Epoch: 691 [22528/54000 (42%)] Loss: -1805.746582\n",
      "Train Epoch: 691 [33792/54000 (63%)] Loss: -1802.743774\n",
      "Train Epoch: 691 [45056/54000 (83%)] Loss: -1792.598022\n",
      "    epoch          : 691\n",
      "    loss           : -1807.5009178305572\n",
      "    ess            : 8.001182556152344\n",
      "    log_marginal   : 1807.5009178305572\n",
      "    val_loss       : -1807.242899576823\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1807.242899576823\n",
      "Train Epoch: 692 [0/54000 (0%)] Loss: -1815.326294\n",
      "Train Epoch: 692 [11264/54000 (21%)] Loss: -1817.545654\n",
      "Train Epoch: 692 [22528/54000 (42%)] Loss: -1793.296387\n",
      "Train Epoch: 692 [33792/54000 (63%)] Loss: -1812.437866\n",
      "Train Epoch: 692 [45056/54000 (83%)] Loss: -1806.105225\n",
      "    epoch          : 692\n",
      "    loss           : -1801.2932693193543\n",
      "    ess            : 8.001182988004864\n",
      "    log_marginal   : 1801.2932693193543\n",
      "    val_loss       : -1799.2162170410156\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1799.2162272135417\n",
      "Train Epoch: 693 [0/54000 (0%)] Loss: -1810.517822\n",
      "Train Epoch: 693 [11264/54000 (21%)] Loss: -1804.839600\n",
      "Train Epoch: 693 [22528/54000 (42%)] Loss: -1801.183594\n",
      "Train Epoch: 693 [33792/54000 (63%)] Loss: -1797.906738\n",
      "Train Epoch: 693 [45056/54000 (83%)] Loss: -1798.742920\n",
      "    epoch          : 693\n",
      "    loss           : -1800.6945489847435\n",
      "    ess            : 8.001183221924979\n",
      "    log_marginal   : 1800.6945489847435\n",
      "    val_loss       : -1799.0647481282551\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1799.0647481282551\n",
      "Train Epoch: 694 [0/54000 (0%)] Loss: -1822.261719\n",
      "Train Epoch: 694 [11264/54000 (21%)] Loss: -1813.377686\n",
      "Train Epoch: 694 [22528/54000 (42%)] Loss: -1804.755493\n",
      "Train Epoch: 694 [33792/54000 (63%)] Loss: -1805.792969\n",
      "Train Epoch: 694 [45056/54000 (83%)] Loss: -1803.596802\n",
      "    epoch          : 694\n",
      "    loss           : -1810.0974938734523\n",
      "    ess            : 8.001183221924979\n",
      "    log_marginal   : 1810.0974938734523\n",
      "    val_loss       : -1812.5565490722656\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1812.5565490722656\n",
      "Train Epoch: 695 [0/54000 (0%)] Loss: -1819.538086\n",
      "Train Epoch: 695 [11264/54000 (21%)] Loss: -1802.911621\n",
      "Train Epoch: 695 [22528/54000 (42%)] Loss: -1794.925537\n",
      "Train Epoch: 695 [33792/54000 (63%)] Loss: -1811.712891\n",
      "Train Epoch: 695 [45056/54000 (83%)] Loss: -1799.238525\n",
      "    epoch          : 695\n",
      "    loss           : -1803.984047943691\n",
      "    ess            : 8.001183797728341\n",
      "    log_marginal   : 1803.984047943691\n",
      "    val_loss       : -1811.029032389323\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1811.029032389323\n",
      "Train Epoch: 696 [0/54000 (0%)] Loss: -1818.767334\n",
      "Train Epoch: 696 [11264/54000 (21%)] Loss: -1817.509644\n",
      "Train Epoch: 696 [22528/54000 (42%)] Loss: -1801.494995\n",
      "Train Epoch: 696 [33792/54000 (63%)] Loss: -1794.107422\n",
      "Train Epoch: 696 [45056/54000 (83%)] Loss: -1801.559082\n",
      "    epoch          : 696\n",
      "    loss           : -1804.3037708210495\n",
      "    ess            : 8.001182538158488\n",
      "    log_marginal   : 1804.3037708210495\n",
      "    val_loss       : -1804.2784830729167\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1804.2784830729167\n",
      "Train Epoch: 697 [0/54000 (0%)] Loss: -1808.252441\n",
      "Train Epoch: 697 [11264/54000 (21%)] Loss: -1811.225098\n",
      "Train Epoch: 697 [22528/54000 (42%)] Loss: -1809.719482\n",
      "Train Epoch: 697 [33792/54000 (63%)] Loss: -1806.315918\n",
      "Train Epoch: 697 [45056/54000 (83%)] Loss: -1818.885742\n",
      "    epoch          : 697\n",
      "    loss           : -1808.0380202959168\n",
      "    ess            : 8.001182808066314\n",
      "    log_marginal   : 1808.0380202959168\n",
      "    val_loss       : -1813.526143391927\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1813.5261535644531\n",
      "Train Epoch: 698 [0/54000 (0%)] Loss: -1826.707031\n",
      "Train Epoch: 698 [11264/54000 (21%)] Loss: -1792.085938\n",
      "Train Epoch: 698 [22528/54000 (42%)] Loss: -1793.275879\n",
      "Train Epoch: 698 [33792/54000 (63%)] Loss: -1799.053223\n",
      "Train Epoch: 698 [45056/54000 (83%)] Loss: -1800.927246\n",
      "    epoch          : 698\n",
      "    loss           : -1799.4828790628685\n",
      "    ess            : 8.00118173743194\n",
      "    log_marginal   : 1799.4828790628685\n",
      "    val_loss       : -1802.2896931966145\n",
      "    val_ess        : 8.001182794570923\n",
      "    val_log_marginal: 1802.2896931966145\n",
      "Train Epoch: 699 [0/54000 (0%)] Loss: -1819.428223\n",
      "Train Epoch: 699 [11264/54000 (21%)] Loss: -1808.520020\n",
      "Train Epoch: 699 [22528/54000 (42%)] Loss: -1810.821655\n",
      "Train Epoch: 699 [33792/54000 (63%)] Loss: -1802.996948\n",
      "Train Epoch: 699 [45056/54000 (83%)] Loss: -1802.523682\n",
      "    epoch          : 699\n",
      "    loss           : -1809.9119550596993\n",
      "    ess            : 8.001181206613216\n",
      "    log_marginal   : 1809.911956211306\n",
      "    val_loss       : -1808.9749552408855\n",
      "    val_ess        : 8.00118080774943\n",
      "    val_log_marginal: 1808.9749552408855\n",
      "Train Epoch: 700 [0/54000 (0%)] Loss: -1831.309814\n",
      "Train Epoch: 700 [11264/54000 (21%)] Loss: -1819.695312\n",
      "Train Epoch: 700 [22528/54000 (42%)] Loss: -1802.158203\n",
      "Train Epoch: 700 [33792/54000 (63%)] Loss: -1779.498535\n",
      "Train Epoch: 700 [45056/54000 (83%)] Loss: -1784.954834\n",
      "    epoch          : 700\n",
      "    loss           : -1798.9119343307782\n",
      "    ess            : 8.001182124299824\n",
      "    log_marginal   : 1798.9119343307782\n",
      "    val_loss       : -1805.589579264323\n",
      "    val_ess        : 8.00118350982666\n",
      "    val_log_marginal: 1805.589579264323\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch700.pth ...\n",
      "Train Epoch: 701 [0/54000 (0%)] Loss: -1805.991943\n",
      "Train Epoch: 701 [11264/54000 (21%)] Loss: -1806.743896\n",
      "Train Epoch: 701 [22528/54000 (42%)] Loss: -1794.893433\n",
      "Train Epoch: 701 [33792/54000 (63%)] Loss: -1806.320068\n",
      "Train Epoch: 701 [45056/54000 (83%)] Loss: -1801.083252\n",
      "    epoch          : 701\n",
      "    loss           : -1801.209319492556\n",
      "    ess            : 8.001182412201503\n",
      "    log_marginal   : 1801.2093206441627\n",
      "    val_loss       : -1808.2374165852864\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1808.2374165852864\n",
      "Train Epoch: 702 [0/54000 (0%)] Loss: -1810.891113\n",
      "Train Epoch: 702 [11264/54000 (21%)] Loss: -1802.937256\n",
      "Train Epoch: 702 [22528/54000 (42%)] Loss: -1791.247803\n",
      "Train Epoch: 702 [33792/54000 (63%)] Loss: -1793.928345\n",
      "Train Epoch: 702 [45056/54000 (83%)] Loss: -1792.315430\n",
      "    epoch          : 702\n",
      "    loss           : -1796.7766435731132\n",
      "    ess            : 8.001182583143127\n",
      "    log_marginal   : 1796.77664472472\n",
      "    val_loss       : -1802.1306762695312\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1802.1306762695312\n",
      "Train Epoch: 703 [0/54000 (0%)] Loss: -1816.847168\n",
      "Train Epoch: 703 [11264/54000 (21%)] Loss: -1812.375000\n",
      "Train Epoch: 703 [22528/54000 (42%)] Loss: -1802.002563\n",
      "Train Epoch: 703 [33792/54000 (63%)] Loss: -1811.730469\n",
      "Train Epoch: 703 [45056/54000 (83%)] Loss: -1803.203613\n",
      "    epoch          : 703\n",
      "    loss           : -1806.7344832510319\n",
      "    ess            : 8.001181971352056\n",
      "    log_marginal   : 1806.7344832510319\n",
      "    val_loss       : -1807.2013753255208\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1807.2013854980469\n",
      "Train Epoch: 704 [0/54000 (0%)] Loss: -1818.669678\n",
      "Train Epoch: 704 [11264/54000 (21%)] Loss: -1756.866699\n",
      "Train Epoch: 704 [22528/54000 (42%)] Loss: -1794.232910\n",
      "Train Epoch: 704 [33792/54000 (63%)] Loss: -1762.593140\n",
      "Train Epoch: 704 [45056/54000 (83%)] Loss: -1790.300049\n",
      "    epoch          : 704\n",
      "    loss           : -1782.6511748691776\n",
      "    ess            : 8.00118372575292\n",
      "    log_marginal   : 1782.6511748691776\n",
      "    val_loss       : -1801.3089497884114\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1801.3089599609375\n",
      "Train Epoch: 705 [0/54000 (0%)] Loss: -1803.978638\n",
      "Train Epoch: 705 [11264/54000 (21%)] Loss: -1798.285645\n",
      "Train Epoch: 705 [22528/54000 (42%)] Loss: -1801.143799\n",
      "Train Epoch: 705 [33792/54000 (63%)] Loss: -1793.535156\n",
      "Train Epoch: 705 [45056/54000 (83%)] Loss: -1793.728027\n",
      "    epoch          : 705\n",
      "    loss           : -1796.409618449661\n",
      "    ess            : 8.001183113961849\n",
      "    log_marginal   : 1796.409618449661\n",
      "    val_loss       : -1806.1416524251301\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1806.1416524251301\n",
      "Train Epoch: 706 [0/54000 (0%)] Loss: -1806.609863\n",
      "Train Epoch: 706 [11264/54000 (21%)] Loss: -1797.341431\n",
      "Train Epoch: 706 [22528/54000 (42%)] Loss: -1818.048462\n",
      "Train Epoch: 706 [33792/54000 (63%)] Loss: -1805.393799\n",
      "Train Epoch: 706 [45056/54000 (83%)] Loss: -1807.314087\n",
      "    epoch          : 706\n",
      "    loss           : -1806.4308437131485\n",
      "    ess            : 8.00118304198643\n",
      "    log_marginal   : 1806.430846016362\n",
      "    val_loss       : -1808.8966166178386\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1808.8966166178386\n",
      "Train Epoch: 707 [0/54000 (0%)] Loss: -1813.395996\n",
      "Train Epoch: 707 [11264/54000 (21%)] Loss: -1796.111328\n",
      "Train Epoch: 707 [22528/54000 (42%)] Loss: -1805.281982\n",
      "Train Epoch: 707 [33792/54000 (63%)] Loss: -1805.624390\n",
      "Train Epoch: 707 [45056/54000 (83%)] Loss: -1803.517822\n",
      "    epoch          : 707\n",
      "    loss           : -1804.612419848172\n",
      "    ess            : 8.001183824719122\n",
      "    log_marginal   : 1804.612419848172\n",
      "    val_loss       : -1813.4031372070312\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1813.4031372070312\n",
      "Train Epoch: 708 [0/54000 (0%)] Loss: -1811.528931\n",
      "Train Epoch: 708 [11264/54000 (21%)] Loss: -1809.143433\n",
      "Train Epoch: 708 [22528/54000 (42%)] Loss: -1793.505737\n",
      "Train Epoch: 708 [33792/54000 (63%)] Loss: -1810.318359\n",
      "Train Epoch: 708 [45056/54000 (83%)] Loss: -1799.928101\n",
      "    epoch          : 708\n",
      "    loss           : -1801.9696839530513\n",
      "    ess            : 8.001183131955704\n",
      "    log_marginal   : 1801.969685104658\n",
      "    val_loss       : -1805.9065653483074\n",
      "    val_ess        : 8.001184145609537\n",
      "    val_log_marginal: 1805.9065653483074\n",
      "Train Epoch: 709 [0/54000 (0%)] Loss: -1803.027100\n",
      "Train Epoch: 709 [11264/54000 (21%)] Loss: -1822.201294\n",
      "Train Epoch: 709 [22528/54000 (42%)] Loss: -1812.106323\n",
      "Train Epoch: 709 [33792/54000 (63%)] Loss: -1802.293945\n",
      "Train Epoch: 709 [45056/54000 (83%)] Loss: -1805.424316\n",
      "    epoch          : 709\n",
      "    loss           : -1808.921701107385\n",
      "    ess            : 8.001183086971068\n",
      "    log_marginal   : 1808.921701107385\n",
      "    val_loss       : -1810.3700358072917\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1810.3700358072917\n",
      "Train Epoch: 710 [0/54000 (0%)] Loss: -1817.076416\n",
      "Train Epoch: 710 [11264/54000 (21%)] Loss: -1811.643188\n",
      "Train Epoch: 710 [22528/54000 (42%)] Loss: -1811.516357\n",
      "Train Epoch: 710 [33792/54000 (63%)] Loss: -1803.934814\n",
      "Train Epoch: 710 [45056/54000 (83%)] Loss: -1803.737549\n",
      "    epoch          : 710\n",
      "    loss           : -1809.763647691259\n",
      "    ess            : 8.001182304238373\n",
      "    log_marginal   : 1809.763647691259\n",
      "    val_loss       : -1807.2587992350261\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1807.2587992350261\n",
      "Train Epoch: 711 [0/54000 (0%)] Loss: -1812.293823\n",
      "Train Epoch: 711 [11264/54000 (21%)] Loss: -1819.994263\n",
      "Train Epoch: 711 [22528/54000 (42%)] Loss: -1800.250244\n",
      "Train Epoch: 711 [33792/54000 (63%)] Loss: -1807.016479\n",
      "Train Epoch: 711 [45056/54000 (83%)] Loss: -1803.373901\n",
      "    epoch          : 711\n",
      "    loss           : -1805.7529849646226\n",
      "    ess            : 8.001182673112401\n",
      "    log_marginal   : 1805.7529849646226\n",
      "    val_loss       : -1801.9347737630208\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.9347737630208\n",
      "Train Epoch: 712 [0/54000 (0%)] Loss: -1821.569946\n",
      "Train Epoch: 712 [11264/54000 (21%)] Loss: -1808.368652\n",
      "Train Epoch: 712 [22528/54000 (42%)] Loss: -1814.410645\n",
      "Train Epoch: 712 [33792/54000 (63%)] Loss: -1804.188110\n",
      "Train Epoch: 712 [45056/54000 (83%)] Loss: -1805.952393\n",
      "    epoch          : 712\n",
      "    loss           : -1808.4220373765477\n",
      "    ess            : 8.001183347881964\n",
      "    log_marginal   : 1808.4220373765477\n",
      "    val_loss       : -1814.6928405761719\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1814.6928405761719\n",
      "Train Epoch: 713 [0/54000 (0%)] Loss: -1814.709473\n",
      "Train Epoch: 713 [11264/54000 (21%)] Loss: -1803.241943\n",
      "Train Epoch: 713 [22528/54000 (42%)] Loss: -1797.666260\n",
      "Train Epoch: 713 [33792/54000 (63%)] Loss: -1780.411987\n",
      "Train Epoch: 713 [45056/54000 (83%)] Loss: -1784.162476\n",
      "    epoch          : 713\n",
      "    loss           : -1791.8512653854657\n",
      "    ess            : 8.001183023992574\n",
      "    log_marginal   : 1791.8512653854657\n",
      "    val_loss       : -1773.5372009277344\n",
      "    val_ess        : 8.001185894012451\n",
      "    val_log_marginal: 1773.5372009277344\n",
      "Train Epoch: 714 [0/54000 (0%)] Loss: -1796.321533\n",
      "Train Epoch: 714 [11264/54000 (21%)] Loss: -1803.774658\n",
      "Train Epoch: 714 [22528/54000 (42%)] Loss: -1801.109985\n",
      "Train Epoch: 714 [33792/54000 (63%)] Loss: -1794.720459\n",
      "Train Epoch: 714 [45056/54000 (83%)] Loss: -1804.521606\n",
      "    epoch          : 714\n",
      "    loss           : -1798.2345638635024\n",
      "    ess            : 8.001184094626948\n",
      "    log_marginal   : 1798.2345638635024\n",
      "    val_loss       : -1794.2328999837239\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1794.2328999837239\n",
      "Train Epoch: 715 [0/54000 (0%)] Loss: -1806.545654\n",
      "Train Epoch: 715 [11264/54000 (21%)] Loss: -1796.126709\n",
      "Train Epoch: 715 [22528/54000 (42%)] Loss: -1804.004517\n",
      "Train Epoch: 715 [33792/54000 (63%)] Loss: -1812.561401\n",
      "Train Epoch: 715 [45056/54000 (83%)] Loss: -1798.989990\n",
      "    epoch          : 715\n",
      "    loss           : -1801.7979172040832\n",
      "    ess            : 8.001182583143127\n",
      "    log_marginal   : 1801.79791835569\n",
      "    val_loss       : -1809.6620585123699\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1809.6620585123699\n",
      "Train Epoch: 716 [0/54000 (0%)] Loss: -1817.663696\n",
      "Train Epoch: 716 [11264/54000 (21%)] Loss: -1813.256592\n",
      "Train Epoch: 716 [22528/54000 (42%)] Loss: -1798.679688\n",
      "Train Epoch: 716 [33792/54000 (63%)] Loss: -1801.293213\n",
      "Train Epoch: 716 [45056/54000 (83%)] Loss: -1816.990234\n",
      "    epoch          : 716\n",
      "    loss           : -1808.1726834279186\n",
      "    ess            : 8.001182646121618\n",
      "    log_marginal   : 1808.1726834279186\n",
      "    val_loss       : -1813.3906656901042\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1813.3906860351562\n",
      "Train Epoch: 717 [0/54000 (0%)] Loss: -1817.583008\n",
      "Train Epoch: 717 [11264/54000 (21%)] Loss: -1817.624268\n",
      "Train Epoch: 717 [22528/54000 (42%)] Loss: -1803.516724\n",
      "Train Epoch: 717 [33792/54000 (63%)] Loss: -1801.774414\n",
      "Train Epoch: 717 [45056/54000 (83%)] Loss: -1797.922363\n",
      "    epoch          : 717\n",
      "    loss           : -1806.9372650722287\n",
      "    ess            : 8.001182349223011\n",
      "    log_marginal   : 1806.9372650722287\n",
      "    val_loss       : -1804.2440388997395\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1804.2440388997395\n",
      "Train Epoch: 718 [0/54000 (0%)] Loss: -1815.343140\n",
      "Train Epoch: 718 [11264/54000 (21%)] Loss: -1805.703247\n",
      "Train Epoch: 718 [22528/54000 (42%)] Loss: -1810.820068\n",
      "Train Epoch: 718 [33792/54000 (63%)] Loss: -1808.512939\n",
      "Train Epoch: 718 [45056/54000 (83%)] Loss: -1810.886963\n",
      "    epoch          : 718\n",
      "    loss           : -1806.873584675339\n",
      "    ess            : 8.001181728435013\n",
      "    log_marginal   : 1806.873584675339\n",
      "    val_loss       : -1811.526346842448\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1811.5263671875\n",
      "Train Epoch: 719 [0/54000 (0%)] Loss: -1816.238525\n",
      "Train Epoch: 719 [11264/54000 (21%)] Loss: -1825.946411\n",
      "Train Epoch: 719 [22528/54000 (42%)] Loss: -1796.682373\n",
      "Train Epoch: 719 [33792/54000 (63%)] Loss: -1800.483032\n",
      "Train Epoch: 719 [45056/54000 (83%)] Loss: -1802.022217\n",
      "    epoch          : 719\n",
      "    loss           : -1807.375314388635\n",
      "    ess            : 8.001182448189214\n",
      "    log_marginal   : 1807.3753155402417\n",
      "    val_loss       : -1812.0878804524739\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1812.0878804524739\n",
      "Train Epoch: 720 [0/54000 (0%)] Loss: -1812.483643\n",
      "Train Epoch: 720 [11264/54000 (21%)] Loss: -1808.370605\n",
      "Train Epoch: 720 [22528/54000 (42%)] Loss: -1813.046753\n",
      "Train Epoch: 720 [33792/54000 (63%)] Loss: -1816.429199\n",
      "Train Epoch: 720 [45056/54000 (83%)] Loss: -1803.525757\n",
      "    epoch          : 720\n",
      "    loss           : -1808.833438513414\n",
      "    ess            : 8.001182106305968\n",
      "    log_marginal   : 1808.833438513414\n",
      "    val_loss       : -1807.9854838053386\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1807.9854838053386\n",
      "Train Epoch: 721 [0/54000 (0%)] Loss: -1814.455322\n",
      "Train Epoch: 721 [11264/54000 (21%)] Loss: -1814.758301\n",
      "Train Epoch: 721 [22528/54000 (42%)] Loss: -1792.536377\n",
      "Train Epoch: 721 [33792/54000 (63%)] Loss: -1794.485596\n",
      "Train Epoch: 721 [45056/54000 (83%)] Loss: -1803.031494\n",
      "    epoch          : 721\n",
      "    loss           : -1804.3634067751327\n",
      "    ess            : 8.001182295241446\n",
      "    log_marginal   : 1804.3634079267395\n",
      "    val_loss       : -1807.5150655110676\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1807.5150655110676\n",
      "Train Epoch: 722 [0/54000 (0%)] Loss: -1814.902344\n",
      "Train Epoch: 722 [11264/54000 (21%)] Loss: -1819.816772\n",
      "Train Epoch: 722 [22528/54000 (42%)] Loss: -1814.193115\n",
      "Train Epoch: 722 [33792/54000 (63%)] Loss: -1812.971924\n",
      "Train Epoch: 722 [45056/54000 (83%)] Loss: -1800.195923\n",
      "    epoch          : 722\n",
      "    loss           : -1811.1425044221698\n",
      "    ess            : 8.001181386551767\n",
      "    log_marginal   : 1811.1425044221698\n",
      "    val_loss       : -1815.1537984212239\n",
      "    val_ess        : 8.001181681950888\n",
      "    val_log_marginal: 1815.15380859375\n",
      "Train Epoch: 723 [0/54000 (0%)] Loss: -1816.296875\n",
      "Train Epoch: 723 [11264/54000 (21%)] Loss: -1817.067627\n",
      "Train Epoch: 723 [22528/54000 (42%)] Loss: -1785.859985\n",
      "Train Epoch: 723 [33792/54000 (63%)] Loss: -1790.346191\n",
      "Train Epoch: 723 [45056/54000 (83%)] Loss: -1780.230469\n",
      "    epoch          : 723\n",
      "    loss           : -1799.2324932746167\n",
      "    ess            : 8.001183122958777\n",
      "    log_marginal   : 1799.2324932746167\n",
      "    val_loss       : -1791.102071126302\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1791.102071126302\n",
      "Train Epoch: 724 [0/54000 (0%)] Loss: -1801.693115\n",
      "Train Epoch: 724 [11264/54000 (21%)] Loss: -1804.901978\n",
      "Train Epoch: 724 [22528/54000 (42%)] Loss: -1809.172607\n",
      "Train Epoch: 724 [33792/54000 (63%)] Loss: -1809.585938\n",
      "Train Epoch: 724 [45056/54000 (83%)] Loss: -1798.101562\n",
      "    epoch          : 724\n",
      "    loss           : -1802.3750690964032\n",
      "    ess            : 8.001183923685327\n",
      "    log_marginal   : 1802.3750690964032\n",
      "    val_loss       : -1792.1767883300781\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1792.1767883300781\n",
      "Train Epoch: 725 [0/54000 (0%)] Loss: -1804.363037\n",
      "Train Epoch: 725 [11264/54000 (21%)] Loss: -1797.472168\n",
      "Train Epoch: 725 [22528/54000 (42%)] Loss: -1790.959717\n",
      "Train Epoch: 725 [33792/54000 (63%)] Loss: -1791.120117\n",
      "Train Epoch: 725 [45056/54000 (83%)] Loss: -1789.400513\n",
      "    epoch          : 725\n",
      "    loss           : -1797.9087685639004\n",
      "    ess            : 8.001183896694544\n",
      "    log_marginal   : 1797.9087685639004\n",
      "    val_loss       : -1806.3574930826824\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1806.3574930826824\n",
      "Train Epoch: 726 [0/54000 (0%)] Loss: -1806.943237\n",
      "Train Epoch: 726 [11264/54000 (21%)] Loss: -1806.621338\n",
      "Train Epoch: 726 [22528/54000 (42%)] Loss: -1800.997070\n",
      "Train Epoch: 726 [33792/54000 (63%)] Loss: -1803.496704\n",
      "Train Epoch: 726 [45056/54000 (83%)] Loss: -1802.547729\n",
      "    epoch          : 726\n",
      "    loss           : -1799.6668033239976\n",
      "    ess            : 8.001182844054025\n",
      "    log_marginal   : 1799.6668033239976\n",
      "    val_loss       : -1798.381591796875\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1798.381591796875\n",
      "Train Epoch: 727 [0/54000 (0%)] Loss: -1815.837646\n",
      "Train Epoch: 727 [11264/54000 (21%)] Loss: -1808.200195\n",
      "Train Epoch: 727 [22528/54000 (42%)] Loss: -1802.326172\n",
      "Train Epoch: 727 [33792/54000 (63%)] Loss: -1791.695435\n",
      "Train Epoch: 727 [45056/54000 (83%)] Loss: -1809.178833\n",
      "    epoch          : 727\n",
      "    loss           : -1805.3930214935879\n",
      "    ess            : 8.001181818404287\n",
      "    log_marginal   : 1805.3930214935879\n",
      "    val_loss       : -1803.3162027994792\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1803.3162027994792\n",
      "Train Epoch: 728 [0/54000 (0%)] Loss: -1827.623291\n",
      "Train Epoch: 728 [11264/54000 (21%)] Loss: -1810.566406\n",
      "Train Epoch: 728 [22528/54000 (42%)] Loss: -1812.711548\n",
      "Train Epoch: 728 [33792/54000 (63%)] Loss: -1803.770508\n",
      "Train Epoch: 728 [45056/54000 (83%)] Loss: -1800.923584\n",
      "    epoch          : 728\n",
      "    loss           : -1809.4257351857311\n",
      "    ess            : 8.001182124299824\n",
      "    log_marginal   : 1809.4257363373379\n",
      "    val_loss       : -1807.4568176269531\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1807.4568176269531\n",
      "Train Epoch: 729 [0/54000 (0%)] Loss: -1817.995605\n",
      "Train Epoch: 729 [11264/54000 (21%)] Loss: -1807.838257\n",
      "Train Epoch: 729 [22528/54000 (42%)] Loss: -1796.383789\n",
      "Train Epoch: 729 [33792/54000 (63%)] Loss: -1801.955322\n",
      "Train Epoch: 729 [45056/54000 (83%)] Loss: -1796.198730\n",
      "    epoch          : 729\n",
      "    loss           : -1803.9349941037735\n",
      "    ess            : 8.00118191737049\n",
      "    log_marginal   : 1803.9349941037735\n",
      "    val_loss       : -1807.5883178710938\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1807.5882975260417\n",
      "Train Epoch: 730 [0/54000 (0%)] Loss: -1815.674072\n",
      "Train Epoch: 730 [11264/54000 (21%)] Loss: -1817.457764\n",
      "Train Epoch: 730 [22528/54000 (42%)] Loss: -1809.684326\n",
      "Train Epoch: 730 [33792/54000 (63%)] Loss: -1789.190186\n",
      "Train Epoch: 730 [45056/54000 (83%)] Loss: -1802.261719\n",
      "    epoch          : 730\n",
      "    loss           : -1807.4356182746167\n",
      "    ess            : 8.001181971352056\n",
      "    log_marginal   : 1807.4356182746167\n",
      "    val_loss       : -1808.9961649576824\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1808.9961649576824\n",
      "Train Epoch: 731 [0/54000 (0%)] Loss: -1810.705078\n",
      "Train Epoch: 731 [11264/54000 (21%)] Loss: -1815.657227\n",
      "Train Epoch: 731 [22528/54000 (42%)] Loss: -1811.915283\n",
      "Train Epoch: 731 [33792/54000 (63%)] Loss: -1806.007080\n",
      "Train Epoch: 731 [45056/54000 (83%)] Loss: -1806.910645\n",
      "    epoch          : 731\n",
      "    loss           : -1808.5381032116009\n",
      "    ess            : 8.00118340186353\n",
      "    log_marginal   : 1808.5381032116009\n",
      "    val_loss       : -1814.583740234375\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1814.583740234375\n",
      "Train Epoch: 732 [0/54000 (0%)] Loss: -1815.816650\n",
      "Train Epoch: 732 [11264/54000 (21%)] Loss: -1805.770752\n",
      "Train Epoch: 732 [22528/54000 (42%)] Loss: -1811.961792\n",
      "Train Epoch: 732 [33792/54000 (63%)] Loss: -1797.810303\n",
      "Train Epoch: 732 [45056/54000 (83%)] Loss: -1806.745605\n",
      "    epoch          : 732\n",
      "    loss           : -1805.5090136258107\n",
      "    ess            : 8.001181827401215\n",
      "    log_marginal   : 1805.5090136258107\n",
      "    val_loss       : -1803.9912719726562\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1803.9912719726562\n",
      "Train Epoch: 733 [0/54000 (0%)] Loss: -1809.457886\n",
      "Train Epoch: 733 [11264/54000 (21%)] Loss: -1808.605225\n",
      "Train Epoch: 733 [22528/54000 (42%)] Loss: -1807.954712\n",
      "Train Epoch: 733 [33792/54000 (63%)] Loss: -1812.361450\n",
      "Train Epoch: 733 [45056/54000 (83%)] Loss: -1806.027588\n",
      "    epoch          : 733\n",
      "    loss           : -1810.7320268738945\n",
      "    ess            : 8.001180513849798\n",
      "    log_marginal   : 1810.7320268738945\n",
      "    val_loss       : -1811.0133972167969\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1811.0133972167969\n",
      "Train Epoch: 734 [0/54000 (0%)] Loss: -1825.747314\n",
      "Train Epoch: 734 [11264/54000 (21%)] Loss: -1804.548706\n",
      "Train Epoch: 734 [22528/54000 (42%)] Loss: -1815.136719\n",
      "Train Epoch: 734 [33792/54000 (63%)] Loss: -1797.327393\n",
      "Train Epoch: 734 [45056/54000 (83%)] Loss: -1803.607666\n",
      "    epoch          : 734\n",
      "    loss           : -1809.1578403688827\n",
      "    ess            : 8.00118058582522\n",
      "    log_marginal   : 1809.1578403688827\n",
      "    val_loss       : -1809.3218078613281\n",
      "    val_ess        : 8.001183986663818\n",
      "    val_log_marginal: 1809.3218078613281\n",
      "Train Epoch: 735 [0/54000 (0%)] Loss: -1815.916870\n",
      "Train Epoch: 735 [11264/54000 (21%)] Loss: -1803.289307\n",
      "Train Epoch: 735 [22528/54000 (42%)] Loss: -1801.614258\n",
      "Train Epoch: 735 [33792/54000 (63%)] Loss: -1817.771851\n",
      "Train Epoch: 735 [45056/54000 (83%)] Loss: -1814.900391\n",
      "    epoch          : 735\n",
      "    loss           : -1806.9776473135319\n",
      "    ess            : 8.001181746428868\n",
      "    log_marginal   : 1806.9776473135319\n",
      "    val_loss       : -1806.959452311198\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1806.959452311198\n",
      "Train Epoch: 736 [0/54000 (0%)] Loss: -1829.257812\n",
      "Train Epoch: 736 [11264/54000 (21%)] Loss: -1807.439453\n",
      "Train Epoch: 736 [22528/54000 (42%)] Loss: -1804.118652\n",
      "Train Epoch: 736 [33792/54000 (63%)] Loss: -1802.710693\n",
      "Train Epoch: 736 [45056/54000 (83%)] Loss: -1806.411743\n",
      "    epoch          : 736\n",
      "    loss           : -1806.2658967791863\n",
      "    ess            : 8.001181782416577\n",
      "    log_marginal   : 1806.2658956275795\n",
      "    val_loss       : -1807.5985310872395\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1807.5985310872395\n",
      "Train Epoch: 737 [0/54000 (0%)] Loss: -1816.999756\n",
      "Train Epoch: 737 [11264/54000 (21%)] Loss: -1816.279297\n",
      "Train Epoch: 737 [22528/54000 (42%)] Loss: -1816.236450\n",
      "Train Epoch: 737 [33792/54000 (63%)] Loss: -1816.381104\n",
      "Train Epoch: 737 [45056/54000 (83%)] Loss: -1813.588623\n",
      "    epoch          : 737\n",
      "    loss           : -1811.592487839033\n",
      "    ess            : 8.001180783757624\n",
      "    log_marginal   : 1811.592487839033\n",
      "    val_loss       : -1816.2903238932292\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1816.2903340657551\n",
      "Train Epoch: 738 [0/54000 (0%)] Loss: -1821.061035\n",
      "Train Epoch: 738 [11264/54000 (21%)] Loss: -1798.722290\n",
      "Train Epoch: 738 [22528/54000 (42%)] Loss: -1787.196777\n",
      "Train Epoch: 738 [33792/54000 (63%)] Loss: -1795.796143\n",
      "Train Epoch: 738 [45056/54000 (83%)] Loss: -1788.796509\n",
      "    epoch          : 738\n",
      "    loss           : -1794.442684533461\n",
      "    ess            : 8.001182736090893\n",
      "    log_marginal   : 1794.442684533461\n",
      "    val_loss       : -1791.8982747395833\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1791.8982645670574\n",
      "Train Epoch: 739 [0/54000 (0%)] Loss: -1798.142822\n",
      "Train Epoch: 739 [11264/54000 (21%)] Loss: -1804.559570\n",
      "Train Epoch: 739 [22528/54000 (42%)] Loss: -1806.165039\n",
      "Train Epoch: 739 [33792/54000 (63%)] Loss: -1808.445801\n",
      "Train Epoch: 739 [45056/54000 (83%)] Loss: -1812.495483\n",
      "    epoch          : 739\n",
      "    loss           : -1803.9685219818691\n",
      "    ess            : 8.001180666797566\n",
      "    log_marginal   : 1803.9685219818691\n",
      "    val_loss       : -1800.9407552083333\n",
      "    val_ess        : 8.001182874043783\n",
      "    val_log_marginal: 1800.9407552083333\n",
      "Train Epoch: 740 [0/54000 (0%)] Loss: -1813.834229\n",
      "Train Epoch: 740 [11264/54000 (21%)] Loss: -1819.573975\n",
      "Train Epoch: 740 [22528/54000 (42%)] Loss: -1806.439697\n",
      "Train Epoch: 740 [33792/54000 (63%)] Loss: -1800.126709\n",
      "Train Epoch: 740 [45056/54000 (83%)] Loss: -1809.760498\n",
      "    epoch          : 740\n",
      "    loss           : -1808.3407016970077\n",
      "    ess            : 8.001180702785277\n",
      "    log_marginal   : 1808.3407016970077\n",
      "    val_loss       : -1808.7406819661458\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1808.7406819661458\n",
      "Train Epoch: 741 [0/54000 (0%)] Loss: -1815.654053\n",
      "Train Epoch: 741 [11264/54000 (21%)] Loss: -1798.210205\n",
      "Train Epoch: 741 [22528/54000 (42%)] Loss: -1793.389526\n",
      "Train Epoch: 741 [33792/54000 (63%)] Loss: -1798.789062\n",
      "Train Epoch: 741 [45056/54000 (83%)] Loss: -1795.285400\n",
      "    epoch          : 741\n",
      "    loss           : -1797.985413749263\n",
      "    ess            : 8.001182268250663\n",
      "    log_marginal   : 1797.985413749263\n",
      "    val_loss       : -1802.5103251139324\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1802.5103251139324\n",
      "Train Epoch: 742 [0/54000 (0%)] Loss: -1814.446899\n",
      "Train Epoch: 742 [11264/54000 (21%)] Loss: -1806.620850\n",
      "Train Epoch: 742 [22528/54000 (42%)] Loss: -1805.837891\n",
      "Train Epoch: 742 [33792/54000 (63%)] Loss: -1801.428711\n",
      "Train Epoch: 742 [45056/54000 (83%)] Loss: -1803.158691\n",
      "    epoch          : 742\n",
      "    loss           : -1804.9464928968898\n",
      "    ess            : 8.001181431536404\n",
      "    log_marginal   : 1804.9464940484966\n",
      "    val_loss       : -1801.1304931640625\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1801.1304931640625\n",
      "Train Epoch: 743 [0/54000 (0%)] Loss: -1816.282227\n",
      "Train Epoch: 743 [11264/54000 (21%)] Loss: -1819.869873\n",
      "Train Epoch: 743 [22528/54000 (42%)] Loss: -1812.170166\n",
      "Train Epoch: 743 [33792/54000 (63%)] Loss: -1799.083374\n",
      "Train Epoch: 743 [45056/54000 (83%)] Loss: -1801.260254\n",
      "    epoch          : 743\n",
      "    loss           : -1808.3897371112175\n",
      "    ess            : 8.00118227724759\n",
      "    log_marginal   : 1808.3897359596108\n",
      "    val_loss       : -1810.7164611816406\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1810.7164611816406\n",
      "Train Epoch: 744 [0/54000 (0%)] Loss: -1811.496216\n",
      "Train Epoch: 744 [11264/54000 (21%)] Loss: -1799.235840\n",
      "Train Epoch: 744 [22528/54000 (42%)] Loss: -1802.733276\n",
      "Train Epoch: 744 [33792/54000 (63%)] Loss: -1796.294922\n",
      "Train Epoch: 744 [45056/54000 (83%)] Loss: -1802.559814\n",
      "    epoch          : 744\n",
      "    loss           : -1800.5028502266362\n",
      "    ess            : 8.00118261013391\n",
      "    log_marginal   : 1800.502851378243\n",
      "    val_loss       : -1806.568094889323\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1806.5681254069011\n",
      "Train Epoch: 745 [0/54000 (0%)] Loss: -1820.382080\n",
      "Train Epoch: 745 [11264/54000 (21%)] Loss: -1820.125000\n",
      "Train Epoch: 745 [22528/54000 (42%)] Loss: -1801.878662\n",
      "Train Epoch: 745 [33792/54000 (63%)] Loss: -1802.760132\n",
      "Train Epoch: 745 [45056/54000 (83%)] Loss: -1806.703613\n",
      "    epoch          : 745\n",
      "    loss           : -1808.7511884581368\n",
      "    ess            : 8.001181512508753\n",
      "    log_marginal   : 1808.7511884581368\n",
      "    val_loss       : -1806.9003499348958\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1806.9003397623699\n",
      "Train Epoch: 746 [0/54000 (0%)] Loss: -1823.879639\n",
      "Train Epoch: 746 [11264/54000 (21%)] Loss: -1815.042725\n",
      "Train Epoch: 746 [22528/54000 (42%)] Loss: -1818.635742\n",
      "Train Epoch: 746 [33792/54000 (63%)] Loss: -1819.280640\n",
      "Train Epoch: 746 [45056/54000 (83%)] Loss: -1806.562256\n",
      "    epoch          : 746\n",
      "    loss           : -1812.9983336250737\n",
      "    ess            : 8.00117960516012\n",
      "    log_marginal   : 1812.9983347766804\n",
      "    val_loss       : -1814.5153706868489\n",
      "    val_ess        : 8.001180489857992\n",
      "    val_log_marginal: 1814.5153706868489\n",
      "Train Epoch: 747 [0/54000 (0%)] Loss: -1821.179932\n",
      "Train Epoch: 747 [11264/54000 (21%)] Loss: -1803.497559\n",
      "Train Epoch: 747 [22528/54000 (42%)] Loss: -1801.061279\n",
      "Train Epoch: 747 [33792/54000 (63%)] Loss: -1802.165649\n",
      "Train Epoch: 747 [45056/54000 (83%)] Loss: -1805.790161\n",
      "    epoch          : 747\n",
      "    loss           : -1800.0341900519604\n",
      "    ess            : 8.001179983031076\n",
      "    log_marginal   : 1800.0341900519604\n",
      "    val_loss       : -1796.951171875\n",
      "    val_ess        : 8.001181523005167\n",
      "    val_log_marginal: 1796.9511820475261\n",
      "Train Epoch: 748 [0/54000 (0%)] Loss: -1800.416748\n",
      "Train Epoch: 748 [11264/54000 (21%)] Loss: -1818.794312\n",
      "Train Epoch: 748 [22528/54000 (42%)] Loss: -1803.854736\n",
      "Train Epoch: 748 [33792/54000 (63%)] Loss: -1809.958252\n",
      "Train Epoch: 748 [45056/54000 (83%)] Loss: -1803.293701\n",
      "    epoch          : 748\n",
      "    loss           : -1805.6420990566037\n",
      "    ess            : 8.001180684791422\n",
      "    log_marginal   : 1805.6421002082104\n",
      "    val_loss       : -1810.4835205078125\n",
      "    val_ess        : 8.001180013020834\n",
      "    val_log_marginal: 1810.4835205078125\n",
      "Train Epoch: 749 [0/54000 (0%)] Loss: -1814.073975\n",
      "Train Epoch: 749 [11264/54000 (21%)] Loss: -1806.703369\n",
      "Train Epoch: 749 [22528/54000 (42%)] Loss: -1797.861450\n",
      "Train Epoch: 749 [33792/54000 (63%)] Loss: -1801.931152\n",
      "Train Epoch: 749 [45056/54000 (83%)] Loss: -1803.894897\n",
      "    epoch          : 749\n",
      "    loss           : -1806.5032728663032\n",
      "    ess            : 8.001180459868234\n",
      "    log_marginal   : 1806.5032728663032\n",
      "    val_loss       : -1811.1575520833333\n",
      "    val_ess        : 8.001181920369467\n",
      "    val_log_marginal: 1811.1575419108074\n",
      "Train Epoch: 750 [0/54000 (0%)] Loss: -1813.585327\n",
      "Train Epoch: 750 [11264/54000 (21%)] Loss: -1819.806396\n",
      "Train Epoch: 750 [22528/54000 (42%)] Loss: -1794.287842\n",
      "Train Epoch: 750 [33792/54000 (63%)] Loss: -1802.719482\n",
      "Train Epoch: 750 [45056/54000 (83%)] Loss: -1795.059082\n",
      "    epoch          : 750\n",
      "    loss           : -1806.2927971605984\n",
      "    ess            : 8.001180324914321\n",
      "    log_marginal   : 1806.2927971605984\n",
      "    val_loss       : -1806.8819885253906\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1806.8819885253906\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch750.pth ...\n",
      "Train Epoch: 751 [0/54000 (0%)] Loss: -1812.037598\n",
      "Train Epoch: 751 [11264/54000 (21%)] Loss: -1750.910889\n",
      "Train Epoch: 751 [22528/54000 (42%)] Loss: -1773.916504\n",
      "Train Epoch: 751 [33792/54000 (63%)] Loss: -1762.493164\n",
      "Train Epoch: 751 [45056/54000 (83%)] Loss: -1787.436768\n",
      "    epoch          : 751\n",
      "    loss           : -1782.2190666918484\n",
      "    ess            : 8.001182961014083\n",
      "    log_marginal   : 1782.2190666918484\n",
      "    val_loss       : -1804.4634602864583\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1804.4634602864583\n",
      "Train Epoch: 752 [0/54000 (0%)] Loss: -1810.541016\n",
      "Train Epoch: 752 [11264/54000 (21%)] Loss: -1798.096436\n",
      "Train Epoch: 752 [22528/54000 (42%)] Loss: -1803.204102\n",
      "Train Epoch: 752 [33792/54000 (63%)] Loss: -1798.553955\n",
      "Train Epoch: 752 [45056/54000 (83%)] Loss: -1795.580933\n",
      "    epoch          : 752\n",
      "    loss           : -1799.6182872844192\n",
      "    ess            : 8.001183203931125\n",
      "    log_marginal   : 1799.6182872844192\n",
      "    val_loss       : -1810.4915364583333\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1810.4915364583333\n",
      "Train Epoch: 753 [0/54000 (0%)] Loss: -1809.155884\n",
      "Train Epoch: 753 [11264/54000 (21%)] Loss: -1814.695557\n",
      "Train Epoch: 753 [22528/54000 (42%)] Loss: -1818.906250\n",
      "Train Epoch: 753 [33792/54000 (63%)] Loss: -1807.523682\n",
      "Train Epoch: 753 [45056/54000 (83%)] Loss: -1806.132935\n",
      "    epoch          : 753\n",
      "    loss           : -1812.0445879090507\n",
      "    ess            : 8.001181746428868\n",
      "    log_marginal   : 1812.0445879090507\n",
      "    val_loss       : -1806.1603597005208\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1806.1603597005208\n",
      "Train Epoch: 754 [0/54000 (0%)] Loss: -1815.940918\n",
      "Train Epoch: 754 [11264/54000 (21%)] Loss: -1799.559082\n",
      "Train Epoch: 754 [22528/54000 (42%)] Loss: -1795.905518\n",
      "Train Epoch: 754 [33792/54000 (63%)] Loss: -1800.776245\n",
      "Train Epoch: 754 [45056/54000 (83%)] Loss: -1809.745361\n",
      "    epoch          : 754\n",
      "    loss           : -1803.9792584113354\n",
      "    ess            : 8.001182880041734\n",
      "    log_marginal   : 1803.9792595629422\n",
      "    val_loss       : -1810.2163899739583\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1810.2164001464844\n",
      "Train Epoch: 755 [0/54000 (0%)] Loss: -1819.749756\n",
      "Train Epoch: 755 [11264/54000 (21%)] Loss: -1812.571289\n",
      "Train Epoch: 755 [22528/54000 (42%)] Loss: -1800.404053\n",
      "Train Epoch: 755 [33792/54000 (63%)] Loss: -1788.168823\n",
      "Train Epoch: 755 [45056/54000 (83%)] Loss: -1801.150513\n",
      "    epoch          : 755\n",
      "    loss           : -1805.9382635152565\n",
      "    ess            : 8.00118202533362\n",
      "    log_marginal   : 1805.9382635152565\n",
      "    val_loss       : -1802.7754720052083\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1802.7754720052083\n",
      "Train Epoch: 756 [0/54000 (0%)] Loss: -1813.976807\n",
      "Train Epoch: 756 [11264/54000 (21%)] Loss: -1815.060791\n",
      "Train Epoch: 756 [22528/54000 (42%)] Loss: -1815.767700\n",
      "Train Epoch: 756 [33792/54000 (63%)] Loss: -1813.066650\n",
      "Train Epoch: 756 [45056/54000 (83%)] Loss: -1807.438477\n",
      "    epoch          : 756\n",
      "    loss           : -1811.064668475457\n",
      "    ess            : 8.00118184539507\n",
      "    log_marginal   : 1811.0646696270637\n",
      "    val_loss       : -1808.920654296875\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1808.920654296875\n",
      "Train Epoch: 757 [0/54000 (0%)] Loss: -1812.923584\n",
      "Train Epoch: 757 [11264/54000 (21%)] Loss: -1779.493896\n",
      "Train Epoch: 757 [22528/54000 (42%)] Loss: -1809.979492\n",
      "Train Epoch: 757 [33792/54000 (63%)] Loss: -1797.863281\n",
      "Train Epoch: 757 [45056/54000 (83%)] Loss: -1809.190186\n",
      "    epoch          : 757\n",
      "    loss           : -1800.0810385650059\n",
      "    ess            : 8.001181395548695\n",
      "    log_marginal   : 1800.0810385650059\n",
      "    val_loss       : -1808.2310384114583\n",
      "    val_ess        : 8.001182874043783\n",
      "    val_log_marginal: 1808.2310384114583\n",
      "Train Epoch: 758 [0/54000 (0%)] Loss: -1813.445557\n",
      "Train Epoch: 758 [11264/54000 (21%)] Loss: -1799.393677\n",
      "Train Epoch: 758 [22528/54000 (42%)] Loss: -1808.431396\n",
      "Train Epoch: 758 [33792/54000 (63%)] Loss: -1802.034180\n",
      "Train Epoch: 758 [45056/54000 (83%)] Loss: -1807.266357\n",
      "    epoch          : 758\n",
      "    loss           : -1806.3329260484227\n",
      "    ess            : 8.001181872385853\n",
      "    log_marginal   : 1806.3329283516362\n",
      "    val_loss       : -1806.6215209960938\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1806.6215209960938\n",
      "Train Epoch: 759 [0/54000 (0%)] Loss: -1824.516846\n",
      "Train Epoch: 759 [11264/54000 (21%)] Loss: -1756.429443\n",
      "Train Epoch: 759 [22528/54000 (42%)] Loss: -1765.125244\n",
      "Train Epoch: 759 [33792/54000 (63%)] Loss: -1775.087891\n",
      "Train Epoch: 759 [45056/54000 (83%)] Loss: -1788.964600\n",
      "    epoch          : 759\n",
      "    loss           : -1776.9561940319134\n",
      "    ess            : 8.001184211587006\n",
      "    log_marginal   : 1776.9561940319134\n",
      "    val_loss       : -1801.4670715332031\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1801.4670715332031\n",
      "Train Epoch: 760 [0/54000 (0%)] Loss: -1801.052246\n",
      "Train Epoch: 760 [11264/54000 (21%)] Loss: -1802.023193\n",
      "Train Epoch: 760 [22528/54000 (42%)] Loss: -1798.020752\n",
      "Train Epoch: 760 [33792/54000 (63%)] Loss: -1793.859863\n",
      "Train Epoch: 760 [45056/54000 (83%)] Loss: -1804.632324\n",
      "    epoch          : 760\n",
      "    loss           : -1793.6544189453125\n",
      "    ess            : 8.001183068977213\n",
      "    log_marginal   : 1793.6544189453125\n",
      "    val_loss       : -1810.203369140625\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1810.203369140625\n",
      "Train Epoch: 761 [0/54000 (0%)] Loss: -1813.431519\n",
      "Train Epoch: 761 [11264/54000 (21%)] Loss: -1818.697510\n",
      "Train Epoch: 761 [22528/54000 (42%)] Loss: -1798.451782\n",
      "Train Epoch: 761 [33792/54000 (63%)] Loss: -1809.063110\n",
      "Train Epoch: 761 [45056/54000 (83%)] Loss: -1802.060059\n",
      "    epoch          : 761\n",
      "    loss           : -1806.792207537957\n",
      "    ess            : 8.001183140952632\n",
      "    log_marginal   : 1806.7922063863502\n",
      "    val_loss       : -1809.3339233398438\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1809.3339233398438\n",
      "Train Epoch: 762 [0/54000 (0%)] Loss: -1815.490234\n",
      "Train Epoch: 762 [11264/54000 (21%)] Loss: -1801.798096\n",
      "Train Epoch: 762 [22528/54000 (42%)] Loss: -1801.146362\n",
      "Train Epoch: 762 [33792/54000 (63%)] Loss: -1798.492920\n",
      "Train Epoch: 762 [45056/54000 (83%)] Loss: -1787.914551\n",
      "    epoch          : 762\n",
      "    loss           : -1801.5354763966686\n",
      "    ess            : 8.001183968669963\n",
      "    log_marginal   : 1801.5354763966686\n",
      "    val_loss       : -1807.4823506673176\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1807.4823404947917\n",
      "Train Epoch: 763 [0/54000 (0%)] Loss: -1801.360840\n",
      "Train Epoch: 763 [11264/54000 (21%)] Loss: -1805.471191\n",
      "Train Epoch: 763 [22528/54000 (42%)] Loss: -1794.962158\n",
      "Train Epoch: 763 [33792/54000 (63%)] Loss: -1791.180298\n",
      "Train Epoch: 763 [45056/54000 (83%)] Loss: -1804.545044\n",
      "    epoch          : 763\n",
      "    loss           : -1798.915452489313\n",
      "    ess            : 8.001182880041734\n",
      "    log_marginal   : 1798.915452489313\n",
      "    val_loss       : -1801.2790222167969\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1801.279032389323\n",
      "Train Epoch: 764 [0/54000 (0%)] Loss: -1816.217651\n",
      "Train Epoch: 764 [11264/54000 (21%)] Loss: -1813.080078\n",
      "Train Epoch: 764 [22528/54000 (42%)] Loss: -1804.580811\n",
      "Train Epoch: 764 [33792/54000 (63%)] Loss: -1798.114502\n",
      "Train Epoch: 764 [45056/54000 (83%)] Loss: -1805.746948\n",
      "    epoch          : 764\n",
      "    loss           : -1804.6231413067512\n",
      "    ess            : 8.0011829340233\n",
      "    log_marginal   : 1804.6231413067512\n",
      "    val_loss       : -1806.4536844889324\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1806.4537048339844\n",
      "Train Epoch: 765 [0/54000 (0%)] Loss: -1818.991211\n",
      "Train Epoch: 765 [11264/54000 (21%)] Loss: -1807.114258\n",
      "Train Epoch: 765 [22528/54000 (42%)] Loss: -1801.242188\n",
      "Train Epoch: 765 [33792/54000 (63%)] Loss: -1789.889160\n",
      "Train Epoch: 765 [45056/54000 (83%)] Loss: -1799.437988\n",
      "    epoch          : 765\n",
      "    loss           : -1804.7783456478478\n",
      "    ess            : 8.001182808066314\n",
      "    log_marginal   : 1804.7783456478478\n",
      "    val_loss       : -1808.0442911783855\n",
      "    val_ess        : 8.001181920369467\n",
      "    val_log_marginal: 1808.0442911783855\n",
      "Train Epoch: 766 [0/54000 (0%)] Loss: -1804.007080\n",
      "Train Epoch: 766 [11264/54000 (21%)] Loss: -1806.664673\n",
      "Train Epoch: 766 [22528/54000 (42%)] Loss: -1794.757446\n",
      "Train Epoch: 766 [33792/54000 (63%)] Loss: -1797.220825\n",
      "Train Epoch: 766 [45056/54000 (83%)] Loss: -1792.132324\n",
      "    epoch          : 766\n",
      "    loss           : -1802.2444031913326\n",
      "    ess            : 8.001182889038661\n",
      "    log_marginal   : 1802.2444031913326\n",
      "    val_loss       : -1808.4532674153645\n",
      "    val_ess        : 8.001182953516642\n",
      "    val_log_marginal: 1808.4532674153645\n",
      "Train Epoch: 767 [0/54000 (0%)] Loss: -1818.777832\n",
      "Train Epoch: 767 [11264/54000 (21%)] Loss: -1813.703735\n",
      "Train Epoch: 767 [22528/54000 (42%)] Loss: -1817.395752\n",
      "Train Epoch: 767 [33792/54000 (63%)] Loss: -1797.268921\n",
      "Train Epoch: 767 [45056/54000 (83%)] Loss: -1790.298828\n",
      "    epoch          : 767\n",
      "    loss           : -1803.9971221348026\n",
      "    ess            : 8.001182322232228\n",
      "    log_marginal   : 1803.9971232864093\n",
      "    val_loss       : -1803.8861897786458\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1803.8861796061199\n",
      "Train Epoch: 768 [0/54000 (0%)] Loss: -1820.405762\n",
      "Train Epoch: 768 [11264/54000 (21%)] Loss: -1814.421387\n",
      "Train Epoch: 768 [22528/54000 (42%)] Loss: -1815.634033\n",
      "Train Epoch: 768 [33792/54000 (63%)] Loss: -1815.157959\n",
      "Train Epoch: 768 [45056/54000 (83%)] Loss: -1805.530029\n",
      "    epoch          : 768\n",
      "    loss           : -1811.7722939545254\n",
      "    ess            : 8.001181620471883\n",
      "    log_marginal   : 1811.7722939545254\n",
      "    val_loss       : -1811.2628885904949\n",
      "    val_ess        : 8.00118112564087\n",
      "    val_log_marginal: 1811.2628885904949\n",
      "Train Epoch: 769 [0/54000 (0%)] Loss: -1830.964355\n",
      "Train Epoch: 769 [11264/54000 (21%)] Loss: -1802.883057\n",
      "Train Epoch: 769 [22528/54000 (42%)] Loss: -1794.807251\n",
      "Train Epoch: 769 [33792/54000 (63%)] Loss: -1817.816040\n",
      "Train Epoch: 769 [45056/54000 (83%)] Loss: -1801.673584\n",
      "    epoch          : 769\n",
      "    loss           : -1806.0173121038472\n",
      "    ess            : 8.001181512508753\n",
      "    log_marginal   : 1806.0173121038472\n",
      "    val_loss       : -1804.5066019694011\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1804.506591796875\n",
      "Train Epoch: 770 [0/54000 (0%)] Loss: -1813.949707\n",
      "Train Epoch: 770 [11264/54000 (21%)] Loss: -1809.887451\n",
      "Train Epoch: 770 [22528/54000 (42%)] Loss: -1797.111938\n",
      "Train Epoch: 770 [33792/54000 (63%)] Loss: -1802.771240\n",
      "Train Epoch: 770 [45056/54000 (83%)] Loss: -1791.004761\n",
      "    epoch          : 770\n",
      "    loss           : -1803.5344307377654\n",
      "    ess            : 8.00118297001101\n",
      "    log_marginal   : 1803.5344295861587\n",
      "    val_loss       : -1810.6477661132812\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1810.6477661132812\n",
      "Train Epoch: 771 [0/54000 (0%)] Loss: -1819.175903\n",
      "Train Epoch: 771 [11264/54000 (21%)] Loss: -1804.924561\n",
      "Train Epoch: 771 [22528/54000 (42%)] Loss: -1799.944824\n",
      "Train Epoch: 771 [33792/54000 (63%)] Loss: -1793.944580\n",
      "Train Epoch: 771 [45056/54000 (83%)] Loss: -1808.469727\n",
      "    epoch          : 771\n",
      "    loss           : -1802.0821775040536\n",
      "    ess            : 8.001181512508753\n",
      "    log_marginal   : 1802.0821775040536\n",
      "    val_loss       : -1803.5677286783855\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1803.5677286783855\n",
      "Train Epoch: 772 [0/54000 (0%)] Loss: -1814.347290\n",
      "Train Epoch: 772 [11264/54000 (21%)] Loss: -1824.135498\n",
      "Train Epoch: 772 [22528/54000 (42%)] Loss: -1795.693359\n",
      "Train Epoch: 772 [33792/54000 (63%)] Loss: -1811.945801\n",
      "Train Epoch: 772 [45056/54000 (83%)] Loss: -1804.626587\n",
      "    epoch          : 772\n",
      "    loss           : -1808.135423192438\n",
      "    ess            : 8.001182556152344\n",
      "    log_marginal   : 1808.135423192438\n",
      "    val_loss       : -1813.6079813639324\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1813.6079813639324\n",
      "Train Epoch: 773 [0/54000 (0%)] Loss: -1823.807373\n",
      "Train Epoch: 773 [11264/54000 (21%)] Loss: -1814.617310\n",
      "Train Epoch: 773 [22528/54000 (42%)] Loss: -1760.421997\n",
      "Train Epoch: 773 [33792/54000 (63%)] Loss: -1765.660400\n",
      "Train Epoch: 773 [45056/54000 (83%)] Loss: -1776.707031\n",
      "    epoch          : 773\n",
      "    loss           : -1783.1902880398732\n",
      "    ess            : 8.001182997001791\n",
      "    log_marginal   : 1783.1902903430866\n",
      "    val_loss       : -1786.8743387858074\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1786.8743387858074\n",
      "Train Epoch: 774 [0/54000 (0%)] Loss: -1799.839600\n",
      "Train Epoch: 774 [11264/54000 (21%)] Loss: -1795.359131\n",
      "Train Epoch: 774 [22528/54000 (42%)] Loss: -1784.842163\n",
      "Train Epoch: 774 [33792/54000 (63%)] Loss: -1794.657959\n",
      "Train Epoch: 774 [45056/54000 (83%)] Loss: -1800.192383\n",
      "    epoch          : 774\n",
      "    loss           : -1795.0983610333137\n",
      "    ess            : 8.001182538158488\n",
      "    log_marginal   : 1795.0983587301002\n",
      "    val_loss       : -1804.4020589192708\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1804.4020589192708\n",
      "Train Epoch: 775 [0/54000 (0%)] Loss: -1818.179443\n",
      "Train Epoch: 775 [11264/54000 (21%)] Loss: -1821.297363\n",
      "Train Epoch: 775 [22528/54000 (42%)] Loss: -1806.572998\n",
      "Train Epoch: 775 [33792/54000 (63%)] Loss: -1795.223022\n",
      "Train Epoch: 775 [45056/54000 (83%)] Loss: -1805.856689\n",
      "    epoch          : 775\n",
      "    loss           : -1806.0013646539653\n",
      "    ess            : 8.00118227724759\n",
      "    log_marginal   : 1806.0013658055718\n",
      "    val_loss       : -1807.925537109375\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1807.925537109375\n",
      "Train Epoch: 776 [0/54000 (0%)] Loss: -1810.669434\n",
      "Train Epoch: 776 [11264/54000 (21%)] Loss: -1807.915527\n",
      "Train Epoch: 776 [22528/54000 (42%)] Loss: -1794.475220\n",
      "Train Epoch: 776 [33792/54000 (63%)] Loss: -1802.358643\n",
      "Train Epoch: 776 [45056/54000 (83%)] Loss: -1795.355713\n",
      "    epoch          : 776\n",
      "    loss           : -1802.0289421801297\n",
      "    ess            : 8.001183482835877\n",
      "    log_marginal   : 1802.0289421801297\n",
      "    val_loss       : -1796.9620463053386\n",
      "    val_ess        : 8.001182794570923\n",
      "    val_log_marginal: 1796.9620463053386\n",
      "Train Epoch: 777 [0/54000 (0%)] Loss: -1807.322388\n",
      "Train Epoch: 777 [11264/54000 (21%)] Loss: -1811.135498\n",
      "Train Epoch: 777 [22528/54000 (42%)] Loss: -1814.470947\n",
      "Train Epoch: 777 [33792/54000 (63%)] Loss: -1811.118652\n",
      "Train Epoch: 777 [45056/54000 (83%)] Loss: -1798.334473\n",
      "    epoch          : 777\n",
      "    loss           : -1805.7307520452534\n",
      "    ess            : 8.001181818404287\n",
      "    log_marginal   : 1805.7307520452534\n",
      "    val_loss       : -1805.8247680664062\n",
      "    val_ess        : 8.001182079315186\n",
      "    val_log_marginal: 1805.8247680664062\n",
      "Train Epoch: 778 [0/54000 (0%)] Loss: -1821.060791\n",
      "Train Epoch: 778 [11264/54000 (21%)] Loss: -1811.593018\n",
      "Train Epoch: 778 [22528/54000 (42%)] Loss: -1807.603271\n",
      "Train Epoch: 778 [33792/54000 (63%)] Loss: -1810.001953\n",
      "Train Epoch: 778 [45056/54000 (83%)] Loss: -1804.282471\n",
      "    epoch          : 778\n",
      "    loss           : -1806.2961563974056\n",
      "    ess            : 8.001182340226084\n",
      "    log_marginal   : 1806.2961575490124\n",
      "    val_loss       : -1802.4907938639324\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1802.4908040364583\n",
      "Train Epoch: 779 [0/54000 (0%)] Loss: -1821.359375\n",
      "Train Epoch: 779 [11264/54000 (21%)] Loss: -1804.654297\n",
      "Train Epoch: 779 [22528/54000 (42%)] Loss: -1809.393066\n",
      "Train Epoch: 779 [33792/54000 (63%)] Loss: -1805.510742\n",
      "Train Epoch: 779 [45056/54000 (83%)] Loss: -1796.473145\n",
      "    epoch          : 779\n",
      "    loss           : -1804.9626787293632\n",
      "    ess            : 8.001182448189214\n",
      "    log_marginal   : 1804.9626787293632\n",
      "    val_loss       : -1802.6595662434895\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1802.6595662434895\n",
      "Train Epoch: 780 [0/54000 (0%)] Loss: -1811.239990\n",
      "Train Epoch: 780 [11264/54000 (21%)] Loss: -1799.964233\n",
      "Train Epoch: 780 [22528/54000 (42%)] Loss: -1815.960083\n",
      "Train Epoch: 780 [33792/54000 (63%)] Loss: -1813.605713\n",
      "Train Epoch: 780 [45056/54000 (83%)] Loss: -1796.909302\n",
      "    epoch          : 780\n",
      "    loss           : -1807.0439913767689\n",
      "    ess            : 8.001182502170778\n",
      "    log_marginal   : 1807.0439913767689\n",
      "    val_loss       : -1811.5896809895833\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1811.5896809895833\n",
      "Train Epoch: 781 [0/54000 (0%)] Loss: -1827.071899\n",
      "Train Epoch: 781 [11264/54000 (21%)] Loss: -1821.404663\n",
      "Train Epoch: 781 [22528/54000 (42%)] Loss: -1801.298584\n",
      "Train Epoch: 781 [33792/54000 (63%)] Loss: -1807.681396\n",
      "Train Epoch: 781 [45056/54000 (83%)] Loss: -1807.287354\n",
      "    epoch          : 781\n",
      "    loss           : -1811.3903658884876\n",
      "    ess            : 8.001182439192286\n",
      "    log_marginal   : 1811.3903647368809\n",
      "    val_loss       : -1807.19384765625\n",
      "    val_ess        : 8.001184145609537\n",
      "    val_log_marginal: 1807.19384765625\n",
      "Train Epoch: 782 [0/54000 (0%)] Loss: -1816.572998\n",
      "Train Epoch: 782 [11264/54000 (21%)] Loss: -1813.176758\n",
      "Train Epoch: 782 [22528/54000 (42%)] Loss: -1804.239258\n",
      "Train Epoch: 782 [33792/54000 (63%)] Loss: -1808.417114\n",
      "Train Epoch: 782 [45056/54000 (83%)] Loss: -1805.509155\n",
      "    epoch          : 782\n",
      "    loss           : -1810.1399363391804\n",
      "    ess            : 8.001181566490317\n",
      "    log_marginal   : 1810.1399363391804\n",
      "    val_loss       : -1809.2935587565105\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1809.2935587565105\n",
      "Train Epoch: 783 [0/54000 (0%)] Loss: -1815.317871\n",
      "Train Epoch: 783 [11264/54000 (21%)] Loss: -1793.886353\n",
      "Train Epoch: 783 [22528/54000 (42%)] Loss: -1799.242676\n",
      "Train Epoch: 783 [33792/54000 (63%)] Loss: -1801.650146\n",
      "Train Epoch: 783 [45056/54000 (83%)] Loss: -1784.703613\n",
      "    epoch          : 783\n",
      "    loss           : -1796.507237848246\n",
      "    ess            : 8.00118261013391\n",
      "    log_marginal   : 1796.5072389998527\n",
      "    val_loss       : -1797.4374287923176\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1797.4374389648438\n",
      "Train Epoch: 784 [0/54000 (0%)] Loss: -1803.186279\n",
      "Train Epoch: 784 [11264/54000 (21%)] Loss: -1809.006104\n",
      "Train Epoch: 784 [22528/54000 (42%)] Loss: -1809.840088\n",
      "Train Epoch: 784 [33792/54000 (63%)] Loss: -1800.092773\n",
      "Train Epoch: 784 [45056/54000 (83%)] Loss: -1793.896729\n",
      "    epoch          : 784\n",
      "    loss           : -1804.8911443746315\n",
      "    ess            : 8.001181116643941\n",
      "    log_marginal   : 1804.8911443746315\n",
      "    val_loss       : -1809.9212849934895\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1809.9212849934895\n",
      "Train Epoch: 785 [0/54000 (0%)] Loss: -1811.502197\n",
      "Train Epoch: 785 [11264/54000 (21%)] Loss: -1805.677734\n",
      "Train Epoch: 785 [22528/54000 (42%)] Loss: -1781.474731\n",
      "Train Epoch: 785 [33792/54000 (63%)] Loss: -1766.832275\n",
      "Train Epoch: 785 [45056/54000 (83%)] Loss: -1780.104858\n",
      "    epoch          : 785\n",
      "    loss           : -1784.5861908534787\n",
      "    ess            : 8.001182583143127\n",
      "    log_marginal   : 1784.5861920050854\n",
      "    val_loss       : -1796.1785278320312\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1796.1785278320312\n",
      "Train Epoch: 786 [0/54000 (0%)] Loss: -1799.244141\n",
      "Train Epoch: 786 [11264/54000 (21%)] Loss: -1800.294922\n",
      "Train Epoch: 786 [22528/54000 (42%)] Loss: -1807.128662\n",
      "Train Epoch: 786 [33792/54000 (63%)] Loss: -1800.158813\n",
      "Train Epoch: 786 [45056/54000 (83%)] Loss: -1791.492188\n",
      "    epoch          : 786\n",
      "    loss           : -1800.628670170622\n",
      "    ess            : 8.001182088312113\n",
      "    log_marginal   : 1800.6286713222287\n",
      "    val_loss       : -1801.4685160319011\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1801.4685160319011\n",
      "Train Epoch: 787 [0/54000 (0%)] Loss: -1807.803711\n",
      "Train Epoch: 787 [11264/54000 (21%)] Loss: -1799.284424\n",
      "Train Epoch: 787 [22528/54000 (42%)] Loss: -1805.464355\n",
      "Train Epoch: 787 [33792/54000 (63%)] Loss: -1796.768433\n",
      "Train Epoch: 787 [45056/54000 (83%)] Loss: -1804.282227\n",
      "    epoch          : 787\n",
      "    loss           : -1799.8894630288178\n",
      "    ess            : 8.00118332988811\n",
      "    log_marginal   : 1799.8894641804245\n",
      "    val_loss       : -1809.2945658365886\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1809.2945658365886\n",
      "Train Epoch: 788 [0/54000 (0%)] Loss: -1818.208862\n",
      "Train Epoch: 788 [11264/54000 (21%)] Loss: -1789.135742\n",
      "Train Epoch: 788 [22528/54000 (42%)] Loss: -1797.004639\n",
      "Train Epoch: 788 [33792/54000 (63%)] Loss: -1810.040771\n",
      "Train Epoch: 788 [45056/54000 (83%)] Loss: -1790.544189\n",
      "    epoch          : 788\n",
      "    loss           : -1803.914981482164\n",
      "    ess            : 8.001182286244518\n",
      "    log_marginal   : 1803.914981482164\n",
      "    val_loss       : -1810.2797546386719\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1810.2797546386719\n",
      "Train Epoch: 789 [0/54000 (0%)] Loss: -1815.776611\n",
      "Train Epoch: 789 [11264/54000 (21%)] Loss: -1829.118164\n",
      "Train Epoch: 789 [22528/54000 (42%)] Loss: -1813.816284\n",
      "Train Epoch: 789 [33792/54000 (63%)] Loss: -1815.589111\n",
      "Train Epoch: 789 [45056/54000 (83%)] Loss: -1804.195068\n",
      "    epoch          : 789\n",
      "    loss           : -1812.770206091539\n",
      "    ess            : 8.001181170625507\n",
      "    log_marginal   : 1812.770206091539\n",
      "    val_loss       : -1808.8753662109375\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1808.8753662109375\n",
      "Train Epoch: 790 [0/54000 (0%)] Loss: -1816.035400\n",
      "Train Epoch: 790 [11264/54000 (21%)] Loss: -1805.900757\n",
      "Train Epoch: 790 [22528/54000 (42%)] Loss: -1799.732422\n",
      "Train Epoch: 790 [33792/54000 (63%)] Loss: -1800.923584\n",
      "Train Epoch: 790 [45056/54000 (83%)] Loss: -1800.416016\n",
      "    epoch          : 790\n",
      "    loss           : -1803.1069946289062\n",
      "    ess            : 8.001182862047878\n",
      "    log_marginal   : 1803.1069946289062\n",
      "    val_loss       : -1808.623270670573\n",
      "    val_ess        : 8.001184701919556\n",
      "    val_log_marginal: 1808.623291015625\n",
      "Train Epoch: 791 [0/54000 (0%)] Loss: -1824.074463\n",
      "Train Epoch: 791 [11264/54000 (21%)] Loss: -1812.325684\n",
      "Train Epoch: 791 [22528/54000 (42%)] Loss: -1789.796631\n",
      "Train Epoch: 791 [33792/54000 (63%)] Loss: -1804.825684\n",
      "Train Epoch: 791 [45056/54000 (83%)] Loss: -1786.100098\n",
      "    epoch          : 791\n",
      "    loss           : -1800.8106896742336\n",
      "    ess            : 8.001181854391998\n",
      "    log_marginal   : 1800.81068737102\n",
      "    val_loss       : -1796.6421813964844\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1796.6422119140625\n",
      "Train Epoch: 792 [0/54000 (0%)] Loss: -1811.150391\n",
      "Train Epoch: 792 [11264/54000 (21%)] Loss: -1763.671631\n",
      "Train Epoch: 792 [22528/54000 (42%)] Loss: -1780.693115\n",
      "Train Epoch: 792 [33792/54000 (63%)] Loss: -1775.193604\n",
      "Train Epoch: 792 [45056/54000 (83%)] Loss: -1790.371094\n",
      "    epoch          : 792\n",
      "    loss           : -1784.274318479142\n",
      "    ess            : 8.001183626786718\n",
      "    log_marginal   : 1784.274318479142\n",
      "    val_loss       : -1801.8210856119792\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1801.8210856119792\n",
      "Train Epoch: 793 [0/54000 (0%)] Loss: -1811.532715\n",
      "Train Epoch: 793 [11264/54000 (21%)] Loss: -1805.699585\n",
      "Train Epoch: 793 [22528/54000 (42%)] Loss: -1798.190186\n",
      "Train Epoch: 793 [33792/54000 (63%)] Loss: -1791.665894\n",
      "Train Epoch: 793 [45056/54000 (83%)] Loss: -1794.266113\n",
      "    epoch          : 793\n",
      "    loss           : -1798.5929669074292\n",
      "    ess            : 8.001182250256809\n",
      "    log_marginal   : 1798.5929657558224\n",
      "    val_loss       : -1805.8375142415364\n",
      "    val_ess        : 8.001182476679483\n",
      "    val_log_marginal: 1805.8375142415364\n",
      "Train Epoch: 794 [0/54000 (0%)] Loss: -1815.607910\n",
      "Train Epoch: 794 [11264/54000 (21%)] Loss: -1815.853027\n",
      "Train Epoch: 794 [22528/54000 (42%)] Loss: -1809.004517\n",
      "Train Epoch: 794 [33792/54000 (63%)] Loss: -1803.043701\n",
      "Train Epoch: 794 [45056/54000 (83%)] Loss: -1802.710938\n",
      "    epoch          : 794\n",
      "    loss           : -1809.1289868624706\n",
      "    ess            : 8.001182556152344\n",
      "    log_marginal   : 1809.1289868624706\n",
      "    val_loss       : -1811.2704366048176\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1811.2704467773438\n",
      "Train Epoch: 795 [0/54000 (0%)] Loss: -1818.263916\n",
      "Train Epoch: 795 [11264/54000 (21%)] Loss: -1817.005005\n",
      "Train Epoch: 795 [22528/54000 (42%)] Loss: -1806.597900\n",
      "Train Epoch: 795 [33792/54000 (63%)] Loss: -1806.300903\n",
      "Train Epoch: 795 [45056/54000 (83%)] Loss: -1800.584229\n",
      "    epoch          : 795\n",
      "    loss           : -1808.3808064010907\n",
      "    ess            : 8.001183410860458\n",
      "    log_marginal   : 1808.3808064010907\n",
      "    val_loss       : -1813.0756022135417\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1813.0756022135417\n",
      "Train Epoch: 796 [0/54000 (0%)] Loss: -1823.733887\n",
      "Train Epoch: 796 [11264/54000 (21%)] Loss: -1821.274780\n",
      "Train Epoch: 796 [22528/54000 (42%)] Loss: -1807.638306\n",
      "Train Epoch: 796 [33792/54000 (63%)] Loss: -1802.846558\n",
      "Train Epoch: 796 [45056/54000 (83%)] Loss: -1801.935547\n",
      "    epoch          : 796\n",
      "    loss           : -1809.5189082307636\n",
      "    ess            : 8.0011815934811\n",
      "    log_marginal   : 1809.5189082307636\n",
      "    val_loss       : -1810.3504028320312\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1810.3504028320312\n",
      "Train Epoch: 797 [0/54000 (0%)] Loss: -1815.527832\n",
      "Train Epoch: 797 [11264/54000 (21%)] Loss: -1807.692627\n",
      "Train Epoch: 797 [22528/54000 (42%)] Loss: -1813.330811\n",
      "Train Epoch: 797 [33792/54000 (63%)] Loss: -1795.425537\n",
      "Train Epoch: 797 [45056/54000 (83%)] Loss: -1800.591553\n",
      "    epoch          : 797\n",
      "    loss           : -1807.589677918632\n",
      "    ess            : 8.001182682109329\n",
      "    log_marginal   : 1807.5896756154186\n",
      "    val_loss       : -1812.8998718261719\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1812.8998921712239\n",
      "Train Epoch: 798 [0/54000 (0%)] Loss: -1820.493164\n",
      "Train Epoch: 798 [11264/54000 (21%)] Loss: -1821.970337\n",
      "Train Epoch: 798 [22528/54000 (42%)] Loss: -1816.633789\n",
      "Train Epoch: 798 [33792/54000 (63%)] Loss: -1809.298828\n",
      "Train Epoch: 798 [45056/54000 (83%)] Loss: -1805.799194\n",
      "    epoch          : 798\n",
      "    loss           : -1811.0663498212707\n",
      "    ess            : 8.001181980348983\n",
      "    log_marginal   : 1811.0663498212707\n",
      "    val_loss       : -1809.357686360677\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1809.3576965332031\n",
      "Train Epoch: 799 [0/54000 (0%)] Loss: -1821.384155\n",
      "Train Epoch: 799 [11264/54000 (21%)] Loss: -1820.595459\n",
      "Train Epoch: 799 [22528/54000 (42%)] Loss: -1809.027588\n",
      "Train Epoch: 799 [33792/54000 (63%)] Loss: -1805.838501\n",
      "Train Epoch: 799 [45056/54000 (83%)] Loss: -1806.054688\n",
      "    epoch          : 799\n",
      "    loss           : -1810.0441537533166\n",
      "    ess            : 8.001181197616289\n",
      "    log_marginal   : 1810.0441502984966\n",
      "    val_loss       : -1810.5877075195312\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1810.5876973470051\n",
      "Train Epoch: 800 [0/54000 (0%)] Loss: -1814.690674\n",
      "Train Epoch: 800 [11264/54000 (21%)] Loss: -1815.629150\n",
      "Train Epoch: 800 [22528/54000 (42%)] Loss: -1800.777588\n",
      "Train Epoch: 800 [33792/54000 (63%)] Loss: -1805.935059\n",
      "Train Epoch: 800 [45056/54000 (83%)] Loss: -1804.614014\n",
      "    epoch          : 800\n",
      "    loss           : -1807.4278552937058\n",
      "    ess            : 8.001182151290605\n",
      "    log_marginal   : 1807.4278552937058\n",
      "    val_loss       : -1805.734130859375\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1805.734130859375\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch800.pth ...\n",
      "Train Epoch: 801 [0/54000 (0%)] Loss: -1814.793213\n",
      "Train Epoch: 801 [11264/54000 (21%)] Loss: -1818.273804\n",
      "Train Epoch: 801 [22528/54000 (42%)] Loss: -1814.924805\n",
      "Train Epoch: 801 [33792/54000 (63%)] Loss: -1804.894531\n",
      "Train Epoch: 801 [45056/54000 (83%)] Loss: -1815.197144\n",
      "    epoch          : 801\n",
      "    loss           : -1812.605216548128\n",
      "    ess            : 8.001180900717682\n",
      "    log_marginal   : 1812.605216548128\n",
      "    val_loss       : -1818.1683451334636\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1818.1683451334636\n",
      "Train Epoch: 802 [0/54000 (0%)] Loss: -1824.873901\n",
      "Train Epoch: 802 [11264/54000 (21%)] Loss: -1814.329956\n",
      "Train Epoch: 802 [22528/54000 (42%)] Loss: -1799.185547\n",
      "Train Epoch: 802 [33792/54000 (63%)] Loss: -1802.749878\n",
      "Train Epoch: 802 [45056/54000 (83%)] Loss: -1803.936157\n",
      "    epoch          : 802\n",
      "    loss           : -1809.3597999428803\n",
      "    ess            : 8.001181494514897\n",
      "    log_marginal   : 1809.3597999428803\n",
      "    val_loss       : -1809.52490234375\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1809.524881998698\n",
      "Train Epoch: 803 [0/54000 (0%)] Loss: -1815.717285\n",
      "Train Epoch: 803 [11264/54000 (21%)] Loss: -1811.281860\n",
      "Train Epoch: 803 [22528/54000 (42%)] Loss: -1801.052246\n",
      "Train Epoch: 803 [33792/54000 (63%)] Loss: -1787.964600\n",
      "Train Epoch: 803 [45056/54000 (83%)] Loss: -1778.908691\n",
      "    epoch          : 803\n",
      "    loss           : -1797.6930138929836\n",
      "    ess            : 8.001181899376636\n",
      "    log_marginal   : 1797.6930127413768\n",
      "    val_loss       : -1795.7241516113281\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1795.7241516113281\n",
      "Train Epoch: 804 [0/54000 (0%)] Loss: -1805.655029\n",
      "Train Epoch: 804 [11264/54000 (21%)] Loss: -1792.869141\n",
      "Train Epoch: 804 [22528/54000 (42%)] Loss: -1773.062500\n",
      "Train Epoch: 804 [33792/54000 (63%)] Loss: -1786.116943\n",
      "Train Epoch: 804 [45056/54000 (83%)] Loss: -1776.434570\n",
      "    epoch          : 804\n",
      "    loss           : -1781.2429233766952\n",
      "    ess            : 8.001184139611587\n",
      "    log_marginal   : 1781.2429233766952\n",
      "    val_loss       : -1800.9262593587239\n",
      "    val_ess        : 8.001185496648153\n",
      "    val_log_marginal: 1800.92626953125\n",
      "Train Epoch: 805 [0/54000 (0%)] Loss: -1803.480713\n",
      "Train Epoch: 805 [11264/54000 (21%)] Loss: -1789.495605\n",
      "Train Epoch: 805 [22528/54000 (42%)] Loss: -1793.251953\n",
      "Train Epoch: 805 [33792/54000 (63%)] Loss: -1788.712891\n",
      "Train Epoch: 805 [45056/54000 (83%)] Loss: -1782.245850\n",
      "    epoch          : 805\n",
      "    loss           : -1788.708090728184\n",
      "    ess            : 8.00118404964231\n",
      "    log_marginal   : 1788.708090728184\n",
      "    val_loss       : -1803.0246988932292\n",
      "    val_ess        : 8.001185178756714\n",
      "    val_log_marginal: 1803.0247294108074\n",
      "Train Epoch: 806 [0/54000 (0%)] Loss: -1811.712158\n",
      "Train Epoch: 806 [11264/54000 (21%)] Loss: -1805.079834\n",
      "Train Epoch: 806 [22528/54000 (42%)] Loss: -1811.702026\n",
      "Train Epoch: 806 [33792/54000 (63%)] Loss: -1804.935913\n",
      "Train Epoch: 806 [45056/54000 (83%)] Loss: -1797.736084\n",
      "    epoch          : 806\n",
      "    loss           : -1803.691203567217\n",
      "    ess            : 8.001182664115474\n",
      "    log_marginal   : 1803.691203567217\n",
      "    val_loss       : -1810.1450093587239\n",
      "    val_ess        : 8.001184145609537\n",
      "    val_log_marginal: 1810.1450297037761\n",
      "Train Epoch: 807 [0/54000 (0%)] Loss: -1819.020508\n",
      "Train Epoch: 807 [11264/54000 (21%)] Loss: -1822.756592\n",
      "Train Epoch: 807 [22528/54000 (42%)] Loss: -1802.193848\n",
      "Train Epoch: 807 [33792/54000 (63%)] Loss: -1811.599854\n",
      "Train Epoch: 807 [45056/54000 (83%)] Loss: -1803.005371\n",
      "    epoch          : 807\n",
      "    loss           : -1807.5462600420105\n",
      "    ess            : 8.001182367216867\n",
      "    log_marginal   : 1807.5462600420105\n",
      "    val_loss       : -1816.0961608886719\n",
      "    val_ess        : 8.001182238260904\n",
      "    val_log_marginal: 1816.0961608886719\n",
      "Train Epoch: 808 [0/54000 (0%)] Loss: -1820.687012\n",
      "Train Epoch: 808 [11264/54000 (21%)] Loss: -1816.726807\n",
      "Train Epoch: 808 [22528/54000 (42%)] Loss: -1806.778198\n",
      "Train Epoch: 808 [33792/54000 (63%)] Loss: -1804.688354\n",
      "Train Epoch: 808 [45056/54000 (83%)] Loss: -1806.155151\n",
      "    epoch          : 808\n",
      "    loss           : -1805.2272695865272\n",
      "    ess            : 8.00118282606017\n",
      "    log_marginal   : 1805.2272695865272\n",
      "    val_loss       : -1808.0336608886719\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1808.0336507161458\n",
      "Train Epoch: 809 [0/54000 (0%)] Loss: -1809.576782\n",
      "Train Epoch: 809 [11264/54000 (21%)] Loss: -1814.936768\n",
      "Train Epoch: 809 [22528/54000 (42%)] Loss: -1811.214844\n",
      "Train Epoch: 809 [33792/54000 (63%)] Loss: -1796.507935\n",
      "Train Epoch: 809 [45056/54000 (83%)] Loss: -1801.987427\n",
      "    epoch          : 809\n",
      "    loss           : -1807.2085813126473\n",
      "    ess            : 8.00118282606017\n",
      "    log_marginal   : 1807.2085813126473\n",
      "    val_loss       : -1795.5916137695312\n",
      "    val_ess        : 8.001182953516642\n",
      "    val_log_marginal: 1795.5916137695312\n",
      "Train Epoch: 810 [0/54000 (0%)] Loss: -1808.990723\n",
      "Train Epoch: 810 [11264/54000 (21%)] Loss: -1781.127197\n",
      "Train Epoch: 810 [22528/54000 (42%)] Loss: -1786.646729\n",
      "Train Epoch: 810 [33792/54000 (63%)] Loss: -1785.721680\n",
      "Train Epoch: 810 [45056/54000 (83%)] Loss: -1766.222656\n",
      "    epoch          : 810\n",
      "    loss           : -1784.8602064600532\n",
      "    ess            : 8.001184589457962\n",
      "    log_marginal   : 1784.8602064600532\n",
      "    val_loss       : -1801.8289896647136\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1801.8289896647136\n",
      "Train Epoch: 811 [0/54000 (0%)] Loss: -1797.217529\n",
      "Train Epoch: 811 [11264/54000 (21%)] Loss: -1791.602539\n",
      "Train Epoch: 811 [22528/54000 (42%)] Loss: -1800.232056\n",
      "Train Epoch: 811 [33792/54000 (63%)] Loss: -1793.513916\n",
      "Train Epoch: 811 [45056/54000 (83%)] Loss: -1785.715088\n",
      "    epoch          : 811\n",
      "    loss           : -1793.7384689618957\n",
      "    ess            : 8.001183851709905\n",
      "    log_marginal   : 1793.7384689618957\n",
      "    val_loss       : -1803.4317932128906\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1803.4317932128906\n",
      "Train Epoch: 812 [0/54000 (0%)] Loss: -1807.362061\n",
      "Train Epoch: 812 [11264/54000 (21%)] Loss: -1819.877930\n",
      "Train Epoch: 812 [22528/54000 (42%)] Loss: -1807.764282\n",
      "Train Epoch: 812 [33792/54000 (63%)] Loss: -1812.236084\n",
      "Train Epoch: 812 [45056/54000 (83%)] Loss: -1804.284424\n",
      "    epoch          : 812\n",
      "    loss           : -1807.1461745927918\n",
      "    ess            : 8.001183167943415\n",
      "    log_marginal   : 1807.146173441185\n",
      "    val_loss       : -1810.7897135416667\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1810.7897135416667\n",
      "Train Epoch: 813 [0/54000 (0%)] Loss: -1812.922607\n",
      "Train Epoch: 813 [11264/54000 (21%)] Loss: -1809.639160\n",
      "Train Epoch: 813 [22528/54000 (42%)] Loss: -1798.014893\n",
      "Train Epoch: 813 [33792/54000 (63%)] Loss: -1796.012573\n",
      "Train Epoch: 813 [45056/54000 (83%)] Loss: -1802.546509\n",
      "    epoch          : 813\n",
      "    loss           : -1804.5890019254864\n",
      "    ess            : 8.001184166602368\n",
      "    log_marginal   : 1804.5890019254864\n",
      "    val_loss       : -1805.8471374511719\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.8471374511719\n",
      "Train Epoch: 814 [0/54000 (0%)] Loss: -1814.070557\n",
      "Train Epoch: 814 [11264/54000 (21%)] Loss: -1811.003174\n",
      "Train Epoch: 814 [22528/54000 (42%)] Loss: -1803.224487\n",
      "Train Epoch: 814 [33792/54000 (63%)] Loss: -1805.538940\n",
      "Train Epoch: 814 [45056/54000 (83%)] Loss: -1804.756348\n",
      "    epoch          : 814\n",
      "    loss           : -1807.786857173128\n",
      "    ess            : 8.001182844054025\n",
      "    log_marginal   : 1807.786857173128\n",
      "    val_loss       : -1817.6085815429688\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1817.6085815429688\n",
      "Train Epoch: 815 [0/54000 (0%)] Loss: -1823.692261\n",
      "Train Epoch: 815 [11264/54000 (21%)] Loss: -1820.629517\n",
      "Train Epoch: 815 [22528/54000 (42%)] Loss: -1793.820557\n",
      "Train Epoch: 815 [33792/54000 (63%)] Loss: -1809.540161\n",
      "Train Epoch: 815 [45056/54000 (83%)] Loss: -1800.054199\n",
      "    epoch          : 815\n",
      "    loss           : -1806.5048851157135\n",
      "    ess            : 8.001182286244518\n",
      "    log_marginal   : 1806.5048851157135\n",
      "    val_loss       : -1807.3860982259114\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1807.3860982259114\n",
      "Train Epoch: 816 [0/54000 (0%)] Loss: -1816.137451\n",
      "Train Epoch: 816 [11264/54000 (21%)] Loss: -1813.714111\n",
      "Train Epoch: 816 [22528/54000 (42%)] Loss: -1790.496826\n",
      "Train Epoch: 816 [33792/54000 (63%)] Loss: -1799.458984\n",
      "Train Epoch: 816 [45056/54000 (83%)] Loss: -1813.014771\n",
      "    epoch          : 816\n",
      "    loss           : -1800.795223595961\n",
      "    ess            : 8.001183392866603\n",
      "    log_marginal   : 1800.795223595961\n",
      "    val_loss       : -1812.129638671875\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1812.129638671875\n",
      "Train Epoch: 817 [0/54000 (0%)] Loss: -1822.682251\n",
      "Train Epoch: 817 [11264/54000 (21%)] Loss: -1801.982178\n",
      "Train Epoch: 817 [22528/54000 (42%)] Loss: -1800.499756\n",
      "Train Epoch: 817 [33792/54000 (63%)] Loss: -1792.798828\n",
      "Train Epoch: 817 [45056/54000 (83%)] Loss: -1804.941528\n",
      "    epoch          : 817\n",
      "    loss           : -1801.6030676499852\n",
      "    ess            : 8.001182268250663\n",
      "    log_marginal   : 1801.6030676499852\n",
      "    val_loss       : -1808.6520080566406\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1808.6520080566406\n",
      "Train Epoch: 818 [0/54000 (0%)] Loss: -1822.949097\n",
      "Train Epoch: 818 [11264/54000 (21%)] Loss: -1818.018799\n",
      "Train Epoch: 818 [22528/54000 (42%)] Loss: -1817.742432\n",
      "Train Epoch: 818 [33792/54000 (63%)] Loss: -1800.665527\n",
      "Train Epoch: 818 [45056/54000 (83%)] Loss: -1796.059692\n",
      "    epoch          : 818\n",
      "    loss           : -1807.473521106648\n",
      "    ess            : 8.001182304238373\n",
      "    log_marginal   : 1807.4735199550412\n",
      "    val_loss       : -1798.8663024902344\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1798.8663024902344\n",
      "Train Epoch: 819 [0/54000 (0%)] Loss: -1804.451660\n",
      "Train Epoch: 819 [11264/54000 (21%)] Loss: -1803.449463\n",
      "Train Epoch: 819 [22528/54000 (42%)] Loss: -1808.640259\n",
      "Train Epoch: 819 [33792/54000 (63%)] Loss: -1801.288696\n",
      "Train Epoch: 819 [45056/54000 (83%)] Loss: -1805.239258\n",
      "    epoch          : 819\n",
      "    loss           : -1802.081236641362\n",
      "    ess            : 8.00118368976521\n",
      "    log_marginal   : 1802.081236641362\n",
      "    val_loss       : -1800.4547526041667\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1800.4547526041667\n",
      "Train Epoch: 820 [0/54000 (0%)] Loss: -1819.399048\n",
      "Train Epoch: 820 [11264/54000 (21%)] Loss: -1803.495117\n",
      "Train Epoch: 820 [22528/54000 (42%)] Loss: -1814.125244\n",
      "Train Epoch: 820 [33792/54000 (63%)] Loss: -1800.228149\n",
      "Train Epoch: 820 [45056/54000 (83%)] Loss: -1799.159424\n",
      "    epoch          : 820\n",
      "    loss           : -1808.1037989202534\n",
      "    ess            : 8.001181692447302\n",
      "    log_marginal   : 1808.1037989202534\n",
      "    val_loss       : -1813.9480692545574\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1813.9480489095051\n",
      "Train Epoch: 821 [0/54000 (0%)] Loss: -1822.991943\n",
      "Train Epoch: 821 [11264/54000 (21%)] Loss: -1801.854126\n",
      "Train Epoch: 821 [22528/54000 (42%)] Loss: -1801.290771\n",
      "Train Epoch: 821 [33792/54000 (63%)] Loss: -1791.566406\n",
      "Train Epoch: 821 [45056/54000 (83%)] Loss: -1791.416992\n",
      "    epoch          : 821\n",
      "    loss           : -1803.548778605911\n",
      "    ess            : 8.001181836398143\n",
      "    log_marginal   : 1803.548778605911\n",
      "    val_loss       : -1809.4182739257812\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1809.4182637532551\n",
      "Train Epoch: 822 [0/54000 (0%)] Loss: -1822.814697\n",
      "Train Epoch: 822 [11264/54000 (21%)] Loss: -1824.652466\n",
      "Train Epoch: 822 [22528/54000 (42%)] Loss: -1803.064575\n",
      "Train Epoch: 822 [33792/54000 (63%)] Loss: -1788.645020\n",
      "Train Epoch: 822 [45056/54000 (83%)] Loss: -1797.118164\n",
      "    epoch          : 822\n",
      "    loss           : -1803.3182753077094\n",
      "    ess            : 8.001182898035589\n",
      "    log_marginal   : 1803.3182753077094\n",
      "    val_loss       : -1802.8041585286458\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1802.8041585286458\n",
      "Train Epoch: 823 [0/54000 (0%)] Loss: -1816.835815\n",
      "Train Epoch: 823 [11264/54000 (21%)] Loss: -1816.752441\n",
      "Train Epoch: 823 [22528/54000 (42%)] Loss: -1819.249023\n",
      "Train Epoch: 823 [33792/54000 (63%)] Loss: -1810.457520\n",
      "Train Epoch: 823 [45056/54000 (83%)] Loss: -1811.231445\n",
      "    epoch          : 823\n",
      "    loss           : -1811.896752699366\n",
      "    ess            : 8.001182817063242\n",
      "    log_marginal   : 1811.8967538509728\n",
      "    val_loss       : -1811.5277099609375\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1811.5277099609375\n",
      "Train Epoch: 824 [0/54000 (0%)] Loss: -1821.978516\n",
      "Train Epoch: 824 [11264/54000 (21%)] Loss: -1816.833740\n",
      "Train Epoch: 824 [22528/54000 (42%)] Loss: -1780.327881\n",
      "Train Epoch: 824 [33792/54000 (63%)] Loss: -1792.221069\n",
      "Train Epoch: 824 [45056/54000 (83%)] Loss: -1779.929688\n",
      "    epoch          : 824\n",
      "    loss           : -1791.4354616561027\n",
      "    ess            : 8.00118318593727\n",
      "    log_marginal   : 1791.4354616561027\n",
      "    val_loss       : -1785.6780090332031\n",
      "    val_ess        : 8.001183350880941\n",
      "    val_log_marginal: 1785.6780192057292\n",
      "Train Epoch: 825 [0/54000 (0%)] Loss: -1798.160889\n",
      "Train Epoch: 825 [11264/54000 (21%)] Loss: -1797.669922\n",
      "Train Epoch: 825 [22528/54000 (42%)] Loss: -1801.631836\n",
      "Train Epoch: 825 [33792/54000 (63%)] Loss: -1802.102295\n",
      "Train Epoch: 825 [45056/54000 (83%)] Loss: -1803.447510\n",
      "    epoch          : 825\n",
      "    loss           : -1805.9576358435288\n",
      "    ess            : 8.001182646121618\n",
      "    log_marginal   : 1805.9576358435288\n",
      "    val_loss       : -1802.3639628092449\n",
      "    val_ess        : 8.001181761423746\n",
      "    val_log_marginal: 1802.3639628092449\n",
      "Train Epoch: 826 [0/54000 (0%)] Loss: -1819.988281\n",
      "Train Epoch: 826 [11264/54000 (21%)] Loss: -1814.812012\n",
      "Train Epoch: 826 [22528/54000 (42%)] Loss: -1785.828857\n",
      "Train Epoch: 826 [33792/54000 (63%)] Loss: -1808.310181\n",
      "Train Epoch: 826 [45056/54000 (83%)] Loss: -1803.684814\n",
      "    epoch          : 826\n",
      "    loss           : -1803.558601811247\n",
      "    ess            : 8.001182664115474\n",
      "    log_marginal   : 1803.558601811247\n",
      "    val_loss       : -1810.4265543619792\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1810.4265543619792\n",
      "Train Epoch: 827 [0/54000 (0%)] Loss: -1815.055908\n",
      "Train Epoch: 827 [11264/54000 (21%)] Loss: -1798.280273\n",
      "Train Epoch: 827 [22528/54000 (42%)] Loss: -1798.609619\n",
      "Train Epoch: 827 [33792/54000 (63%)] Loss: -1793.468994\n",
      "Train Epoch: 827 [45056/54000 (83%)] Loss: -1808.736206\n",
      "    epoch          : 827\n",
      "    loss           : -1802.0855148603332\n",
      "    ess            : 8.00118238521072\n",
      "    log_marginal   : 1802.08551601194\n",
      "    val_loss       : -1802.1900329589844\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1802.1900329589844\n",
      "Train Epoch: 828 [0/54000 (0%)] Loss: -1818.031616\n",
      "Train Epoch: 828 [11264/54000 (21%)] Loss: -1817.354980\n",
      "Train Epoch: 828 [22528/54000 (42%)] Loss: -1807.565796\n",
      "Train Epoch: 828 [33792/54000 (63%)] Loss: -1801.403809\n",
      "Train Epoch: 828 [45056/54000 (83%)] Loss: -1812.054077\n",
      "    epoch          : 828\n",
      "    loss           : -1809.118420870799\n",
      "    ess            : 8.001182592140054\n",
      "    log_marginal   : 1809.118420870799\n",
      "    val_loss       : -1809.6353149414062\n",
      "    val_ess        : 8.001182635625204\n",
      "    val_log_marginal: 1809.6353149414062\n",
      "Train Epoch: 829 [0/54000 (0%)] Loss: -1825.467529\n",
      "Train Epoch: 829 [11264/54000 (21%)] Loss: -1820.893555\n",
      "Train Epoch: 829 [22528/54000 (42%)] Loss: -1787.916870\n",
      "Train Epoch: 829 [33792/54000 (63%)] Loss: -1810.566650\n",
      "Train Epoch: 829 [45056/54000 (83%)] Loss: -1800.746704\n",
      "    epoch          : 829\n",
      "    loss           : -1805.9920780973614\n",
      "    ess            : 8.00118271809704\n",
      "    log_marginal   : 1805.9920769457547\n",
      "    val_loss       : -1811.6447855631511\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1811.644795735677\n",
      "Train Epoch: 830 [0/54000 (0%)] Loss: -1819.669067\n",
      "Train Epoch: 830 [11264/54000 (21%)] Loss: -1813.244873\n",
      "Train Epoch: 830 [22528/54000 (42%)] Loss: -1810.712402\n",
      "Train Epoch: 830 [33792/54000 (63%)] Loss: -1794.525635\n",
      "Train Epoch: 830 [45056/54000 (83%)] Loss: -1801.397583\n",
      "    epoch          : 830\n",
      "    loss           : -1808.3328534971993\n",
      "    ess            : 8.00118202533362\n",
      "    log_marginal   : 1808.3328523455925\n",
      "    val_loss       : -1808.992167154948\n",
      "    val_ess        : 8.001182794570923\n",
      "    val_log_marginal: 1808.992167154948\n",
      "Train Epoch: 831 [0/54000 (0%)] Loss: -1811.085327\n",
      "Train Epoch: 831 [11264/54000 (21%)] Loss: -1814.338623\n",
      "Train Epoch: 831 [22528/54000 (42%)] Loss: -1807.456787\n",
      "Train Epoch: 831 [33792/54000 (63%)] Loss: -1796.118530\n",
      "Train Epoch: 831 [45056/54000 (83%)] Loss: -1793.304932\n",
      "    epoch          : 831\n",
      "    loss           : -1805.5734713572376\n",
      "    ess            : 8.001182214269098\n",
      "    log_marginal   : 1805.5734713572376\n",
      "    val_loss       : -1804.4049275716145\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1804.4049275716145\n",
      "Train Epoch: 832 [0/54000 (0%)] Loss: -1814.076050\n",
      "Train Epoch: 832 [11264/54000 (21%)] Loss: -1819.406860\n",
      "Train Epoch: 832 [22528/54000 (42%)] Loss: -1814.634766\n",
      "Train Epoch: 832 [33792/54000 (63%)] Loss: -1807.249878\n",
      "Train Epoch: 832 [45056/54000 (83%)] Loss: -1813.661133\n",
      "    epoch          : 832\n",
      "    loss           : -1809.9211322136646\n",
      "    ess            : 8.001180738772986\n",
      "    log_marginal   : 1809.9211322136646\n",
      "    val_loss       : -1812.2669779459636\n",
      "    val_ess        : 8.001182158788046\n",
      "    val_log_marginal: 1812.2669779459636\n",
      "Train Epoch: 833 [0/54000 (0%)] Loss: -1820.697266\n",
      "Train Epoch: 833 [11264/54000 (21%)] Loss: -1820.536743\n",
      "Train Epoch: 833 [22528/54000 (42%)] Loss: -1806.715820\n",
      "Train Epoch: 833 [33792/54000 (63%)] Loss: -1770.317139\n",
      "Train Epoch: 833 [45056/54000 (83%)] Loss: -1797.764160\n",
      "    epoch          : 833\n",
      "    loss           : -1805.3864515772407\n",
      "    ess            : 8.001182034330547\n",
      "    log_marginal   : 1805.3864527288472\n",
      "    val_loss       : -1809.040791829427\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1809.040791829427\n",
      "Train Epoch: 834 [0/54000 (0%)] Loss: -1805.558350\n",
      "Train Epoch: 834 [11264/54000 (21%)] Loss: -1811.631958\n",
      "Train Epoch: 834 [22528/54000 (42%)] Loss: -1810.892090\n",
      "Train Epoch: 834 [33792/54000 (63%)] Loss: -1814.475830\n",
      "Train Epoch: 834 [45056/54000 (83%)] Loss: -1799.868530\n",
      "    epoch          : 834\n",
      "    loss           : -1804.532441912957\n",
      "    ess            : 8.001181215610144\n",
      "    log_marginal   : 1804.532441912957\n",
      "    val_loss       : -1813.1078999837239\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1813.1078999837239\n",
      "Train Epoch: 835 [0/54000 (0%)] Loss: -1811.692505\n",
      "Train Epoch: 835 [11264/54000 (21%)] Loss: -1807.756958\n",
      "Train Epoch: 835 [22528/54000 (42%)] Loss: -1798.395508\n",
      "Train Epoch: 835 [33792/54000 (63%)] Loss: -1801.521484\n",
      "Train Epoch: 835 [45056/54000 (83%)] Loss: -1806.179688\n",
      "    epoch          : 835\n",
      "    loss           : -1801.7849051997346\n",
      "    ess            : 8.001181827401215\n",
      "    log_marginal   : 1801.7849051997346\n",
      "    val_loss       : -1804.4647521972656\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1804.4647521972656\n",
      "Train Epoch: 836 [0/54000 (0%)] Loss: -1816.701416\n",
      "Train Epoch: 836 [11264/54000 (21%)] Loss: -1818.567017\n",
      "Train Epoch: 836 [22528/54000 (42%)] Loss: -1818.456787\n",
      "Train Epoch: 836 [33792/54000 (63%)] Loss: -1816.162354\n",
      "Train Epoch: 836 [45056/54000 (83%)] Loss: -1818.783203\n",
      "    epoch          : 836\n",
      "    loss           : -1812.8714035322082\n",
      "    ess            : 8.001180621812928\n",
      "    log_marginal   : 1812.8714035322082\n",
      "    val_loss       : -1811.5406087239583\n",
      "    val_ess        : 8.001181761423746\n",
      "    val_log_marginal: 1811.5406087239583\n",
      "Train Epoch: 837 [0/54000 (0%)] Loss: -1823.905762\n",
      "Train Epoch: 837 [11264/54000 (21%)] Loss: -1816.437012\n",
      "Train Epoch: 837 [22528/54000 (42%)] Loss: -1807.002930\n",
      "Train Epoch: 837 [33792/54000 (63%)] Loss: -1805.984375\n",
      "Train Epoch: 837 [45056/54000 (83%)] Loss: -1806.910156\n",
      "    epoch          : 837\n",
      "    loss           : -1806.8708231224205\n",
      "    ess            : 8.001181296582493\n",
      "    log_marginal   : 1806.8708231224205\n",
      "    val_loss       : -1807.3966979980469\n",
      "    val_ess        : 8.001182238260904\n",
      "    val_log_marginal: 1807.3967183430989\n",
      "Train Epoch: 838 [0/54000 (0%)] Loss: -1814.035400\n",
      "Train Epoch: 838 [11264/54000 (21%)] Loss: -1818.595093\n",
      "Train Epoch: 838 [22528/54000 (42%)] Loss: -1792.734619\n",
      "Train Epoch: 838 [33792/54000 (63%)] Loss: -1800.559814\n",
      "Train Epoch: 838 [45056/54000 (83%)] Loss: -1801.631348\n",
      "    epoch          : 838\n",
      "    loss           : -1802.4769068304097\n",
      "    ess            : 8.001181215610144\n",
      "    log_marginal   : 1802.4769091336232\n",
      "    val_loss       : -1801.9878031412761\n",
      "    val_ess        : 8.001181364059448\n",
      "    val_log_marginal: 1801.9878031412761\n",
      "Train Epoch: 839 [0/54000 (0%)] Loss: -1808.803711\n",
      "Train Epoch: 839 [11264/54000 (21%)] Loss: -1814.397705\n",
      "Train Epoch: 839 [22528/54000 (42%)] Loss: -1813.984131\n",
      "Train Epoch: 839 [33792/54000 (63%)] Loss: -1804.391357\n",
      "Train Epoch: 839 [45056/54000 (83%)] Loss: -1812.752686\n",
      "    epoch          : 839\n",
      "    loss           : -1811.653262732164\n",
      "    ess            : 8.00118003701264\n",
      "    log_marginal   : 1811.653262732164\n",
      "    val_loss       : -1813.0594787597656\n",
      "    val_ess        : 8.001181284586588\n",
      "    val_log_marginal: 1813.0594787597656\n",
      "Train Epoch: 840 [0/54000 (0%)] Loss: -1827.742920\n",
      "Train Epoch: 840 [11264/54000 (21%)] Loss: -1821.689697\n",
      "Train Epoch: 840 [22528/54000 (42%)] Loss: -1783.135010\n",
      "Train Epoch: 840 [33792/54000 (63%)] Loss: -1808.499023\n",
      "Train Epoch: 840 [45056/54000 (83%)] Loss: -1802.691650\n",
      "    epoch          : 840\n",
      "    loss           : -1807.5881808298939\n",
      "    ess            : 8.00118141354255\n",
      "    log_marginal   : 1807.5881808298939\n",
      "    val_loss       : -1812.4855244954426\n",
      "    val_ess        : 8.001181920369467\n",
      "    val_log_marginal: 1812.4855244954426\n",
      "Train Epoch: 841 [0/54000 (0%)] Loss: -1821.905762\n",
      "Train Epoch: 841 [11264/54000 (21%)] Loss: -1822.036133\n",
      "Train Epoch: 841 [22528/54000 (42%)] Loss: -1790.190552\n",
      "Train Epoch: 841 [33792/54000 (63%)] Loss: -1792.738770\n",
      "Train Epoch: 841 [45056/54000 (83%)] Loss: -1799.200562\n",
      "    epoch          : 841\n",
      "    loss           : -1804.302155116819\n",
      "    ess            : 8.001181062662377\n",
      "    log_marginal   : 1804.302155116819\n",
      "    val_loss       : -1806.4161071777344\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1806.4161071777344\n",
      "Train Epoch: 842 [0/54000 (0%)] Loss: -1812.159424\n",
      "Train Epoch: 842 [11264/54000 (21%)] Loss: -1817.086914\n",
      "Train Epoch: 842 [22528/54000 (42%)] Loss: -1808.926758\n",
      "Train Epoch: 842 [33792/54000 (63%)] Loss: -1814.442627\n",
      "Train Epoch: 842 [45056/54000 (83%)] Loss: -1815.317383\n",
      "    epoch          : 842\n",
      "    loss           : -1814.2041107753537\n",
      "    ess            : 8.001179866071018\n",
      "    log_marginal   : 1814.2041119269604\n",
      "    val_loss       : -1814.811747233073\n",
      "    val_ess        : 8.001181284586588\n",
      "    val_log_marginal: 1814.811747233073\n",
      "Train Epoch: 843 [0/54000 (0%)] Loss: -1820.860596\n",
      "Train Epoch: 843 [11264/54000 (21%)] Loss: -1818.412842\n",
      "Train Epoch: 843 [22528/54000 (42%)] Loss: -1789.414062\n",
      "Train Epoch: 843 [33792/54000 (63%)] Loss: -1806.575684\n",
      "Train Epoch: 843 [45056/54000 (83%)] Loss: -1816.794434\n",
      "    epoch          : 843\n",
      "    loss           : -1806.5917911169663\n",
      "    ess            : 8.001181782416577\n",
      "    log_marginal   : 1806.5917911169663\n",
      "    val_loss       : -1810.8507181803386\n",
      "    val_ess        : 8.001182953516642\n",
      "    val_log_marginal: 1810.8507181803386\n",
      "Train Epoch: 844 [0/54000 (0%)] Loss: -1815.230713\n",
      "Train Epoch: 844 [11264/54000 (21%)] Loss: -1822.013428\n",
      "Train Epoch: 844 [22528/54000 (42%)] Loss: -1800.926514\n",
      "Train Epoch: 844 [33792/54000 (63%)] Loss: -1796.620972\n",
      "Train Epoch: 844 [45056/54000 (83%)] Loss: -1804.361816\n",
      "    epoch          : 844\n",
      "    loss           : -1806.093965350457\n",
      "    ess            : 8.001180792754552\n",
      "    log_marginal   : 1806.0939641988502\n",
      "    val_loss       : -1805.4305419921875\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1805.4305419921875\n",
      "Train Epoch: 845 [0/54000 (0%)] Loss: -1817.113281\n",
      "Train Epoch: 845 [11264/54000 (21%)] Loss: -1819.688477\n",
      "Train Epoch: 845 [22528/54000 (42%)] Loss: -1807.386841\n",
      "Train Epoch: 845 [33792/54000 (63%)] Loss: -1810.865234\n",
      "Train Epoch: 845 [45056/54000 (83%)] Loss: -1823.416748\n",
      "    epoch          : 845\n",
      "    loss           : -1812.9207556382664\n",
      "    ess            : 8.001180090994206\n",
      "    log_marginal   : 1812.9207567898732\n",
      "    val_loss       : -1816.4669392903645\n",
      "    val_ess        : 8.001181761423746\n",
      "    val_log_marginal: 1816.4669291178386\n",
      "Train Epoch: 846 [0/54000 (0%)] Loss: -1828.626587\n",
      "Train Epoch: 846 [11264/54000 (21%)] Loss: -1828.165527\n",
      "Train Epoch: 846 [22528/54000 (42%)] Loss: -1782.119019\n",
      "Train Epoch: 846 [33792/54000 (63%)] Loss: -1781.105591\n",
      "Train Epoch: 846 [45056/54000 (83%)] Loss: -1783.082275\n",
      "    epoch          : 846\n",
      "    loss           : -1796.677866809773\n",
      "    ess            : 8.00118105366545\n",
      "    log_marginal   : 1796.6778679613797\n",
      "    val_loss       : -1795.5821533203125\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1795.5821329752605\n",
      "Train Epoch: 847 [0/54000 (0%)] Loss: -1797.282837\n",
      "Train Epoch: 847 [11264/54000 (21%)] Loss: -1795.523682\n",
      "Train Epoch: 847 [22528/54000 (42%)] Loss: -1799.377808\n",
      "Train Epoch: 847 [33792/54000 (63%)] Loss: -1801.631104\n",
      "Train Epoch: 847 [45056/54000 (83%)] Loss: -1808.541016\n",
      "    epoch          : 847\n",
      "    loss           : -1797.49825761903\n",
      "    ess            : 8.001181539499536\n",
      "    log_marginal   : 1797.49825761903\n",
      "    val_loss       : -1807.4493001302083\n",
      "    val_ess        : 8.00118350982666\n",
      "    val_log_marginal: 1807.4493001302083\n",
      "Train Epoch: 848 [0/54000 (0%)] Loss: -1809.623291\n",
      "Train Epoch: 848 [11264/54000 (21%)] Loss: -1804.690674\n",
      "Train Epoch: 848 [22528/54000 (42%)] Loss: -1810.862061\n",
      "Train Epoch: 848 [33792/54000 (63%)] Loss: -1798.866211\n",
      "Train Epoch: 848 [45056/54000 (83%)] Loss: -1804.724854\n",
      "    epoch          : 848\n",
      "    loss           : -1805.4793839364681\n",
      "    ess            : 8.001180297923538\n",
      "    log_marginal   : 1805.4793839364681\n",
      "    val_loss       : -1807.6756896972656\n",
      "    val_ess        : 8.001181999842325\n",
      "    val_log_marginal: 1807.6756896972656\n",
      "Train Epoch: 849 [0/54000 (0%)] Loss: -1825.059326\n",
      "Train Epoch: 849 [11264/54000 (21%)] Loss: -1814.859375\n",
      "Train Epoch: 849 [22528/54000 (42%)] Loss: -1808.605713\n",
      "Train Epoch: 849 [33792/54000 (63%)] Loss: -1805.793823\n",
      "Train Epoch: 849 [45056/54000 (83%)] Loss: -1798.913696\n",
      "    epoch          : 849\n",
      "    loss           : -1806.3232871001621\n",
      "    ess            : 8.001180630809856\n",
      "    log_marginal   : 1806.3232871001621\n",
      "    val_loss       : -1804.6977437337239\n",
      "    val_ess        : 8.001182238260904\n",
      "    val_log_marginal: 1804.6977437337239\n",
      "Train Epoch: 850 [0/54000 (0%)] Loss: -1813.134277\n",
      "Train Epoch: 850 [11264/54000 (21%)] Loss: -1823.676147\n",
      "Train Epoch: 850 [22528/54000 (42%)] Loss: -1788.496338\n",
      "Train Epoch: 850 [33792/54000 (63%)] Loss: -1799.863647\n",
      "Train Epoch: 850 [45056/54000 (83%)] Loss: -1803.528442\n",
      "    epoch          : 850\n",
      "    loss           : -1801.3927335919075\n",
      "    ess            : 8.001181611474955\n",
      "    log_marginal   : 1801.392731288694\n",
      "    val_loss       : -1802.0459391276042\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1802.0459391276042\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch850.pth ...\n",
      "Train Epoch: 851 [0/54000 (0%)] Loss: -1813.835205\n",
      "Train Epoch: 851 [11264/54000 (21%)] Loss: -1816.104248\n",
      "Train Epoch: 851 [22528/54000 (42%)] Loss: -1816.311768\n",
      "Train Epoch: 851 [33792/54000 (63%)] Loss: -1804.215698\n",
      "Train Epoch: 851 [45056/54000 (83%)] Loss: -1797.981689\n",
      "    epoch          : 851\n",
      "    loss           : -1803.6891905586674\n",
      "    ess            : 8.001180900717682\n",
      "    log_marginal   : 1803.6891917102741\n",
      "    val_loss       : -1769.8133646647136\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1769.8133646647136\n",
      "Train Epoch: 852 [0/54000 (0%)] Loss: -1787.575928\n",
      "Train Epoch: 852 [11264/54000 (21%)] Loss: -1768.937500\n",
      "Train Epoch: 852 [22528/54000 (42%)] Loss: -1792.725220\n",
      "Train Epoch: 852 [33792/54000 (63%)] Loss: -1800.629150\n",
      "Train Epoch: 852 [45056/54000 (83%)] Loss: -1801.825806\n",
      "    epoch          : 852\n",
      "    loss           : -1788.3428033792748\n",
      "    ess            : 8.001184418516338\n",
      "    log_marginal   : 1788.342802227668\n",
      "    val_loss       : -1790.5895080566406\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1790.5895080566406\n",
      "Train Epoch: 853 [0/54000 (0%)] Loss: -1810.342529\n",
      "Train Epoch: 853 [11264/54000 (21%)] Loss: -1798.650391\n",
      "Train Epoch: 853 [22528/54000 (42%)] Loss: -1792.229248\n",
      "Train Epoch: 853 [33792/54000 (63%)] Loss: -1794.864014\n",
      "Train Epoch: 853 [45056/54000 (83%)] Loss: -1797.597412\n",
      "    epoch          : 853\n",
      "    loss           : -1797.5977046174823\n",
      "    ess            : 8.001182493173852\n",
      "    log_marginal   : 1797.5977046174823\n",
      "    val_loss       : -1804.264383951823\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1804.264404296875\n",
      "Train Epoch: 854 [0/54000 (0%)] Loss: -1817.366089\n",
      "Train Epoch: 854 [11264/54000 (21%)] Loss: -1809.099121\n",
      "Train Epoch: 854 [22528/54000 (42%)] Loss: -1817.518311\n",
      "Train Epoch: 854 [33792/54000 (63%)] Loss: -1816.604858\n",
      "Train Epoch: 854 [45056/54000 (83%)] Loss: -1811.541748\n",
      "    epoch          : 854\n",
      "    loss           : -1813.5271180350826\n",
      "    ess            : 8.001182223266026\n",
      "    log_marginal   : 1813.5271180350826\n",
      "    val_loss       : -1812.3811136881511\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1812.3811340332031\n",
      "Train Epoch: 855 [0/54000 (0%)] Loss: -1831.273438\n",
      "Train Epoch: 855 [11264/54000 (21%)] Loss: -1810.947510\n",
      "Train Epoch: 855 [22528/54000 (42%)] Loss: -1815.767578\n",
      "Train Epoch: 855 [33792/54000 (63%)] Loss: -1802.977661\n",
      "Train Epoch: 855 [45056/54000 (83%)] Loss: -1802.725342\n",
      "    epoch          : 855\n",
      "    loss           : -1807.9614994840802\n",
      "    ess            : 8.001182475179997\n",
      "    log_marginal   : 1807.9614994840802\n",
      "    val_loss       : -1800.0404256184895\n",
      "    val_ess        : 8.001182953516642\n",
      "    val_log_marginal: 1800.0404154459636\n",
      "Train Epoch: 856 [0/54000 (0%)] Loss: -1801.894775\n",
      "Train Epoch: 856 [11264/54000 (21%)] Loss: -1803.269531\n",
      "Train Epoch: 856 [22528/54000 (42%)] Loss: -1813.684570\n",
      "Train Epoch: 856 [33792/54000 (63%)] Loss: -1798.088135\n",
      "Train Epoch: 856 [45056/54000 (83%)] Loss: -1804.100708\n",
      "    epoch          : 856\n",
      "    loss           : -1803.3254290886646\n",
      "    ess            : 8.001182511167706\n",
      "    log_marginal   : 1803.3254290886646\n",
      "    val_loss       : -1808.8489074707031\n",
      "    val_ess        : 8.001181523005167\n",
      "    val_log_marginal: 1808.8489379882812\n",
      "Train Epoch: 857 [0/54000 (0%)] Loss: -1816.048340\n",
      "Train Epoch: 857 [11264/54000 (21%)] Loss: -1808.433594\n",
      "Train Epoch: 857 [22528/54000 (42%)] Loss: -1805.044434\n",
      "Train Epoch: 857 [33792/54000 (63%)] Loss: -1807.853638\n",
      "Train Epoch: 857 [45056/54000 (83%)] Loss: -1810.564819\n",
      "    epoch          : 857\n",
      "    loss           : -1809.0584601636203\n",
      "    ess            : 8.001180855733043\n",
      "    log_marginal   : 1809.0584590120136\n",
      "    val_loss       : -1806.8060404459636\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1806.8060404459636\n",
      "Train Epoch: 858 [0/54000 (0%)] Loss: -1823.012451\n",
      "Train Epoch: 858 [11264/54000 (21%)] Loss: -1807.377441\n",
      "Train Epoch: 858 [22528/54000 (42%)] Loss: -1801.010498\n",
      "Train Epoch: 858 [33792/54000 (63%)] Loss: -1801.352783\n",
      "Train Epoch: 858 [45056/54000 (83%)] Loss: -1795.776611\n",
      "    epoch          : 858\n",
      "    loss           : -1804.5115287348908\n",
      "    ess            : 8.001181566490317\n",
      "    log_marginal   : 1804.5115287348908\n",
      "    val_loss       : -1800.010274251302\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1800.010274251302\n",
      "Train Epoch: 859 [0/54000 (0%)] Loss: -1813.826782\n",
      "Train Epoch: 859 [11264/54000 (21%)] Loss: -1804.479736\n",
      "Train Epoch: 859 [22528/54000 (42%)] Loss: -1798.970703\n",
      "Train Epoch: 859 [33792/54000 (63%)] Loss: -1805.822266\n",
      "Train Epoch: 859 [45056/54000 (83%)] Loss: -1810.252930\n",
      "    epoch          : 859\n",
      "    loss           : -1807.0299486844044\n",
      "    ess            : 8.001181098650086\n",
      "    log_marginal   : 1807.0299486844044\n",
      "    val_loss       : -1808.2168579101562\n",
      "    val_ess        : 8.001181840896606\n",
      "    val_log_marginal: 1808.2168579101562\n",
      "Train Epoch: 860 [0/54000 (0%)] Loss: -1826.619873\n",
      "Train Epoch: 860 [11264/54000 (21%)] Loss: -1805.677734\n",
      "Train Epoch: 860 [22528/54000 (42%)] Loss: -1807.561401\n",
      "Train Epoch: 860 [33792/54000 (63%)] Loss: -1783.933838\n",
      "Train Epoch: 860 [45056/54000 (83%)] Loss: -1808.622314\n",
      "    epoch          : 860\n",
      "    loss           : -1804.7373899063973\n",
      "    ess            : 8.001181863388926\n",
      "    log_marginal   : 1804.7373899063973\n",
      "    val_loss       : -1805.1143697102864\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1805.1143697102864\n",
      "Train Epoch: 861 [0/54000 (0%)] Loss: -1813.062256\n",
      "Train Epoch: 861 [11264/54000 (21%)] Loss: -1813.367676\n",
      "Train Epoch: 861 [22528/54000 (42%)] Loss: -1811.624268\n",
      "Train Epoch: 861 [33792/54000 (63%)] Loss: -1822.001953\n",
      "Train Epoch: 861 [45056/54000 (83%)] Loss: -1811.230225\n",
      "    epoch          : 861\n",
      "    loss           : -1813.0405215857163\n",
      "    ess            : 8.001180576828292\n",
      "    log_marginal   : 1813.0405215857163\n",
      "    val_loss       : -1819.1800333658855\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1819.1800333658855\n",
      "Train Epoch: 862 [0/54000 (0%)] Loss: -1824.521729\n",
      "Train Epoch: 862 [11264/54000 (21%)] Loss: -1809.707520\n",
      "Train Epoch: 862 [22528/54000 (42%)] Loss: -1796.084839\n",
      "Train Epoch: 862 [33792/54000 (63%)] Loss: -1807.876343\n",
      "Train Epoch: 862 [45056/54000 (83%)] Loss: -1805.058716\n",
      "    epoch          : 862\n",
      "    loss           : -1807.0344802568543\n",
      "    ess            : 8.001181296582493\n",
      "    log_marginal   : 1807.0344802568543\n",
      "    val_loss       : -1808.5959065755208\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1808.5959065755208\n",
      "Train Epoch: 863 [0/54000 (0%)] Loss: -1816.346436\n",
      "Train Epoch: 863 [11264/54000 (21%)] Loss: -1809.650879\n",
      "Train Epoch: 863 [22528/54000 (42%)] Loss: -1807.389893\n",
      "Train Epoch: 863 [33792/54000 (63%)] Loss: -1801.683472\n",
      "Train Epoch: 863 [45056/54000 (83%)] Loss: -1798.659302\n",
      "    epoch          : 863\n",
      "    loss           : -1802.597921119546\n",
      "    ess            : 8.001181296582493\n",
      "    log_marginal   : 1802.597921119546\n",
      "    val_loss       : -1804.2603047688801\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1804.2602945963542\n",
      "Train Epoch: 864 [0/54000 (0%)] Loss: -1799.997559\n",
      "Train Epoch: 864 [11264/54000 (21%)] Loss: -1810.476685\n",
      "Train Epoch: 864 [22528/54000 (42%)] Loss: -1801.283936\n",
      "Train Epoch: 864 [33792/54000 (63%)] Loss: -1802.511719\n",
      "Train Epoch: 864 [45056/54000 (83%)] Loss: -1808.003662\n",
      "    epoch          : 864\n",
      "    loss           : -1807.7080112673202\n",
      "    ess            : 8.001180324914321\n",
      "    log_marginal   : 1807.708012418927\n",
      "    val_loss       : -1818.2080891927083\n",
      "    val_ess        : 8.001181602478027\n",
      "    val_log_marginal: 1818.2080891927083\n",
      "Train Epoch: 865 [0/54000 (0%)] Loss: -1824.361816\n",
      "Train Epoch: 865 [11264/54000 (21%)] Loss: -1815.072998\n",
      "Train Epoch: 865 [22528/54000 (42%)] Loss: -1812.468750\n",
      "Train Epoch: 865 [33792/54000 (63%)] Loss: -1810.727539\n",
      "Train Epoch: 865 [45056/54000 (83%)] Loss: -1808.987915\n",
      "    epoch          : 865\n",
      "    loss           : -1811.3209965543927\n",
      "    ess            : 8.001180774760696\n",
      "    log_marginal   : 1811.3209977059994\n",
      "    val_loss       : -1812.4265950520833\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1812.4265950520833\n",
      "Train Epoch: 866 [0/54000 (0%)] Loss: -1822.179810\n",
      "Train Epoch: 866 [11264/54000 (21%)] Loss: -1813.578369\n",
      "Train Epoch: 866 [22528/54000 (42%)] Loss: -1806.999268\n",
      "Train Epoch: 866 [33792/54000 (63%)] Loss: -1808.913086\n",
      "Train Epoch: 866 [45056/54000 (83%)] Loss: -1808.991211\n",
      "    epoch          : 866\n",
      "    loss           : -1812.6600744859227\n",
      "    ess            : 8.001180846736116\n",
      "    log_marginal   : 1812.6600756375294\n",
      "    val_loss       : -1804.112528483073\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1804.112548828125\n",
      "Train Epoch: 867 [0/54000 (0%)] Loss: -1817.169434\n",
      "Train Epoch: 867 [11264/54000 (21%)] Loss: -1791.112793\n",
      "Train Epoch: 867 [22528/54000 (42%)] Loss: -1802.297119\n",
      "Train Epoch: 867 [33792/54000 (63%)] Loss: -1796.855713\n",
      "Train Epoch: 867 [45056/54000 (83%)] Loss: -1794.551636\n",
      "    epoch          : 867\n",
      "    loss           : -1801.471646290905\n",
      "    ess            : 8.001181080656231\n",
      "    log_marginal   : 1801.4716485941185\n",
      "    val_loss       : -1810.1598510742188\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1810.1598612467449\n",
      "Train Epoch: 868 [0/54000 (0%)] Loss: -1822.515625\n",
      "Train Epoch: 868 [11264/54000 (21%)] Loss: -1814.144775\n",
      "Train Epoch: 868 [22528/54000 (42%)] Loss: -1807.588257\n",
      "Train Epoch: 868 [33792/54000 (63%)] Loss: -1798.485596\n",
      "Train Epoch: 868 [45056/54000 (83%)] Loss: -1802.080811\n",
      "    epoch          : 868\n",
      "    loss           : -1807.4605597729953\n",
      "    ess            : 8.001180702785277\n",
      "    log_marginal   : 1807.4605597729953\n",
      "    val_loss       : -1810.6473286946614\n",
      "    val_ess        : 8.001183986663818\n",
      "    val_log_marginal: 1810.6473286946614\n",
      "Train Epoch: 869 [0/54000 (0%)] Loss: -1823.433105\n",
      "Train Epoch: 869 [11264/54000 (21%)] Loss: -1811.067383\n",
      "Train Epoch: 869 [22528/54000 (42%)] Loss: -1801.199463\n",
      "Train Epoch: 869 [33792/54000 (63%)] Loss: -1805.674927\n",
      "Train Epoch: 869 [45056/54000 (83%)] Loss: -1802.493164\n",
      "    epoch          : 869\n",
      "    loss           : -1809.2809943433078\n",
      "    ess            : 8.001180369898957\n",
      "    log_marginal   : 1809.2809943433078\n",
      "    val_loss       : -1811.114725748698\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1811.1147359212239\n",
      "Train Epoch: 870 [0/54000 (0%)] Loss: -1810.630005\n",
      "Train Epoch: 870 [11264/54000 (21%)] Loss: -1810.264160\n",
      "Train Epoch: 870 [22528/54000 (42%)] Loss: -1809.667480\n",
      "Train Epoch: 870 [33792/54000 (63%)] Loss: -1796.508301\n",
      "Train Epoch: 870 [45056/54000 (83%)] Loss: -1807.838867\n",
      "    epoch          : 870\n",
      "    loss           : -1812.214637612397\n",
      "    ess            : 8.001179479203135\n",
      "    log_marginal   : 1812.214637612397\n",
      "    val_loss       : -1811.7938842773438\n",
      "    val_ess        : 8.001181046168009\n",
      "    val_log_marginal: 1811.7938741048176\n",
      "Train Epoch: 871 [0/54000 (0%)] Loss: -1825.213623\n",
      "Train Epoch: 871 [11264/54000 (21%)] Loss: -1800.594971\n",
      "Train Epoch: 871 [22528/54000 (42%)] Loss: -1789.413818\n",
      "Train Epoch: 871 [33792/54000 (63%)] Loss: -1789.590210\n",
      "Train Epoch: 871 [45056/54000 (83%)] Loss: -1783.846436\n",
      "    epoch          : 871\n",
      "    loss           : -1795.0903354860702\n",
      "    ess            : 8.001181368557912\n",
      "    log_marginal   : 1795.090336637677\n",
      "    val_loss       : -1804.2619730631511\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1804.2619730631511\n",
      "Train Epoch: 872 [0/54000 (0%)] Loss: -1809.730713\n",
      "Train Epoch: 872 [11264/54000 (21%)] Loss: -1803.469360\n",
      "Train Epoch: 872 [22528/54000 (42%)] Loss: -1816.906250\n",
      "Train Epoch: 872 [33792/54000 (63%)] Loss: -1797.707397\n",
      "Train Epoch: 872 [45056/54000 (83%)] Loss: -1794.411377\n",
      "    epoch          : 872\n",
      "    loss           : -1802.7763395489387\n",
      "    ess            : 8.001182187278316\n",
      "    log_marginal   : 1802.7763395489387\n",
      "    val_loss       : -1804.9312235514324\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1804.9312235514324\n",
      "Train Epoch: 873 [0/54000 (0%)] Loss: -1817.297363\n",
      "Train Epoch: 873 [11264/54000 (21%)] Loss: -1807.183105\n",
      "Train Epoch: 873 [22528/54000 (42%)] Loss: -1803.253906\n",
      "Train Epoch: 873 [33792/54000 (63%)] Loss: -1803.603516\n",
      "Train Epoch: 873 [45056/54000 (83%)] Loss: -1797.126709\n",
      "    epoch          : 873\n",
      "    loss           : -1806.0146346182194\n",
      "    ess            : 8.001182673112401\n",
      "    log_marginal   : 1806.0146346182194\n",
      "    val_loss       : -1811.8039143880208\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1811.8039143880208\n",
      "Train Epoch: 874 [0/54000 (0%)] Loss: -1814.679810\n",
      "Train Epoch: 874 [11264/54000 (21%)] Loss: -1812.783447\n",
      "Train Epoch: 874 [22528/54000 (42%)] Loss: -1811.511475\n",
      "Train Epoch: 874 [33792/54000 (63%)] Loss: -1808.757812\n",
      "Train Epoch: 874 [45056/54000 (83%)] Loss: -1798.241699\n",
      "    epoch          : 874\n",
      "    loss           : -1807.0536395378833\n",
      "    ess            : 8.001182151290605\n",
      "    log_marginal   : 1807.0536395378833\n",
      "    val_loss       : -1807.8704935709636\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1807.8704935709636\n",
      "Train Epoch: 875 [0/54000 (0%)] Loss: -1823.961426\n",
      "Train Epoch: 875 [11264/54000 (21%)] Loss: -1818.846069\n",
      "Train Epoch: 875 [22528/54000 (42%)] Loss: -1814.695923\n",
      "Train Epoch: 875 [33792/54000 (63%)] Loss: -1809.332642\n",
      "Train Epoch: 875 [45056/54000 (83%)] Loss: -1784.923096\n",
      "    epoch          : 875\n",
      "    loss           : -1810.8763024672023\n",
      "    ess            : 8.001180738772986\n",
      "    log_marginal   : 1810.8763024672023\n",
      "    val_loss       : -1801.8049723307292\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1801.8049723307292\n",
      "Train Epoch: 876 [0/54000 (0%)] Loss: -1811.945801\n",
      "Train Epoch: 876 [11264/54000 (21%)] Loss: -1813.495117\n",
      "Train Epoch: 876 [22528/54000 (42%)] Loss: -1804.273682\n",
      "Train Epoch: 876 [33792/54000 (63%)] Loss: -1805.023315\n",
      "Train Epoch: 876 [45056/54000 (83%)] Loss: -1803.943359\n",
      "    epoch          : 876\n",
      "    loss           : -1807.7500391546284\n",
      "    ess            : 8.00118184539507\n",
      "    log_marginal   : 1807.7500391546284\n",
      "    val_loss       : -1814.6338602701824\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1814.6338602701824\n",
      "Train Epoch: 877 [0/54000 (0%)] Loss: -1817.600464\n",
      "Train Epoch: 877 [11264/54000 (21%)] Loss: -1805.939331\n",
      "Train Epoch: 877 [22528/54000 (42%)] Loss: -1805.404053\n",
      "Train Epoch: 877 [33792/54000 (63%)] Loss: -1796.540771\n",
      "Train Epoch: 877 [45056/54000 (83%)] Loss: -1801.659912\n",
      "    epoch          : 877\n",
      "    loss           : -1804.5556963074882\n",
      "    ess            : 8.001181818404287\n",
      "    log_marginal   : 1804.5556963074882\n",
      "    val_loss       : -1808.1831563313801\n",
      "    val_ess        : 8.001182635625204\n",
      "    val_log_marginal: 1808.1831563313801\n",
      "Train Epoch: 878 [0/54000 (0%)] Loss: -1817.211548\n",
      "Train Epoch: 878 [11264/54000 (21%)] Loss: -1817.757812\n",
      "Train Epoch: 878 [22528/54000 (42%)] Loss: -1816.601074\n",
      "Train Epoch: 878 [33792/54000 (63%)] Loss: -1815.120850\n",
      "Train Epoch: 878 [45056/54000 (83%)] Loss: -1795.969238\n",
      "    epoch          : 878\n",
      "    loss           : -1810.9650187942218\n",
      "    ess            : 8.001181296582493\n",
      "    log_marginal   : 1810.9650187942218\n",
      "    val_loss       : -1804.9063924153645\n",
      "    val_ess        : 8.001182635625204\n",
      "    val_log_marginal: 1804.9063924153645\n",
      "Train Epoch: 879 [0/54000 (0%)] Loss: -1807.853638\n",
      "Train Epoch: 879 [11264/54000 (21%)] Loss: -1803.711426\n",
      "Train Epoch: 879 [22528/54000 (42%)] Loss: -1815.932373\n",
      "Train Epoch: 879 [33792/54000 (63%)] Loss: -1802.909302\n",
      "Train Epoch: 879 [45056/54000 (83%)] Loss: -1815.395752\n",
      "    epoch          : 879\n",
      "    loss           : -1809.1819331331073\n",
      "    ess            : 8.001181575487244\n",
      "    log_marginal   : 1809.1819331331073\n",
      "    val_loss       : -1814.4457499186199\n",
      "    val_ess        : 8.001182079315186\n",
      "    val_log_marginal: 1814.4457499186199\n",
      "Train Epoch: 880 [0/54000 (0%)] Loss: -1813.772095\n",
      "Train Epoch: 880 [11264/54000 (21%)] Loss: -1819.056030\n",
      "Train Epoch: 880 [22528/54000 (42%)] Loss: -1811.221436\n",
      "Train Epoch: 880 [33792/54000 (63%)] Loss: -1794.172363\n",
      "Train Epoch: 880 [45056/54000 (83%)] Loss: -1810.863403\n",
      "    epoch          : 880\n",
      "    loss           : -1808.5147290499706\n",
      "    ess            : 8.001181566490317\n",
      "    log_marginal   : 1808.5147290499706\n",
      "    val_loss       : -1808.9649556477864\n",
      "    val_ess        : 8.001183907190958\n",
      "    val_log_marginal: 1808.9649556477864\n",
      "Train Epoch: 881 [0/54000 (0%)] Loss: -1820.909912\n",
      "Train Epoch: 881 [11264/54000 (21%)] Loss: -1810.467896\n",
      "Train Epoch: 881 [22528/54000 (42%)] Loss: -1797.456787\n",
      "Train Epoch: 881 [33792/54000 (63%)] Loss: -1804.602539\n",
      "Train Epoch: 881 [45056/54000 (83%)] Loss: -1798.471191\n",
      "    epoch          : 881\n",
      "    loss           : -1803.7316353276092\n",
      "    ess            : 8.001181296582493\n",
      "    log_marginal   : 1803.7316364792157\n",
      "    val_loss       : -1805.3646443684895\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1805.3646443684895\n",
      "Train Epoch: 882 [0/54000 (0%)] Loss: -1819.401733\n",
      "Train Epoch: 882 [11264/54000 (21%)] Loss: -1817.284912\n",
      "Train Epoch: 882 [22528/54000 (42%)] Loss: -1803.507568\n",
      "Train Epoch: 882 [33792/54000 (63%)] Loss: -1801.453003\n",
      "Train Epoch: 882 [45056/54000 (83%)] Loss: -1802.044678\n",
      "    epoch          : 882\n",
      "    loss           : -1805.7586508696934\n",
      "    ess            : 8.001181656459593\n",
      "    log_marginal   : 1805.7586508696934\n",
      "    val_loss       : -1800.1832682291667\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1800.1832682291667\n",
      "Train Epoch: 883 [0/54000 (0%)] Loss: -1818.354858\n",
      "Train Epoch: 883 [11264/54000 (21%)] Loss: -1817.020874\n",
      "Train Epoch: 883 [22528/54000 (42%)] Loss: -1812.278198\n",
      "Train Epoch: 883 [33792/54000 (63%)] Loss: -1812.563843\n",
      "Train Epoch: 883 [45056/54000 (83%)] Loss: -1808.810181\n",
      "    epoch          : 883\n",
      "    loss           : -1811.4965451798348\n",
      "    ess            : 8.001181314576346\n",
      "    log_marginal   : 1811.4965451798348\n",
      "    val_loss       : -1804.9986368815105\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1804.9986165364583\n",
      "Train Epoch: 884 [0/54000 (0%)] Loss: -1812.465332\n",
      "Train Epoch: 884 [11264/54000 (21%)] Loss: -1809.086670\n",
      "Train Epoch: 884 [22528/54000 (42%)] Loss: -1801.700928\n",
      "Train Epoch: 884 [33792/54000 (63%)] Loss: -1814.254883\n",
      "Train Epoch: 884 [45056/54000 (83%)] Loss: -1801.509277\n",
      "    epoch          : 884\n",
      "    loss           : -1806.5720606390034\n",
      "    ess            : 8.001180927708464\n",
      "    log_marginal   : 1806.5720617906102\n",
      "    val_loss       : -1811.6621907552083\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1811.6621907552083\n",
      "Train Epoch: 885 [0/54000 (0%)] Loss: -1821.308105\n",
      "Train Epoch: 885 [11264/54000 (21%)] Loss: -1813.224365\n",
      "Train Epoch: 885 [22528/54000 (42%)] Loss: -1784.117920\n",
      "Train Epoch: 885 [33792/54000 (63%)] Loss: -1805.360107\n",
      "Train Epoch: 885 [45056/54000 (83%)] Loss: -1807.644775\n",
      "    epoch          : 885\n",
      "    loss           : -1803.3223819372788\n",
      "    ess            : 8.001181962355128\n",
      "    log_marginal   : 1803.3223819372788\n",
      "    val_loss       : -1807.2794596354167\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1807.2794596354167\n",
      "Train Epoch: 886 [0/54000 (0%)] Loss: -1819.351074\n",
      "Train Epoch: 886 [11264/54000 (21%)] Loss: -1824.072266\n",
      "Train Epoch: 886 [22528/54000 (42%)] Loss: -1809.830444\n",
      "Train Epoch: 886 [33792/54000 (63%)] Loss: -1811.553223\n",
      "Train Epoch: 886 [45056/54000 (83%)] Loss: -1800.757446\n",
      "    epoch          : 886\n",
      "    loss           : -1812.1483062168338\n",
      "    ess            : 8.001180657800639\n",
      "    log_marginal   : 1812.1483062168338\n",
      "    val_loss       : -1805.3297322591145\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1805.3297322591145\n",
      "Train Epoch: 887 [0/54000 (0%)] Loss: -1811.649170\n",
      "Train Epoch: 887 [11264/54000 (21%)] Loss: -1800.450439\n",
      "Train Epoch: 887 [22528/54000 (42%)] Loss: -1808.108643\n",
      "Train Epoch: 887 [33792/54000 (63%)] Loss: -1809.416626\n",
      "Train Epoch: 887 [45056/54000 (83%)] Loss: -1807.115967\n",
      "    epoch          : 887\n",
      "    loss           : -1801.996941332547\n",
      "    ess            : 8.001181926367417\n",
      "    log_marginal   : 1801.996941332547\n",
      "    val_loss       : -1804.7155049641926\n",
      "    val_ess        : 8.001182953516642\n",
      "    val_log_marginal: 1804.7155049641926\n",
      "Train Epoch: 888 [0/54000 (0%)] Loss: -1817.885742\n",
      "Train Epoch: 888 [11264/54000 (21%)] Loss: -1819.436768\n",
      "Train Epoch: 888 [22528/54000 (42%)] Loss: -1792.297485\n",
      "Train Epoch: 888 [33792/54000 (63%)] Loss: -1799.087891\n",
      "Train Epoch: 888 [45056/54000 (83%)] Loss: -1792.660889\n",
      "    epoch          : 888\n",
      "    loss           : -1801.3167148806015\n",
      "    ess            : 8.001182475179997\n",
      "    log_marginal   : 1801.3167160322082\n",
      "    val_loss       : -1798.4043273925781\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1798.4043273925781\n",
      "Train Epoch: 889 [0/54000 (0%)] Loss: -1817.257568\n",
      "Train Epoch: 889 [11264/54000 (21%)] Loss: -1809.688721\n",
      "Train Epoch: 889 [22528/54000 (42%)] Loss: -1814.248779\n",
      "Train Epoch: 889 [33792/54000 (63%)] Loss: -1813.601074\n",
      "Train Epoch: 889 [45056/54000 (83%)] Loss: -1801.476562\n",
      "    epoch          : 889\n",
      "    loss           : -1810.5945664891656\n",
      "    ess            : 8.001181278588637\n",
      "    log_marginal   : 1810.5945676407723\n",
      "    val_loss       : -1808.2181498209636\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1808.2181498209636\n",
      "Train Epoch: 890 [0/54000 (0%)] Loss: -1822.265381\n",
      "Train Epoch: 890 [11264/54000 (21%)] Loss: -1818.755249\n",
      "Train Epoch: 890 [22528/54000 (42%)] Loss: -1782.759766\n",
      "Train Epoch: 890 [33792/54000 (63%)] Loss: -1794.487427\n",
      "Train Epoch: 890 [45056/54000 (83%)] Loss: -1775.198608\n",
      "    epoch          : 890\n",
      "    loss           : -1795.8445538214917\n",
      "    ess            : 8.001181872385853\n",
      "    log_marginal   : 1795.8445538214917\n",
      "    val_loss       : -1780.8740132649739\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1780.8739929199219\n",
      "Train Epoch: 891 [0/54000 (0%)] Loss: -1792.092163\n",
      "Train Epoch: 891 [11264/54000 (21%)] Loss: -1785.807373\n",
      "Train Epoch: 891 [22528/54000 (42%)] Loss: -1768.693359\n",
      "Train Epoch: 891 [33792/54000 (63%)] Loss: -1770.006348\n",
      "Train Epoch: 891 [45056/54000 (83%)] Loss: -1787.810303\n",
      "    epoch          : 891\n",
      "    loss           : -1776.5882821712853\n",
      "    ess            : 8.001184211587006\n",
      "    log_marginal   : 1776.5882821712853\n",
      "    val_loss       : -1803.7052001953125\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1803.7052001953125\n",
      "Train Epoch: 892 [0/54000 (0%)] Loss: -1803.751221\n",
      "Train Epoch: 892 [11264/54000 (21%)] Loss: -1789.794678\n",
      "Train Epoch: 892 [22528/54000 (42%)] Loss: -1792.717041\n",
      "Train Epoch: 892 [33792/54000 (63%)] Loss: -1789.228882\n",
      "Train Epoch: 892 [45056/54000 (83%)] Loss: -1781.190796\n",
      "    epoch          : 892\n",
      "    loss           : -1792.7286595758403\n",
      "    ess            : 8.001182106305968\n",
      "    log_marginal   : 1792.7286595758403\n",
      "    val_loss       : -1808.7085673014324\n",
      "    val_ess        : 8.00118350982666\n",
      "    val_log_marginal: 1808.7085876464844\n",
      "Train Epoch: 893 [0/54000 (0%)] Loss: -1815.631592\n",
      "Train Epoch: 893 [11264/54000 (21%)] Loss: -1817.092163\n",
      "Train Epoch: 893 [22528/54000 (42%)] Loss: -1805.047363\n",
      "Train Epoch: 893 [33792/54000 (63%)] Loss: -1800.163818\n",
      "Train Epoch: 893 [45056/54000 (83%)] Loss: -1797.315308\n",
      "    epoch          : 893\n",
      "    loss           : -1809.0978646908166\n",
      "    ess            : 8.001182286244518\n",
      "    log_marginal   : 1809.09786699403\n",
      "    val_loss       : -1811.2030232747395\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1811.2030029296875\n",
      "Train Epoch: 894 [0/54000 (0%)] Loss: -1823.332275\n",
      "Train Epoch: 894 [11264/54000 (21%)] Loss: -1819.636475\n",
      "Train Epoch: 894 [22528/54000 (42%)] Loss: -1805.522461\n",
      "Train Epoch: 894 [33792/54000 (63%)] Loss: -1808.164795\n",
      "Train Epoch: 894 [45056/54000 (83%)] Loss: -1808.463135\n",
      "    epoch          : 894\n",
      "    loss           : -1810.0921043539947\n",
      "    ess            : 8.001182358219939\n",
      "    log_marginal   : 1810.0921020507812\n",
      "    val_loss       : -1810.1987406412761\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1810.1987406412761\n",
      "Train Epoch: 895 [0/54000 (0%)] Loss: -1813.344482\n",
      "Train Epoch: 895 [11264/54000 (21%)] Loss: -1808.070068\n",
      "Train Epoch: 895 [22528/54000 (42%)] Loss: -1790.804932\n",
      "Train Epoch: 895 [33792/54000 (63%)] Loss: -1805.165039\n",
      "Train Epoch: 895 [45056/54000 (83%)] Loss: -1803.697266\n",
      "    epoch          : 895\n",
      "    loss           : -1803.995369389372\n",
      "    ess            : 8.001181890379709\n",
      "    log_marginal   : 1803.995369389372\n",
      "    val_loss       : -1802.0661722819011\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1802.0661926269531\n",
      "Train Epoch: 896 [0/54000 (0%)] Loss: -1814.084839\n",
      "Train Epoch: 896 [11264/54000 (21%)] Loss: -1813.148071\n",
      "Train Epoch: 896 [22528/54000 (42%)] Loss: -1818.466064\n",
      "Train Epoch: 896 [33792/54000 (63%)] Loss: -1814.015259\n",
      "Train Epoch: 896 [45056/54000 (83%)] Loss: -1814.744385\n",
      "    epoch          : 896\n",
      "    loss           : -1815.3453864331516\n",
      "    ess            : 8.001180891720754\n",
      "    log_marginal   : 1815.345388736365\n",
      "    val_loss       : -1810.8092549641926\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1810.8092549641926\n",
      "Train Epoch: 897 [0/54000 (0%)] Loss: -1821.014648\n",
      "Train Epoch: 897 [11264/54000 (21%)] Loss: -1812.144897\n",
      "Train Epoch: 897 [22528/54000 (42%)] Loss: -1809.055176\n",
      "Train Epoch: 897 [33792/54000 (63%)] Loss: -1800.907959\n",
      "Train Epoch: 897 [45056/54000 (83%)] Loss: -1790.761230\n",
      "    epoch          : 897\n",
      "    loss           : -1805.0395864810584\n",
      "    ess            : 8.001182088312113\n",
      "    log_marginal   : 1805.0395864810584\n",
      "    val_loss       : -1806.1483662923176\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1806.1483662923176\n",
      "Train Epoch: 898 [0/54000 (0%)] Loss: -1810.555908\n",
      "Train Epoch: 898 [11264/54000 (21%)] Loss: -1811.666870\n",
      "Train Epoch: 898 [22528/54000 (42%)] Loss: -1809.536133\n",
      "Train Epoch: 898 [33792/54000 (63%)] Loss: -1805.535156\n",
      "Train Epoch: 898 [45056/54000 (83%)] Loss: -1801.475098\n",
      "    epoch          : 898\n",
      "    loss           : -1807.267168153007\n",
      "    ess            : 8.001181611474955\n",
      "    log_marginal   : 1807.267168153007\n",
      "    val_loss       : -1805.8352355957031\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1805.8352355957031\n",
      "Train Epoch: 899 [0/54000 (0%)] Loss: -1818.320801\n",
      "Train Epoch: 899 [11264/54000 (21%)] Loss: -1812.515381\n",
      "Train Epoch: 899 [22528/54000 (42%)] Loss: -1824.455322\n",
      "Train Epoch: 899 [33792/54000 (63%)] Loss: -1807.034302\n",
      "Train Epoch: 899 [45056/54000 (83%)] Loss: -1820.007812\n",
      "    epoch          : 899\n",
      "    loss           : -1811.7201791439416\n",
      "    ess            : 8.001181746428868\n",
      "    log_marginal   : 1811.7201791439416\n",
      "    val_loss       : -1817.7832946777344\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1817.7832946777344\n",
      "Train Epoch: 900 [0/54000 (0%)] Loss: -1823.727539\n",
      "Train Epoch: 900 [11264/54000 (21%)] Loss: -1821.519043\n",
      "Train Epoch: 900 [22528/54000 (42%)] Loss: -1811.238647\n",
      "Train Epoch: 900 [33792/54000 (63%)] Loss: -1810.254028\n",
      "Train Epoch: 900 [45056/54000 (83%)] Loss: -1810.986938\n",
      "    epoch          : 900\n",
      "    loss           : -1812.1086264556309\n",
      "    ess            : 8.001181620471883\n",
      "    log_marginal   : 1812.1086276072376\n",
      "    val_loss       : -1807.3943990071614\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1807.3943990071614\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch900.pth ...\n",
      "Train Epoch: 901 [0/54000 (0%)] Loss: -1814.573242\n",
      "Train Epoch: 901 [11264/54000 (21%)] Loss: -1809.326782\n",
      "Train Epoch: 901 [22528/54000 (42%)] Loss: -1808.408447\n",
      "Train Epoch: 901 [33792/54000 (63%)] Loss: -1802.080444\n",
      "Train Epoch: 901 [45056/54000 (83%)] Loss: -1806.989258\n",
      "    epoch          : 901\n",
      "    loss           : -1805.4110326227153\n",
      "    ess            : 8.001181944361273\n",
      "    log_marginal   : 1805.4110337743218\n",
      "    val_loss       : -1803.7376200358074\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1803.7376200358074\n",
      "Train Epoch: 902 [0/54000 (0%)] Loss: -1813.069214\n",
      "Train Epoch: 902 [11264/54000 (21%)] Loss: -1810.561157\n",
      "Train Epoch: 902 [22528/54000 (42%)] Loss: -1804.872314\n",
      "Train Epoch: 902 [33792/54000 (63%)] Loss: -1804.133789\n",
      "Train Epoch: 902 [45056/54000 (83%)] Loss: -1794.693115\n",
      "    epoch          : 902\n",
      "    loss           : -1803.9337561265477\n",
      "    ess            : 8.001181035671594\n",
      "    log_marginal   : 1803.9337561265477\n",
      "    val_loss       : -1808.8274841308594\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1808.8274841308594\n",
      "Train Epoch: 903 [0/54000 (0%)] Loss: -1817.926758\n",
      "Train Epoch: 903 [11264/54000 (21%)] Loss: -1824.287231\n",
      "Train Epoch: 903 [22528/54000 (42%)] Loss: -1808.544678\n",
      "Train Epoch: 903 [33792/54000 (63%)] Loss: -1811.239014\n",
      "Train Epoch: 903 [45056/54000 (83%)] Loss: -1807.572876\n",
      "    epoch          : 903\n",
      "    loss           : -1809.6866305369251\n",
      "    ess            : 8.001181287585565\n",
      "    log_marginal   : 1809.6866316885319\n",
      "    val_loss       : -1809.9358520507812\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1809.9358723958333\n",
      "Train Epoch: 904 [0/54000 (0%)] Loss: -1822.836792\n",
      "Train Epoch: 904 [11264/54000 (21%)] Loss: -1817.837158\n",
      "Train Epoch: 904 [22528/54000 (42%)] Loss: -1809.241455\n",
      "Train Epoch: 904 [33792/54000 (63%)] Loss: -1806.009521\n",
      "Train Epoch: 904 [45056/54000 (83%)] Loss: -1796.684204\n",
      "    epoch          : 904\n",
      "    loss           : -1811.926633438974\n",
      "    ess            : 8.001181296582493\n",
      "    log_marginal   : 1811.926633438974\n",
      "    val_loss       : -1814.4940592447917\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1814.4940592447917\n",
      "Train Epoch: 905 [0/54000 (0%)] Loss: -1821.369019\n",
      "Train Epoch: 905 [11264/54000 (21%)] Loss: -1806.071167\n",
      "Train Epoch: 905 [22528/54000 (42%)] Loss: -1801.734863\n",
      "Train Epoch: 905 [33792/54000 (63%)] Loss: -1801.041748\n",
      "Train Epoch: 905 [45056/54000 (83%)] Loss: -1803.210693\n",
      "    epoch          : 905\n",
      "    loss           : -1807.9559026754127\n",
      "    ess            : 8.001181314576346\n",
      "    log_marginal   : 1807.955901523806\n",
      "    val_loss       : -1807.5943705240886\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1807.5943806966145\n",
      "Train Epoch: 906 [0/54000 (0%)] Loss: -1807.666992\n",
      "Train Epoch: 906 [11264/54000 (21%)] Loss: -1822.607178\n",
      "Train Epoch: 906 [22528/54000 (42%)] Loss: -1806.843018\n",
      "Train Epoch: 906 [33792/54000 (63%)] Loss: -1811.121948\n",
      "Train Epoch: 906 [45056/54000 (83%)] Loss: -1801.705933\n",
      "    epoch          : 906\n",
      "    loss           : -1808.8281549417748\n",
      "    ess            : 8.001181548496461\n",
      "    log_marginal   : 1808.8281549417748\n",
      "    val_loss       : -1804.2522481282551\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1804.2522583007812\n",
      "Train Epoch: 907 [0/54000 (0%)] Loss: -1821.596924\n",
      "Train Epoch: 907 [11264/54000 (21%)] Loss: -1772.430908\n",
      "Train Epoch: 907 [22528/54000 (42%)] Loss: -1778.019531\n",
      "Train Epoch: 907 [33792/54000 (63%)] Loss: -1793.802856\n",
      "Train Epoch: 907 [45056/54000 (83%)] Loss: -1780.554565\n",
      "    epoch          : 907\n",
      "    loss           : -1787.1789147718898\n",
      "    ess            : 8.001182439192286\n",
      "    log_marginal   : 1787.1789147718898\n",
      "    val_loss       : -1793.1619669596355\n",
      "    val_ess        : 8.001184066136679\n",
      "    val_log_marginal: 1793.1619669596355\n",
      "Train Epoch: 908 [0/54000 (0%)] Loss: -1798.926025\n",
      "Train Epoch: 908 [11264/54000 (21%)] Loss: -1803.738892\n",
      "Train Epoch: 908 [22528/54000 (42%)] Loss: -1795.362305\n",
      "Train Epoch: 908 [33792/54000 (63%)] Loss: -1789.173828\n",
      "Train Epoch: 908 [45056/54000 (83%)] Loss: -1794.241455\n",
      "    epoch          : 908\n",
      "    loss           : -1792.9003376510907\n",
      "    ess            : 8.001182961014083\n",
      "    log_marginal   : 1792.9003376510907\n",
      "    val_loss       : -1808.6290791829426\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1808.6290791829426\n",
      "Train Epoch: 909 [0/54000 (0%)] Loss: -1815.196045\n",
      "Train Epoch: 909 [11264/54000 (21%)] Loss: -1803.350342\n",
      "Train Epoch: 909 [22528/54000 (42%)] Loss: -1801.404053\n",
      "Train Epoch: 909 [33792/54000 (63%)] Loss: -1798.802979\n",
      "Train Epoch: 909 [45056/54000 (83%)] Loss: -1791.000610\n",
      "    epoch          : 909\n",
      "    loss           : -1802.8018706699588\n",
      "    ess            : 8.001182511167706\n",
      "    log_marginal   : 1802.801869518352\n",
      "    val_loss       : -1805.1187947591145\n",
      "    val_ess        : 8.001183350880941\n",
      "    val_log_marginal: 1805.1187845865886\n",
      "Train Epoch: 910 [0/54000 (0%)] Loss: -1830.915405\n",
      "Train Epoch: 910 [11264/54000 (21%)] Loss: -1820.192383\n",
      "Train Epoch: 910 [22528/54000 (42%)] Loss: -1806.256958\n",
      "Train Epoch: 910 [33792/54000 (63%)] Loss: -1810.391968\n",
      "Train Epoch: 910 [45056/54000 (83%)] Loss: -1803.600098\n",
      "    epoch          : 910\n",
      "    loss           : -1812.0666526938385\n",
      "    ess            : 8.001181296582493\n",
      "    log_marginal   : 1812.0666538454452\n",
      "    val_loss       : -1809.2758483886719\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1809.2758483886719\n",
      "Train Epoch: 911 [0/54000 (0%)] Loss: -1818.736572\n",
      "Train Epoch: 911 [11264/54000 (21%)] Loss: -1809.683594\n",
      "Train Epoch: 911 [22528/54000 (42%)] Loss: -1791.642578\n",
      "Train Epoch: 911 [33792/54000 (63%)] Loss: -1791.312866\n",
      "Train Epoch: 911 [45056/54000 (83%)] Loss: -1794.289062\n",
      "    epoch          : 911\n",
      "    loss           : -1801.3478186265477\n",
      "    ess            : 8.001182682109329\n",
      "    log_marginal   : 1801.3478186265477\n",
      "    val_loss       : -1805.0157267252605\n",
      "    val_ess        : 8.001184145609537\n",
      "    val_log_marginal: 1805.0157267252605\n",
      "Train Epoch: 912 [0/54000 (0%)] Loss: -1807.206665\n",
      "Train Epoch: 912 [11264/54000 (21%)] Loss: -1797.488647\n",
      "Train Epoch: 912 [22528/54000 (42%)] Loss: -1796.630249\n",
      "Train Epoch: 912 [33792/54000 (63%)] Loss: -1792.133789\n",
      "Train Epoch: 912 [45056/54000 (83%)] Loss: -1797.987061\n",
      "    epoch          : 912\n",
      "    loss           : -1800.7368613189121\n",
      "    ess            : 8.001181224607071\n",
      "    log_marginal   : 1800.7368613189121\n",
      "    val_loss       : -1807.8101704915364\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1807.8101704915364\n",
      "Train Epoch: 913 [0/54000 (0%)] Loss: -1817.517212\n",
      "Train Epoch: 913 [11264/54000 (21%)] Loss: -1811.992065\n",
      "Train Epoch: 913 [22528/54000 (42%)] Loss: -1810.169922\n",
      "Train Epoch: 913 [33792/54000 (63%)] Loss: -1809.882812\n",
      "Train Epoch: 913 [45056/54000 (83%)] Loss: -1808.314453\n",
      "    epoch          : 913\n",
      "    loss           : -1809.9018531655365\n",
      "    ess            : 8.001182466183069\n",
      "    log_marginal   : 1809.9018531655365\n",
      "    val_loss       : -1810.0435485839844\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1810.0435485839844\n",
      "Train Epoch: 914 [0/54000 (0%)] Loss: -1818.059082\n",
      "Train Epoch: 914 [11264/54000 (21%)] Loss: -1816.402954\n",
      "Train Epoch: 914 [22528/54000 (42%)] Loss: -1792.436523\n",
      "Train Epoch: 914 [33792/54000 (63%)] Loss: -1786.909180\n",
      "Train Epoch: 914 [45056/54000 (83%)] Loss: -1779.618042\n",
      "    epoch          : 914\n",
      "    loss           : -1799.1537613778744\n",
      "    ess            : 8.001183167943415\n",
      "    log_marginal   : 1799.1537602262677\n",
      "    val_loss       : -1808.6268819173176\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1808.6268819173176\n",
      "Train Epoch: 915 [0/54000 (0%)] Loss: -1810.568359\n",
      "Train Epoch: 915 [11264/54000 (21%)] Loss: -1809.382080\n",
      "Train Epoch: 915 [22528/54000 (42%)] Loss: -1805.511841\n",
      "Train Epoch: 915 [33792/54000 (63%)] Loss: -1793.721924\n",
      "Train Epoch: 915 [45056/54000 (83%)] Loss: -1808.007812\n",
      "    epoch          : 915\n",
      "    loss           : -1800.541239036704\n",
      "    ess            : 8.001183176940343\n",
      "    log_marginal   : 1800.5412378850972\n",
      "    val_loss       : -1806.4886779785156\n",
      "    val_ess        : 8.001183907190958\n",
      "    val_log_marginal: 1806.4886678059895\n",
      "Train Epoch: 916 [0/54000 (0%)] Loss: -1811.533936\n",
      "Train Epoch: 916 [11264/54000 (21%)] Loss: -1816.121094\n",
      "Train Epoch: 916 [22528/54000 (42%)] Loss: -1813.112793\n",
      "Train Epoch: 916 [33792/54000 (63%)] Loss: -1797.857300\n",
      "Train Epoch: 916 [45056/54000 (83%)] Loss: -1803.177124\n",
      "    epoch          : 916\n",
      "    loss           : -1809.0463556253685\n",
      "    ess            : 8.001182574146199\n",
      "    log_marginal   : 1809.0463556253685\n",
      "    val_loss       : -1811.6454976399739\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1811.6455078125\n",
      "Train Epoch: 917 [0/54000 (0%)] Loss: -1815.502808\n",
      "Train Epoch: 917 [11264/54000 (21%)] Loss: -1811.998535\n",
      "Train Epoch: 917 [22528/54000 (42%)] Loss: -1803.105225\n",
      "Train Epoch: 917 [33792/54000 (63%)] Loss: -1814.979492\n",
      "Train Epoch: 917 [45056/54000 (83%)] Loss: -1805.913330\n",
      "    epoch          : 917\n",
      "    loss           : -1809.7043284290241\n",
      "    ess            : 8.001182214269098\n",
      "    log_marginal   : 1809.7043284290241\n",
      "    val_loss       : -1811.3745524088542\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1811.3745422363281\n",
      "Train Epoch: 918 [0/54000 (0%)] Loss: -1827.470215\n",
      "Train Epoch: 918 [11264/54000 (21%)] Loss: -1824.945312\n",
      "Train Epoch: 918 [22528/54000 (42%)] Loss: -1809.925049\n",
      "Train Epoch: 918 [33792/54000 (63%)] Loss: -1800.348389\n",
      "Train Epoch: 918 [45056/54000 (83%)] Loss: -1798.506836\n",
      "    epoch          : 918\n",
      "    loss           : -1809.1275795990566\n",
      "    ess            : 8.001181836398143\n",
      "    log_marginal   : 1809.1275795990566\n",
      "    val_loss       : -1805.4742533365886\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1805.4742533365886\n",
      "Train Epoch: 919 [0/54000 (0%)] Loss: -1799.494873\n",
      "Train Epoch: 919 [11264/54000 (21%)] Loss: -1805.418335\n",
      "Train Epoch: 919 [22528/54000 (42%)] Loss: -1808.086914\n",
      "Train Epoch: 919 [33792/54000 (63%)] Loss: -1800.757690\n",
      "Train Epoch: 919 [45056/54000 (83%)] Loss: -1805.088379\n",
      "    epoch          : 919\n",
      "    loss           : -1805.1831319557045\n",
      "    ess            : 8.001182160287533\n",
      "    log_marginal   : 1805.1831319557045\n",
      "    val_loss       : -1811.3982543945312\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1811.3982543945312\n",
      "Train Epoch: 920 [0/54000 (0%)] Loss: -1820.130859\n",
      "Train Epoch: 920 [11264/54000 (21%)] Loss: -1809.801025\n",
      "Train Epoch: 920 [22528/54000 (42%)] Loss: -1810.378418\n",
      "Train Epoch: 920 [33792/54000 (63%)] Loss: -1808.269043\n",
      "Train Epoch: 920 [45056/54000 (83%)] Loss: -1804.497559\n",
      "    epoch          : 920\n",
      "    loss           : -1809.7200847121906\n",
      "    ess            : 8.001181260594782\n",
      "    log_marginal   : 1809.7200847121906\n",
      "    val_loss       : -1810.6763102213542\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1810.6763203938801\n",
      "Train Epoch: 921 [0/54000 (0%)] Loss: -1820.797241\n",
      "Train Epoch: 921 [11264/54000 (21%)] Loss: -1822.893066\n",
      "Train Epoch: 921 [22528/54000 (42%)] Loss: -1810.563843\n",
      "Train Epoch: 921 [33792/54000 (63%)] Loss: -1805.315430\n",
      "Train Epoch: 921 [45056/54000 (83%)] Loss: -1798.763916\n",
      "    epoch          : 921\n",
      "    loss           : -1811.2179692106427\n",
      "    ess            : 8.001181665456519\n",
      "    log_marginal   : 1811.2179692106427\n",
      "    val_loss       : -1805.0221455891926\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1805.0221455891926\n",
      "Train Epoch: 922 [0/54000 (0%)] Loss: -1810.977539\n",
      "Train Epoch: 922 [11264/54000 (21%)] Loss: -1803.512329\n",
      "Train Epoch: 922 [22528/54000 (42%)] Loss: -1802.761841\n",
      "Train Epoch: 922 [33792/54000 (63%)] Loss: -1796.797119\n",
      "Train Epoch: 922 [45056/54000 (83%)] Loss: -1813.644775\n",
      "    epoch          : 922\n",
      "    loss           : -1804.108042591023\n",
      "    ess            : 8.00118180940736\n",
      "    log_marginal   : 1804.108042591023\n",
      "    val_loss       : -1810.4490152994792\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1810.4490152994792\n",
      "Train Epoch: 923 [0/54000 (0%)] Loss: -1815.500122\n",
      "Train Epoch: 923 [11264/54000 (21%)] Loss: -1819.495361\n",
      "Train Epoch: 923 [22528/54000 (42%)] Loss: -1801.869507\n",
      "Train Epoch: 923 [33792/54000 (63%)] Loss: -1800.583496\n",
      "Train Epoch: 923 [45056/54000 (83%)] Loss: -1802.046143\n",
      "    epoch          : 923\n",
      "    loss           : -1807.3258217865566\n",
      "    ess            : 8.001180837739188\n",
      "    log_marginal   : 1807.3258229381634\n",
      "    val_loss       : -1806.1235249837239\n",
      "    val_ess        : 8.001182635625204\n",
      "    val_log_marginal: 1806.1235453287761\n",
      "Train Epoch: 924 [0/54000 (0%)] Loss: -1818.008789\n",
      "Train Epoch: 924 [11264/54000 (21%)] Loss: -1812.694824\n",
      "Train Epoch: 924 [22528/54000 (42%)] Loss: -1806.820557\n",
      "Train Epoch: 924 [33792/54000 (63%)] Loss: -1803.616211\n",
      "Train Epoch: 924 [45056/54000 (83%)] Loss: -1802.454590\n",
      "    epoch          : 924\n",
      "    loss           : -1808.4169207878833\n",
      "    ess            : 8.00118177341965\n",
      "    log_marginal   : 1808.4169207878833\n",
      "    val_loss       : -1811.945068359375\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1811.945068359375\n",
      "Train Epoch: 925 [0/54000 (0%)] Loss: -1820.149536\n",
      "Train Epoch: 925 [11264/54000 (21%)] Loss: -1802.107056\n",
      "Train Epoch: 925 [22528/54000 (42%)] Loss: -1806.174316\n",
      "Train Epoch: 925 [33792/54000 (63%)] Loss: -1793.849976\n",
      "Train Epoch: 925 [45056/54000 (83%)] Loss: -1799.415039\n",
      "    epoch          : 925\n",
      "    loss           : -1803.2243663859817\n",
      "    ess            : 8.001181611474955\n",
      "    log_marginal   : 1803.2243686891952\n",
      "    val_loss       : -1804.0230509440105\n",
      "    val_ess        : 8.001182079315186\n",
      "    val_log_marginal: 1804.0230509440105\n",
      "Train Epoch: 926 [0/54000 (0%)] Loss: -1820.399902\n",
      "Train Epoch: 926 [11264/54000 (21%)] Loss: -1821.918457\n",
      "Train Epoch: 926 [22528/54000 (42%)] Loss: -1808.384277\n",
      "Train Epoch: 926 [33792/54000 (63%)] Loss: -1808.628052\n",
      "Train Epoch: 926 [45056/54000 (83%)] Loss: -1814.707520\n",
      "    epoch          : 926\n",
      "    loss           : -1808.2695842239093\n",
      "    ess            : 8.001181467524114\n",
      "    log_marginal   : 1808.2695853755158\n",
      "    val_loss       : -1811.5651957194011\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1811.5651957194011\n",
      "Train Epoch: 927 [0/54000 (0%)] Loss: -1821.205566\n",
      "Train Epoch: 927 [11264/54000 (21%)] Loss: -1819.655273\n",
      "Train Epoch: 927 [22528/54000 (42%)] Loss: -1805.380249\n",
      "Train Epoch: 927 [33792/54000 (63%)] Loss: -1806.698730\n",
      "Train Epoch: 927 [45056/54000 (83%)] Loss: -1815.343262\n",
      "    epoch          : 927\n",
      "    loss           : -1812.4668394844487\n",
      "    ess            : 8.001182556152344\n",
      "    log_marginal   : 1812.4668394844487\n",
      "    val_loss       : -1810.1566569010417\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1810.1566569010417\n",
      "Train Epoch: 928 [0/54000 (0%)] Loss: -1831.862061\n",
      "Train Epoch: 928 [11264/54000 (21%)] Loss: -1826.124268\n",
      "Train Epoch: 928 [22528/54000 (42%)] Loss: -1806.451172\n",
      "Train Epoch: 928 [33792/54000 (63%)] Loss: -1815.041870\n",
      "Train Epoch: 928 [45056/54000 (83%)] Loss: -1803.049561\n",
      "    epoch          : 928\n",
      "    loss           : -1811.9538608766952\n",
      "    ess            : 8.001181188619361\n",
      "    log_marginal   : 1811.9538608766952\n",
      "    val_loss       : -1806.8997701009114\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1806.8997701009114\n",
      "Train Epoch: 929 [0/54000 (0%)] Loss: -1810.857422\n",
      "Train Epoch: 929 [11264/54000 (21%)] Loss: -1800.233154\n",
      "Train Epoch: 929 [22528/54000 (42%)] Loss: -1788.061401\n",
      "Train Epoch: 929 [33792/54000 (63%)] Loss: -1800.441406\n",
      "Train Epoch: 929 [45056/54000 (83%)] Loss: -1790.248047\n",
      "    epoch          : 929\n",
      "    loss           : -1800.9938538749263\n",
      "    ess            : 8.001181962355128\n",
      "    log_marginal   : 1800.9938538749263\n",
      "    val_loss       : -1802.1107584635417\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1802.1107584635417\n",
      "Train Epoch: 930 [0/54000 (0%)] Loss: -1805.992920\n",
      "Train Epoch: 930 [11264/54000 (21%)] Loss: -1801.922119\n",
      "Train Epoch: 930 [22528/54000 (42%)] Loss: -1810.123535\n",
      "Train Epoch: 930 [33792/54000 (63%)] Loss: -1793.059570\n",
      "Train Epoch: 930 [45056/54000 (83%)] Loss: -1798.236694\n",
      "    epoch          : 930\n",
      "    loss           : -1801.412278661188\n",
      "    ess            : 8.00118101767774\n",
      "    log_marginal   : 1801.412278661188\n",
      "    val_loss       : -1810.9754231770833\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1810.9754231770833\n",
      "Train Epoch: 931 [0/54000 (0%)] Loss: -1812.986206\n",
      "Train Epoch: 931 [11264/54000 (21%)] Loss: -1821.815552\n",
      "Train Epoch: 931 [22528/54000 (42%)] Loss: -1802.019775\n",
      "Train Epoch: 931 [33792/54000 (63%)] Loss: -1804.435791\n",
      "Train Epoch: 931 [45056/54000 (83%)] Loss: -1805.864014\n",
      "    epoch          : 931\n",
      "    loss           : -1810.1614644752358\n",
      "    ess            : 8.001180423880523\n",
      "    log_marginal   : 1810.161463323629\n",
      "    val_loss       : -1811.7144165039062\n",
      "    val_ess        : 8.00118088722229\n",
      "    val_log_marginal: 1811.7144165039062\n",
      "Train Epoch: 932 [0/54000 (0%)] Loss: -1829.921143\n",
      "Train Epoch: 932 [11264/54000 (21%)] Loss: -1813.523071\n",
      "Train Epoch: 932 [22528/54000 (42%)] Loss: -1815.813477\n",
      "Train Epoch: 932 [33792/54000 (63%)] Loss: -1793.089600\n",
      "Train Epoch: 932 [45056/54000 (83%)] Loss: -1797.476562\n",
      "    epoch          : 932\n",
      "    loss           : -1803.9068833836968\n",
      "    ess            : 8.00118082874226\n",
      "    log_marginal   : 1803.9068822320903\n",
      "    val_loss       : -1804.0118815104167\n",
      "    val_ess        : 8.001182317733765\n",
      "    val_log_marginal: 1804.0118815104167\n",
      "Train Epoch: 933 [0/54000 (0%)] Loss: -1819.928711\n",
      "Train Epoch: 933 [11264/54000 (21%)] Loss: -1816.408203\n",
      "Train Epoch: 933 [22528/54000 (42%)] Loss: -1800.088379\n",
      "Train Epoch: 933 [33792/54000 (63%)] Loss: -1799.877808\n",
      "Train Epoch: 933 [45056/54000 (83%)] Loss: -1798.871826\n",
      "    epoch          : 933\n",
      "    loss           : -1801.258747604658\n",
      "    ess            : 8.001182592140054\n",
      "    log_marginal   : 1801.258747604658\n",
      "    val_loss       : -1802.8028869628906\n",
      "    val_ess        : 8.001184066136679\n",
      "    val_log_marginal: 1802.8028869628906\n",
      "Train Epoch: 934 [0/54000 (0%)] Loss: -1815.123291\n",
      "Train Epoch: 934 [11264/54000 (21%)] Loss: -1817.014893\n",
      "Train Epoch: 934 [22528/54000 (42%)] Loss: -1812.186157\n",
      "Train Epoch: 934 [33792/54000 (63%)] Loss: -1814.852295\n",
      "Train Epoch: 934 [45056/54000 (83%)] Loss: -1815.862549\n",
      "    epoch          : 934\n",
      "    loss           : -1813.168299261129\n",
      "    ess            : 8.001180225948119\n",
      "    log_marginal   : 1813.168299261129\n",
      "    val_loss       : -1815.9729410807292\n",
      "    val_ess        : 8.001181523005167\n",
      "    val_log_marginal: 1815.9729410807292\n",
      "Train Epoch: 935 [0/54000 (0%)] Loss: -1827.270020\n",
      "Train Epoch: 935 [11264/54000 (21%)] Loss: -1820.532349\n",
      "Train Epoch: 935 [22528/54000 (42%)] Loss: -1791.683472\n",
      "Train Epoch: 935 [33792/54000 (63%)] Loss: -1814.388794\n",
      "Train Epoch: 935 [45056/54000 (83%)] Loss: -1809.091675\n",
      "    epoch          : 935\n",
      "    loss           : -1807.7523066682636\n",
      "    ess            : 8.001181305579419\n",
      "    log_marginal   : 1807.7523066682636\n",
      "    val_loss       : -1807.4475708007812\n",
      "    val_ess        : 8.001182476679483\n",
      "    val_log_marginal: 1807.4475606282551\n",
      "Train Epoch: 936 [0/54000 (0%)] Loss: -1817.395996\n",
      "Train Epoch: 936 [11264/54000 (21%)] Loss: -1806.370728\n",
      "Train Epoch: 936 [22528/54000 (42%)] Loss: -1789.702637\n",
      "Train Epoch: 936 [33792/54000 (63%)] Loss: -1792.358643\n",
      "Train Epoch: 936 [45056/54000 (83%)] Loss: -1792.898682\n",
      "    epoch          : 936\n",
      "    loss           : -1801.475494960569\n",
      "    ess            : 8.001182430195358\n",
      "    log_marginal   : 1801.475494960569\n",
      "    val_loss       : -1799.6460367838542\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1799.6460367838542\n",
      "Train Epoch: 937 [0/54000 (0%)] Loss: -1806.967529\n",
      "Train Epoch: 937 [11264/54000 (21%)] Loss: -1811.291260\n",
      "Train Epoch: 937 [22528/54000 (42%)] Loss: -1808.221069\n",
      "Train Epoch: 937 [33792/54000 (63%)] Loss: -1800.510132\n",
      "Train Epoch: 937 [45056/54000 (83%)] Loss: -1804.119141\n",
      "    epoch          : 937\n",
      "    loss           : -1808.2994062315743\n",
      "    ess            : 8.00118007300035\n",
      "    log_marginal   : 1808.2994062315743\n",
      "    val_loss       : -1808.902323404948\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1808.902323404948\n",
      "Train Epoch: 938 [0/54000 (0%)] Loss: -1827.958008\n",
      "Train Epoch: 938 [11264/54000 (21%)] Loss: -1820.088623\n",
      "Train Epoch: 938 [22528/54000 (42%)] Loss: -1814.576660\n",
      "Train Epoch: 938 [33792/54000 (63%)] Loss: -1811.670166\n",
      "Train Epoch: 938 [45056/54000 (83%)] Loss: -1796.364258\n",
      "    epoch          : 938\n",
      "    loss           : -1811.4537215322819\n",
      "    ess            : 8.00117971312325\n",
      "    log_marginal   : 1811.4537215322819\n",
      "    val_loss       : -1811.2852986653645\n",
      "    val_ess        : 8.001181920369467\n",
      "    val_log_marginal: 1811.2852986653645\n",
      "Train Epoch: 939 [0/54000 (0%)] Loss: -1821.336548\n",
      "Train Epoch: 939 [11264/54000 (21%)] Loss: -1813.051514\n",
      "Train Epoch: 939 [22528/54000 (42%)] Loss: -1806.460449\n",
      "Train Epoch: 939 [33792/54000 (63%)] Loss: -1785.003540\n",
      "Train Epoch: 939 [45056/54000 (83%)] Loss: -1792.286987\n",
      "    epoch          : 939\n",
      "    loss           : -1803.861642513635\n",
      "    ess            : 8.00118069378835\n",
      "    log_marginal   : 1803.861642513635\n",
      "    val_loss       : -1795.3641153971355\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1795.3641153971355\n",
      "Train Epoch: 940 [0/54000 (0%)] Loss: -1809.392944\n",
      "Train Epoch: 940 [11264/54000 (21%)] Loss: -1774.724609\n",
      "Train Epoch: 940 [22528/54000 (42%)] Loss: -1769.234131\n",
      "Train Epoch: 940 [33792/54000 (63%)] Loss: -1776.684448\n",
      "Train Epoch: 940 [45056/54000 (83%)] Loss: -1775.411133\n",
      "    epoch          : 940\n",
      "    loss           : -1777.7081310344192\n",
      "    ess            : 8.001184517482542\n",
      "    log_marginal   : 1777.7081310344192\n",
      "    val_loss       : -1796.1710917154949\n",
      "    val_ess        : 8.001185258229574\n",
      "    val_log_marginal: 1796.1710917154949\n",
      "Train Epoch: 941 [0/54000 (0%)] Loss: -1804.119629\n",
      "Train Epoch: 941 [11264/54000 (21%)] Loss: -1801.561768\n",
      "Train Epoch: 941 [22528/54000 (42%)] Loss: -1789.219482\n",
      "Train Epoch: 941 [33792/54000 (63%)] Loss: -1789.491211\n",
      "Train Epoch: 941 [45056/54000 (83%)] Loss: -1782.441284\n",
      "    epoch          : 941\n",
      "    loss           : -1795.2668399450913\n",
      "    ess            : 8.001182736090893\n",
      "    log_marginal   : 1795.2668387934846\n",
      "    val_loss       : -1800.099833170573\n",
      "    val_ess        : 8.001183907190958\n",
      "    val_log_marginal: 1800.0998128255208\n",
      "Train Epoch: 942 [0/54000 (0%)] Loss: -1813.910645\n",
      "Train Epoch: 942 [11264/54000 (21%)] Loss: -1813.458984\n",
      "Train Epoch: 942 [22528/54000 (42%)] Loss: -1813.233521\n",
      "Train Epoch: 942 [33792/54000 (63%)] Loss: -1804.804199\n",
      "Train Epoch: 942 [45056/54000 (83%)] Loss: -1798.026978\n",
      "    epoch          : 942\n",
      "    loss           : -1807.4351403578273\n",
      "    ess            : 8.001182790072459\n",
      "    log_marginal   : 1807.4351403578273\n",
      "    val_loss       : -1811.3906656901042\n",
      "    val_ess        : 8.001184542973837\n",
      "    val_log_marginal: 1811.3906656901042\n",
      "Train Epoch: 943 [0/54000 (0%)] Loss: -1814.941284\n",
      "Train Epoch: 943 [11264/54000 (21%)] Loss: -1821.918823\n",
      "Train Epoch: 943 [22528/54000 (42%)] Loss: -1805.283936\n",
      "Train Epoch: 943 [33792/54000 (63%)] Loss: -1806.428345\n",
      "Train Epoch: 943 [45056/54000 (83%)] Loss: -1796.715820\n",
      "    epoch          : 943\n",
      "    loss           : -1808.8234609927772\n",
      "    ess            : 8.001182241259881\n",
      "    log_marginal   : 1808.8234609927772\n",
      "    val_loss       : -1804.0741577148438\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1804.0741780598958\n",
      "Train Epoch: 944 [0/54000 (0%)] Loss: -1823.810791\n",
      "Train Epoch: 944 [11264/54000 (21%)] Loss: -1810.770752\n",
      "Train Epoch: 944 [22528/54000 (42%)] Loss: -1815.781250\n",
      "Train Epoch: 944 [33792/54000 (63%)] Loss: -1804.712158\n",
      "Train Epoch: 944 [45056/54000 (83%)] Loss: -1806.930054\n",
      "    epoch          : 944\n",
      "    loss           : -1811.250537800339\n",
      "    ess            : 8.001181656459593\n",
      "    log_marginal   : 1811.250537800339\n",
      "    val_loss       : -1812.9230550130208\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1812.9230550130208\n",
      "Train Epoch: 945 [0/54000 (0%)] Loss: -1822.283447\n",
      "Train Epoch: 945 [11264/54000 (21%)] Loss: -1817.079956\n",
      "Train Epoch: 945 [22528/54000 (42%)] Loss: -1799.382446\n",
      "Train Epoch: 945 [33792/54000 (63%)] Loss: -1800.559082\n",
      "Train Epoch: 945 [45056/54000 (83%)] Loss: -1794.116821\n",
      "    epoch          : 945\n",
      "    loss           : -1806.9422803195018\n",
      "    ess            : 8.001182358219939\n",
      "    log_marginal   : 1806.9422826227153\n",
      "    val_loss       : -1809.1425069173176\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1809.1425374348958\n",
      "Train Epoch: 946 [0/54000 (0%)] Loss: -1818.288086\n",
      "Train Epoch: 946 [11264/54000 (21%)] Loss: -1821.423706\n",
      "Train Epoch: 946 [22528/54000 (42%)] Loss: -1815.936890\n",
      "Train Epoch: 946 [33792/54000 (63%)] Loss: -1800.061523\n",
      "Train Epoch: 946 [45056/54000 (83%)] Loss: -1806.913696\n",
      "    epoch          : 946\n",
      "    loss           : -1809.166582215507\n",
      "    ess            : 8.001180936705392\n",
      "    log_marginal   : 1809.166582215507\n",
      "    val_loss       : -1804.3593037923176\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1804.3593037923176\n",
      "Train Epoch: 947 [0/54000 (0%)] Loss: -1822.498047\n",
      "Train Epoch: 947 [11264/54000 (21%)] Loss: -1810.087646\n",
      "Train Epoch: 947 [22528/54000 (42%)] Loss: -1805.771484\n",
      "Train Epoch: 947 [33792/54000 (63%)] Loss: -1807.919189\n",
      "Train Epoch: 947 [45056/54000 (83%)] Loss: -1800.122314\n",
      "    epoch          : 947\n",
      "    loss           : -1805.7974980192364\n",
      "    ess            : 8.00118152150568\n",
      "    log_marginal   : 1805.7974980192364\n",
      "    val_loss       : -1813.6678568522136\n",
      "    val_ess        : 8.0011834303538\n",
      "    val_log_marginal: 1813.6678771972656\n",
      "Train Epoch: 948 [0/54000 (0%)] Loss: -1812.905273\n",
      "Train Epoch: 948 [11264/54000 (21%)] Loss: -1812.214478\n",
      "Train Epoch: 948 [22528/54000 (42%)] Loss: -1800.167236\n",
      "Train Epoch: 948 [33792/54000 (63%)] Loss: -1802.441650\n",
      "Train Epoch: 948 [45056/54000 (83%)] Loss: -1804.090088\n",
      "    epoch          : 948\n",
      "    loss           : -1806.1954518444134\n",
      "    ess            : 8.00118177341965\n",
      "    log_marginal   : 1806.1954518444134\n",
      "    val_loss       : -1806.99462890625\n",
      "    val_ess        : 8.001182715098063\n",
      "    val_log_marginal: 1806.9946390787761\n",
      "Train Epoch: 949 [0/54000 (0%)] Loss: -1821.207031\n",
      "Train Epoch: 949 [11264/54000 (21%)] Loss: -1814.514893\n",
      "Train Epoch: 949 [22528/54000 (42%)] Loss: -1812.295776\n",
      "Train Epoch: 949 [33792/54000 (63%)] Loss: -1807.476807\n",
      "Train Epoch: 949 [45056/54000 (83%)] Loss: -1809.371948\n",
      "    epoch          : 949\n",
      "    loss           : -1814.3245227741745\n",
      "    ess            : 8.001182655118546\n",
      "    log_marginal   : 1814.3245216225678\n",
      "    val_loss       : -1821.3059895833333\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1821.3059895833333\n",
      "Train Epoch: 950 [0/54000 (0%)] Loss: -1831.250366\n",
      "Train Epoch: 950 [11264/54000 (21%)] Loss: -1808.707153\n",
      "Train Epoch: 950 [22528/54000 (42%)] Loss: -1787.076660\n",
      "Train Epoch: 950 [33792/54000 (63%)] Loss: -1809.685791\n",
      "Train Epoch: 950 [45056/54000 (83%)] Loss: -1796.459351\n",
      "    epoch          : 950\n",
      "    loss           : -1805.8947477520637\n",
      "    ess            : 8.001181836398143\n",
      "    log_marginal   : 1805.8947489036705\n",
      "    val_loss       : -1809.7650451660156\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1809.7650451660156\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch950.pth ...\n",
      "Train Epoch: 951 [0/54000 (0%)] Loss: -1821.412231\n",
      "Train Epoch: 951 [11264/54000 (21%)] Loss: -1811.517334\n",
      "Train Epoch: 951 [22528/54000 (42%)] Loss: -1811.715088\n",
      "Train Epoch: 951 [33792/54000 (63%)] Loss: -1807.010254\n",
      "Train Epoch: 951 [45056/54000 (83%)] Loss: -1790.072632\n",
      "    epoch          : 951\n",
      "    loss           : -1806.783943608122\n",
      "    ess            : 8.001181638465738\n",
      "    log_marginal   : 1806.783943608122\n",
      "    val_loss       : -1805.1366475423176\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1805.1366271972656\n",
      "Train Epoch: 952 [0/54000 (0%)] Loss: -1821.553955\n",
      "Train Epoch: 952 [11264/54000 (21%)] Loss: -1801.618652\n",
      "Train Epoch: 952 [22528/54000 (42%)] Loss: -1804.369995\n",
      "Train Epoch: 952 [33792/54000 (63%)] Loss: -1807.364258\n",
      "Train Epoch: 952 [45056/54000 (83%)] Loss: -1818.034180\n",
      "    epoch          : 952\n",
      "    loss           : -1808.3592667489681\n",
      "    ess            : 8.00118206132133\n",
      "    log_marginal   : 1808.3592667489681\n",
      "    val_loss       : -1818.9617513020833\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1818.9617513020833\n",
      "Train Epoch: 953 [0/54000 (0%)] Loss: -1818.574951\n",
      "Train Epoch: 953 [11264/54000 (21%)] Loss: -1809.128540\n",
      "Train Epoch: 953 [22528/54000 (42%)] Loss: -1797.042480\n",
      "Train Epoch: 953 [33792/54000 (63%)] Loss: -1801.898926\n",
      "Train Epoch: 953 [45056/54000 (83%)] Loss: -1799.682861\n",
      "    epoch          : 953\n",
      "    loss           : -1808.8688170235112\n",
      "    ess            : 8.001181305579419\n",
      "    log_marginal   : 1808.8688158719044\n",
      "    val_loss       : -1798.7030944824219\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1798.7030944824219\n",
      "Train Epoch: 954 [0/54000 (0%)] Loss: -1799.824707\n",
      "Train Epoch: 954 [11264/54000 (21%)] Loss: -1782.046265\n",
      "Train Epoch: 954 [22528/54000 (42%)] Loss: -1766.621948\n",
      "Train Epoch: 954 [33792/54000 (63%)] Loss: -1770.306396\n",
      "Train Epoch: 954 [45056/54000 (83%)] Loss: -1789.434082\n",
      "    epoch          : 954\n",
      "    loss           : -1785.4879564969044\n",
      "    ess            : 8.001184004657674\n",
      "    log_marginal   : 1785.4879564969044\n",
      "    val_loss       : -1804.3033548990886\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1804.3033548990886\n",
      "Train Epoch: 955 [0/54000 (0%)] Loss: -1808.064209\n",
      "Train Epoch: 955 [11264/54000 (21%)] Loss: -1796.210693\n",
      "Train Epoch: 955 [22528/54000 (42%)] Loss: -1797.825562\n",
      "Train Epoch: 955 [33792/54000 (63%)] Loss: -1787.803589\n",
      "Train Epoch: 955 [45056/54000 (83%)] Loss: -1798.514282\n",
      "    epoch          : 955\n",
      "    loss           : -1798.7779333726414\n",
      "    ess            : 8.001182007339766\n",
      "    log_marginal   : 1798.7779333726414\n",
      "    val_loss       : -1811.1749471028645\n",
      "    val_ess        : 8.001184145609537\n",
      "    val_log_marginal: 1811.1749471028645\n",
      "Train Epoch: 956 [0/54000 (0%)] Loss: -1830.857666\n",
      "Train Epoch: 956 [11264/54000 (21%)] Loss: -1820.055298\n",
      "Train Epoch: 956 [22528/54000 (42%)] Loss: -1805.961426\n",
      "Train Epoch: 956 [33792/54000 (63%)] Loss: -1794.574951\n",
      "Train Epoch: 956 [45056/54000 (83%)] Loss: -1798.343018\n",
      "    epoch          : 956\n",
      "    loss           : -1812.1570411538178\n",
      "    ess            : 8.001181908373562\n",
      "    log_marginal   : 1812.1570411538178\n",
      "    val_loss       : -1809.0551147460938\n",
      "    val_ess        : 8.001185576121012\n",
      "    val_log_marginal: 1809.0550944010417\n",
      "Train Epoch: 957 [0/54000 (0%)] Loss: -1821.827393\n",
      "Train Epoch: 957 [11264/54000 (21%)] Loss: -1813.666382\n",
      "Train Epoch: 957 [22528/54000 (42%)] Loss: -1811.054810\n",
      "Train Epoch: 957 [33792/54000 (63%)] Loss: -1799.116943\n",
      "Train Epoch: 957 [45056/54000 (83%)] Loss: -1799.244385\n",
      "    epoch          : 957\n",
      "    loss           : -1806.9022389537884\n",
      "    ess            : 8.001182520164633\n",
      "    log_marginal   : 1806.9022389537884\n",
      "    val_loss       : -1809.9614359537761\n",
      "    val_ess        : 8.001184940338135\n",
      "    val_log_marginal: 1809.9614359537761\n",
      "Train Epoch: 958 [0/54000 (0%)] Loss: -1820.949585\n",
      "Train Epoch: 958 [11264/54000 (21%)] Loss: -1820.291504\n",
      "Train Epoch: 958 [22528/54000 (42%)] Loss: -1799.159180\n",
      "Train Epoch: 958 [33792/54000 (63%)] Loss: -1796.344971\n",
      "Train Epoch: 958 [45056/54000 (83%)] Loss: -1788.004517\n",
      "    epoch          : 958\n",
      "    loss           : -1803.7321224572524\n",
      "    ess            : 8.001181800410432\n",
      "    log_marginal   : 1803.7321224572524\n",
      "    val_loss       : -1802.5584513346355\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1802.5584513346355\n",
      "Train Epoch: 959 [0/54000 (0%)] Loss: -1816.748535\n",
      "Train Epoch: 959 [11264/54000 (21%)] Loss: -1810.968628\n",
      "Train Epoch: 959 [22528/54000 (42%)] Loss: -1815.667358\n",
      "Train Epoch: 959 [33792/54000 (63%)] Loss: -1805.171265\n",
      "Train Epoch: 959 [45056/54000 (83%)] Loss: -1807.208252\n",
      "    epoch          : 959\n",
      "    loss           : -1810.2529158682194\n",
      "    ess            : 8.00118274508782\n",
      "    log_marginal   : 1810.2529158682194\n",
      "    val_loss       : -1816.1634521484375\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1816.1634623209636\n",
      "Train Epoch: 960 [0/54000 (0%)] Loss: -1827.266357\n",
      "Train Epoch: 960 [11264/54000 (21%)] Loss: -1819.051758\n",
      "Train Epoch: 960 [22528/54000 (42%)] Loss: -1808.206421\n",
      "Train Epoch: 960 [33792/54000 (63%)] Loss: -1794.074097\n",
      "Train Epoch: 960 [45056/54000 (83%)] Loss: -1802.240112\n",
      "    epoch          : 960\n",
      "    loss           : -1807.3904246204304\n",
      "    ess            : 8.001181233604\n",
      "    log_marginal   : 1807.3904246204304\n",
      "    val_loss       : -1801.121073404948\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1801.121073404948\n",
      "Train Epoch: 961 [0/54000 (0%)] Loss: -1802.339355\n",
      "Train Epoch: 961 [11264/54000 (21%)] Loss: -1816.673828\n",
      "Train Epoch: 961 [22528/54000 (42%)] Loss: -1812.732422\n",
      "Train Epoch: 961 [33792/54000 (63%)] Loss: -1807.330566\n",
      "Train Epoch: 961 [45056/54000 (83%)] Loss: -1807.837769\n",
      "    epoch          : 961\n",
      "    loss           : -1811.0406378979953\n",
      "    ess            : 8.001181089653159\n",
      "    log_marginal   : 1811.040639049602\n",
      "    val_loss       : -1812.40673828125\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1812.40673828125\n",
      "Train Epoch: 962 [0/54000 (0%)] Loss: -1821.712891\n",
      "Train Epoch: 962 [11264/54000 (21%)] Loss: -1814.189941\n",
      "Train Epoch: 962 [22528/54000 (42%)] Loss: -1805.494141\n",
      "Train Epoch: 962 [33792/54000 (63%)] Loss: -1805.642822\n",
      "Train Epoch: 962 [45056/54000 (83%)] Loss: -1807.754883\n",
      "    epoch          : 962\n",
      "    loss           : -1810.3268202295844\n",
      "    ess            : 8.001181548496461\n",
      "    log_marginal   : 1810.326821381191\n",
      "    val_loss       : -1811.2505798339844\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1811.2505594889324\n",
      "Train Epoch: 963 [0/54000 (0%)] Loss: -1820.703125\n",
      "Train Epoch: 963 [11264/54000 (21%)] Loss: -1815.053223\n",
      "Train Epoch: 963 [22528/54000 (42%)] Loss: -1803.083008\n",
      "Train Epoch: 963 [33792/54000 (63%)] Loss: -1797.607666\n",
      "Train Epoch: 963 [45056/54000 (83%)] Loss: -1801.334961\n",
      "    epoch          : 963\n",
      "    loss           : -1802.8130366487323\n",
      "    ess            : 8.001182070318258\n",
      "    log_marginal   : 1802.8130389519458\n",
      "    val_loss       : -1798.6068318684895\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1798.6068013509114\n",
      "Train Epoch: 964 [0/54000 (0%)] Loss: -1808.296997\n",
      "Train Epoch: 964 [11264/54000 (21%)] Loss: -1811.732056\n",
      "Train Epoch: 964 [22528/54000 (42%)] Loss: -1809.518433\n",
      "Train Epoch: 964 [33792/54000 (63%)] Loss: -1811.516357\n",
      "Train Epoch: 964 [45056/54000 (83%)] Loss: -1801.638916\n",
      "    epoch          : 964\n",
      "    loss           : -1807.7854303323998\n",
      "    ess            : 8.001181890379709\n",
      "    log_marginal   : 1807.7854303323998\n",
      "    val_loss       : -1809.8937072753906\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1809.8937072753906\n",
      "Train Epoch: 965 [0/54000 (0%)] Loss: -1827.677734\n",
      "Train Epoch: 965 [11264/54000 (21%)] Loss: -1813.536377\n",
      "Train Epoch: 965 [22528/54000 (42%)] Loss: -1796.624756\n",
      "Train Epoch: 965 [33792/54000 (63%)] Loss: -1807.379761\n",
      "Train Epoch: 965 [45056/54000 (83%)] Loss: -1807.318604\n",
      "    epoch          : 965\n",
      "    loss           : -1805.147593372273\n",
      "    ess            : 8.001181728435013\n",
      "    log_marginal   : 1805.147593372273\n",
      "    val_loss       : -1802.72705078125\n",
      "    val_ess        : 8.001184225082397\n",
      "    val_log_marginal: 1802.72705078125\n",
      "Train Epoch: 966 [0/54000 (0%)] Loss: -1820.548828\n",
      "Train Epoch: 966 [11264/54000 (21%)] Loss: -1810.755859\n",
      "Train Epoch: 966 [22528/54000 (42%)] Loss: -1811.145386\n",
      "Train Epoch: 966 [33792/54000 (63%)] Loss: -1806.650757\n",
      "Train Epoch: 966 [45056/54000 (83%)] Loss: -1803.838623\n",
      "    epoch          : 966\n",
      "    loss           : -1808.6441938292305\n",
      "    ess            : 8.001181746428868\n",
      "    log_marginal   : 1808.6441938292305\n",
      "    val_loss       : -1810.3808695475261\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1810.3808695475261\n",
      "Train Epoch: 967 [0/54000 (0%)] Loss: -1812.390625\n",
      "Train Epoch: 967 [11264/54000 (21%)] Loss: -1816.617310\n",
      "Train Epoch: 967 [22528/54000 (42%)] Loss: -1797.594482\n",
      "Train Epoch: 967 [33792/54000 (63%)] Loss: -1794.934937\n",
      "Train Epoch: 967 [45056/54000 (83%)] Loss: -1812.471802\n",
      "    epoch          : 967\n",
      "    loss           : -1806.2366713038032\n",
      "    ess            : 8.001182538158488\n",
      "    log_marginal   : 1806.2366713038032\n",
      "    val_loss       : -1811.3004353841145\n",
      "    val_ess        : 8.001183032989502\n",
      "    val_log_marginal: 1811.3004353841145\n",
      "Train Epoch: 968 [0/54000 (0%)] Loss: -1817.166748\n",
      "Train Epoch: 968 [11264/54000 (21%)] Loss: -1808.736572\n",
      "Train Epoch: 968 [22528/54000 (42%)] Loss: -1792.926270\n",
      "Train Epoch: 968 [33792/54000 (63%)] Loss: -1806.399292\n",
      "Train Epoch: 968 [45056/54000 (83%)] Loss: -1787.117432\n",
      "    epoch          : 968\n",
      "    loss           : -1800.4358416863208\n",
      "    ess            : 8.001182412201503\n",
      "    log_marginal   : 1800.4358416863208\n",
      "    val_loss       : -1793.419209798177\n",
      "    val_ess        : 8.001184701919556\n",
      "    val_log_marginal: 1793.419209798177\n",
      "Train Epoch: 969 [0/54000 (0%)] Loss: -1798.766113\n",
      "Train Epoch: 969 [11264/54000 (21%)] Loss: -1811.294189\n",
      "Train Epoch: 969 [22528/54000 (42%)] Loss: -1802.956787\n",
      "Train Epoch: 969 [33792/54000 (63%)] Loss: -1809.509888\n",
      "Train Epoch: 969 [45056/54000 (83%)] Loss: -1808.373779\n",
      "    epoch          : 969\n",
      "    loss           : -1806.8745082639298\n",
      "    ess            : 8.00118227724759\n",
      "    log_marginal   : 1806.874507112323\n",
      "    val_loss       : -1808.0872294108074\n",
      "    val_ess        : 8.001181840896606\n",
      "    val_log_marginal: 1808.0872497558594\n",
      "Train Epoch: 970 [0/54000 (0%)] Loss: -1822.088501\n",
      "Train Epoch: 970 [11264/54000 (21%)] Loss: -1814.024170\n",
      "Train Epoch: 970 [22528/54000 (42%)] Loss: -1809.040527\n",
      "Train Epoch: 970 [33792/54000 (63%)] Loss: -1800.920410\n",
      "Train Epoch: 970 [45056/54000 (83%)] Loss: -1801.055176\n",
      "    epoch          : 970\n",
      "    loss           : -1808.4987827516952\n",
      "    ess            : 8.001180010021857\n",
      "    log_marginal   : 1808.4987827516952\n",
      "    val_loss       : -1807.95947265625\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1807.959452311198\n",
      "Train Epoch: 971 [0/54000 (0%)] Loss: -1820.511719\n",
      "Train Epoch: 971 [11264/54000 (21%)] Loss: -1804.349243\n",
      "Train Epoch: 971 [22528/54000 (42%)] Loss: -1811.184937\n",
      "Train Epoch: 971 [33792/54000 (63%)] Loss: -1803.789062\n",
      "Train Epoch: 971 [45056/54000 (83%)] Loss: -1801.547119\n",
      "    epoch          : 971\n",
      "    loss           : -1805.21902753722\n",
      "    ess            : 8.001181242600927\n",
      "    log_marginal   : 1805.21902753722\n",
      "    val_loss       : -1810.781005859375\n",
      "    val_ess        : 8.00118358929952\n",
      "    val_log_marginal: 1810.781005859375\n",
      "Train Epoch: 972 [0/54000 (0%)] Loss: -1822.295654\n",
      "Train Epoch: 972 [11264/54000 (21%)] Loss: -1816.522339\n",
      "Train Epoch: 972 [22528/54000 (42%)] Loss: -1785.750000\n",
      "Train Epoch: 972 [33792/54000 (63%)] Loss: -1810.109375\n",
      "Train Epoch: 972 [45056/54000 (83%)] Loss: -1798.074585\n",
      "    epoch          : 972\n",
      "    loss           : -1804.694373940522\n",
      "    ess            : 8.001181170625507\n",
      "    log_marginal   : 1804.694373940522\n",
      "    val_loss       : -1804.7425638834636\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1804.7425638834636\n",
      "Train Epoch: 973 [0/54000 (0%)] Loss: -1822.426147\n",
      "Train Epoch: 973 [11264/54000 (21%)] Loss: -1820.051025\n",
      "Train Epoch: 973 [22528/54000 (42%)] Loss: -1814.854980\n",
      "Train Epoch: 973 [33792/54000 (63%)] Loss: -1807.304688\n",
      "Train Epoch: 973 [45056/54000 (83%)] Loss: -1819.307129\n",
      "    epoch          : 973\n",
      "    loss           : -1814.2521120467277\n",
      "    ess            : 8.00118007300035\n",
      "    log_marginal   : 1814.2521120467277\n",
      "    val_loss       : -1812.6870218912761\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1812.6870218912761\n",
      "Train Epoch: 974 [0/54000 (0%)] Loss: -1826.290771\n",
      "Train Epoch: 974 [11264/54000 (21%)] Loss: -1816.588867\n",
      "Train Epoch: 974 [22528/54000 (42%)] Loss: -1812.643799\n",
      "Train Epoch: 974 [33792/54000 (63%)] Loss: -1811.164795\n",
      "Train Epoch: 974 [45056/54000 (83%)] Loss: -1805.761963\n",
      "    epoch          : 974\n",
      "    loss           : -1811.1392983490566\n",
      "    ess            : 8.001179803092525\n",
      "    log_marginal   : 1811.1392983490566\n",
      "    val_loss       : -1808.8807779947917\n",
      "    val_ess        : 8.001180489857992\n",
      "    val_log_marginal: 1808.8807779947917\n",
      "Train Epoch: 975 [0/54000 (0%)] Loss: -1823.382324\n",
      "Train Epoch: 975 [11264/54000 (21%)] Loss: -1815.302612\n",
      "Train Epoch: 975 [22528/54000 (42%)] Loss: -1810.115234\n",
      "Train Epoch: 975 [33792/54000 (63%)] Loss: -1782.361328\n",
      "Train Epoch: 975 [45056/54000 (83%)] Loss: -1806.927856\n",
      "    epoch          : 975\n",
      "    loss           : -1804.4286948150059\n",
      "    ess            : 8.001181530502608\n",
      "    log_marginal   : 1804.4286936633991\n",
      "    val_loss       : -1792.8337707519531\n",
      "    val_ess        : 8.001184304555258\n",
      "    val_log_marginal: 1792.8337707519531\n",
      "Train Epoch: 976 [0/54000 (0%)] Loss: -1802.559326\n",
      "Train Epoch: 976 [11264/54000 (21%)] Loss: -1804.595459\n",
      "Train Epoch: 976 [22528/54000 (42%)] Loss: -1811.825806\n",
      "Train Epoch: 976 [33792/54000 (63%)] Loss: -1804.079834\n",
      "Train Epoch: 976 [45056/54000 (83%)] Loss: -1812.522095\n",
      "    epoch          : 976\n",
      "    loss           : -1806.699089770047\n",
      "    ess            : 8.001181233604\n",
      "    log_marginal   : 1806.6990909216538\n",
      "    val_loss       : -1812.7258707682292\n",
      "    val_ess        : 8.001182874043783\n",
      "    val_log_marginal: 1812.7258605957031\n",
      "Train Epoch: 977 [0/54000 (0%)] Loss: -1821.163208\n",
      "Train Epoch: 977 [11264/54000 (21%)] Loss: -1806.288818\n",
      "Train Epoch: 977 [22528/54000 (42%)] Loss: -1804.665771\n",
      "Train Epoch: 977 [33792/54000 (63%)] Loss: -1781.304077\n",
      "Train Epoch: 977 [45056/54000 (83%)] Loss: -1783.939941\n",
      "    epoch          : 977\n",
      "    loss           : -1793.0014821178509\n",
      "    ess            : 8.001182241259881\n",
      "    log_marginal   : 1793.0014821178509\n",
      "    val_loss       : -1801.5672403971355\n",
      "    val_ess        : 8.001185099283854\n",
      "    val_log_marginal: 1801.5672403971355\n",
      "Train Epoch: 978 [0/54000 (0%)] Loss: -1808.894043\n",
      "Train Epoch: 978 [11264/54000 (21%)] Loss: -1808.812988\n",
      "Train Epoch: 978 [22528/54000 (42%)] Loss: -1808.692749\n",
      "Train Epoch: 978 [33792/54000 (63%)] Loss: -1813.838379\n",
      "Train Epoch: 978 [45056/54000 (83%)] Loss: -1792.497803\n",
      "    epoch          : 978\n",
      "    loss           : -1804.503124309036\n",
      "    ess            : 8.001182484176924\n",
      "    log_marginal   : 1804.5031220058224\n",
      "    val_loss       : -1805.9939880371094\n",
      "    val_ess        : 8.001184781392416\n",
      "    val_log_marginal: 1805.9939880371094\n",
      "Train Epoch: 979 [0/54000 (0%)] Loss: -1815.890625\n",
      "Train Epoch: 979 [11264/54000 (21%)] Loss: -1825.242920\n",
      "Train Epoch: 979 [22528/54000 (42%)] Loss: -1809.583008\n",
      "Train Epoch: 979 [33792/54000 (63%)] Loss: -1809.781616\n",
      "Train Epoch: 979 [45056/54000 (83%)] Loss: -1803.991699\n",
      "    epoch          : 979\n",
      "    loss           : -1811.0658776625148\n",
      "    ess            : 8.001180810748407\n",
      "    log_marginal   : 1811.0658776625148\n",
      "    val_loss       : -1813.9650980631511\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1813.9650980631511\n",
      "Train Epoch: 980 [0/54000 (0%)] Loss: -1822.576172\n",
      "Train Epoch: 980 [11264/54000 (21%)] Loss: -1814.070557\n",
      "Train Epoch: 980 [22528/54000 (42%)] Loss: -1805.244019\n",
      "Train Epoch: 980 [33792/54000 (63%)] Loss: -1806.510010\n",
      "Train Epoch: 980 [45056/54000 (83%)] Loss: -1809.044067\n",
      "    epoch          : 980\n",
      "    loss           : -1808.9261232771964\n",
      "    ess            : 8.001181170625507\n",
      "    log_marginal   : 1808.9261232771964\n",
      "    val_loss       : -1811.1895955403645\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1811.1895751953125\n",
      "Train Epoch: 981 [0/54000 (0%)] Loss: -1828.934570\n",
      "Train Epoch: 981 [11264/54000 (21%)] Loss: -1817.857178\n",
      "Train Epoch: 981 [22528/54000 (42%)] Loss: -1807.516602\n",
      "Train Epoch: 981 [33792/54000 (63%)] Loss: -1805.012939\n",
      "Train Epoch: 981 [45056/54000 (83%)] Loss: -1804.862061\n",
      "    epoch          : 981\n",
      "    loss           : -1807.0794332252358\n",
      "    ess            : 8.001181710441157\n",
      "    log_marginal   : 1807.0794343768425\n",
      "    val_loss       : -1808.2873229980469\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1808.2873229980469\n",
      "Train Epoch: 982 [0/54000 (0%)] Loss: -1813.540039\n",
      "Train Epoch: 982 [11264/54000 (21%)] Loss: -1816.189453\n",
      "Train Epoch: 982 [22528/54000 (42%)] Loss: -1812.958252\n",
      "Train Epoch: 982 [33792/54000 (63%)] Loss: -1819.228271\n",
      "Train Epoch: 982 [45056/54000 (83%)] Loss: -1805.529175\n",
      "    epoch          : 982\n",
      "    loss           : -1813.8962436891952\n",
      "    ess            : 8.001180522846726\n",
      "    log_marginal   : 1813.8962436891952\n",
      "    val_loss       : -1810.0184020996094\n",
      "    val_ess        : 8.001182158788046\n",
      "    val_log_marginal: 1810.0184020996094\n",
      "Train Epoch: 983 [0/54000 (0%)] Loss: -1827.398193\n",
      "Train Epoch: 983 [11264/54000 (21%)] Loss: -1786.351196\n",
      "Train Epoch: 983 [22528/54000 (42%)] Loss: -1809.002075\n",
      "Train Epoch: 983 [33792/54000 (63%)] Loss: -1783.054443\n",
      "Train Epoch: 983 [45056/54000 (83%)] Loss: -1802.461426\n",
      "    epoch          : 983\n",
      "    loss           : -1801.6579417102741\n",
      "    ess            : 8.001182052324403\n",
      "    log_marginal   : 1801.6579417102741\n",
      "    val_loss       : -1811.173583984375\n",
      "    val_ess        : 8.001182556152344\n",
      "    val_log_marginal: 1811.173583984375\n",
      "Train Epoch: 984 [0/54000 (0%)] Loss: -1815.338989\n",
      "Train Epoch: 984 [11264/54000 (21%)] Loss: -1814.948364\n",
      "Train Epoch: 984 [22528/54000 (42%)] Loss: -1794.330078\n",
      "Train Epoch: 984 [33792/54000 (63%)] Loss: -1799.398438\n",
      "Train Epoch: 984 [45056/54000 (83%)] Loss: -1795.120605\n",
      "    epoch          : 984\n",
      "    loss           : -1802.0118200913914\n",
      "    ess            : 8.001181782416577\n",
      "    log_marginal   : 1802.0118200913914\n",
      "    val_loss       : -1804.7455647786458\n",
      "    val_ess        : 8.00118319193522\n",
      "    val_log_marginal: 1804.7455647786458\n",
      "Train Epoch: 985 [0/54000 (0%)] Loss: -1815.801636\n",
      "Train Epoch: 985 [11264/54000 (21%)] Loss: -1819.968018\n",
      "Train Epoch: 985 [22528/54000 (42%)] Loss: -1816.696167\n",
      "Train Epoch: 985 [33792/54000 (63%)] Loss: -1817.259766\n",
      "Train Epoch: 985 [45056/54000 (83%)] Loss: -1817.213135\n",
      "    epoch          : 985\n",
      "    loss           : -1814.1818824624115\n",
      "    ess            : 8.00118094570232\n",
      "    log_marginal   : 1814.1818824624115\n",
      "    val_loss       : -1801.7876688639324\n",
      "    val_ess        : 8.001182397206625\n",
      "    val_log_marginal: 1801.7876688639324\n",
      "Train Epoch: 986 [0/54000 (0%)] Loss: -1812.584595\n",
      "Train Epoch: 986 [11264/54000 (21%)] Loss: -1798.682129\n",
      "Train Epoch: 986 [22528/54000 (42%)] Loss: -1806.772705\n",
      "Train Epoch: 986 [33792/54000 (63%)] Loss: -1799.432373\n",
      "Train Epoch: 986 [45056/54000 (83%)] Loss: -1816.881836\n",
      "    epoch          : 986\n",
      "    loss           : -1803.4401498470666\n",
      "    ess            : 8.001180765763769\n",
      "    log_marginal   : 1803.4401498470666\n",
      "    val_loss       : -1807.9749043782551\n",
      "    val_ess        : 8.001183668772379\n",
      "    val_log_marginal: 1807.9749043782551\n",
      "Train Epoch: 987 [0/54000 (0%)] Loss: -1819.584106\n",
      "Train Epoch: 987 [11264/54000 (21%)] Loss: -1799.690796\n",
      "Train Epoch: 987 [22528/54000 (42%)] Loss: -1810.900024\n",
      "Train Epoch: 987 [33792/54000 (63%)] Loss: -1802.070068\n",
      "Train Epoch: 987 [45056/54000 (83%)] Loss: -1796.462646\n",
      "    epoch          : 987\n",
      "    loss           : -1805.1982744324882\n",
      "    ess            : 8.001179758107886\n",
      "    log_marginal   : 1805.1982744324882\n",
      "    val_loss       : -1807.7832438151042\n",
      "    val_ess        : 8.001182874043783\n",
      "    val_log_marginal: 1807.7832438151042\n",
      "Train Epoch: 988 [0/54000 (0%)] Loss: -1809.252686\n",
      "Train Epoch: 988 [11264/54000 (21%)] Loss: -1818.508057\n",
      "Train Epoch: 988 [22528/54000 (42%)] Loss: -1817.316650\n",
      "Train Epoch: 988 [33792/54000 (63%)] Loss: -1815.072510\n",
      "Train Epoch: 988 [45056/54000 (83%)] Loss: -1812.548584\n",
      "    epoch          : 988\n",
      "    loss           : -1813.6291077811763\n",
      "    ess            : 8.001181782416577\n",
      "    log_marginal   : 1813.6291077811763\n",
      "    val_loss       : -1813.6363321940105\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1813.6363321940105\n",
      "Train Epoch: 989 [0/54000 (0%)] Loss: -1823.760986\n",
      "Train Epoch: 989 [11264/54000 (21%)] Loss: -1812.453369\n",
      "Train Epoch: 989 [22528/54000 (42%)] Loss: -1808.604614\n",
      "Train Epoch: 989 [33792/54000 (63%)] Loss: -1800.601074\n",
      "Train Epoch: 989 [45056/54000 (83%)] Loss: -1807.598145\n",
      "    epoch          : 989\n",
      "    loss           : -1805.1455446639152\n",
      "    ess            : 8.001180414883596\n",
      "    log_marginal   : 1805.145545815522\n",
      "    val_loss       : -1805.479268391927\n",
      "    val_ess        : 8.001182953516642\n",
      "    val_log_marginal: 1805.479268391927\n",
      "Train Epoch: 990 [0/54000 (0%)] Loss: -1823.832764\n",
      "Train Epoch: 990 [11264/54000 (21%)] Loss: -1811.290039\n",
      "Train Epoch: 990 [22528/54000 (42%)] Loss: -1809.905762\n",
      "Train Epoch: 990 [33792/54000 (63%)] Loss: -1808.232666\n",
      "Train Epoch: 990 [45056/54000 (83%)] Loss: -1803.364990\n",
      "    epoch          : 990\n",
      "    loss           : -1809.5572221863945\n",
      "    ess            : 8.00117956917241\n",
      "    log_marginal   : 1809.5572221863945\n",
      "    val_loss       : -1810.0061848958333\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1810.0061848958333\n",
      "Train Epoch: 991 [0/54000 (0%)] Loss: -1819.640137\n",
      "Train Epoch: 991 [11264/54000 (21%)] Loss: -1813.896240\n",
      "Train Epoch: 991 [22528/54000 (42%)] Loss: -1813.997803\n",
      "Train Epoch: 991 [33792/54000 (63%)] Loss: -1806.677734\n",
      "Train Epoch: 991 [45056/54000 (83%)] Loss: -1810.168701\n",
      "    epoch          : 991\n",
      "    loss           : -1808.2720613299675\n",
      "    ess            : 8.001181692447302\n",
      "    log_marginal   : 1808.2720613299675\n",
      "    val_loss       : -1806.6076965332031\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1806.607686360677\n",
      "Train Epoch: 992 [0/54000 (0%)] Loss: -1812.320068\n",
      "Train Epoch: 992 [11264/54000 (21%)] Loss: -1810.774902\n",
      "Train Epoch: 992 [22528/54000 (42%)] Loss: -1813.409912\n",
      "Train Epoch: 992 [33792/54000 (63%)] Loss: -1811.980347\n",
      "Train Epoch: 992 [45056/54000 (83%)] Loss: -1799.647095\n",
      "    epoch          : 992\n",
      "    loss           : -1812.0429203825177\n",
      "    ess            : 8.00118202533362\n",
      "    log_marginal   : 1812.042919230911\n",
      "    val_loss       : -1812.4141845703125\n",
      "    val_ess        : 8.001182953516642\n",
      "    val_log_marginal: 1812.4141845703125\n",
      "Train Epoch: 993 [0/54000 (0%)] Loss: -1811.820068\n",
      "Train Epoch: 993 [11264/54000 (21%)] Loss: -1784.023438\n",
      "Train Epoch: 993 [22528/54000 (42%)] Loss: -1774.212158\n",
      "Train Epoch: 993 [33792/54000 (63%)] Loss: -1767.501709\n",
      "Train Epoch: 993 [45056/54000 (83%)] Loss: -1793.722778\n",
      "    epoch          : 993\n",
      "    loss           : -1783.011549463812\n",
      "    ess            : 8.001183815722195\n",
      "    log_marginal   : 1783.011549463812\n",
      "    val_loss       : -1790.7580057779949\n",
      "    val_ess        : 8.001184622446695\n",
      "    val_log_marginal: 1790.7580261230469\n",
      "Train Epoch: 994 [0/54000 (0%)] Loss: -1797.450562\n",
      "Train Epoch: 994 [11264/54000 (21%)] Loss: -1801.380981\n",
      "Train Epoch: 994 [22528/54000 (42%)] Loss: -1802.192505\n",
      "Train Epoch: 994 [33792/54000 (63%)] Loss: -1803.229736\n",
      "Train Epoch: 994 [45056/54000 (83%)] Loss: -1795.899292\n",
      "    epoch          : 994\n",
      "    loss           : -1800.3806129311615\n",
      "    ess            : 8.00118177341965\n",
      "    log_marginal   : 1800.3806129311615\n",
      "    val_loss       : -1803.6332194010417\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1803.6332194010417\n",
      "Train Epoch: 995 [0/54000 (0%)] Loss: -1819.545776\n",
      "Train Epoch: 995 [11264/54000 (21%)] Loss: -1812.032227\n",
      "Train Epoch: 995 [22528/54000 (42%)] Loss: -1822.885498\n",
      "Train Epoch: 995 [33792/54000 (63%)] Loss: -1801.368164\n",
      "Train Epoch: 995 [45056/54000 (83%)] Loss: -1801.895874\n",
      "    epoch          : 995\n",
      "    loss           : -1808.9342444077977\n",
      "    ess            : 8.001180423880523\n",
      "    log_marginal   : 1808.9342455594044\n",
      "    val_loss       : -1804.8296610514324\n",
      "    val_ess        : 8.001181999842325\n",
      "    val_log_marginal: 1804.8296508789062\n",
      "Train Epoch: 996 [0/54000 (0%)] Loss: -1817.850586\n",
      "Train Epoch: 996 [11264/54000 (21%)] Loss: -1802.364502\n",
      "Train Epoch: 996 [22528/54000 (42%)] Loss: -1795.752441\n",
      "Train Epoch: 996 [33792/54000 (63%)] Loss: -1804.678711\n",
      "Train Epoch: 996 [45056/54000 (83%)] Loss: -1806.867310\n",
      "    epoch          : 996\n",
      "    loss           : -1803.233149690448\n",
      "    ess            : 8.001181782416577\n",
      "    log_marginal   : 1803.2331473872346\n",
      "    val_loss       : -1810.917744954427\n",
      "    val_ess        : 8.001184463500977\n",
      "    val_log_marginal: 1810.917744954427\n",
      "Train Epoch: 997 [0/54000 (0%)] Loss: -1819.297852\n",
      "Train Epoch: 997 [11264/54000 (21%)] Loss: -1812.600342\n",
      "Train Epoch: 997 [22528/54000 (42%)] Loss: -1787.981323\n",
      "Train Epoch: 997 [33792/54000 (63%)] Loss: -1800.836182\n",
      "Train Epoch: 997 [45056/54000 (83%)] Loss: -1798.441772\n",
      "    epoch          : 997\n",
      "    loss           : -1802.6607366597877\n",
      "    ess            : 8.001181890379709\n",
      "    log_marginal   : 1802.6607378113945\n",
      "    val_loss       : -1802.1474100748699\n",
      "    val_ess        : 8.001184860865274\n",
      "    val_log_marginal: 1802.1474202473958\n",
      "Train Epoch: 998 [0/54000 (0%)] Loss: -1813.303101\n",
      "Train Epoch: 998 [11264/54000 (21%)] Loss: -1814.326660\n",
      "Train Epoch: 998 [22528/54000 (42%)] Loss: -1804.319580\n",
      "Train Epoch: 998 [33792/54000 (63%)] Loss: -1807.008057\n",
      "Train Epoch: 998 [45056/54000 (83%)] Loss: -1805.405029\n",
      "    epoch          : 998\n",
      "    loss           : -1807.7969728865714\n",
      "    ess            : 8.001182520164633\n",
      "    log_marginal   : 1807.7969728865714\n",
      "    val_loss       : -1811.7884928385417\n",
      "    val_ess        : 8.0011838277181\n",
      "    val_log_marginal: 1811.7884928385417\n",
      "Train Epoch: 999 [0/54000 (0%)] Loss: -1818.499023\n",
      "Train Epoch: 999 [11264/54000 (21%)] Loss: -1825.204102\n",
      "Train Epoch: 999 [22528/54000 (42%)] Loss: -1805.980225\n",
      "Train Epoch: 999 [33792/54000 (63%)] Loss: -1811.798096\n",
      "Train Epoch: 999 [45056/54000 (83%)] Loss: -1802.309570\n",
      "    epoch          : 999\n",
      "    loss           : -1809.1813642393868\n",
      "    ess            : 8.00118216928446\n",
      "    log_marginal   : 1809.1813642393868\n",
      "    val_loss       : -1808.5902099609375\n",
      "    val_ess        : 8.001185735066732\n",
      "    val_log_marginal: 1808.5901896158855\n",
      "Train Epoch: 1000 [0/54000 (0%)] Loss: -1819.186523\n",
      "Train Epoch: 1000 [11264/54000 (21%)] Loss: -1794.569702\n",
      "Train Epoch: 1000 [22528/54000 (42%)] Loss: -1783.068848\n",
      "Train Epoch: 1000 [33792/54000 (63%)] Loss: -1797.735962\n",
      "Train Epoch: 1000 [45056/54000 (83%)] Loss: -1786.066406\n",
      "    epoch          : 1000\n",
      "    loss           : -1793.2544417471256\n",
      "    ess            : 8.001183536817443\n",
      "    log_marginal   : 1793.2544417471256\n",
      "    val_loss       : -1798.52978515625\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1798.529805501302\n",
      "Saving checkpoint: saved/models/Mnist_Ppc/0222_160001/checkpoint-epoch1000.pth ...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faa5b900-0121-4cf6-91b6-2c9b0b12716f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParticleDict(  (z_what): Parameter containing: [torch.FloatTensor of size 8x6000x1x10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "trainer.model.cpu()\n",
    "trainer.cpu()\n",
    "trainer.train_particles.cpu()\n",
    "trainer.valid_particles.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9d528cb-d45e-4a37-be86-9622aec9ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._load_particles(range(trainer.data_loader.batch_size), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e26726a6-74e5-4e0f-81d5-746dba00f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in trainer.model.graph.nodes:\n",
    "    trainer.model.graph.nodes[site]['is_observed'] = trainer.model.graph.nodes[site]['value'] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cb94c06-7112-4c5e-b781-0ac215a7e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60a2eeb8-32e3-467e-9e9a-0ac1e2af9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyro.plate_stack(\"forward\", (trainer.num_particles, trainer.data_loader.batch_size)):\n",
    "    model = pyro.condition(trainer.model, data={k: v['value'] for k, v in trainer.model.graph.nodes.items()})\n",
    "    xs = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6e9ba59-7b26-4ef3-9bfd-23f7c8a26464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b1509d2-9afc-444e-821f-b7ccda42aa28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm50lEQVR4nO3df3DUdZ7n8de3O0nnh53GCPklMZOZ0R0XWOpGXZASRW7MmarhxsG5Q93bhbsZV0ewjmLmvGGtW7mtK2K5K8cfrMytNcfojo7W3KljLd5oZhAYj2EWKSxZxvVwAIlCjETSHZLQSbo/9wdDdgMI/f6Y8EnI81HVVZL0y88n33y7X+mk+92Rc84JAIAAYqE3AACYvCghAEAwlBAAIBhKCAAQDCUEAAiGEgIABEMJAQCCoYQAAMEUhd7AmfL5vI4cOaJkMqkoikJvBwBg5JxTT0+P6uvrFYud/7HOuCuhI0eOqKGhIfQ2AACfUXt7u6ZPn37e64y7Ekomk5Kkm2esVFE8UXDOvXPAvFasrPD//wgl9lzuk+PmTPzylDmjyP4bVpc9aV9HUhT3+G1u3j4lKt/Xb85EiRJzRpLcwJB9rbj9EXtUYt+fz4Qtr++RJJfL20ODg/Z1hnL2deJxe8bjvJM8v7eVleZM3uP+IVZeZs5I8jt+RkNuQNu6fzx8f34+Y1ZCTzzxhP7yL/9SR48e1YwZM7R+/XrNnz//grnTv4IriidsJRQVm/cYi/zuqBSz5yKP/cU91vEqocjjDkdS5LGWIo8SijyKwfN76zx+BexzHHz25+RRQj7fI3meEx7Hznl8bxV53Il6nHeS5/fW43abv5j3Xz7Hz1Mhf1IZkycmPP/881q5cqUefvhh7dmzR/Pnz1dLS4sOHz48FssBACaoMSmhdevW6Zvf/Ka+9a1v6dprr9X69evV0NCgjRs3jsVyAIAJatRLaGBgQLt371Zzc/OIjzc3N2vHjh1nXT+bzSqTyYy4AAAmh1EvoWPHjimXy6mmpmbEx2tqatTR0XHW9VtbW5VKpYYvPDMOACaPMXux6pl/kHLOnfOPVKtXr1Y6nR6+tLe3j9WWAADjzKg/O27q1KmKx+NnPerp7Ow869GRJCUSCSUSnk+VBgBMaKP+SKikpETXXXed2traRny8ra1N8+bNG+3lAAAT2Ji8TmjVqlX64z/+Y11//fW68cYb9Td/8zc6fPiw7r///rFYDgAwQY1JCS1ZskRdXV36i7/4Cx09elQzZ87UK6+8osbGxrFYDgAwQUXOZxbIGMpkMkqlUlqY/CMVGV4R7DXstNivg93JrDkTeawVldnHcvjsTSX2V2tLkvo9xv14jAxxAwP2dXIe42AkxeprzRl3PG1fyGecjsdNNZ/2e8mDz7kXeZxHzuMc8lnHd2yP8zmPfMYrefxd3Hc0lc9ki9zHx0zXH3KDen3ofyudTqvyAmOMeCsHAEAwlBAAIBhKCAAQDCUEAAiGEgIABEMJAQCCoYQAAMFQQgCAYCghAEAwlBAAIBhKCAAQDCUEAAhmTKZoj4YoikxDSfNZ++DOeEW5OSNJ+cwJe8hnTmyRx7fHJzMwaM/Ib7ij16BGlzdnfAalStLQwfftS33pi+ZM1N1jzuSPd5szsdT5h0d+etB+/PKfsw9/je0/bF/nRK99ncsqzBlJiiVT5szQkaP2dTyGirr+fnNGknTN58yRyHjMIxeXCjzFeSQEAAiGEgIABEMJAQCCoYQAAMFQQgCAYCghAEAwlBAAIBhKCAAQDCUEAAiGEgIABEMJAQCCoYQAAMFQQgCAYMbtFG1Nu0KKFz5xOer42LxE7pPj5owkxSrKzBk3OGTO5NP2SctRsf1bmvecxhv3mNCcP24/5pHHZPDIc0J6/Ioqc8Z90GHO5Hr7zBkfUc5j4ruvt9LmSG5wwJyJ11SbM67H7zhEKfs08XgyaV/I43YbFRfb15HkjtjvK5UoMV09Mgy+55EQACAYSggAEAwlBAAIhhICAARDCQEAgqGEAADBUEIAgGAoIQBAMJQQACAYSggAEAwlBAAIhhICAAQzbgeY5g62K4oKH9AXv6zCvEY0JWXOSJKG7MNIo8je9/n+k+aM8xgIGSUKHxQ7Qt7Z1yqzD3+NeXyfcp0eQxolKZezZzy+t/HKy+zLeAyMdemMOSNJkc8QzrxhauXv+AwWzX/Sbc7EykrNGe+1Ku3HzmvAqucA05zH12QdjJx3gwVfl0dCAIBgKCEAQDCUEAAgGEoIABAMJQQACIYSAgAEQwkBAIKhhAAAwVBCAIBgKCEAQDCUEAAgGEoIABDMuB1gGhUXKYrG5/byGfuwwViFfXBn3GNgZf5ErznjMyBU8huOGaurMWdyU+zDPut+kjZnJOn6ykPmTN7Zf5abVmQ/dk+2zzdnDn7wRXNGku67brs5c23ph+bML3t+z5xp77/cnPn1237H4dr/8ltzJnesy5zxuQ3mu/3O8SKP26AzDm2O5QekzgKva94NAACjhBICAAQz6iW0Zs0aRVE04lJbWzvaywAALgFj8keXGTNm6Oc///nwv+Px+FgsAwCY4MakhIqKinj0AwC4oDH5m9D+/ftVX1+vpqYm3XXXXTpw4MCnXjebzSqTyYy4AAAmh1EvoTlz5ujpp5/Wq6++qieffFIdHR2aN2+eurrO/bTF1tZWpVKp4UtDQ8NobwkAME6Negm1tLTozjvv1KxZs/SVr3xFmzdvliQ99dRT57z+6tWrlU6nhy/t7e2jvSUAwDg15q8Graio0KxZs7R///5zfj6RSCiRSIz1NgAA49CYv04om83qnXfeUV1d3VgvBQCYYEa9hL773e9q27ZtOnjwoH7961/rG9/4hjKZjJYuXTraSwEAJrhR/3XcBx98oLvvvlvHjh3TtGnTNHfuXO3cuVONjY2jvRQAYIKLnHMu9Cb+uUwmo1QqpYXJP1JRVFJwLiotNa8Vldszkt/gTp/9uZMn7etUVJgz+ePd5owkpf/1H9gz/6bHnLm2+iNz5s8b/s6ckaSfn/h9c+b+Kf9ozrw3lDdnevKF3x5OmxLLmjOS1FRkf4H5sfyAOZOK2dd5b9Ce2dJ7rTkjSZue+1fmzOeefM++kHFAqCQp73nXXeTx2CNrO4+G3IB+kfmR0um0KivPP4iZ2XEAgGAoIQBAMJQQACAYSggAEAwlBAAIhhICAARDCQEAgqGEAADBUEIAgGAoIQBAMJQQACAYSggAEMyYv6mdryiKFEVRwdfPHz9uXiM2lDRnJCnfbx8sGvcZYDowaM+c/MSciSrKzRlJmrK705w5eU+xOdNUce63hj+fF9NfNmck6aVD9qGsO6o+b87Mu/yAOXNZ3H7ezUh8YM5I0vtD9rUai+wDVn2Gnk6L58yZZMz+9UhS6oB90Kzr67cvlLevE5X73W59hqXmjQNM867w+y4eCQEAgqGEAADBUEIAgGAoIQBAMJQQACAYSggAEAwlBAAIhhICAARDCQEAgqGEAADBUEIAgGAoIQBAMJQQACCYcTtFW9VTpXii4KvH++xTcoeOdJgzkhQrsU+CVtHFOdRRWZk9NGifqitJQ1PtU8ir/nvcnPmHrt8zZ6IP7RO+JamuyD6FvL/Ufsx/EbdP6/axseUOr1z3LPs5UVHda84MZO23i6rN9uN9xf89as5IUlX+Q3PGeUzMj4rst4t8pseckaQobl8rlij8vliSYi6SChy8zSMhAEAwlBAAIBhKCAAQDCUEAAiGEgIABEMJAQCCoYQAAMFQQgCAYCghAEAwlBAAIBhKCAAQDCUEAAhm/A4wPdYlRSUFXz3Xbx9gGk9VmjOSJI8Bpu6kfX9Ruccw0sj+c0VUUW5fR1JxR7c54zyHLppVX+GXO2offOqSFeZMdKLPnOn/gwZzJl/4TWiEz73kzJmydo9BuB0eg0V9Bu4mL7NnJLle+1BWn2HFbjBvzsRqppkzkt9t0GUHbNd3hZ8/PBICAARDCQEAgqGEAADBUEIAgGAoIQBAMJQQACAYSggAEAwlBAAIhhICAARDCQEAgqGEAADBUEIAgGDG7QBTNzAoF0Vjukauu9sv6DEkND7NPlDT9fWbMxoc9Mgk7BlJkccAWNdrH9wZu6LKvk7M7+cr11hvzuz/k5Q5k6uwD6ws+Thuznz+J5+YM5Kk37abI/ls1pxxuZw5E5XYp7LGr7jcnJEk120flhorsw8ezh/vtmd6TpgzkhTF7eeRrPfFrvDzm0dCAIBgKCEAQDDmEtq+fbsWLVqk+vp6RVGkl156acTnnXNas2aN6uvrVVZWpgULFmjfvn2jtV8AwCXEXEK9vb2aPXu2NmzYcM7PP/bYY1q3bp02bNigXbt2qba2Vrfddpt6ei7Sm5kBACYM8xMTWlpa1NLScs7POee0fv16Pfzww1q8eLEk6amnnlJNTY2effZZ3XfffZ9ttwCAS8qo/k3o4MGD6ujoUHNz8/DHEomEbrnlFu3YseOcmWw2q0wmM+ICAJgcRrWEOjo6JEk1NTUjPl5TUzP8uTO1trYqlUoNXxoaGkZzSwCAcWxMnh0XnfGccufcWR87bfXq1Uqn08OX9nb76xMAABPTqL5Ytba2VtKpR0R1dXXDH+/s7Dzr0dFpiURCiYTfiyUBABPbqD4SampqUm1trdra2oY/NjAwoG3btmnevHmjuRQA4BJgfiR04sQJvffee8P/PnjwoN566y1VVVXpqquu0sqVK7V27VpdffXVuvrqq7V27VqVl5frnnvuGdWNAwAmPnMJvfnmm7r11luH/71q1SpJ0tKlS/XDH/5QDz30kPr7+/XAAw/o+PHjmjNnjl577TUlk8nR2zUA4JIQOedc6E38c5lMRqlUSgvL71JRVPigwqik2L5YsX0QoiRFCY9c3P6bz6H2I+ZMrNT+97XI829y+T6PYaTl5ebM0JeuMmcGLvf73n7l0V+aM9eXHzRnvv2LPzFnfv+/HTVnXGWFOSNJUVe3OZM71mXOxK+su/CVzuBO9JozkcdQ0VNreQwJ9Rhw7Prtw4ojj9u6dGo4tDkzaBvkOuQG9frgT5ROp1VZef5Bx8yOAwAEQwkBAIKhhAAAwVBCAIBgKCEAQDCUEAAgGEoIABAMJQQACIYSAgAEQwkBAIKhhAAAwVBCAIBgKCEAQDCj+s6qoykqr1AUM0xCzmbNa+SPHzdnJCk2JWUPeUyujflMBvcQVdgnW0tSNDBgD9VONUc6brRPgr5z6VZzRpLuTr1pzpx0cXOm5ct7zZlDfR7fJ4+J05LkplWZM1GPfeJ0/mP75G2Xy5kzMeMU6OG1PN5kwJ20T8SOX1lrzvgcO0mKyu0TxfOfdJuu71zh3yMeCQEAgqGEAADBUEIAgGAoIQBAMJQQACAYSggAEAwlBAAIhhICAARDCQEAgqGEAADBUEIAgGAoIQBAMON2gKnyOUmFD8FzHsM0oyK/L9/1n7SvVZqwL+QxqFExj58rfNaRFJUYBswOhyJzpOxj+xDJVFGfOSNJOdn3Ny2eN2eeuHKnOXPf/7nRnHnjxX9hzkhSw2tpcybmc47n7d9bedzWfYaeSlIUtw+n9fiK5NI9HiGflSRF9vsI6/c25iKpwJsgj4QAAMFQQgCAYCghAEAwlBAAIBhKCAAQDCUEAAiGEgIABEMJAQCCoYQAAMFQQgCAYCghAEAwlBAAIJjIOd8peGMjk8kolUrpX1YtU1Gs8AGZrtdjYKXHcEJJispK7aGBQXPE61uTtw/TzPf5Dfssqqm2r+XzffL4mqI6+94k6eP5tebM7937jjmzvmGzOfNRzv4zY2nkN7hzd/ZKc2bDf1piziR3f2jOqMjjdnsya89Icln7sNT8iV5zJuZxn+I7lNVl7cciKiszXX/IDWhLzzNKp9OqrKw873V5JAQACIYSAgAEQwkBAIKhhAAAwVBCAIBgKCEAQDCUEAAgGEoIABAMJQQACIYSAgAEQwkBAIKhhAAAwYzfAaZX/HvTAFMNDdkXKyqyZyQ5jwGF0WUV9oU8hp6qpNie8TwFomL7Wm7QY5Crx9DTqDRhzpwK2n8uc/395swHK75szvy7pW3mzH++Yr85I0nf77YPMC2N2b+3j//gG+ZMw999bM7ow4/sGUluwD7A1OcuNT5tqn2dkyfNGcnvdmsdysoAUwDAhEAJAQCCMZfQ9u3btWjRItXX1yuKIr300ksjPr9s2TJFUTTiMnfu3NHaLwDgEmIuod7eXs2ePVsbNmz41OvcfvvtOnr06PDllVde+UybBABcmsx/mW9paVFLS8t5r5NIJFRba3+HSgDA5DImfxPaunWrqqurdc011+jee+9VZ2fnp143m80qk8mMuAAAJodRL6GWlhY988wz2rJlix5//HHt2rVLCxcuVPZT3te8tbVVqVRq+NLQ0DDaWwIAjFN+L5Q5jyVLlgz/98yZM3X99dersbFRmzdv1uLFi8+6/urVq7Vq1arhf2cyGYoIACaJUS+hM9XV1amxsVH795/7RXOJREKJhOcLCwEAE9qYv06oq6tL7e3tqqurG+ulAAATjPmR0IkTJ/Tee+8N//vgwYN66623VFVVpaqqKq1Zs0Z33nmn6urqdOjQIf3Zn/2Zpk6dqq9//eujunEAwMRnLqE333xTt9566/C/T/89Z+nSpdq4caP27t2rp59+Wt3d3aqrq9Ott96q559/XslkcvR2DQC4JIzfAaZTv2kaYOozzC+Ke/42Mm8/ZPl+j/0V2/9kF5WXmTP5dI85I0luyD6wsmi6fTCmS3s8bb+s1J6RvIbGRlPOP6DxXFzG45hXTTFHPplTbV9H0owV/2DO7E9PM2e+1fhLc+bve75gzvx2fmTOSJJiF2eyWVRfY864j455reUzcDcqs92vMMAUADAhUEIAgGAoIQBAMJQQACAYSggAEAwlBAAIhhICAARDCQEAgqGEAADBUEIAgGAoIQBAMJQQACAYSggAEMyYv7OqL9ffJxcNFXz9WKXHW0V4Tsh1PSfsS11W4bFQ3h7p85iQW+r3zrax0pQ54zMR2w0Vfh6cFnlMw5Yk5XL2zMmsORKVl5sz7njanKna6Tckf8b3jtgzl9kzf/vhjebMkZ83mDPTs78yZySpqMY+hXyo4yP7Ov32c0ie7wIQXVFlDxnfBSBS4VPLeSQEAAiGEgIABEMJAQCCoYQAAMFQQgCAYCghAEAwlBAAIBhKCAAQDCUEAAiGEgIABEMJAQCCoYQAAMGM2wGmUTyuKIoXfP388W7zGrEp9gGckuScfShk4eP8/tk6g/bBnYoXfsxOixIl9nUkuZO2oYaSFBV5nHIex9tlPQZCSnIDA+aM109yZaXmSHb258yZD/7Ub5DrD1N7zZmdJ6eZM/c3HDNn/sf2xeZMzGNgrK9Y0j5MOfdRp30d36/JZ7hvsfF2my98+DKPhAAAwVBCAIBgKCEAQDCUEAAgGEoIABAMJQQACIYSAgAEQwkBAIKhhAAAwVBCAIBgKCEAQDCUEAAgmPE7wLQyqSiWKDxwote8Rr63z5yRJNffbw8lDF/L70RX1trXOZ4xR3wGkUqSBu2DEKMK+9DFXNr+NcUv9xtOG5WXmTP5Rvv36b177EMuv7bg782ZTdO2mzOn2Efu1halzZmHlj9gzlR8+JE546wDOE/nfM7xhjr7QgcO2zMlxfaMpHzPCXsol7Ot4Qo/bjwSAgAEQwkBAIKhhAAAwVBCAIBgKCEAQDCUEAAgGEoIABAMJQQACIYSAgAEQwkBAIKhhAAAwVBCAIBgxu0AUw0OSbHCO9INDdnXMA7lOy3yGEYam1plzrjOY+aM4nFzxGdopyS5E3l7KLIPxox5DGqMUpXmjCS990378Mn/+o3nzJkbStvNmY6cffhrucfxlqT3h+zH/L6/+o/mTN2v3jFnnDkh5fv9hvS6bNacKSq2Hzvncbt1fR6DlCVFJSX2TMKWieUHpK4Cr2veDQAAo4QSAgAEYyqh1tZW3XDDDUomk6qurtYdd9yhd999d8R1nHNas2aN6uvrVVZWpgULFmjfvn2jumkAwKXBVELbtm3T8uXLtXPnTrW1tWloaEjNzc3q7f2nN5R77LHHtG7dOm3YsEG7du1SbW2tbrvtNvX09Iz65gEAE5vpiQk/+9nPRvx706ZNqq6u1u7du3XzzTfLOaf169fr4Ycf1uLFiyVJTz31lGpqavTss8/qvvvuG72dAwAmvM/0N6F0+tTb+VZVnXrm18GDB9XR0aHm5ubh6yQSCd1yyy3asWPHOf8f2WxWmUxmxAUAMDl4l5BzTqtWrdJNN92kmTNnSpI6OjokSTU1NSOuW1NTM/y5M7W2tiqVSg1fGhoafLcEAJhgvEtoxYoVevvtt/XjH//4rM9FZ7w2wTl31sdOW716tdLp9PClvd3++gkAwMTk9WLVBx98UC+//LK2b9+u6dOnD3+8trZW0qlHRHV1//Siv87OzrMeHZ2WSCSU8HjxJwBg4jM9EnLOacWKFXrhhRe0ZcsWNTU1jfh8U1OTamtr1dbWNvyxgYEBbdu2TfPmzRudHQMALhmmR0LLly/Xs88+q5/+9KdKJpPDf+dJpVIqKytTFEVauXKl1q5dq6uvvlpXX3211q5dq/Lyct1zzz1j8gUAACYuUwlt3LhRkrRgwYIRH9+0aZOWLVsmSXrooYfU39+vBx54QMePH9ecOXP02muvKZlMjsqGAQCXjsg55zMPcMxkMhmlUiktTP6RiqLCh+ZFHkMufQeYupx9cKfrtw8bjH3O/kxBd+Qjc8Z3gKmm2IeEDjRcbs68f6/9eP/t3B+YM5IUi+xrlUb28ygZ2Qfufpy3/+30R11+vwb/7VftA3d9hud6DeG8Yoo909Vtz/gq8vhTu8egVO/Bw9kBe8h4XznkBvSLzI+UTqdVWXn++wlmxwEAgqGEAADBUEIAgGAoIQBAMJQQACAYSggAEAwlBAAIhhICAARDCQEAgqGEAADBUEIAgGAoIQBAMJQQACAYr3dWvRiiRKmiWOFTtH24k/bJtZIUldj3dXzJ9eZM6j98YM7MnHLSnFk0Zac5I0n7sleaM1cWHzdnZpV0mDNVMb+fr17ts39NDcVd5syf/vlKcyaRtk/4rvj7Q+aMJEWRPeN6TpgzeZ/p0UP2CeS+Io+J2LmuT8yZ+GUVHuvYb0uSFJuSMmfyJ3pt13eDBV+XR0IAgGAoIQBAMJQQACAYSggAEAwlBAAIhhICAARDCQEAgqGEAADBUEIAgGAoIQBAMJQQACAYSggAEMy4HWCqygopnij46u7IR+YlorJSc0aSolJ7rvKQfbDo2s+/YM7kZJ88+fmiAXNGkmJRuzkzJWY/DrtONpgzyXi/OSNJf/7iXebMF/9npzlT1fEbc0bxuD3j7ENPJUllZX45q1zOHMn39pkzPgNCJXntr6hmmjnjM0w5Ki38/nHEWn0ex29qlW2N/IBU4NxhHgkBAIKhhAAAwVBCAIBgKCEAQDCUEAAgGEoIABAMJQQACIYSAgAEQwkBAIKhhAAAwVBCAIBgKCEAQDDjdoCp++hjuaik8EBkH9zpBgbNGUmKPIY7Fv2/D82ZF9PXmTP/dsouc+b9oWJzRpJ+v7jXnPn+cfvX9L+eXGjOVO+2702SvvjOP9pDcfvPclG5/RzyGdwZeexNknKfHLevVeRxdxLZ9xevSpoz+e60OXNqMfvQ2GjAYyCwzzrl5fZ1JLmeHntm0HZf6fKFX59HQgCAYCghAEAwlBAAIBhKCAAQDCUEAAiGEgIABEMJAQCCoYQAAMFQQgCAYCghAEAwlBAAIBhKCAAQzLgdYGoVlSbsmWK/wZ3WYX6SlM9kzJnd86eYM2/232TO+IpKDANmf8d5DHesT3kMFfWVy5kjrr/fnImmpMwZn735DPaVpFjyMq+clc8QzqjEfruNXVFlzkiS67N/b+Vx/xDzOR/yeXtGkjzuKzU0ZLu+K/z6PBICAARDCQEAgjGVUGtrq2644QYlk0lVV1frjjvu0LvvvjviOsuWLVMURSMuc+fOHdVNAwAuDaYS2rZtm5YvX66dO3eqra1NQ0NDam5uVm/vyDcQu/3223X06NHhyyuvvDKqmwYAXBpMT0z42c9+NuLfmzZtUnV1tXbv3q2bb755+OOJREK1tbWjs0MAwCXrM/1NKJ0+9Za5VVUjn3mydetWVVdX65prrtG9996rzs7OT/1/ZLNZZTKZERcAwOTgXULOOa1atUo33XSTZs6cOfzxlpYWPfPMM9qyZYsef/xx7dq1SwsXLlQ2mz3n/6e1tVWpVGr40tDQ4LslAMAEEznnnE9w+fLl2rx5s9544w1Nnz79U6939OhRNTY26rnnntPixYvP+nw2mx1RUJlMRg0NDVpYcbeKosJfh3JRXyfkccjy3Wlzxut1OB6vWfF1sV4nFEtVmjPeBo2vh5Dn1+TxupB8pseciYo8Xwro8VocL5H952Cf1wk56+tcTucu1uuEfF7H5Pk6oYtxHzHkBvSL7r9VOp1WZeX5b79eZ+iDDz6ol19+Wdu3bz9vAUlSXV2dGhsbtX///nN+PpFIKJHwePEUAGDCM5WQc04PPvigXnzxRW3dulVNTU0XzHR1dam9vV11dXXemwQAXJpMj4WXL1+uH/3oR3r22WeVTCbV0dGhjo4O9f/u4d2JEyf03e9+V7/61a906NAhbd26VYsWLdLUqVP19a9/fUy+AADAxGV6JLRx40ZJ0oIFC0Z8fNOmTVq2bJni8bj27t2rp59+Wt3d3aqrq9Ott96q559/XslkctQ2DQC4NJh/HXc+ZWVlevXVVz/ThgAAk8f4naKdy0lR4VODXW+ffY3LKuwZSe5E74WvdIbIY5pxFPd45pDH1+ROnvvp8xdcy+dZSj6ToH2eseb3pM+Lxmd/kccTeKJKv2nYPue418TuQfszC92nvNzjvBmf805SVFZqD3k8I9H1ehzveNyekeRyHs+qMz4TzzFFGwAwEVBCAIBgKCEAQDCUEAAgGEoIABAMJQQACIYSAgAEQwkBAIKhhAAAwVBCAIBgKCEAQDCUEAAgmHE7wDSWSikWK/zto32G8rk+j6Gn8nyrYI9hg5HHWx/Lc1Cjj7zHkMv4lfY3N3TH7W+N7vP22ZLfWx/7DI31eqvuYvvNNX/sE3NGkhTzGJ6bsL/de+Tx1u35T7rNGd+391bWPmDVh8+A43zvCb/FYh6DZvO2gbsMMAUATAiUEAAgGEoIABAMJQQACIYSAgAEQwkBAIKhhAAAwVBCAIBgKCEAQDCUEAAgGEoIABDMuJsd59ypGUVDedvMJpf3mB3n/OZCOTfoEbLvL+azP2efHed/HDzWymc91rHvL/JYR5Kc8bw7tZZtrpYk5X2+Jo9zyDLDa2TQY3acfXuK8vaZij7Hzus2Kymyf2v91pHH7DjPr0nOY3acsx2Iod/tzRWQi1wh17qIPvjgAzU0NITeBgDgM2pvb9f06dPPe51xV0L5fF5HjhxRMpk8a7JsJpNRQ0OD2tvbVVlpn757qeA4nMJxOIXjcArH4ZTxcBycc+rp6VF9fb1iF5jIPu5+HReLxS7YnJWVlZP6JDuN43AKx+EUjsMpHIdTQh+HVKqwt1PhiQkAgGAoIQBAMBOqhBKJhB555BElEonQWwmK43AKx+EUjsMpHIdTJtpxGHdPTAAATB4T6pEQAODSQgkBAIKhhAAAwVBCAIBgJlQJPfHEE2pqalJpaamuu+46/fKXvwy9pYtqzZo1iqJoxKW2tjb0tsbc9u3btWjRItXX1yuKIr300ksjPu+c05o1a1RfX6+ysjItWLBA+/btC7PZMXSh47Bs2bKzzo+5c+eG2ewYaW1t1Q033KBkMqnq6mrdcccdevfdd0dcZzKcD4Uch4lyPkyYEnr++ee1cuVKPfzww9qzZ4/mz5+vlpYWHT58OPTWLqoZM2bo6NGjw5e9e/eG3tKY6+3t1ezZs7Vhw4Zzfv6xxx7TunXrtGHDBu3atUu1tbW67bbb1NPTc5F3OrYudBwk6fbbbx9xfrzyyisXcYdjb9u2bVq+fLl27typtrY2DQ0Nqbm5Wb29vcPXmQznQyHHQZog54ObIP7wD//Q3X///SM+9qUvfcl973vfC7Sji++RRx5xs2fPDr2NoCS5F198cfjf+Xze1dbWukcffXT4YydPnnSpVMp9//vfD7DDi+PM4+Ccc0uXLnVf+9rXguwnlM7OTifJbdu2zTk3ec+HM4+DcxPnfJgQj4QGBga0e/duNTc3j/h4c3OzduzYEWhXYezfv1/19fVqamrSXXfdpQMHDoTeUlAHDx5UR0fHiHMjkUjolltumXTnhiRt3bpV1dXVuuaaa3Tvvfeqs7Mz9JbGVDqdliRVVVVJmrznw5nH4bSJcD5MiBI6duyYcrmcampqRny8pqZGHR0dgXZ18c2ZM0dPP/20Xn31VT355JPq6OjQvHnz1NXVFXprwZz+/k/2c0OSWlpa9Mwzz2jLli16/PHHtWvXLi1cuFDZrN97K413zjmtWrVKN910k2bOnClpcp4P5zoO0sQ5H8bdFO3zOfOtHZxzZ33sUtbS0jL837NmzdKNN96oL3zhC3rqqae0atWqgDsLb7KfG5K0ZMmS4f+eOXOmrr/+ejU2Nmrz5s1avHhxwJ2NjRUrVujtt9/WG2+8cdbnJtP58GnHYaKcDxPikdDUqVMVj8fP+kmms7PzrJ94JpOKigrNmjVL+/fvD72VYE4/O5Bz42x1dXVqbGy8JM+PBx98UC+//LJef/31EW/9MtnOh087DucyXs+HCVFCJSUluu6669TW1jbi421tbZo3b16gXYWXzWb1zjvvqK6uLvRWgmlqalJtbe2Ic2NgYEDbtm2b1OeGJHV1dam9vf2SOj+cc1qxYoVeeOEFbdmyRU1NTSM+P1nOhwsdh3MZt+dDwCdFmDz33HOuuLjY/eAHP3C/+c1v3MqVK11FRYU7dOhQ6K1dNN/5znfc1q1b3YEDB9zOnTvdV7/6VZdMJi/5Y9DT0+P27Nnj9uzZ4yS5devWuT179rj333/fOefco48+6lKplHvhhRfc3r173d133+3q6upcJpMJvPPRdb7j0NPT477zne+4HTt2uIMHD7rXX3/d3Xjjje7KK6+8pI7Dt7/9bZdKpdzWrVvd0aNHhy99fX3D15kM58OFjsNEOh8mTAk559xf//Vfu8bGRldSUuK+/OUvj3g64mSwZMkSV1dX54qLi119fb1bvHix27dvX+htjbnXX3/dSTrrsnTpUufcqaflPvLII662ttYlEgl38803u71794bd9Bg433Ho6+tzzc3Nbtq0aa64uNhdddVVbunSpe7w4cOhtz2qzvX1S3KbNm0avs5kOB8udBwm0vnAWzkAAIKZEH8TAgBcmighAEAwlBAAIBhKCAAQDCUEAAiGEgIABEMJAQCCoYQAAMFQQgCAYCghAEAwlBAAIBhKCAAQzP8HLoL+l6mzK40AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    plt.imshow(xs.mean(dim=0)[i].squeeze().detach().cpu().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2029315-afe9-4ae8-9f64-c342ef8b830b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ppc] *",
   "language": "python",
   "name": "conda-env-ppc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
