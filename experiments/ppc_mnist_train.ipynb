{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ef93e3-8eb5-4883-8265-5fbe97d06e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eli/AnacondaProjects/ppc_experiments\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94520ed3-45be-4735-a0f2-b8f380838c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import numpy as np\n",
    "import pyro\n",
    "import torch\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "import trainer.trainer as module_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ac4be4-19eb-4713-a88d-9bec118c7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.enable_validation(True)\n",
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75ea7b9-6fe5-4f8a-a825-3572e421242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1445d1-45bb-43f9-893d-7a2b2bad1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "\n",
    "config = read_json(\"experiments/ppc_mnist_config.json\")\n",
    "config = ConfigParser(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca35026-5f64-47f8-98c7-a5e228013d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MnistPpc(\n",
      "  (digit_features): DigitFeatures()\n",
      "  (decoder): DigitDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=10, out_features=200, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=200, out_features=400, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=400, out_features=784, bias=True)\n",
      "      (5): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (graph): GraphicalModel()\n",
      ")\n",
      "Trainable parameters: 396984\n",
      "Initialize particles: train batch 0\n",
      "Initialize particles: train batch 1\n",
      "Initialize particles: train batch 2\n",
      "Initialize particles: train batch 3\n",
      "Initialize particles: train batch 4\n",
      "Initialize particles: train batch 5\n",
      "Initialize particles: train batch 6\n",
      "Initialize particles: train batch 7\n",
      "Initialize particles: train batch 8\n",
      "Initialize particles: train batch 9\n",
      "Initialize particles: train batch 10\n",
      "Initialize particles: train batch 11\n",
      "Initialize particles: train batch 12\n",
      "Initialize particles: train batch 13\n",
      "Initialize particles: train batch 14\n",
      "Initialize particles: train batch 15\n",
      "Initialize particles: train batch 16\n",
      "Initialize particles: train batch 17\n",
      "Initialize particles: train batch 18\n",
      "Initialize particles: train batch 19\n",
      "Initialize particles: train batch 20\n",
      "Initialize particles: train batch 21\n",
      "Initialize particles: train batch 22\n",
      "Initialize particles: train batch 23\n",
      "Initialize particles: train batch 24\n",
      "Initialize particles: train batch 25\n",
      "Initialize particles: train batch 26\n",
      "Initialize particles: train batch 27\n",
      "Initialize particles: train batch 28\n",
      "Initialize particles: train batch 29\n",
      "Initialize particles: train batch 30\n",
      "Initialize particles: train batch 31\n",
      "Initialize particles: train batch 32\n",
      "Initialize particles: train batch 33\n",
      "Initialize particles: train batch 34\n",
      "Initialize particles: train batch 35\n",
      "Initialize particles: train batch 36\n",
      "Initialize particles: train batch 37\n",
      "Initialize particles: train batch 38\n",
      "Initialize particles: train batch 39\n",
      "Initialize particles: train batch 40\n",
      "Initialize particles: train batch 41\n",
      "Initialize particles: train batch 42\n",
      "Initialize particles: train batch 43\n",
      "Initialize particles: train batch 44\n",
      "Initialize particles: train batch 45\n",
      "Initialize particles: train batch 46\n",
      "Initialize particles: train batch 47\n",
      "Initialize particles: train batch 48\n",
      "Initialize particles: train batch 49\n",
      "Initialize particles: train batch 50\n",
      "Initialize particles: train batch 51\n",
      "Initialize particles: train batch 52\n",
      "Initialize particles: train batch 53\n",
      "Initialize particles: train batch 54\n",
      "Initialize particles: train batch 55\n",
      "Initialize particles: train batch 56\n",
      "Initialize particles: train batch 57\n",
      "Initialize particles: train batch 58\n",
      "Initialize particles: train batch 59\n",
      "Initialize particles: train batch 60\n",
      "Initialize particles: train batch 61\n",
      "Initialize particles: train batch 62\n",
      "Initialize particles: train batch 63\n",
      "Initialize particles: train batch 64\n",
      "Initialize particles: train batch 65\n",
      "Initialize particles: train batch 66\n",
      "Initialize particles: train batch 67\n",
      "Initialize particles: train batch 68\n",
      "Initialize particles: train batch 69\n",
      "Initialize particles: train batch 70\n",
      "Initialize particles: train batch 71\n",
      "Initialize particles: train batch 72\n",
      "Initialize particles: train batch 73\n",
      "Initialize particles: train batch 74\n",
      "Initialize particles: train batch 75\n",
      "Initialize particles: train batch 76\n",
      "Initialize particles: train batch 77\n",
      "Initialize particles: train batch 78\n",
      "Initialize particles: train batch 79\n",
      "Initialize particles: train batch 80\n",
      "Initialize particles: train batch 81\n",
      "Initialize particles: train batch 82\n",
      "Initialize particles: train batch 83\n",
      "Initialize particles: train batch 84\n",
      "Initialize particles: train batch 85\n",
      "Initialize particles: train batch 86\n",
      "Initialize particles: train batch 87\n",
      "Initialize particles: train batch 88\n",
      "Initialize particles: train batch 89\n",
      "Initialize particles: train batch 90\n",
      "Initialize particles: train batch 91\n",
      "Initialize particles: train batch 92\n",
      "Initialize particles: train batch 93\n",
      "Initialize particles: train batch 94\n",
      "Initialize particles: train batch 95\n",
      "Initialize particles: train batch 96\n",
      "Initialize particles: train batch 97\n",
      "Initialize particles: train batch 98\n",
      "Initialize particles: train batch 99\n",
      "Initialize particles: train batch 100\n",
      "Initialize particles: train batch 101\n",
      "Initialize particles: train batch 102\n",
      "Initialize particles: train batch 103\n",
      "Initialize particles: train batch 104\n",
      "Initialize particles: train batch 105\n",
      "Initialize particles: valid batch 0\n",
      "Initialize particles: valid batch 1\n",
      "Initialize particles: valid batch 2\n",
      "Initialize particles: valid batch 3\n",
      "Initialize particles: valid batch 4\n",
      "Initialize particles: valid batch 5\n",
      "Initialize particles: valid batch 6\n",
      "Initialize particles: valid batch 7\n",
      "Initialize particles: valid batch 8\n",
      "Initialize particles: valid batch 9\n",
      "Initialize particles: valid batch 10\n",
      "Initialize particles: valid batch 11\n"
     ]
    }
   ],
   "source": [
    "logger = config.get_logger('train')\n",
    "\n",
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data)\n",
    "valid_data_loader = data_loader.split_validation()\n",
    "\n",
    "# build model architecture, then print to console\n",
    "model = config.init_obj('arch', module_arch)\n",
    "logger.info(model)\n",
    "\n",
    "# get function handles of metrics\n",
    "metrics = [getattr(module_metric, met) for met in config['metrics']]\n",
    "\n",
    "# build optimizer.\n",
    "optimizer = config.init_obj('optimizer', pyro.optim)\n",
    "\n",
    "# build trainer\n",
    "# kwargs = config['trainer'].pop('args')\n",
    "trainer = config.init_obj('trainer', module_trainer, model, metrics, optimizer,\n",
    "                          config=config, data_loader=data_loader,\n",
    "                          valid_data_loader=valid_data_loader,\n",
    "                          lr_scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5cdd31-7fae-4e6f-8fe2-ef5c023919de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved/log/Mnist_Ppc/0201_173737\n"
     ]
    }
   ],
   "source": [
    "logger.info(trainer.config.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05daa11b-4367-42e6-b449-cf630e65b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVITIES = [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]\n",
    "# SCHEDULE = torch.profiler.schedule(wait=1, warmup=1, active=2, repeat=1)\n",
    "# ON_TRACE_READY = torch.profiler.tensorboard_trace_handler(trainer.config.log_dir)\n",
    "# with torch.profiler.profile(activities=ACTIVITIES, on_trace_ready=ON_TRACE_READY, profile_memory=True, schedule=SCHEDULE, with_stack=True) as profiler:\n",
    "#     trainer.train(profiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9820b982-1a39-4bde-850e-1a1d9a92557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54000 (0%)] Loss: 16.100006\n",
      "Train Epoch: 1 [11264/54000 (21%)] Loss: -1325.323853\n",
      "Train Epoch: 1 [22528/54000 (42%)] Loss: -1322.447021\n",
      "Train Epoch: 1 [33792/54000 (63%)] Loss: -1393.537354\n",
      "Train Epoch: 1 [45056/54000 (83%)] Loss: -1364.360840\n",
      "    epoch          : 1\n",
      "    loss           : -1324.0911142601158\n",
      "    ess            : 8.00109882624644\n",
      "    log_marginal   : 1324.091113108509\n",
      "    val_loss       : -1387.2535603841145\n",
      "    val_ess        : 8.001166741053263\n",
      "    val_log_marginal: 1387.2535502115886\n",
      "Train Epoch: 2 [0/54000 (0%)] Loss: -1407.207031\n",
      "Train Epoch: 2 [11264/54000 (21%)] Loss: -1417.651978\n",
      "Train Epoch: 2 [22528/54000 (42%)] Loss: -1362.216187\n",
      "Train Epoch: 2 [33792/54000 (63%)] Loss: -1417.052246\n",
      "Train Epoch: 2 [45056/54000 (83%)] Loss: -1381.800781\n",
      "    epoch          : 2\n",
      "    loss           : -1397.1772725807045\n",
      "    ess            : 8.001166298704327\n",
      "    log_marginal   : 1397.1772714290978\n",
      "    val_loss       : -1403.6777954101562\n",
      "    val_ess        : 8.00116483370463\n",
      "    val_log_marginal: 1403.6777954101562\n",
      "Train Epoch: 3 [0/54000 (0%)] Loss: -1425.916016\n",
      "Train Epoch: 3 [11264/54000 (21%)] Loss: -1441.692871\n",
      "Train Epoch: 3 [22528/54000 (42%)] Loss: -1400.647949\n",
      "Train Epoch: 3 [33792/54000 (63%)] Loss: -1465.570557\n",
      "Train Epoch: 3 [45056/54000 (83%)] Loss: -1435.498169\n",
      "    epoch          : 3\n",
      "    loss           : -1434.8055627211086\n",
      "    ess            : 8.00117384712651\n",
      "    log_marginal   : 1434.8055638727153\n",
      "    val_loss       : -1457.2221577962239\n",
      "    val_ess        : 8.001177151997885\n",
      "    val_log_marginal: 1457.2221577962239\n",
      "Train Epoch: 4 [0/54000 (0%)] Loss: -1478.802490\n",
      "Train Epoch: 4 [11264/54000 (21%)] Loss: -1493.333862\n",
      "Train Epoch: 4 [22528/54000 (42%)] Loss: -1436.683838\n",
      "Train Epoch: 4 [33792/54000 (63%)] Loss: -1500.900146\n",
      "Train Epoch: 4 [45056/54000 (83%)] Loss: -1482.403076\n",
      "    epoch          : 4\n",
      "    loss           : -1473.7138936744545\n",
      "    ess            : 8.00118082874226\n",
      "    log_marginal   : 1473.7138936744545\n",
      "    val_loss       : -1495.8056335449219\n",
      "    val_ess        : 8.001180171966553\n",
      "    val_log_marginal: 1495.8056335449219\n",
      "Train Epoch: 5 [0/54000 (0%)] Loss: -1516.519287\n",
      "Train Epoch: 5 [11264/54000 (21%)] Loss: -1519.528564\n",
      "Train Epoch: 5 [22528/54000 (42%)] Loss: -1485.747314\n",
      "Train Epoch: 5 [33792/54000 (63%)] Loss: -1534.734131\n",
      "Train Epoch: 5 [45056/54000 (83%)] Loss: -1507.233398\n",
      "    epoch          : 5\n",
      "    loss           : -1507.9090368882664\n",
      "    ess            : 8.001182754084748\n",
      "    log_marginal   : 1507.9090368882664\n",
      "    val_loss       : -1529.5372416178386\n",
      "    val_ess        : 8.001183112462362\n",
      "    val_log_marginal: 1529.5372416178386\n",
      "Train Epoch: 6 [0/54000 (0%)] Loss: -1551.014893\n",
      "Train Epoch: 6 [11264/54000 (21%)] Loss: -1555.536621\n",
      "Train Epoch: 6 [22528/54000 (42%)] Loss: -1529.600098\n",
      "Train Epoch: 6 [33792/54000 (63%)] Loss: -1576.017456\n",
      "Train Epoch: 6 [45056/54000 (83%)] Loss: -1547.470093\n",
      "    epoch          : 6\n",
      "    loss           : -1550.3260544111145\n",
      "    ess            : 8.001185291218308\n",
      "    log_marginal   : 1550.3260555627212\n",
      "    val_loss       : -1572.8816324869792\n",
      "    val_ess        : 8.001185417175293\n",
      "    val_log_marginal: 1572.8816324869792\n",
      "Train Epoch: 7 [0/54000 (0%)] Loss: -1593.622192\n",
      "Train Epoch: 7 [11264/54000 (21%)] Loss: -1596.218872\n",
      "Train Epoch: 7 [22528/54000 (42%)] Loss: -1555.497559\n",
      "Train Epoch: 7 [33792/54000 (63%)] Loss: -1603.705322\n",
      "Train Epoch: 7 [45056/54000 (83%)] Loss: -1568.069702\n",
      "    epoch          : 7\n",
      "    loss           : -1578.703034023069\n",
      "    ess            : 8.001185992978654\n",
      "    log_marginal   : 1578.703034023069\n",
      "    val_loss       : -1596.5889282226562\n",
      "    val_ess        : 8.00118581453959\n",
      "    val_log_marginal: 1596.5889282226562\n",
      "Train Epoch: 8 [0/54000 (0%)] Loss: -1616.203125\n",
      "Train Epoch: 8 [11264/54000 (21%)] Loss: -1621.157715\n",
      "Train Epoch: 8 [22528/54000 (42%)] Loss: -1580.047363\n",
      "Train Epoch: 8 [33792/54000 (63%)] Loss: -1612.464844\n",
      "Train Epoch: 8 [45056/54000 (83%)] Loss: -1580.679688\n",
      "    epoch          : 8\n",
      "    loss           : -1596.1511195920548\n",
      "    ess            : 8.001185849027813\n",
      "    log_marginal   : 1596.1511195920548\n",
      "    val_loss       : -1606.4608866373699\n",
      "    val_ess        : 8.00118605295817\n",
      "    val_log_marginal: 1606.4608866373699\n",
      "Train Epoch: 9 [0/54000 (0%)] Loss: -1628.351318\n",
      "Train Epoch: 9 [11264/54000 (21%)] Loss: -1628.602783\n",
      "Train Epoch: 9 [22528/54000 (42%)] Loss: -1585.463135\n",
      "Train Epoch: 9 [33792/54000 (63%)] Loss: -1640.304443\n",
      "Train Epoch: 9 [45056/54000 (83%)] Loss: -1604.721680\n",
      "    epoch          : 9\n",
      "    loss           : -1611.3305214935879\n",
      "    ess            : 8.001185903009379\n",
      "    log_marginal   : 1611.3305214935879\n",
      "    val_loss       : -1614.9407552083333\n",
      "    val_ess        : 8.00118605295817\n",
      "    val_log_marginal: 1614.9407552083333\n",
      "Train Epoch: 10 [0/54000 (0%)] Loss: -1636.059937\n",
      "Train Epoch: 10 [11264/54000 (21%)] Loss: -1635.459229\n",
      "Train Epoch: 10 [22528/54000 (42%)] Loss: -1605.421021\n",
      "Train Epoch: 10 [33792/54000 (63%)] Loss: -1644.810913\n",
      "Train Epoch: 10 [45056/54000 (83%)] Loss: -1614.587769\n",
      "    epoch          : 10\n",
      "    loss           : -1620.373664136203\n",
      "    ess            : 8.001186082947928\n",
      "    log_marginal   : 1620.373664136203\n",
      "    val_loss       : -1636.7505594889324\n",
      "    val_ess        : 8.00118605295817\n",
      "    val_log_marginal: 1636.7505594889324\n",
      "Train Epoch: 11 [0/54000 (0%)] Loss: -1657.527710\n",
      "Train Epoch: 11 [11264/54000 (21%)] Loss: -1655.860840\n",
      "Train Epoch: 11 [22528/54000 (42%)] Loss: -1621.217285\n",
      "Train Epoch: 11 [33792/54000 (63%)] Loss: -1653.046387\n",
      "Train Epoch: 11 [45056/54000 (83%)] Loss: -1623.025635\n",
      "    epoch          : 11\n",
      "    loss           : -1631.987921948703\n",
      "    ess            : 8.001186208904913\n",
      "    log_marginal   : 1631.9879231003094\n",
      "    val_loss       : -1648.8932495117188\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1648.8932495117188\n",
      "Train Epoch: 12 [0/54000 (0%)] Loss: -1668.814453\n",
      "Train Epoch: 12 [11264/54000 (21%)] Loss: -1666.518555\n",
      "Train Epoch: 12 [22528/54000 (42%)] Loss: -1637.829834\n",
      "Train Epoch: 12 [33792/54000 (63%)] Loss: -1651.732910\n",
      "Train Epoch: 12 [45056/54000 (83%)] Loss: -1630.195801\n",
      "    epoch          : 12\n",
      "    loss           : -1644.2746109872494\n",
      "    ess            : 8.001186028966364\n",
      "    log_marginal   : 1644.2746109872494\n",
      "    val_loss       : -1654.2992553710938\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1654.2992350260417\n",
      "Train Epoch: 13 [0/54000 (0%)] Loss: -1676.016602\n",
      "Train Epoch: 13 [11264/54000 (21%)] Loss: -1680.260620\n",
      "Train Epoch: 13 [22528/54000 (42%)] Loss: -1646.510254\n",
      "Train Epoch: 13 [33792/54000 (63%)] Loss: -1670.038696\n",
      "Train Epoch: 13 [45056/54000 (83%)] Loss: -1642.501587\n",
      "    epoch          : 13\n",
      "    loss           : -1655.4384120725235\n",
      "    ess            : 8.00118615492335\n",
      "    log_marginal   : 1655.4384132241303\n",
      "    val_loss       : -1663.8368631998699\n",
      "    val_ess        : 8.00118605295817\n",
      "    val_log_marginal: 1663.8368733723958\n",
      "Train Epoch: 14 [0/54000 (0%)] Loss: -1682.536987\n",
      "Train Epoch: 14 [11264/54000 (21%)] Loss: -1684.353271\n",
      "Train Epoch: 14 [22528/54000 (42%)] Loss: -1658.117188\n",
      "Train Epoch: 14 [33792/54000 (63%)] Loss: -1687.441406\n",
      "Train Epoch: 14 [45056/54000 (83%)] Loss: -1651.069580\n",
      "    epoch          : 14\n",
      "    loss           : -1665.4735107421875\n",
      "    ess            : 8.001186226898769\n",
      "    log_marginal   : 1665.4735095905808\n",
      "    val_loss       : -1677.2511596679688\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1677.2511596679688\n",
      "Train Epoch: 15 [0/54000 (0%)] Loss: -1696.437988\n",
      "Train Epoch: 15 [11264/54000 (21%)] Loss: -1690.127441\n",
      "Train Epoch: 15 [22528/54000 (42%)] Loss: -1668.250000\n",
      "Train Epoch: 15 [33792/54000 (63%)] Loss: -1698.479492\n",
      "Train Epoch: 15 [45056/54000 (83%)] Loss: -1667.299561\n",
      "    epoch          : 15\n",
      "    loss           : -1675.882328825177\n",
      "    ess            : 8.001186280880335\n",
      "    log_marginal   : 1675.882328825177\n",
      "    val_loss       : -1689.0680033365886\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1689.0679931640625\n",
      "Train Epoch: 16 [0/54000 (0%)] Loss: -1706.074829\n",
      "Train Epoch: 16 [11264/54000 (21%)] Loss: -1703.962646\n",
      "Train Epoch: 16 [22528/54000 (42%)] Loss: -1682.111572\n",
      "Train Epoch: 16 [33792/54000 (63%)] Loss: -1706.557373\n",
      "Train Epoch: 16 [45056/54000 (83%)] Loss: -1662.448242\n",
      "    epoch          : 16\n",
      "    loss           : -1684.3519690171727\n",
      "    ess            : 8.001186334861899\n",
      "    log_marginal   : 1684.3519690171727\n",
      "    val_loss       : -1692.4906311035156\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1692.4906107584636\n",
      "Train Epoch: 17 [0/54000 (0%)] Loss: -1708.529907\n",
      "Train Epoch: 17 [11264/54000 (21%)] Loss: -1705.839600\n",
      "Train Epoch: 17 [22528/54000 (42%)] Loss: -1688.561279\n",
      "Train Epoch: 17 [33792/54000 (63%)] Loss: -1722.747437\n",
      "Train Epoch: 17 [45056/54000 (83%)] Loss: -1681.472778\n",
      "    epoch          : 17\n",
      "    loss           : -1694.9581863115418\n",
      "    ess            : 8.00118637084961\n",
      "    log_marginal   : 1694.9581863115418\n",
      "    val_loss       : -1708.5462849934895\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1708.5462849934895\n",
      "Train Epoch: 18 [0/54000 (0%)] Loss: -1722.962646\n",
      "Train Epoch: 18 [11264/54000 (21%)] Loss: -1715.684692\n",
      "Train Epoch: 18 [22528/54000 (42%)] Loss: -1699.093750\n",
      "Train Epoch: 18 [33792/54000 (63%)] Loss: -1721.259521\n",
      "Train Epoch: 18 [45056/54000 (83%)] Loss: -1684.279053\n",
      "    epoch          : 18\n",
      "    loss           : -1702.8498627284787\n",
      "    ess            : 8.001186352855754\n",
      "    log_marginal   : 1702.8498638800854\n",
      "    val_loss       : -1715.8029378255208\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1715.8029378255208\n",
      "Train Epoch: 19 [0/54000 (0%)] Loss: -1728.097168\n",
      "Train Epoch: 19 [11264/54000 (21%)] Loss: -1721.775635\n",
      "Train Epoch: 19 [22528/54000 (42%)] Loss: -1705.953735\n",
      "Train Epoch: 19 [33792/54000 (63%)] Loss: -1725.484375\n",
      "Train Epoch: 19 [45056/54000 (83%)] Loss: -1685.208740\n",
      "    epoch          : 19\n",
      "    loss           : -1706.4229575103184\n",
      "    ess            : 8.00118637084961\n",
      "    log_marginal   : 1706.4229575103184\n",
      "    val_loss       : -1711.1892191569011\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1711.1892191569011\n",
      "Train Epoch: 20 [0/54000 (0%)] Loss: -1722.949463\n",
      "Train Epoch: 20 [11264/54000 (21%)] Loss: -1725.802490\n",
      "Train Epoch: 20 [22528/54000 (42%)] Loss: -1703.142822\n",
      "Train Epoch: 20 [33792/54000 (63%)] Loss: -1739.040039\n",
      "Train Epoch: 20 [45056/54000 (83%)] Loss: -1705.570190\n",
      "    epoch          : 20\n",
      "    loss           : -1714.2421137971698\n",
      "    ess            : 8.001186352855754\n",
      "    log_marginal   : 1714.2421137971698\n",
      "    val_loss       : -1724.8426513671875\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1724.8426513671875\n",
      "Train Epoch: 21 [0/54000 (0%)] Loss: -1736.756592\n",
      "Train Epoch: 21 [11264/54000 (21%)] Loss: -1735.196899\n",
      "Train Epoch: 21 [22528/54000 (42%)] Loss: -1713.486206\n",
      "Train Epoch: 21 [33792/54000 (63%)] Loss: -1749.657715\n",
      "Train Epoch: 21 [45056/54000 (83%)] Loss: -1713.720703\n",
      "    epoch          : 21\n",
      "    loss           : -1723.2373046875\n",
      "    ess            : 8.00118637084961\n",
      "    log_marginal   : 1723.2373058391067\n",
      "    val_loss       : -1732.9877319335938\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1732.9877319335938\n",
      "Train Epoch: 22 [0/54000 (0%)] Loss: -1744.109009\n",
      "Train Epoch: 22 [11264/54000 (21%)] Loss: -1743.541138\n",
      "Train Epoch: 22 [22528/54000 (42%)] Loss: -1720.850220\n",
      "Train Epoch: 22 [33792/54000 (63%)] Loss: -1745.966309\n",
      "Train Epoch: 22 [45056/54000 (83%)] Loss: -1710.546143\n",
      "    epoch          : 22\n",
      "    loss           : -1725.5843908921727\n",
      "    ess            : 8.001186334861899\n",
      "    log_marginal   : 1725.5843908921727\n",
      "    val_loss       : -1722.8991800944011\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1722.8991800944011\n",
      "Train Epoch: 23 [0/54000 (0%)] Loss: -1734.772827\n",
      "Train Epoch: 23 [11264/54000 (21%)] Loss: -1744.508423\n",
      "Train Epoch: 23 [22528/54000 (42%)] Loss: -1720.651123\n",
      "Train Epoch: 23 [33792/54000 (63%)] Loss: -1749.712402\n",
      "Train Epoch: 23 [45056/54000 (83%)] Loss: -1719.391357\n",
      "    epoch          : 23\n",
      "    loss           : -1728.6560001013413\n",
      "    ess            : 8.00118637084961\n",
      "    log_marginal   : 1728.656001252948\n",
      "    val_loss       : -1737.2149759928386\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1737.2149759928386\n",
      "Train Epoch: 24 [0/54000 (0%)] Loss: -1748.230225\n",
      "Train Epoch: 24 [11264/54000 (21%)] Loss: -1752.015625\n",
      "Train Epoch: 24 [22528/54000 (42%)] Loss: -1723.235474\n",
      "Train Epoch: 24 [33792/54000 (63%)] Loss: -1753.257080\n",
      "Train Epoch: 24 [45056/54000 (83%)] Loss: -1724.570557\n",
      "    epoch          : 24\n",
      "    loss           : -1733.8176108306309\n",
      "    ess            : 8.00118637084961\n",
      "    log_marginal   : 1733.8176119822376\n",
      "    val_loss       : -1743.8817138671875\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1743.8817138671875\n",
      "Train Epoch: 25 [0/54000 (0%)] Loss: -1754.353760\n",
      "Train Epoch: 25 [11264/54000 (21%)] Loss: -1754.529053\n",
      "Train Epoch: 25 [22528/54000 (42%)] Loss: -1730.168701\n",
      "Train Epoch: 25 [33792/54000 (63%)] Loss: -1758.141479\n",
      "Train Epoch: 25 [45056/54000 (83%)] Loss: -1727.216064\n",
      "    epoch          : 25\n",
      "    loss           : -1738.1561555682488\n",
      "    ess            : 8.00118629887419\n",
      "    log_marginal   : 1738.1561555682488\n",
      "    val_loss       : -1745.1072184244792\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1745.1072285970051\n",
      "Train Epoch: 26 [0/54000 (0%)] Loss: -1755.291992\n",
      "Train Epoch: 26 [11264/54000 (21%)] Loss: -1751.764526\n",
      "Train Epoch: 26 [22528/54000 (42%)] Loss: -1726.011963\n",
      "Train Epoch: 26 [33792/54000 (63%)] Loss: -1761.036621\n",
      "Train Epoch: 26 [45056/54000 (83%)] Loss: -1729.576660\n",
      "    epoch          : 26\n",
      "    loss           : -1738.145656369767\n",
      "    ess            : 8.00118629887419\n",
      "    log_marginal   : 1738.145656369767\n",
      "    val_loss       : -1749.8478190104167\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1749.8478190104167\n",
      "Train Epoch: 27 [0/54000 (0%)] Loss: -1759.585938\n",
      "Train Epoch: 27 [11264/54000 (21%)] Loss: -1753.015259\n",
      "Train Epoch: 27 [22528/54000 (42%)] Loss: -1737.349609\n",
      "Train Epoch: 27 [33792/54000 (63%)] Loss: -1765.561768\n",
      "Train Epoch: 27 [45056/54000 (83%)] Loss: -1731.907227\n",
      "    epoch          : 27\n",
      "    loss           : -1743.1324969597583\n",
      "    ess            : 8.00118637084961\n",
      "    log_marginal   : 1743.1324992629718\n",
      "    val_loss       : -1749.5157572428386\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1749.5157267252605\n",
      "Train Epoch: 28 [0/54000 (0%)] Loss: -1759.192383\n",
      "Train Epoch: 28 [11264/54000 (21%)] Loss: -1755.183350\n",
      "Train Epoch: 28 [22528/54000 (42%)] Loss: -1738.785645\n",
      "Train Epoch: 28 [33792/54000 (63%)] Loss: -1770.160645\n",
      "Train Epoch: 28 [45056/54000 (83%)] Loss: -1726.502197\n",
      "    epoch          : 28\n",
      "    loss           : -1744.0918129974941\n",
      "    ess            : 8.00118637084961\n",
      "    log_marginal   : 1744.0918141491009\n",
      "    val_loss       : -1750.1979573567708\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1750.1979573567708\n",
      "Train Epoch: 29 [0/54000 (0%)] Loss: -1760.757324\n",
      "Train Epoch: 29 [11264/54000 (21%)] Loss: -1757.815186\n",
      "Train Epoch: 29 [22528/54000 (42%)] Loss: -1736.021240\n",
      "Train Epoch: 29 [33792/54000 (63%)] Loss: -1773.029175\n",
      "Train Epoch: 29 [45056/54000 (83%)] Loss: -1737.767578\n",
      "    epoch          : 29\n",
      "    loss           : -1746.9929959279186\n",
      "    ess            : 8.00118637084961\n",
      "    log_marginal   : 1746.9929959279186\n",
      "    val_loss       : -1757.790018717448\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1757.790018717448\n",
      "Train Epoch: 30 [0/54000 (0%)] Loss: -1767.819580\n",
      "Train Epoch: 30 [11264/54000 (21%)] Loss: -1763.363159\n",
      "Train Epoch: 30 [22528/54000 (42%)] Loss: -1744.934692\n",
      "Train Epoch: 30 [33792/54000 (63%)] Loss: -1772.214844\n",
      "Train Epoch: 30 [45056/54000 (83%)] Loss: -1734.523438\n",
      "    epoch          : 30\n",
      "    loss           : -1750.220057073629\n",
      "    ess            : 8.001186352855754\n",
      "    log_marginal   : 1750.220057073629\n",
      "    val_loss       : -1747.8209228515625\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1747.8209228515625\n",
      "Train Epoch: 31 [0/54000 (0%)] Loss: -1757.885254\n",
      "Train Epoch: 31 [11264/54000 (21%)] Loss: -1754.457031\n",
      "Train Epoch: 31 [22528/54000 (42%)] Loss: -1746.165894\n",
      "Train Epoch: 31 [33792/54000 (63%)] Loss: -1773.241943\n",
      "Train Epoch: 31 [45056/54000 (83%)] Loss: -1739.140991\n",
      "    epoch          : 31\n",
      "    loss           : -1749.737338084095\n",
      "    ess            : 8.001186352855754\n",
      "    log_marginal   : 1749.737338084095\n",
      "    val_loss       : -1758.8375244140625\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1758.8375244140625\n",
      "Train Epoch: 32 [0/54000 (0%)] Loss: -1769.308838\n",
      "Train Epoch: 32 [11264/54000 (21%)] Loss: -1768.140137\n",
      "Train Epoch: 32 [22528/54000 (42%)] Loss: -1744.798828\n",
      "Train Epoch: 32 [33792/54000 (63%)] Loss: -1770.973877\n",
      "Train Epoch: 32 [45056/54000 (83%)] Loss: -1740.531494\n",
      "    epoch          : 32\n",
      "    loss           : -1753.0098761792453\n",
      "    ess            : 8.001186226898769\n",
      "    log_marginal   : 1753.0098738760319\n",
      "    val_loss       : -1757.7046203613281\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1757.7046305338542\n",
      "Train Epoch: 33 [0/54000 (0%)] Loss: -1767.955322\n",
      "Train Epoch: 33 [11264/54000 (21%)] Loss: -1769.041626\n",
      "Train Epoch: 33 [22528/54000 (42%)] Loss: -1747.328857\n",
      "Train Epoch: 33 [33792/54000 (63%)] Loss: -1773.298828\n",
      "Train Epoch: 33 [45056/54000 (83%)] Loss: -1744.637451\n",
      "    epoch          : 33\n",
      "    loss           : -1754.8250905162884\n",
      "    ess            : 8.001186244892624\n",
      "    log_marginal   : 1754.8250905162884\n",
      "    val_loss       : -1761.5487263997395\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1761.5487263997395\n",
      "Train Epoch: 34 [0/54000 (0%)] Loss: -1771.045532\n",
      "Train Epoch: 34 [11264/54000 (21%)] Loss: -1765.889404\n",
      "Train Epoch: 34 [22528/54000 (42%)] Loss: -1749.993286\n",
      "Train Epoch: 34 [33792/54000 (63%)] Loss: -1780.255249\n",
      "Train Epoch: 34 [45056/54000 (83%)] Loss: -1745.502197\n",
      "    epoch          : 34\n",
      "    loss           : -1756.7453417508107\n",
      "    ess            : 8.001186352855754\n",
      "    log_marginal   : 1756.7453417508107\n",
      "    val_loss       : -1763.6142171223958\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1763.6142069498699\n",
      "Train Epoch: 35 [0/54000 (0%)] Loss: -1773.733398\n",
      "Train Epoch: 35 [11264/54000 (21%)] Loss: -1774.783447\n",
      "Train Epoch: 35 [22528/54000 (42%)] Loss: -1755.616455\n",
      "Train Epoch: 35 [33792/54000 (63%)] Loss: -1781.560791\n",
      "Train Epoch: 35 [45056/54000 (83%)] Loss: -1748.029175\n",
      "    epoch          : 35\n",
      "    loss           : -1759.9252561173348\n",
      "    ess            : 8.001186244892624\n",
      "    log_marginal   : 1759.9252561173348\n",
      "    val_loss       : -1754.7775472005208\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1754.7775472005208\n",
      "Train Epoch: 36 [0/54000 (0%)] Loss: -1763.455566\n",
      "Train Epoch: 36 [11264/54000 (21%)] Loss: -1772.689209\n",
      "Train Epoch: 36 [22528/54000 (42%)] Loss: -1757.402344\n",
      "Train Epoch: 36 [33792/54000 (63%)] Loss: -1784.020264\n",
      "Train Epoch: 36 [45056/54000 (83%)] Loss: -1752.023315\n",
      "    epoch          : 36\n",
      "    loss           : -1760.8913401477741\n",
      "    ess            : 8.00118629887419\n",
      "    log_marginal   : 1760.8913378445607\n",
      "    val_loss       : -1763.1007690429688\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1763.1007690429688\n",
      "Train Epoch: 37 [0/54000 (0%)] Loss: -1773.081543\n",
      "Train Epoch: 37 [11264/54000 (21%)] Loss: -1772.630737\n",
      "Train Epoch: 37 [22528/54000 (42%)] Loss: -1752.957275\n",
      "Train Epoch: 37 [33792/54000 (63%)] Loss: -1783.349121\n",
      "Train Epoch: 37 [45056/54000 (83%)] Loss: -1752.706055\n",
      "    epoch          : 37\n",
      "    loss           : -1761.4570911335495\n",
      "    ess            : 8.001186280880335\n",
      "    log_marginal   : 1761.4570911335495\n",
      "    val_loss       : -1767.6395670572917\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1767.6395670572917\n",
      "Train Epoch: 38 [0/54000 (0%)] Loss: -1777.072876\n",
      "Train Epoch: 38 [11264/54000 (21%)] Loss: -1775.076904\n",
      "Train Epoch: 38 [22528/54000 (42%)] Loss: -1757.576660\n",
      "Train Epoch: 38 [33792/54000 (63%)] Loss: -1785.769531\n",
      "Train Epoch: 38 [45056/54000 (83%)] Loss: -1753.677734\n",
      "    epoch          : 38\n",
      "    loss           : -1763.6394446031102\n",
      "    ess            : 8.001186280880335\n",
      "    log_marginal   : 1763.6394446031102\n",
      "    val_loss       : -1765.9059244791667\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1765.9059244791667\n",
      "Train Epoch: 39 [0/54000 (0%)] Loss: -1775.574585\n",
      "Train Epoch: 39 [11264/54000 (21%)] Loss: -1781.837524\n",
      "Train Epoch: 39 [22528/54000 (42%)] Loss: -1760.977051\n",
      "Train Epoch: 39 [33792/54000 (63%)] Loss: -1788.208496\n",
      "Train Epoch: 39 [45056/54000 (83%)] Loss: -1749.982300\n",
      "    epoch          : 39\n",
      "    loss           : -1765.1688232421875\n",
      "    ess            : 8.00118615492335\n",
      "    log_marginal   : 1765.1688220905808\n",
      "    val_loss       : -1761.8486938476562\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1761.8486938476562\n",
      "Train Epoch: 40 [0/54000 (0%)] Loss: -1771.854614\n",
      "Train Epoch: 40 [11264/54000 (21%)] Loss: -1778.103027\n",
      "Train Epoch: 40 [22528/54000 (42%)] Loss: -1758.400146\n",
      "Train Epoch: 40 [33792/54000 (63%)] Loss: -1789.268799\n",
      "Train Epoch: 40 [45056/54000 (83%)] Loss: -1757.247314\n",
      "    epoch          : 40\n",
      "    loss           : -1765.5899450913914\n",
      "    ess            : 8.001186226898769\n",
      "    log_marginal   : 1765.5899450913914\n",
      "    val_loss       : -1771.2758178710938\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1771.2758178710938\n",
      "Train Epoch: 41 [0/54000 (0%)] Loss: -1779.959717\n",
      "Train Epoch: 41 [11264/54000 (21%)] Loss: -1780.736328\n",
      "Train Epoch: 41 [22528/54000 (42%)] Loss: -1750.393311\n",
      "Train Epoch: 41 [33792/54000 (63%)] Loss: -1785.277710\n",
      "Train Epoch: 41 [45056/54000 (83%)] Loss: -1751.098877\n",
      "    epoch          : 41\n",
      "    loss           : -1764.9951690098026\n",
      "    ess            : 8.001186280880335\n",
      "    log_marginal   : 1764.9951690098026\n",
      "    val_loss       : -1769.6406555175781\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1769.6406555175781\n",
      "Train Epoch: 42 [0/54000 (0%)] Loss: -1779.189087\n",
      "Train Epoch: 42 [11264/54000 (21%)] Loss: -1782.688965\n",
      "Train Epoch: 42 [22528/54000 (42%)] Loss: -1757.662354\n",
      "Train Epoch: 42 [33792/54000 (63%)] Loss: -1791.121582\n",
      "Train Epoch: 42 [45056/54000 (83%)] Loss: -1758.457520\n",
      "    epoch          : 42\n",
      "    loss           : -1768.432411049897\n",
      "    ess            : 8.001186208904913\n",
      "    log_marginal   : 1768.432411049897\n",
      "    val_loss       : -1768.599589029948\n",
      "    val_ess        : 8.00118621190389\n",
      "    val_log_marginal: 1768.599589029948\n",
      "Train Epoch: 43 [0/54000 (0%)] Loss: -1777.992554\n",
      "Train Epoch: 43 [11264/54000 (21%)] Loss: -1773.994751\n",
      "Train Epoch: 43 [22528/54000 (42%)] Loss: -1760.857422\n",
      "Train Epoch: 43 [33792/54000 (63%)] Loss: -1792.452881\n",
      "Train Epoch: 43 [45056/54000 (83%)] Loss: -1759.826660\n",
      "    epoch          : 43\n",
      "    loss           : -1766.9865250497494\n",
      "    ess            : 8.001186280880335\n",
      "    log_marginal   : 1766.9865262013561\n",
      "    val_loss       : -1771.2056477864583\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1771.2056477864583\n",
      "Train Epoch: 44 [0/54000 (0%)] Loss: -1780.721313\n",
      "Train Epoch: 44 [11264/54000 (21%)] Loss: -1775.189453\n",
      "Train Epoch: 44 [22528/54000 (42%)] Loss: -1761.930542\n",
      "Train Epoch: 44 [33792/54000 (63%)] Loss: -1788.457520\n",
      "Train Epoch: 44 [45056/54000 (83%)] Loss: -1752.486816\n",
      "    epoch          : 44\n",
      "    loss           : -1765.2301140551297\n",
      "    ess            : 8.00118629887419\n",
      "    log_marginal   : 1765.2301140551297\n",
      "    val_loss       : -1762.1123758951824\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1762.1123758951824\n",
      "Train Epoch: 45 [0/54000 (0%)] Loss: -1772.283203\n",
      "Train Epoch: 45 [11264/54000 (21%)] Loss: -1776.238525\n",
      "Train Epoch: 45 [22528/54000 (42%)] Loss: -1761.706909\n",
      "Train Epoch: 45 [33792/54000 (63%)] Loss: -1790.756958\n",
      "Train Epoch: 45 [45056/54000 (83%)] Loss: -1749.509888\n",
      "    epoch          : 45\n",
      "    loss           : -1763.513998931309\n",
      "    ess            : 8.001186226898769\n",
      "    log_marginal   : 1763.513998931309\n",
      "    val_loss       : -1770.5958455403645\n",
      "    val_ess        : 8.00118637084961\n",
      "    val_log_marginal: 1770.5958455403645\n",
      "Train Epoch: 46 [0/54000 (0%)] Loss: -1780.083374\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5b900-0121-4cf6-91b6-2c9b0b12716f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.model.eval()\n",
    "trainer.cpu()\n",
    "trainer.train_particles.cpu()\n",
    "trainer.valid_particles.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f27b30-f96c-4b88-8915-c90b844f7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in trainer.model.graph.nodes:\n",
    "    trainer.model.graph.nodes[site]['is_observed'] = trainer.model.graph.nodes[site]['value'] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025dc77-7e7f-415a-ba70-db3e9b33e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b98478-8039-4c69-819b-0ac639fb15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyro.plate_stack(\"forward\", (trainer.num_particles, trainer.data_loader.batch_size)):\n",
    "    model = pyro.condition(trainer.model, data={k: v['value'] for k, v in trainer.model.graph.nodes.items()})\n",
    "    xs = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c76257-aef8-40e3-934a-a1e534924256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750bfb2-80aa-4fad-885e-752772706a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.imshow(xs.mean(dim=0)[i].squeeze().detach().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b487d4-288a-40d7-8f4d-c79e2e15385e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ppc] *",
   "language": "python",
   "name": "conda-env-ppc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
